[2025-07-24 16:04:20,314] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-07-24 16:04:25.876 | INFO     | __main__:<module>:136 - --------- config key ---------        ------ value ------
seed                                  42
ref_num_nodes                         4
ref_num_gpus_per_node                 1
reward_num_nodes                      1
reward_num_gpus_per_node              2
actor_num_nodes                       4
actor_num_gpus_per_node               1
critic_num_nodes                      4
critic_num_gpus_per_node              1
colocate_critic_reward                True
colocate_actor_ref                    True
colocate_all                          True
vllm_num_engines                      4
vllm_tensor_parallel_size             1
vllm_sync_backend                     nccl
local_rank                            -1
pretrain                              /home/a/anokhin/links/scratch/Qwen2.5-1.5B
critic_pretrain
reward_pretrain                       <class 'NoneType'>
ckpt_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
save_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
tensorboard_log_dir                   /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
prompt_data                           <class 'omegaconf.listconfig.ListConfig'>
load_checkpoint                       False
zero_stage                            3
bf16                                  True
zpg                                   1
adam_offload                          False
flash_attn                            True
grad_accum_dtype                      <class 'NoneType'>
disable_trace_cache                   False
gradient_checkpointing                True
gradient_checkpointing_use_reentrant  False
disable_fast_tokenizer                False
target_modules                        all-linear
enable_prefix_caching                 True
enable_chunked_prefill                False
max_num_batched_tokens                2048
enforce_eager                         False
gpu_memory_utilization                0.25
eval_steps                            -1
save_steps                            -1
save_interval                         50
actor_learning_rate                   1e-06
critic_learning_rate                  5e-06
num_episodes                          20
max_epochs                            1
prompt_max_len                        2048
generate_max_len                      8000
train_batch_size                      256
micro_train_batch_size                1
rollout_batch_size                    128
micro_rollout_batch_size              128
micro_forward_batch_size              1
policy_update_steps                   1
critic_update_steps                   12
max_len                               8192
max_norm                              1.0
num_warmup_steps                      50
l2                                    0.0
eps_clip                              0.2
value_clip                            0.2
lambd                                 1.0
gamma                                 1.0
normalize_reward                      True
top_p                                 1.0
temperature                           1.0
freezing_actor_steps                  -1
n_samples_per_prompt                  64
kl_target                             <class 'NoneType'>
init_kl_coef                          0
use_kl_estimator_k3                   True
use_abs_kl                            False
use_kl_loss                           True
kl_loss_coef                          0.0
adam_betas                            (0.9, 0.95)
reward_clip_range                     (-10, 10)
use_compute_reward_fn                 True
advantage_normalize                   True
value_head_prefix                     value_head
ref_reward_offload                    False
enable_eval                           True
eval_interval                         10
update_ref_every_epoch                True
use_orm_score                         False
total_num_nodes                       4
exp_name                              binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
eval_prompt_data                      <class 'omegaconf.listconfig.ListConfig'>
prompt_data_probs                     <class 'omegaconf.listconfig.ListConfig'>
packing_max_len                       10048
top_k                                 -1
stop                                  <class 'omegaconf.listconfig.ListConfig'>
use_grpo                              True
wandb: Currently logged in as: avecplezir (irina-rish). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.21.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/wandb/run-20250724_160427-8405gkw1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/irina-rish/open-reasoner-zero
wandb: üöÄ View run at https://wandb.ai/irina-rish/open-reasoner-zero/runs/8405gkw1
2025-07-24 16:04:30,542	INFO worker.py:1841 -- Started a local Ray instance.
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it]
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it]
[36m(LLMActor pid=1435060)[0m 
[36m(LLMActor pid=1435061)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m 
[36m(LLMActor pid=1435062)[0m 
[36m(LLMActor pid=1435061)[0m 
2025-07-24 16:05:08.022 | INFO     | orz.ppo.utils:create_vllm_engines:452 - Offloaded all vLLM engines to CPU
2025-07-24 16:05:08.244 | INFO     | playground.orz_7b_ppo:train_dataset:580 - Start processing 1603 dialogues
2025-07-24 16:05:10.312 | INFO     | playground.orz_7b_ppo:train_dataset:589 - Finished processing 1603 dialogues
2025-07-24 16:05:10.316 | INFO     | playground.orz_7b_ppo:eval_dataset:603 - Start processing 687 dialogues
2025-07-24 16:05:10.753 | INFO     | playground.orz_7b_ppo:eval_dataset:612 - Finished processing 687 dialogues
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
[36m(RefRayActorBase pid=1436501)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(LLMActor pid=1435061)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it][32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 4x across cluster][0m
[36m(PolicyRayActorBase pid=1435816)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...
[36m(PolicyRayActorBase pid=1435815)[0m Detected CUDA files, patching ldflags
[36m(PolicyRayActorBase pid=1435815)[0m Emitting ninja build file /home/a/anokhin/.cache/torch_extensions/py312_cu122/fused_adam/build.ninja...
[36m(PolicyRayActorBase pid=1435815)[0m /home/a/anokhin/envs/reasoner3.12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(PolicyRayActorBase pid=1435815)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(PolicyRayActorBase pid=1435815)[0m   warnings.warn(
[36m(PolicyRayActorBase pid=1435815)[0m Building extension module fused_adam...
[36m(PolicyRayActorBase pid=1435815)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(PolicyRayActorBase pid=1435815)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435815)[0m Loading extension module fused_adam...
2025-07-24 16:05:48.652 | INFO     | orz.ppo.trainer:build_models:815 - init policy/ref/critic/reward models done
2025-07-24 16:05:49.316 | INFO     | orz.ppo.trainer:train:74 - Create vllm engine gourps done.
2025-07-24 16:05:51.913 | INFO     | orz.ppo.trainer:train:76 - Sync actor weights to vllm engines, time cost: 2.60s
2025-07-24 16:05:52.208 | INFO     | orz.ppo.trainer:train:80 - Offload policy model to cpu, time cost: 0.29s
math_verify is not installed in this environment
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:38 __init__.py:207] Automatically detected platform cuda.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435061)[0m WARNING 07-24 16:04:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=1435061)[0m WARNING 07-24 16:04:54 config.py:685] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=44, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:39 __init__.py:207] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(LLMActor pid=1435060)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435062)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:55 cuda.py:229] Using Flash Attention backend.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:56 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...
[36m(LLMActor pid=1435062)[0m INFO 07-24 16:05:04 model_runner.py:1115] Loading model weights took 2.9105 GB
[36m(LLMActor pid=1435059)[0m WARNING 07-24 16:04:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m WARNING 07-24 16:04:54 config.py:685] Async output processing is not supported on the current platform type cuda.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=43, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, [32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:55 cuda.py:229] Using Flash Attention backend.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:56 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] Memory profiling takes 0.51 seconds
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:05 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:05 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:07 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.60 seconds
[36m(pid=1435640)[0m [2025-07-24 16:05:13,702] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 model_runner.py:1115] Loading model weights took 2.9105 GB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] Memory profiling takes 0.49 seconds[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:05 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:05 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:07 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.58 seconds[32m [repeated 3x across cluster][0m
[36m(pid=1435816)[0m [2025-07-24 16:05:21,478] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=1435818)[0m [2025-07-24 16:05:21,675] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:25,874] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:25,874] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(pid=1436500)[0m [2025-07-24 16:05:29,104] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 3x across cluster][0m
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:33,170] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:33,280] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 16:05:33,199] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:36,755] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(pid=1436499)[0m [2025-07-24 16:05:29,310] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 2x across cluster][0m
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:38,261] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 4x across cluster][0m
[36m(RefRayActorBase pid=1436499)[0m [2025-07-24 16:05:33,311] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,358] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,359] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,367] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,368] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,568] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,569] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.71 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,569] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.38 GB, percent = 10.4%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,709] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,710] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,710] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.38 GB, percent = 10.4%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,711] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7e4375e80>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:38,832] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:38,833] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:42,327] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(PolicyRayActorBase pid=1435815)[0m ninja: no work to do.
[36m(PolicyRayActorBase pid=1435815)[0m Time to load fused_adam op: 1.1178452968597412 seconds
[36m(PolicyRayActorBase pid=1435815)[0m [2025-07-24 16:05:44,887] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435815)[0m [2025-07-24 16:05:38,841] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,958] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,959] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,981] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,984] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,984] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,242] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,243] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.6 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,243] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,244] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,244] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,417] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,418] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,418] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,567] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,567] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,568] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,697] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,697] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,698] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.73 GB         CA 0.72 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.72 GB         Max_CA 1 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.88 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,922] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,923] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,923] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,054] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 3.59 GB         CA 4.32 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,286] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,286] [INFO] [utils.py:782:see_memory_usage] MA 4.53 GB         Max_MA 5.39 GB         CA 5.76 GB         Max_CA 6 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.46 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7e72ecb7a690>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(PolicyRayActorBase pid=1435640)[0m     "partition_activations": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "contiguous_memory_optimization": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "cpu_checkpointing": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "number_checkpoints": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "synchronize_checkpoint_boundary": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "profile": false
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "start_step": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "end_step": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "metric_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "arg_mappings": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "metric": "throughput", 
[36m(PolicyRayActorBase pid=1435640)[0m     "model_info": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "results_dir": "autotuning_results", 
[36m(PolicyRayActorBase pid=1435640)[0m     "exps_dir": "autotuning_exps", 
[36m(PolicyRayActorBase pid=1435640)[0m     "overwrite": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "fast": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "start_profile_step": 3, 
[36m(PolicyRayActorBase pid=1435640)[0m     "end_profile_step": 5, 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_type": "gridsearch", 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_early_stopping": 5, 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_num_trials": 50, 
[36m(PolicyRayActorBase pid=1435640)[0m     "model_info_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "mp_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "max_train_batch_size": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "min_train_batch_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(PolicyRayActorBase pid=1435640)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "num_tuning_micro_batch_sizes": 3
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7e72ecb79a90>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "recompute_fwd_factor": 0.0, 
[36m(PolicyRayActorBase pid=1435640)[0m     "profile_step": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "module_depth": -1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "top_modules": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "detailed": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "output_file": null
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "persistent_storage_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "persistent_time_interval": 100, 
[36m(PolicyRayActorBase pid=1435640)[0m     "num_of_version_in_retention": 2, 
[36m(PolicyRayActorBase pid=1435640)[0m     "enable_nebula_load": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "load_path": null
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:989:print_user_config]   json = {
[36m(PolicyRayActorBase pid=1435640)[0m     "steps_per_print": 100, 
[36m(PolicyRayActorBase pid=1435640)[0m     "zero_optimization": {
[36m(PolicyRayActorBase pid=1435640)[0m         "stage": 3, 
[36m(PolicyRayActorBase pid=1435640)[0m         "offload_param": {
[36m(PolicyRayActorBase pid=1435640)[0m             "device": "none"
[36m(PolicyRayActorBase pid=1435640)[0m         }, 
[36m(PolicyRayActorBase pid=1435640)[0m         "offload_optimizer": {
[36m(PolicyRayActorBase pid=1435640)[0m             "device": "none", 
[36m(PolicyRayActorBase pid=1435640)[0m             "pin_memory": true
[36m(PolicyRayActorBase pid=1435640)[0m         }, 
[36m(PolicyRayActorBase pid=1435640)[0m         "sub_group_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_max_live_parameters": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_max_reuse_distance": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "reduce_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_hpz_partition_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_quantized_weights": false, 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_quantized_gradients": false
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "bf16": {
[36m(PolicyRayActorBase pid=1435640)[0m         "enabled": true
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "gradient_clipping": 1.0, 
[36m(PolicyRayActorBase pid=1435640)[0m     "prescale_gradients": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "wall_clock_breakdown": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "data_types": {
[36m(PolicyRayActorBase pid=1435640)[0m         "grad_accum_dtype": "fp32"
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(LLMActor pid=1435061)[0m init_process_group: master_address=10.224.3.58, master_port=41739,  rank=3, world_size=5, group_name=openrlhf
[36m(PolicyRayActorBase pid=1435640)[0m WARNING:using --vllm_sync_backend=gloo for vLLM version > 0.4.2 (or export NCCL_P2P_DISABLE=1)
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435818)[0m Time to load fused_adam op: 1.216843605041504 seconds[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435818)[0m [2025-07-24 16:05:44,986] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 3x across cluster][0m
Episode [1/20]:   0%|          | 0/13 [00:00<?, ?it/s]2025-07-24 16:05:52.312 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(PolicyRayActorBase pid=1435640)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435818)[0m Loading extension module fused_adam...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<00:59,  2.87it/s, est. speed input: 512.95 toks/s, output: 20.06 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 2/172 [00:00<00:36,  4.64it/s, est. speed input: 767.99 toks/s, output: 40.31 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:04<00:00, 20.63it/s, est. speed input: 6303.86 toks/s, output: 2760.14 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:05<00:01, 12.39it/s, est. speed input: 5149.97 toks/s, output: 2432.96 toks/s][32m [repeated 110x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00,  1.88it/s, est. speed input: 3136.79 toks/s, output: 1923.58 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00, 17.30it/s, est. speed input: 3136.79 toks/s, output: 1923.58 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:08<00:00,  2.74it/s, est. speed input: 3468.63 toks/s, output: 1831.23 toks/s][32m [repeated 30x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:05<00:01,  8.87it/s, est. speed input: 5068.50 toks/s, output: 2273.45 toks/s]
2025-07-24 16:06:07.922 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 494.2853,strategyqa_test/accuracy: 0.3115,eval_accuracy: 0.3115
2025-07-24 16:06:08.418 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:07:55.866 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:07:56.045 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:07:56.046 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 107.63s
2025-07-24 16:08:14.553 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0094,avg_pass_at_n: 1.0000,avg_num_tokens: 114.4545,std_num_tokens: 132.9845,avg_correct_num_tokens: 105.4537,std_correct_num_tokens: 84.9028,avg_incorrect_num_tokens: 129.5815,std_incorrect_num_tokens: 186.8873
2025-07-24 16:08:14.903 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 18.86s
2025-07-24 16:08:18.212 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.31s
2025-07-24 16:08:47.873 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:08:47.874 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.66s
2025-07-24 16:08:49.264 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 16:08:49.265 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0006267745350284185, avg_kl: 0.0, avg_response_length: 116.57365384164335, avg_orm_score: 0.0, avg_custom_rewards: 0.0006267745350284185
2025-07-24 16:08:49.328 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter0_replay_buffer.jsonl
2025-07-24 16:08:51.234 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00,  1.23s/it, est. speed input: 2194.44 toks/s, output: 1255.84 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00, 12.11it/s, est. speed input: 2194.44 toks/s, output: 1255.84 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:12<00:00,  1.09it/s, est. speed input: 2445.90 toks/s, output: 1474.87 toks/s][32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.14, ret=-0.00164, glen=103, tlen=264, kl=0, act_lr=0, ent=1.73]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<01:24,  1.49s/it, pg=0.14, ret=-0.00164, glen=103, tlen=264, kl=0, act_lr=0, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:02<01:24,  1.49s/it, pg=0.057, ret=-0.000267, glen=104, tlen=264, kl=0, act_lr=0, ent=1.67]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<01:09,  1.24s/it, pg=0.057, ret=-0.000267, glen=104, tlen=264, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:04<01:09,  1.24s/it, pg=0.069, ret=-0.001, glen=139, tlen=299, kl=0, act_lr=0, ent=1.95]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:04<01:15,  1.37s/it, pg=0.069, ret=-0.001, glen=139, tlen=299, kl=0, act_lr=0, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:04<01:15,  1.37s/it, pg=0.287, ret=-0.000979, glen=126, tlen=287, kl=0, act_lr=0, ent=1.76]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<01:03,  1.17s/it, pg=0.287, ret=-0.000979, glen=126, tlen=287, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:05<01:03,  1.17s/it, pg=-0.055, ret=-0.000206, glen=105, tlen=265, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:56,  1.07s/it, pg=-0.055, ret=-0.000206, glen=105, tlen=265, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:06<00:56,  1.07s/it, pg=-0.107, ret=-0.000409, glen=115, tlen=275, kl=0, act_lr=0, ent=1.63]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:52,  1.01s/it, pg=-0.107, ret=-0.000409, glen=115, tlen=275, kl=0, act_lr=0, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:07<00:52,  1.01s/it, pg=-0.0383, ret=-0.000243, glen=112, tlen=272, kl=0, act_lr=0, ent=1.64]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:49,  1.04it/s, pg=-0.0383, ret=-0.000243, glen=112, tlen=272, kl=0, act_lr=0, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:08<00:49,  1.04it/s, pg=-0.0483, ret=0.00091, glen=104, tlen=264, kl=0, act_lr=0, ent=1.78]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:46,  1.08it/s, pg=-0.0483, ret=0.00091, glen=104, tlen=264, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:09<00:46,  1.08it/s, pg=-0.0175, ret=-0.000894, glen=106, tlen=266, kl=0, act_lr=0, ent=1.59]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:09<00:44,  1.11it/s, pg=-0.0175, ret=-0.000894, glen=106, tlen=266, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:10<00:44,  1.11it/s, pg=0.0853, ret=-0.000294, glen=117, tlen=277, kl=0, act_lr=0, ent=1.73] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:10<00:43,  1.10it/s, pg=0.0853, ret=-0.000294, glen=117, tlen=277, kl=0, act_lr=0, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:11<00:43,  1.10it/s, pg=0.118, ret=-0.000719, glen=116, tlen=276, kl=0, act_lr=0, ent=1.75] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:11<00:41,  1.12it/s, pg=0.118, ret=-0.000719, glen=116, tlen=276, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:11<00:41,  1.12it/s, pg=0.00751, ret=0.000226, glen=112, tlen=273, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.14it/s, pg=0.00751, ret=0.000226, glen=112, tlen=273, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:12<00:40,  1.14it/s, pg=0.126, ret=-0.00022, glen=108, tlen=268, kl=0, act_lr=0, ent=1.69]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:39,  1.15it/s, pg=0.126, ret=-0.00022, glen=108, tlen=268, kl=0, act_lr=0, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:13<00:39,  1.15it/s, pg=-0.179, ret=0.00049, glen=98.3, tlen=259, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.16it/s, pg=-0.179, ret=0.00049, glen=98.3, tlen=259, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:14<00:37,  1.16it/s, pg=-0.0616, ret=3.93e-5, glen=109, tlen=269, kl=0, act_lr=0, ent=1.77]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:36,  1.17it/s, pg=-0.0616, ret=3.93e-5, glen=109, tlen=269, kl=0, act_lr=0, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:15<00:36,  1.17it/s, pg=0.0941, ret=-0.000905, glen=123, tlen=284, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:15<00:35,  1.17it/s, pg=0.0941, ret=-0.000905, glen=123, tlen=284, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:16<00:35,  1.17it/s, pg=-0.103, ret=-0.000127, glen=94.4, tlen=255, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:16<00:34,  1.17it/s, pg=-0.103, ret=-0.000127, glen=94.4, tlen=255, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:16<00:34,  1.17it/s, pg=0.0759, ret=0.000215, glen=123, tlen=284, kl=0, act_lr=0, ent=1.76]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=0.0759, ret=0.000215, glen=123, tlen=284, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:17<00:34,  1.17it/s, pg=-0.154, ret=0.00225, glen=121, tlen=282, kl=0, act_lr=0, ent=1.62] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.18it/s, pg=-0.154, ret=0.00225, glen=121, tlen=282, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:18<00:33,  1.18it/s, pg=0.0566, ret=-0.000261, glen=116, tlen=276, kl=0, act_lr=0, ent=1.72]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.18it/s, pg=0.0566, ret=-0.000261, glen=116, tlen=276, kl=0, act_lr=0, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:19<00:32,  1.18it/s, pg=-0.0662, ret=0.000428, glen=101, tlen=262, kl=0, act_lr=0, ent=1.59]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.18it/s, pg=-0.0662, ret=0.000428, glen=101, tlen=262, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:20<00:31,  1.18it/s, pg=-0.102, ret=-0.000148, glen=124, tlen=284, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.18it/s, pg=-0.102, ret=-0.000148, glen=124, tlen=284, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:21<00:30,  1.18it/s, pg=0.0445, ret=-0.000576, glen=91.7, tlen=252, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:21<00:29,  1.18it/s, pg=0.0445, ret=-0.000576, glen=91.7, tlen=252, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:22<00:29,  1.18it/s, pg=-0.0389, ret=-0.000265, glen=112, tlen=273, kl=0, act_lr=0, ent=1.82]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:22<00:28,  1.18it/s, pg=-0.0389, ret=-0.000265, glen=112, tlen=273, kl=0, act_lr=0, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:22<00:28,  1.18it/s, pg=-0.00873, ret=0.00307, glen=179, tlen=339, kl=0, act_lr=0, ent=2.23] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.17it/s, pg=-0.00873, ret=0.00307, glen=179, tlen=339, kl=0, act_lr=0, ent=2.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:23<00:28,  1.17it/s, pg=-0.203, ret=0.00135, glen=116, tlen=277, kl=0, act_lr=0, ent=1.68]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.17it/s, pg=-0.203, ret=0.00135, glen=116, tlen=277, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:24<00:27,  1.17it/s, pg=0.111, ret=-0.00221, glen=118, tlen=278, kl=0, act_lr=0, ent=1.84]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.111, ret=-0.00221, glen=118, tlen=278, kl=0, act_lr=0, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:25<00:26,  1.17it/s, pg=-0.00208, ret=-0.000165, glen=115, tlen=276, kl=0, act_lr=0, ent=1.54]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.18it/s, pg=-0.00208, ret=-0.000165, glen=115, tlen=276, kl=0, act_lr=0, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:26<00:25,  1.18it/s, pg=-0.113, ret=0.000108, glen=107, tlen=267, kl=0, act_lr=0, ent=1.67]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.113, ret=0.000108, glen=107, tlen=267, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:27<00:27,  1.07it/s, pg=-0.0209, ret=-0.000352, glen=108, tlen=269, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.10it/s, pg=-0.0209, ret=-0.000352, glen=108, tlen=269, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:28<00:25,  1.10it/s, pg=-0.00781, ret=-0.00215, glen=91.8, tlen=252, kl=0, act_lr=0, ent=1.54]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.12it/s, pg=-0.00781, ret=-0.00215, glen=91.8, tlen=252, kl=0, act_lr=0, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:29<00:24,  1.12it/s, pg=0.244, ret=-0.000828, glen=137, tlen=298, kl=0, act_lr=0, ent=2.15]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:22,  1.14it/s, pg=0.244, ret=-0.000828, glen=137, tlen=298, kl=0, act_lr=0, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:30<00:22,  1.14it/s, pg=-0.0299, ret=0.000116, glen=122, tlen=282, kl=0, act_lr=0, ent=2.02]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:30<00:21,  1.15it/s, pg=-0.0299, ret=0.000116, glen=122, tlen=282, kl=0, act_lr=0, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:30<00:21,  1.15it/s, pg=0.0894, ret=-0.000191, glen=108, tlen=268, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.16it/s, pg=0.0894, ret=-0.000191, glen=108, tlen=268, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:31<00:20,  1.16it/s, pg=0.102, ret=0.00018, glen=116, tlen=276, kl=0, act_lr=0, ent=1.83]   Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=0.102, ret=0.00018, glen=116, tlen=276, kl=0, act_lr=0, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:32<00:19,  1.16it/s, pg=-0.0702, ret=-0.000848, glen=108, tlen=269, kl=0, act_lr=0, ent=1.78]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=-0.0702, ret=-0.000848, glen=108, tlen=269, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:33<00:18,  1.17it/s, pg=-0.0553, ret=-0.000724, glen=101, tlen=261, kl=0, act_lr=0, ent=1.8] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=-0.0553, ret=-0.000724, glen=101, tlen=261, kl=0, act_lr=0, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:34<00:17,  1.17it/s, pg=-0.0718, ret=-0.000542, glen=102, tlen=263, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.17it/s, pg=-0.0718, ret=-0.000542, glen=102, tlen=263, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:35<00:17,  1.17it/s, pg=-0.136, ret=0.00152, glen=128, tlen=288, kl=0, act_lr=0, ent=1.91]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.17it/s, pg=-0.136, ret=0.00152, glen=128, tlen=288, kl=0, act_lr=0, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.17it/s, pg=0.176, ret=-0.000703, glen=119, tlen=280, kl=0, act_lr=0, ent=1.81]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.18it/s, pg=0.176, ret=-0.000703, glen=119, tlen=280, kl=0, act_lr=0, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:36<00:15,  1.18it/s, pg=0.217, ret=0.000893, glen=163, tlen=324, kl=0, act_lr=0, ent=2.15] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.217, ret=0.000893, glen=163, tlen=324, kl=0, act_lr=0, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:37<00:14,  1.16it/s, pg=-0.0223, ret=-0.00191, glen=116, tlen=277, kl=0, act_lr=0, ent=1.77]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.15it/s, pg=-0.0223, ret=-0.00191, glen=116, tlen=277, kl=0, act_lr=0, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:38<00:13,  1.15it/s, pg=-0.11, ret=0.00112, glen=116, tlen=277, kl=0, act_lr=0, ent=1.66]   Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:13,  1.15it/s, pg=-0.11, ret=0.00112, glen=116, tlen=277, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:39<00:13,  1.15it/s, pg=-0.045, ret=0.0016, glen=125, tlen=286, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.16it/s, pg=-0.045, ret=0.0016, glen=125, tlen=286, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:40<00:12,  1.16it/s, pg=-0.22, ret=0.0033, glen=127, tlen=288, kl=0, act_lr=0, ent=2.1]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=-0.22, ret=0.0033, glen=127, tlen=288, kl=0, act_lr=0, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:41<00:11,  1.17it/s, pg=-0.105, ret=0.000793, glen=140, tlen=300, kl=0, act_lr=0, ent=1.99]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.17it/s, pg=-0.105, ret=0.000793, glen=140, tlen=300, kl=0, act_lr=0, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:42<00:10,  1.17it/s, pg=-0.178, ret=0.00118, glen=128, tlen=288, kl=0, act_lr=0, ent=1.88] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:42<00:09,  1.17it/s, pg=-0.178, ret=0.00118, glen=128, tlen=288, kl=0, act_lr=0, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:42<00:09,  1.17it/s, pg=0.188, ret=-0.00214, glen=113, tlen=274, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=0.188, ret=-0.00214, glen=113, tlen=274, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:43<00:08,  1.17it/s, pg=0.219, ret=-0.0027, glen=122, tlen=283, kl=0, act_lr=0, ent=1.52] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=0.219, ret=-0.0027, glen=122, tlen=283, kl=0, act_lr=0, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:44<00:07,  1.17it/s, pg=0.0356, ret=0.00127, glen=114, tlen=274, kl=0, act_lr=0, ent=1.93]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=0.0356, ret=0.00127, glen=114, tlen=274, kl=0, act_lr=0, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:45<00:06,  1.17it/s, pg=-0.0473, ret=0.001, glen=108, tlen=268, kl=0, act_lr=0, ent=1.62] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=-0.0473, ret=0.001, glen=108, tlen=268, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:46<00:05,  1.17it/s, pg=-0.137, ret=0.00144, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.18it/s, pg=-0.137, ret=0.00144, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:47<00:05,  1.18it/s, pg=-0.071, ret=-8.45e-5, glen=137, tlen=297, kl=0, act_lr=0, ent=1.98]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.18it/s, pg=-0.071, ret=-8.45e-5, glen=137, tlen=297, kl=0, act_lr=0, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.18it/s, pg=-0.0575, ret=0.000681, glen=113, tlen=274, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.18it/s, pg=-0.0575, ret=0.000681, glen=113, tlen=274, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:48<00:03,  1.18it/s, pg=0.00867, ret=-0.000882, glen=122, tlen=282, kl=0, act_lr=0, ent=1.91]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.18it/s, pg=0.00867, ret=-0.000882, glen=122, tlen=282, kl=0, act_lr=0, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:49<00:02,  1.18it/s, pg=-0.179, ret=0.00125, glen=111, tlen=271, kl=0, act_lr=0, ent=1.59]   Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.18it/s, pg=-0.179, ret=0.00125, glen=111, tlen=271, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:50<00:01,  1.18it/s, pg=-0.203, ret=0.00141, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.18it/s, pg=-0.203, ret=0.00141, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]
2025-07-24 16:09:42.917 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 51.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:51<00:00,  1.18it/s, pg=0.0981, ret=-0.00132, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:51<00:00,  1.11it/s, pg=0.0981, ret=-0.00132, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.77]
2025-07-24 16:09:43.796 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 16:09:46.357 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 16:09:48.828 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 57.52s
2025-07-24 16:09:48.836 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.007191941655915359, 'actor_lr': 3.448275841112169e-10, 'clip_ratio': 0.0, 'entropy': 1.7657057540170078, 'kl': 0.0, 'response_length': 116.28467454581425, 'total_length': 276.7488821621599, 'teacher_total_length': 289.0060219600283, 'return': -1.278092644622967e-06, 'policy_update_steps': 1.0}
Episode [1/20]:   8%|‚ñä         | 1/13 [03:56<47:19, 236.63s/it]2025-07-24 16:09:48.895 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:11:20.756 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:11:20.934 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:11:20.934 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 92.04s
2025-07-24 16:11:23.095 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 109.7526,std_num_tokens: 111.6655,avg_correct_num_tokens: 102.7106,std_correct_num_tokens: 88.0398,avg_incorrect_num_tokens: 121.4100,std_incorrect_num_tokens: 141.6397
2025-07-24 16:11:23.425 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.49s
2025-07-24 16:11:26.606 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 16:11:55.557 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 225
2025-07-24 16:11:55.558 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 16:11:56.918 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.97s
2025-07-24 16:11:56.918 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0004019819732962383, avg_kl: 0.0, avg_response_length: 111.20896928575304, avg_orm_score: 0.0, avg_custom_rewards: -0.0004019819732962383
2025-07-24 16:11:56.953 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter1_replay_buffer.jsonl
2025-07-24 16:11:58.810 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.86s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.122, ret=0.00139, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.62]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.122, ret=0.00139, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.229, ret=0.00258, glen=99.7, tlen=261, kl=0, act_lr=2e-8, ent=1.64]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.08it/s, pg=-0.229, ret=0.00258, glen=99.7, tlen=261, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.08it/s, pg=-0.00248, ret=-0.000607, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.54]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=-0.00248, ret=-0.000607, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=-0.146, ret=0.00126, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.64]    Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=-0.146, ret=0.00126, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=-0.136, ret=-1.2e-5, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.136, ret=-1.2e-5, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.0627, ret=-0.00105, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0627, ret=-0.00105, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.0963, ret=0.000658, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.14it/s, pg=-0.0963, ret=0.000658, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.14it/s, pg=0.046, ret=0.00101, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.98]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=0.046, ret=0.00101, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=-0.0628, ret=-0.000412, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.0628, ret=-0.000412, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.134, ret=-0.000758, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.88]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=0.134, ret=-0.000758, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=0.0674, ret=-0.00141, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.74]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.17it/s, pg=0.0674, ret=-0.00141, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.17it/s, pg=0.0914, ret=0.00054, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.62] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=0.0914, ret=0.00054, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=-0.0233, ret=-0.000625, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.75]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.0233, ret=-0.000625, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.256, ret=0.00166, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.59]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=-0.256, ret=0.00166, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0753, ret=-0.000334, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.59]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0753, ret=-0.000334, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.101, ret=0.000645, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.57] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.101, ret=0.000645, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.1, ret=0.00133, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.1, ret=0.00133, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.0258, ret=-0.000727, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=0.0258, ret=-0.000727, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=0.107, ret=-0.00128, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.65]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.18it/s, pg=0.107, ret=-0.00128, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.18it/s, pg=-0.0837, ret=0.000457, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.18it/s, pg=-0.0837, ret=0.000457, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.18it/s, pg=-0.184, ret=0.00276, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.69]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.18it/s, pg=-0.184, ret=0.00276, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.18it/s, pg=0.0781, ret=5.91e-5, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.82]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.18it/s, pg=0.0781, ret=5.91e-5, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.18it/s, pg=0.253, ret=-0.00223, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.81]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.18it/s, pg=0.253, ret=-0.00223, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.18it/s, pg=0.0104, ret=-0.00124, glen=110, tlen=271, kl=0, act_lr=2e-8, ent=1.8]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.18it/s, pg=0.0104, ret=-0.00124, glen=110, tlen=271, kl=0, act_lr=2e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.18it/s, pg=0.171, ret=-0.00157, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=2.06]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.18it/s, pg=0.171, ret=-0.00157, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.18it/s, pg=-0.0759, ret=0.00144, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.78]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.18it/s, pg=-0.0759, ret=0.00144, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.18it/s, pg=0.0108, ret=1.68e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.86] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=0.0108, ret=1.68e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=0.277, ret=-0.0011, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.86] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.18it/s, pg=0.277, ret=-0.0011, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.18it/s, pg=0.035, ret=-0.000798, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:27,  1.02it/s, pg=0.035, ret=-0.000798, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:27,  1.02it/s, pg=-0.104, ret=0.00169, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.53] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.06it/s, pg=-0.104, ret=0.00169, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.06it/s, pg=0.151, ret=-0.00124, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=1.89]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=0.151, ret=-0.00124, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.0665, ret=0.000756, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.11it/s, pg=-0.0665, ret=0.000756, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.00211, ret=-0.00148, glen=125, tlen=285, kl=0, act_lr=2e-8, ent=1.78]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.00211, ret=-0.00148, glen=125, tlen=285, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=0.0208, ret=-0.000786, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.66]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.13it/s, pg=0.0208, ret=-0.000786, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.13it/s, pg=-0.198, ret=0.000905, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=2.12] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=-0.198, ret=0.000905, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=-0.00299, ret=0.000586, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.59]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=-0.00299, ret=0.000586, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.163, ret=0.00107, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.58]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.163, ret=0.00107, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0769, ret=-2.25e-5, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0769, ret=-2.25e-5, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.194, ret=-0.000224, glen=114, tlen=275, kl=0, act_lr=2e-8, ent=1.74]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.194, ret=-0.000224, glen=114, tlen=275, kl=0, act_lr=2e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0107, ret=-0.000238, glen=96.3, tlen=257, kl=0, act_lr=2e-8, ent=1.57]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0107, ret=-0.000238, glen=96.3, tlen=257, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0047, ret=6.06e-5, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.6]     Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0047, ret=6.06e-5, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.00222, ret=0.000656, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.00222, ret=0.000656, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=0.0798, ret=-0.000233, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.58] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=0.0798, ret=-0.000233, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.0158, ret=-0.0013, glen=99.7, tlen=260, kl=0, act_lr=2e-8, ent=1.58]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=-0.0158, ret=-0.0013, glen=99.7, tlen=260, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=0.202, ret=-0.00102, glen=116, tlen=278, kl=0, act_lr=2e-8, ent=2.01]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.202, ret=-0.00102, glen=116, tlen=278, kl=0, act_lr=2e-8, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.136, ret=-0.00071, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.64]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.18it/s, pg=0.136, ret=-0.00071, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.0179, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.0179, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=-0.0343, ret=0.000563, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.6] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=-0.0343, ret=0.000563, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=-0.125, ret=0.000842, glen=103, tlen=263, kl=0, act_lr=2e-8, ent=1.97]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.125, ret=0.000842, glen=103, tlen=263, kl=0, act_lr=2e-8, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=0.0212, ret=-0.00376, glen=120, tlen=282, kl=0, act_lr=2e-8, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.0212, ret=-0.00376, glen=120, tlen=282, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.0927, ret=0.000311, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.18it/s, pg=-0.0927, ret=0.000311, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.18it/s, pg=0.163, ret=-0.000887, glen=129, tlen=290, kl=0, act_lr=2e-8, ent=2.04]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.163, ret=-0.000887, glen=129, tlen=290, kl=0, act_lr=2e-8, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.057, ret=0.000675, glen=98.6, tlen=259, kl=0, act_lr=2e-8, ent=1.57]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.16it/s, pg=-0.057, ret=0.000675, glen=98.6, tlen=259, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.16it/s, pg=0.0166, ret=-0.00109, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.67] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.16it/s, pg=0.0166, ret=-0.00109, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=-0.0638, ret=0.00119, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0638, ret=0.00119, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.16, ret=-0.00194, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.16, ret=-0.00194, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.73]
2025-07-24 16:12:48.426 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.43s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.181, ret=0.00102, glen=143, tlen=304, kl=0, act_lr=4e-8, ent=1.55]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.181, ret=0.00102, glen=143, tlen=304, kl=0, act_lr=4e-8, ent=1.55]
2025-07-24 16:12:49.303 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:12:51.663 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 16:12:52.008 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.13s
2025-07-24 16:12:52.015 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0045301872387267, 'actor_lr': 2.035087706930059e-08, 'clip_ratio': 0.0, 'entropy': 1.7246172030766804, 'kl': 0.0, 'response_length': 111.14298368755139, 'total_length': 271.7544030975877, 'teacher_total_length': 283.5741668165776, 'return': -7.169973717904405e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  15%|‚ñà‚ñå        | 2/13 [06:59<37:37, 205.19s/it]2025-07-24 16:12:52.058 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:15:32.344 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:15:32.523 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:15:32.524 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 160.47s
2025-07-24 16:15:34.999 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 112.6627,std_num_tokens: 145.2815,avg_correct_num_tokens: 103.4203,std_correct_num_tokens: 86.0646,avg_incorrect_num_tokens: 127.2372,std_incorrect_num_tokens: 205.8219
2025-07-24 16:15:35.377 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.85s
2025-07-24 16:15:38.311 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.93s
2025-07-24 16:16:07.259 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 226
2025-07-24 16:16:07.260 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 16:16:08.665 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 16:16:08.665 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0024170190746488, avg_kl: 0.000875793727098313, avg_response_length: 118.18322855181398, avg_orm_score: 0.0, avg_custom_rewards: -0.0024170190746488
2025-07-24 16:16:08.703 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter2_replay_buffer.jsonl
2025-07-24 16:16:10.603 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.0745, ret=-8.99e-5, glen=129, tlen=290, kl=0.000856, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0745, ret=-8.99e-5, glen=129, tlen=290, kl=0.000856, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0279, ret=1.71e-5, glen=111, tlen=271, kl=0.000875, act_lr=4e-8, ent=1.76] Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=-0.0279, ret=1.71e-5, glen=111, tlen=271, kl=0.000875, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=-0.117, ret=-0.000587, glen=116, tlen=276, kl=0.000868, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=-0.117, ret=-0.000587, glen=116, tlen=276, kl=0.000868, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=0.0988, ret=-0.0011, glen=118, tlen=278, kl=0.000878, act_lr=4e-8, ent=1.68]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=0.0988, ret=-0.0011, glen=118, tlen=278, kl=0.000878, act_lr=4e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=0.0905, ret=0.000838, glen=136, tlen=296, kl=0.000888, act_lr=4e-8, ent=1.94]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=0.0905, ret=0.000838, glen=136, tlen=296, kl=0.000888, act_lr=4e-8, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=-0.0579, ret=-0.000396, glen=104, tlen=264, kl=0.000914, act_lr=4e-8, ent=1.84]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.16it/s, pg=-0.0579, ret=-0.000396, glen=104, tlen=264, kl=0.000914, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.16it/s, pg=-0.195, ret=0.00119, glen=120, tlen=281, kl=0.000845, act_lr=4e-8, ent=1.85]   Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=-0.195, ret=0.00119, glen=120, tlen=281, kl=0.000845, act_lr=4e-8, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=-0.14, ret=0.000105, glen=103, tlen=264, kl=0.000918, act_lr=4e-8, ent=1.6] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:41,  1.17it/s, pg=-0.14, ret=0.000105, glen=103, tlen=264, kl=0.000918, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:41,  1.17it/s, pg=-0.183, ret=0.000671, glen=104, tlen=264, kl=0.00087, act_lr=4e-8, ent=1.6]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.183, ret=0.000671, glen=104, tlen=264, kl=0.00087, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.0739, ret=-0.000439, glen=112, tlen=273, kl=0.000913, act_lr=4e-8, ent=1.9]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=-0.0739, ret=-0.000439, glen=112, tlen=273, kl=0.000913, act_lr=4e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.139, ret=0.00231, glen=121, tlen=281, kl=0.000895, act_lr=4e-8, ent=1.6]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.15it/s, pg=-0.139, ret=0.00231, glen=121, tlen=281, kl=0.000895, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.15it/s, pg=-0.012, ret=0.000149, glen=110, tlen=270, kl=0.000875, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.012, ret=0.000149, glen=110, tlen=270, kl=0.000875, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.26, ret=-0.00287, glen=114, tlen=273, kl=0.000817, act_lr=4e-8, ent=1.48]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=0.26, ret=-0.00287, glen=114, tlen=273, kl=0.000817, act_lr=4e-8, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=0.00775, ret=0.000437, glen=112, tlen=273, kl=0.000891, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.16it/s, pg=0.00775, ret=0.000437, glen=112, tlen=273, kl=0.000891, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.16it/s, pg=-0.13, ret=0.000705, glen=103, tlen=263, kl=0.000855, act_lr=4e-8, ent=1.8]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.13, ret=0.000705, glen=103, tlen=263, kl=0.000855, act_lr=4e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.0493, ret=0.00138, glen=137, tlen=297, kl=0.000841, act_lr=4e-8, ent=2.22]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0493, ret=0.00138, glen=137, tlen=297, kl=0.000841, act_lr=4e-8, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.276, ret=-0.000972, glen=120, tlen=280, kl=0.000904, act_lr=4e-8, ent=1.86]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.276, ret=-0.000972, glen=120, tlen=280, kl=0.000904, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.086, ret=0.000553, glen=103, tlen=263, kl=0.00088, act_lr=4e-8, ent=1.61] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.086, ret=0.000553, glen=103, tlen=263, kl=0.00088, act_lr=4e-8, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.195, ret=0.00143, glen=118, tlen=278, kl=0.000857, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.195, ret=0.00143, glen=118, tlen=278, kl=0.000857, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.188, ret=0.00116, glen=116, tlen=277, kl=0.000867, act_lr=4e-8, ent=1.73]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.188, ret=0.00116, glen=116, tlen=277, kl=0.000867, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.00564, ret=0.00012, glen=113, tlen=274, kl=0.000859, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.00564, ret=0.00012, glen=113, tlen=274, kl=0.000859, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.113, ret=-0.00015, glen=131, tlen=291, kl=0.000892, act_lr=4e-8, ent=1.78]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.113, ret=-0.00015, glen=131, tlen=291, kl=0.000892, act_lr=4e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0288, ret=-0.0014, glen=108, tlen=267, kl=0.000916, act_lr=4e-8, ent=1.64]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.17it/s, pg=0.0288, ret=-0.0014, glen=108, tlen=267, kl=0.000916, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0328, ret=-0.000227, glen=130, tlen=290, kl=0.000848, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0328, ret=-0.000227, glen=130, tlen=290, kl=0.000848, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0914, ret=-0.00149, glen=113, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.73] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0914, ret=-0.00149, glen=113, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0418, ret=0.000939, glen=111, tlen=271, kl=0.000896, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0418, ret=0.000939, glen=111, tlen=271, kl=0.000896, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.108, ret=-0.00106, glen=103, tlen=263, kl=0.000921, act_lr=4e-8, ent=1.7]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.108, ret=-0.00106, glen=103, tlen=263, kl=0.000921, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=-0.0739, ret=0.00138, glen=116, tlen=277, kl=0.000908, act_lr=4e-8, ent=1.85]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=-0.0739, ret=0.00138, glen=116, tlen=277, kl=0.000908, act_lr=4e-8, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0544, ret=-0.000121, glen=108, tlen=269, kl=0.00084, act_lr=4e-8, ent=1.63]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:27,  1.01it/s, pg=-0.0544, ret=-0.000121, glen=108, tlen=269, kl=0.00084, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:27,  1.01it/s, pg=-0.159, ret=0.00108, glen=114, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.65]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.05it/s, pg=-0.159, ret=0.00108, glen=114, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.05it/s, pg=-0.207, ret=0.00114, glen=99, tlen=260, kl=0.00088, act_lr=4e-8, ent=1.63]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.207, ret=0.00114, glen=99, tlen=260, kl=0.00088, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.09it/s, pg=-0.171, ret=0.00131, glen=108, tlen=269, kl=0.000911, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=-0.171, ret=0.00131, glen=108, tlen=269, kl=0.000911, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.233, ret=-0.00321, glen=108, tlen=268, kl=0.000928, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.233, ret=-0.00321, glen=108, tlen=268, kl=0.000928, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=-0.00635, ret=0.00115, glen=124, tlen=284, kl=0.000877, act_lr=4e-8, ent=1.86]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.14it/s, pg=-0.00635, ret=0.00115, glen=124, tlen=284, kl=0.000877, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.14it/s, pg=-0.00122, ret=0.000491, glen=102, tlen=262, kl=0.000856, act_lr=4e-8, ent=1.66]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.15it/s, pg=-0.00122, ret=0.000491, glen=102, tlen=262, kl=0.000856, act_lr=4e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.15it/s, pg=-0.0938, ret=0.000208, glen=112, tlen=273, kl=0.000917, act_lr=4e-8, ent=1.84] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0938, ret=0.000208, glen=112, tlen=273, kl=0.000917, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.0966, ret=-0.000281, glen=93.6, tlen=254, kl=0.000879, act_lr=4e-8, ent=1.52]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.0966, ret=-0.000281, glen=93.6, tlen=254, kl=0.000879, act_lr=4e-8, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0652, ret=-0.000276, glen=96.6, tlen=257, kl=0.000896, act_lr=4e-8, ent=1.67]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0652, ret=-0.000276, glen=96.6, tlen=257, kl=0.000896, act_lr=4e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=-0.0344, ret=0.00203, glen=109, tlen=270, kl=0.000886, act_lr=4e-8, ent=1.98]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0344, ret=0.00203, glen=109, tlen=270, kl=0.000886, act_lr=4e-8, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.11, ret=0.00118, glen=114, tlen=274, kl=0.000892, act_lr=4e-8, ent=1.7]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.11, ret=0.00118, glen=114, tlen=274, kl=0.000892, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.242, ret=-0.00316, glen=115, tlen=275, kl=0.000875, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.242, ret=-0.00316, glen=115, tlen=275, kl=0.000875, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0181, ret=3.29e-5, glen=107, tlen=266, kl=0.000891, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0181, ret=3.29e-5, glen=107, tlen=266, kl=0.000891, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.154, ret=0.000425, glen=101, tlen=261, kl=0.000907, act_lr=4e-8, ent=1.64]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.154, ret=0.000425, glen=101, tlen=261, kl=0.000907, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.00129, glen=109, tlen=269, kl=0.00086, act_lr=4e-8, ent=1.84]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.00129, glen=109, tlen=269, kl=0.00086, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.0341, ret=0.000607, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.77]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0341, ret=0.000607, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.218, ret=0.00251, glen=115, tlen=276, kl=0.000878, act_lr=4e-8, ent=1.86]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.218, ret=0.00251, glen=115, tlen=276, kl=0.000878, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.244, ret=-0.00315, glen=125, tlen=285, kl=0.000886, act_lr=4e-8, ent=1.7] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.244, ret=-0.00315, glen=125, tlen=285, kl=0.000886, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.149, ret=-0.00317, glen=126, tlen=286, kl=0.000907, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.149, ret=-0.00317, glen=126, tlen=286, kl=0.000907, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.431, ret=-0.0183, glen=410, tlen=570, kl=0.000707, act_lr=4e-8, ent=2.71] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:07,  1.14it/s, pg=0.431, ret=-0.0183, glen=410, tlen=570, kl=0.000707, act_lr=4e-8, ent=2.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:07,  1.14it/s, pg=0.14, ret=-0.00188, glen=118, tlen=278, kl=0.000865, act_lr=4e-8, ent=1.57]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.15it/s, pg=0.14, ret=-0.00188, glen=118, tlen=278, kl=0.000865, act_lr=4e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.15it/s, pg=-0.0471, ret=-0.000547, glen=108, tlen=269, kl=0.000831, act_lr=4e-8, ent=1.8]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.0471, ret=-0.000547, glen=108, tlen=269, kl=0.000831, act_lr=4e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=-0.0851, ret=0.00087, glen=112, tlen=272, kl=0.00089, act_lr=4e-8, ent=1.74]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=-0.0851, ret=0.00087, glen=112, tlen=272, kl=0.00089, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.151, ret=0.000737, glen=110, tlen=270, kl=0.000897, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.151, ret=0.000737, glen=110, tlen=270, kl=0.000897, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.00233, ret=0.000999, glen=112, tlen=272, kl=0.000839, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.00233, ret=0.000999, glen=112, tlen=272, kl=0.000839, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0515, ret=-0.000648, glen=108, tlen=269, kl=0.000858, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0515, ret=-0.000648, glen=108, tlen=269, kl=0.000858, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.148, ret=-0.000656, glen=128, tlen=289, kl=0.000846, act_lr=4e-8, ent=1.94]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.148, ret=-0.000656, glen=128, tlen=289, kl=0.000846, act_lr=4e-8, ent=1.94]
2025-07-24 16:17:00.357 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.57s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.00983, ret=0.000313, glen=110, tlen=271, kl=0.00084, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.00983, ret=0.000313, glen=110, tlen=271, kl=0.00084, act_lr=6e-8, ent=1.69]
2025-07-24 16:17:01.248 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:17:03.773 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 16:17:04.101 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.43s
2025-07-24 16:17:04.151 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01652477080361885, 'actor_lr': 4.0350876916587013e-08, 'clip_ratio': 0.0, 'entropy': 1.7695354474218268, 'kl': 0.0008754730224609375, 'response_length': 118.31446369907312, 'total_length': 278.56398947197096, 'teacher_total_length': 290.05270921138293, 'return': -0.0002886613505675964, 'policy_update_steps': 1.0}
Episode [1/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:11<37:46, 226.63s/it]2025-07-24 16:17:04.191 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:18:50.122 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:18:50.301 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:18:50.302 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 106.11s
2025-07-24 16:18:52.281 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 109.1659,std_num_tokens: 128.0053,avg_correct_num_tokens: 101.0823,std_correct_num_tokens: 86.4976,avg_incorrect_num_tokens: 121.6989,std_incorrect_num_tokens: 173.0066
2025-07-24 16:18:52.733 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.43s
2025-07-24 16:18:55.681 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.95s
2025-07-24 16:19:24.217 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 224
2025-07-24 16:19:24.218 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.53s
2025-07-24 16:19:25.583 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 16:19:25.583 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0010229884017982321, avg_kl: 0.0009026101657322475, avg_response_length: 111.71290295464652, avg_orm_score: 0.0, avg_custom_rewards: -0.0010229884017982321
2025-07-24 16:19:25.637 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter3_replay_buffer.jsonl
2025-07-24 16:19:27.483 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.169, ret=-0.00226, glen=187, tlen=347, kl=0.000835, act_lr=6e-8, ent=2.19]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:59,  1.08s/it, pg=0.169, ret=-0.00226, glen=187, tlen=347, kl=0.000835, act_lr=6e-8, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:59,  1.08s/it, pg=-0.114, ret=0.00101, glen=100, tlen=260, kl=0.000943, act_lr=6e-8, ent=1.74]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:51,  1.06it/s, pg=-0.114, ret=0.00101, glen=100, tlen=260, kl=0.000943, act_lr=6e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:51,  1.06it/s, pg=0.0869, ret=0.000845, glen=111, tlen=272, kl=0.000916, act_lr=6e-8, ent=1.79]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.10it/s, pg=0.0869, ret=0.000845, glen=111, tlen=272, kl=0.000916, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.10it/s, pg=-0.111, ret=0.0013, glen=106, tlen=267, kl=0.000919, act_lr=6e-8, ent=1.65]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.13it/s, pg=-0.111, ret=0.0013, glen=106, tlen=267, kl=0.000919, act_lr=6e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.13it/s, pg=0.12, ret=-0.00134, glen=120, tlen=280, kl=0.000899, act_lr=6e-8, ent=2.09]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.12, ret=-0.00134, glen=120, tlen=280, kl=0.000899, act_lr=6e-8, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=0.0226, ret=-0.000796, glen=97.6, tlen=258, kl=0.000943, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=0.0226, ret=-0.000796, glen=97.6, tlen=258, kl=0.000943, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.102, ret=0.0021, glen=108, tlen=269, kl=0.000917, act_lr=6e-8, ent=1.66]    Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=-0.102, ret=0.0021, glen=108, tlen=269, kl=0.000917, act_lr=6e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.0961, ret=-0.000353, glen=94.6, tlen=255, kl=0.000871, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0961, ret=-0.000353, glen=94.6, tlen=255, kl=0.000871, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.157, ret=0.00116, glen=108, tlen=269, kl=0.000896, act_lr=6e-8, ent=1.64]    Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.157, ret=0.00116, glen=108, tlen=269, kl=0.000896, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0488, ret=0.000599, glen=103, tlen=264, kl=0.000929, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0488, ret=0.000599, glen=103, tlen=264, kl=0.000929, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=0.173, ret=-0.00187, glen=118, tlen=278, kl=0.00083, act_lr=6e-8, ent=1.93]   Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=0.173, ret=-0.00187, glen=118, tlen=278, kl=0.00083, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=-0.108, ret=0.000618, glen=91.4, tlen=252, kl=0.000933, act_lr=6e-8, ent=1.7]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.16it/s, pg=-0.108, ret=0.000618, glen=91.4, tlen=252, kl=0.000933, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.16it/s, pg=0.0664, ret=-0.000441, glen=116, tlen=277, kl=0.000853, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.16it/s, pg=0.0664, ret=-0.000441, glen=116, tlen=277, kl=0.000853, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.16it/s, pg=-0.137, ret=0.000305, glen=146, tlen=306, kl=0.000909, act_lr=6e-8, ent=2.09] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.17it/s, pg=-0.137, ret=0.000305, glen=146, tlen=306, kl=0.000909, act_lr=6e-8, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.17it/s, pg=-0.0267, ret=-0.00147, glen=109, tlen=269, kl=0.000905, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.0267, ret=-0.00147, glen=109, tlen=269, kl=0.000905, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.00988, ret=-0.000825, glen=96.3, tlen=257, kl=0.000897, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.16it/s, pg=-0.00988, ret=-0.000825, glen=96.3, tlen=257, kl=0.000897, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.00235, ret=-0.000202, glen=91.3, tlen=252, kl=0.000913, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.00235, ret=-0.000202, glen=91.3, tlen=252, kl=0.000913, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.0131, ret=-0.000867, glen=109, tlen=269, kl=0.000899, act_lr=6e-8, ent=1.79]   Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.0131, ret=-0.000867, glen=109, tlen=269, kl=0.000899, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0363, ret=-0.000524, glen=108, tlen=269, kl=0.000918, act_lr=6e-8, ent=1.61]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0363, ret=-0.000524, glen=108, tlen=269, kl=0.000918, act_lr=6e-8, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.086, ret=-0.000445, glen=158, tlen=318, kl=0.000839, act_lr=6e-8, ent=2.21]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.086, ret=-0.000445, glen=158, tlen=318, kl=0.000839, act_lr=6e-8, ent=2.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.201, ret=0.00272, glen=115, tlen=275, kl=0.000888, act_lr=6e-8, ent=1.93] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.201, ret=0.00272, glen=115, tlen=275, kl=0.000888, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=0.148, ret=-0.0012, glen=127, tlen=287, kl=0.000909, act_lr=6e-8, ent=1.76] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.148, ret=-0.0012, glen=127, tlen=287, kl=0.000909, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.021, ret=0.00115, glen=105, tlen=266, kl=0.000918, act_lr=6e-8, ent=1.78]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=0.021, ret=0.00115, glen=105, tlen=266, kl=0.000918, act_lr=6e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.104, ret=-0.000369, glen=98.4, tlen=259, kl=0.000913, act_lr=6e-8, ent=1.53]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.104, ret=-0.000369, glen=98.4, tlen=259, kl=0.000913, act_lr=6e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.125, ret=-0.000261, glen=137, tlen=297, kl=0.000834, act_lr=6e-8, ent=2.22]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.125, ret=-0.000261, glen=137, tlen=297, kl=0.000834, act_lr=6e-8, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.0552, ret=0.00125, glen=109, tlen=268, kl=0.00092, act_lr=6e-8, ent=1.68] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.0552, ret=0.00125, glen=109, tlen=268, kl=0.00092, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.045, ret=-0.00055, glen=97.8, tlen=258, kl=0.000957, act_lr=6e-8, ent=1.59]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.045, ret=-0.00055, glen=97.8, tlen=258, kl=0.000957, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.00842, ret=0.000603, glen=119, tlen=279, kl=0.00087, act_lr=6e-8, ent=1.87]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.00842, ret=0.000603, glen=119, tlen=279, kl=0.00087, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.0585, ret=0.000429, glen=98, tlen=258, kl=0.000882, act_lr=6e-8, ent=1.65] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.0585, ret=0.000429, glen=98, tlen=258, kl=0.000882, act_lr=6e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=0.256, ret=-0.00329, glen=105, tlen=266, kl=0.00092, act_lr=6e-8, ent=1.7]   Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.256, ret=-0.00329, glen=105, tlen=266, kl=0.00092, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=0.00259, ret=-0.000603, glen=113, tlen=274, kl=0.000949, act_lr=6e-8, ent=1.77]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.00259, ret=-0.000603, glen=113, tlen=274, kl=0.000949, act_lr=6e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.115, ret=0.000115, glen=112, tlen=272, kl=0.000887, act_lr=6e-8, ent=1.8]   Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.13it/s, pg=-0.115, ret=0.000115, glen=112, tlen=272, kl=0.000887, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.0431, ret=0.00139, glen=113, tlen=273, kl=0.000855, act_lr=6e-8, ent=1.82]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=-0.0431, ret=0.00139, glen=113, tlen=273, kl=0.000855, act_lr=6e-8, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=0.0608, ret=-0.000599, glen=102, tlen=263, kl=0.000956, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=0.0608, ret=-0.000599, glen=102, tlen=263, kl=0.000956, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=-0.0305, ret=-0.000302, glen=98.9, tlen=259, kl=0.000947, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.0305, ret=-0.000302, glen=98.9, tlen=259, kl=0.000947, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=-0.0984, ret=0.00109, glen=118, tlen=279, kl=0.000922, act_lr=6e-8, ent=1.87]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=-0.0984, ret=0.00109, glen=118, tlen=279, kl=0.000922, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=0.0214, ret=0.000823, glen=123, tlen=284, kl=0.000901, act_lr=6e-8, ent=1.78]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=0.0214, ret=0.000823, glen=123, tlen=284, kl=0.000901, act_lr=6e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.234, ret=0.00201, glen=112, tlen=272, kl=0.000901, act_lr=6e-8, ent=1.79] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:32<00:15,  1.17it/s, pg=-0.234, ret=0.00201, glen=112, tlen=272, kl=0.000901, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0181, ret=-0.00049, glen=109, tlen=269, kl=0.000919, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.16it/s, pg=0.0181, ret=-0.00049, glen=109, tlen=269, kl=0.000919, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=0.115, ret=-0.000841, glen=89.2, tlen=249, kl=0.000929, act_lr=6e-8, ent=1.64]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.16it/s, pg=0.115, ret=-0.000841, glen=89.2, tlen=249, kl=0.000929, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.16it/s, pg=0.253, ret=-0.00188, glen=136, tlen=296, kl=0.000793, act_lr=6e-8, ent=1.51]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.16it/s, pg=0.253, ret=-0.00188, glen=136, tlen=296, kl=0.000793, act_lr=6e-8, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=0.0101, ret=-0.0004, glen=110, tlen=270, kl=0.000902, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.16it/s, pg=0.0101, ret=-0.0004, glen=110, tlen=270, kl=0.000902, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.16it/s, pg=-0.148, ret=3.85e-5, glen=97.2, tlen=257, kl=0.000912, act_lr=6e-8, ent=1.58]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=3.85e-5, glen=97.2, tlen=257, kl=0.000912, act_lr=6e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0665, ret=-0.00255, glen=96, tlen=256, kl=0.000932, act_lr=6e-8, ent=1.59] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0665, ret=-0.00255, glen=96, tlen=256, kl=0.000932, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.19, ret=-0.00182, glen=113, tlen=273, kl=0.000876, act_lr=6e-8, ent=1.84] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.17it/s, pg=0.19, ret=-0.00182, glen=113, tlen=273, kl=0.000876, act_lr=6e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.079, ret=0.000555, glen=107, tlen=268, kl=0.000908, act_lr=6e-8, ent=1.62]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.079, ret=0.000555, glen=107, tlen=268, kl=0.000908, act_lr=6e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.113, ret=0.00129, glen=130, tlen=291, kl=0.000882, act_lr=6e-8, ent=1.73] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.18it/s, pg=-0.113, ret=0.00129, glen=130, tlen=291, kl=0.000882, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.18it/s, pg=0.0896, ret=-0.000923, glen=97.5, tlen=258, kl=0.000916, act_lr=6e-8, ent=1.54]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=0.0896, ret=-0.000923, glen=97.5, tlen=258, kl=0.000916, act_lr=6e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.0272, ret=0.00111, glen=114, tlen=274, kl=0.00092, act_lr=6e-8, ent=1.67]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.0272, ret=0.00111, glen=114, tlen=274, kl=0.00092, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=-0.0159, ret=-0.000171, glen=120, tlen=280, kl=0.00091, act_lr=6e-8, ent=2.06]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.0159, ret=-0.000171, glen=120, tlen=280, kl=0.00091, act_lr=6e-8, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0125, ret=-0.000249, glen=101, tlen=261, kl=0.000925, act_lr=6e-8, ent=1.59]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0125, ret=-0.000249, glen=101, tlen=261, kl=0.000925, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0162, ret=0.000158, glen=114, tlen=274, kl=0.000901, act_lr=6e-8, ent=1.88]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=-0.0162, ret=0.000158, glen=114, tlen=274, kl=0.000901, act_lr=6e-8, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.0827, ret=0.000486, glen=119, tlen=280, kl=0.000891, act_lr=6e-8, ent=1.73]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.0827, ret=0.000486, glen=119, tlen=280, kl=0.000891, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0355, ret=-0.000383, glen=96.4, tlen=257, kl=0.000915, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0355, ret=-0.000383, glen=96.4, tlen=257, kl=0.000915, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.0361, ret=-0.000192, glen=115, tlen=275, kl=0.000886, act_lr=6e-8, ent=1.7]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=-0.0361, ret=-0.000192, glen=115, tlen=275, kl=0.000886, act_lr=6e-8, ent=1.7]
2025-07-24 16:20:16.148 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=-0.194, ret=0.000859, glen=109, tlen=270, kl=0.000936, act_lr=8e-8, ent=1.97] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.194, ret=0.000859, glen=109, tlen=270, kl=0.000936, act_lr=8e-8, ent=1.97]
2025-07-24 16:20:17.016 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:20:19.425 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.41s
2025-07-24 16:20:19.750 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.20s
2025-07-24 16:20:19.758 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.010056640420641218, 'actor_lr': 6.03571407456879e-08, 'clip_ratio': 0.0, 'entropy': 1.7597153910568781, 'kl': 0.0009026101657322475, 'response_length': 111.71290343148368, 'total_length': 272.0588773999895, 'teacher_total_length': 284.70700345720564, 'return': -7.95320956967771e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:27<32:09, 214.38s/it]2025-07-24 16:20:19.806 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:23:00.801 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:23:00.983 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:23:00.984 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 161.18s
2025-07-24 16:23:03.047 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0100,avg_pass_at_n: 1.0000,avg_num_tokens: 113.3406,std_num_tokens: 152.2395,avg_correct_num_tokens: 103.6449,std_correct_num_tokens: 88.5879,avg_incorrect_num_tokens: 128.1518,std_incorrect_num_tokens: 215.0134
2025-07-24 16:23:03.500 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.52s
2025-07-24 16:23:06.581 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.08s
2025-07-24 16:23:36.209 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:23:36.209 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.63s
2025-07-24 16:23:37.920 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.24s
2025-07-24 16:23:37.920 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0005573580287067488, avg_kl: 0.0008828129830839332, avg_response_length: 118.33821242553178, avg_orm_score: 0.0, avg_custom_rewards: -0.0005573580287067488
2025-07-24 16:23:37.956 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter4_replay_buffer.jsonl
2025-07-24 16:23:39.880 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.93s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.198, ret=0.00151, glen=102, tlen=263, kl=0.000929, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.198, ret=0.00151, glen=102, tlen=263, kl=0.000929, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.00903, ret=-0.00139, glen=109, tlen=270, kl=0.000844, act_lr=8e-8, ent=1.6]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.06it/s, pg=-0.00903, ret=-0.00139, glen=109, tlen=270, kl=0.000844, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.06it/s, pg=-0.138, ret=0.000485, glen=110, tlen=270, kl=0.000888, act_lr=8e-8, ent=1.75] Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.11it/s, pg=-0.138, ret=0.000485, glen=110, tlen=270, kl=0.000888, act_lr=8e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.11it/s, pg=0.0961, ret=-0.00124, glen=117, tlen=278, kl=0.000882, act_lr=8e-8, ent=1.58]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.13it/s, pg=0.0961, ret=-0.00124, glen=117, tlen=278, kl=0.000882, act_lr=8e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.13it/s, pg=-0.0509, ret=-0.000247, glen=103, tlen=264, kl=0.000858, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:46,  1.15it/s, pg=-0.0509, ret=-0.000247, glen=103, tlen=264, kl=0.000858, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:46,  1.15it/s, pg=-0.0633, ret=0.00111, glen=111, tlen=272, kl=0.000883, act_lr=8e-8, ent=1.76]  Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:44,  1.16it/s, pg=-0.0633, ret=0.00111, glen=111, tlen=272, kl=0.000883, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:44,  1.16it/s, pg=0.114, ret=-0.00224, glen=356, tlen=517, kl=0.000704, act_lr=8e-8, ent=1.33] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.13it/s, pg=0.114, ret=-0.00224, glen=356, tlen=517, kl=0.000704, act_lr=8e-8, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.13it/s, pg=-0.0398, ret=-0.000177, glen=102, tlen=262, kl=0.000905, act_lr=8e-8, ent=1.64]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=-0.0398, ret=-0.000177, glen=102, tlen=262, kl=0.000905, act_lr=8e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=-0.0136, ret=-0.000271, glen=111, tlen=272, kl=0.000905, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.14it/s, pg=-0.0136, ret=-0.000271, glen=111, tlen=272, kl=0.000905, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.14it/s, pg=0.171, ret=-0.00211, glen=106, tlen=266, kl=0.000896, act_lr=8e-8, ent=1.54]   Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.15it/s, pg=0.171, ret=-0.00211, glen=106, tlen=266, kl=0.000896, act_lr=8e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.15it/s, pg=0.0506, ret=-0.000404, glen=111, tlen=272, kl=0.000831, act_lr=8e-8, ent=1.95]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.16it/s, pg=0.0506, ret=-0.000404, glen=111, tlen=272, kl=0.000831, act_lr=8e-8, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.16it/s, pg=0.0435, ret=-0.00103, glen=105, tlen=265, kl=0.000893, act_lr=8e-8, ent=1.59] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.16it/s, pg=0.0435, ret=-0.00103, glen=105, tlen=265, kl=0.000893, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.16it/s, pg=0.0495, ret=-0.00215, glen=108, tlen=269, kl=0.000892, act_lr=8e-8, ent=1.71]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=0.0495, ret=-0.00215, glen=108, tlen=269, kl=0.000892, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=0.0609, ret=-0.00107, glen=106, tlen=267, kl=0.000899, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=0.0609, ret=-0.00107, glen=106, tlen=267, kl=0.000899, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=-0.142, ret=0.00198, glen=118, tlen=278, kl=0.000916, act_lr=8e-8, ent=1.74] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:37,  1.15it/s, pg=-0.142, ret=0.00198, glen=118, tlen=278, kl=0.000916, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:37,  1.15it/s, pg=-0.0829, ret=-0.000248, glen=108, tlen=269, kl=0.000873, act_lr=8e-8, ent=1.57]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.0829, ret=-0.000248, glen=108, tlen=269, kl=0.000873, act_lr=8e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.051, ret=0.000987, glen=112, tlen=273, kl=0.000872, act_lr=8e-8, ent=1.72]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.051, ret=0.000987, glen=112, tlen=273, kl=0.000872, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.0227, ret=-0.00132, glen=101, tlen=262, kl=0.000894, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=0.0227, ret=-0.00132, glen=101, tlen=262, kl=0.000894, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=0.153, ret=-0.00147, glen=116, tlen=276, kl=0.000903, act_lr=8e-8, ent=1.78] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=0.153, ret=-0.00147, glen=116, tlen=276, kl=0.000903, act_lr=8e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.00244, ret=0.000159, glen=126, tlen=287, kl=0.000901, act_lr=8e-8, ent=1.92]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.00244, ret=0.000159, glen=126, tlen=287, kl=0.000901, act_lr=8e-8, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.105, ret=0.000885, glen=113, tlen=274, kl=0.000881, act_lr=8e-8, ent=1.66] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.105, ret=0.000885, glen=113, tlen=274, kl=0.000881, act_lr=8e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=-0.161, ret=0.00229, glen=122, tlen=282, kl=0.000855, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.161, ret=0.00229, glen=122, tlen=282, kl=0.000855, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0652, ret=0.000451, glen=111, tlen=272, kl=0.000863, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.17it/s, pg=-0.0652, ret=0.000451, glen=111, tlen=272, kl=0.000863, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.168, ret=0.00189, glen=129, tlen=290, kl=0.000857, act_lr=8e-8, ent=2.03]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:28,  1.17it/s, pg=-0.168, ret=0.00189, glen=129, tlen=290, kl=0.000857, act_lr=8e-8, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:28,  1.17it/s, pg=-0.166, ret=0.00144, glen=132, tlen=294, kl=0.000873, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.18it/s, pg=-0.166, ret=0.00144, glen=132, tlen=294, kl=0.000873, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.18it/s, pg=0.239, ret=-0.00231, glen=134, tlen=295, kl=0.000848, act_lr=8e-8, ent=1.9] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.18it/s, pg=0.239, ret=-0.00231, glen=134, tlen=295, kl=0.000848, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.18it/s, pg=-0.0405, ret=-3.71e-5, glen=132, tlen=293, kl=0.000905, act_lr=8e-8, ent=1.81]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.18it/s, pg=-0.0405, ret=-3.71e-5, glen=132, tlen=293, kl=0.000905, act_lr=8e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.18it/s, pg=-0.126, ret=-0.000624, glen=134, tlen=294, kl=0.000897, act_lr=8e-8, ent=1.87]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=-0.126, ret=-0.000624, glen=134, tlen=294, kl=0.000897, act_lr=8e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.102, ret=-0.000284, glen=106, tlen=267, kl=0.000874, act_lr=8e-8, ent=1.62]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.07it/s, pg=-0.102, ret=-0.000284, glen=106, tlen=267, kl=0.000874, act_lr=8e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.00371, ret=9.82e-5, glen=126, tlen=286, kl=0.000901, act_lr=8e-8, ent=1.91]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=-0.00371, ret=9.82e-5, glen=126, tlen=286, kl=0.000901, act_lr=8e-8, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.10it/s, pg=0.149, ret=-0.00141, glen=138, tlen=298, kl=0.000907, act_lr=8e-8, ent=2]     Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.149, ret=-0.00141, glen=138, tlen=298, kl=0.000907, act_lr=8e-8, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.0511, ret=2.9e-5, glen=123, tlen=284, kl=0.000866, act_lr=8e-8, ent=1.71]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:22,  1.14it/s, pg=0.0511, ret=2.9e-5, glen=123, tlen=284, kl=0.000866, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.14it/s, pg=0.0269, ret=-0.000232, glen=118, tlen=279, kl=0.000917, act_lr=8e-8, ent=1.78]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.15it/s, pg=0.0269, ret=-0.000232, glen=118, tlen=279, kl=0.000917, act_lr=8e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.15it/s, pg=0.081, ret=-0.000541, glen=105, tlen=266, kl=0.000872, act_lr=8e-8, ent=1.6]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.16it/s, pg=0.081, ret=-0.000541, glen=105, tlen=266, kl=0.000872, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.16it/s, pg=0.0626, ret=-4.4e-5, glen=96.2, tlen=257, kl=0.00088, act_lr=8e-8, ent=1.55]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=0.0626, ret=-4.4e-5, glen=96.2, tlen=257, kl=0.00088, act_lr=8e-8, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=-0.187, ret=0.000526, glen=114, tlen=275, kl=0.000891, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.17it/s, pg=-0.187, ret=0.000526, glen=114, tlen=275, kl=0.000891, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=0.107, ret=-0.000542, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:17,  1.17it/s, pg=0.107, ret=-0.000542, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=0.218, ret=-0.00381, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.218, ret=-0.00381, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=-0.301, ret=0.00221, glen=105, tlen=266, kl=0.000881, act_lr=8e-8, ent=1.59]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.17it/s, pg=-0.301, ret=0.00221, glen=105, tlen=266, kl=0.000881, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=-0.159, ret=0.00148, glen=117, tlen=278, kl=0.000934, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=-0.159, ret=0.00148, glen=117, tlen=278, kl=0.000934, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=-0.0958, ret=8.63e-5, glen=112, tlen=273, kl=0.000878, act_lr=8e-8, ent=1.49]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0958, ret=8.63e-5, glen=112, tlen=273, kl=0.000878, act_lr=8e-8, ent=1.49]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=-0.134, ret=0.00187, glen=111, tlen=273, kl=0.000899, act_lr=8e-8, ent=1.74] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.16it/s, pg=-0.134, ret=0.00187, glen=111, tlen=273, kl=0.000899, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.16it/s, pg=-0.0825, ret=-0.000351, glen=116, tlen=278, kl=0.000887, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=-0.0825, ret=-0.000351, glen=116, tlen=278, kl=0.000887, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=0.0884, ret=0.000805, glen=116, tlen=277, kl=0.000852, act_lr=8e-8, ent=1.9]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:12,  1.17it/s, pg=0.0884, ret=0.000805, glen=116, tlen=277, kl=0.000852, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.17it/s, pg=-0.0308, ret=6.35e-5, glen=95.1, tlen=256, kl=0.000897, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0308, ret=6.35e-5, glen=95.1, tlen=256, kl=0.000897, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.0798, ret=-0.000667, glen=120, tlen=281, kl=0.000873, act_lr=8e-8, ent=1.91]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.17it/s, pg=0.0798, ret=-0.000667, glen=120, tlen=281, kl=0.000873, act_lr=8e-8, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=-0.188, ret=0.000712, glen=97.3, tlen=258, kl=0.000892, act_lr=8e-8, ent=1.51]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.188, ret=0.000712, glen=97.3, tlen=258, kl=0.000892, act_lr=8e-8, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=-0.0296, ret=0.000783, glen=108, tlen=269, kl=0.000907, act_lr=8e-8, ent=1.62]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.17it/s, pg=-0.0296, ret=0.000783, glen=108, tlen=269, kl=0.000907, act_lr=8e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=0.0599, ret=-0.000454, glen=98.9, tlen=260, kl=0.000945, act_lr=8e-8, ent=1.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.17it/s, pg=0.0599, ret=-0.000454, glen=98.9, tlen=260, kl=0.000945, act_lr=8e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=-0.0551, ret=0.0086, glen=115, tlen=277, kl=0.000934, act_lr=8e-8, ent=2.12]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.18it/s, pg=-0.0551, ret=0.0086, glen=115, tlen=277, kl=0.000934, act_lr=8e-8, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.18it/s, pg=-0.0998, ret=0.000231, glen=108, tlen=268, kl=0.000911, act_lr=8e-8, ent=1.54]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.18it/s, pg=-0.0998, ret=0.000231, glen=108, tlen=268, kl=0.000911, act_lr=8e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.18it/s, pg=0.0715, ret=-0.00159, glen=108, tlen=269, kl=0.000924, act_lr=8e-8, ent=1.53] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:44<00:05,  1.18it/s, pg=0.0715, ret=-0.00159, glen=108, tlen=269, kl=0.000924, act_lr=8e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.18it/s, pg=-0.0443, ret=0.000486, glen=108, tlen=269, kl=0.00084, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.18it/s, pg=-0.0443, ret=0.000486, glen=108, tlen=269, kl=0.00084, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.18it/s, pg=-0.0603, ret=0.00058, glen=105, tlen=266, kl=0.000932, act_lr=8e-8, ent=1.64]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.18it/s, pg=-0.0603, ret=0.00058, glen=105, tlen=266, kl=0.000932, act_lr=8e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.18it/s, pg=0.134, ret=0.000693, glen=156, tlen=317, kl=0.000748, act_lr=8e-8, ent=2.34] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.16it/s, pg=0.134, ret=0.000693, glen=156, tlen=317, kl=0.000748, act_lr=8e-8, ent=2.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.16it/s, pg=0.141, ret=-0.000664, glen=113, tlen=273, kl=0.000916, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.16it/s, pg=0.141, ret=-0.000664, glen=113, tlen=273, kl=0.000916, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.16it/s, pg=0.0629, ret=-0.000506, glen=122, tlen=283, kl=0.000863, act_lr=8e-8, ent=1.75]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=0.0629, ret=-0.000506, glen=122, tlen=283, kl=0.000863, act_lr=8e-8, ent=1.75]
2025-07-24 16:24:30.303 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.0569, ret=0.0016, glen=116, tlen=277, kl=0.000896, act_lr=1e-7, ent=1.78]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.14it/s, pg=-0.0569, ret=0.0016, glen=116, tlen=277, kl=0.000896, act_lr=1e-7, ent=1.78]
2025-07-24 16:24:31.175 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-24 16:24:33.619 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.44s
2025-07-24 16:24:33.946 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.99s
2025-07-24 16:24:33.953 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.015775351688779634, 'actor_lr': 8.034482712854038e-08, 'clip_ratio': 0.0, 'entropy': 1.7241236361963996, 'kl': 0.0008834004402160645, 'response_length': 118.16882981925175, 'total_length': 278.961176115891, 'teacher_total_length': 291.66929363382275, 'return': 7.941008265459396e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:41<30:29, 228.74s/it]2025-07-24 16:24:33.997 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:27:03.390 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:27:03.580 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 16:27:03.581 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 149.58s
2025-07-24 16:27:05.690 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0079,avg_pass_at_n: 1.0000,avg_num_tokens: 112.2317,std_num_tokens: 149.0196,avg_correct_num_tokens: 101.6415,std_correct_num_tokens: 81.0229,avg_incorrect_num_tokens: 129.6903,std_incorrect_num_tokens: 217.9545
2025-07-24 16:27:06.131 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.55s
2025-07-24 16:27:09.389 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.25s
2025-07-24 16:27:38.368 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 16:27:38.369 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.98s
2025-07-24 16:27:39.785 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.01s
2025-07-24 16:27:39.786 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0008574851802940017, avg_kl: 0.0008850013632081153, avg_response_length: 116.88514386819848, avg_orm_score: 0.0, avg_custom_rewards: 0.0008574851802940017
2025-07-24 16:27:39.818 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter5_replay_buffer.jsonl
2025-07-24 16:27:41.721 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0393, ret=-0.00192, glen=107, tlen=267, kl=0.000883, act_lr=1e-7, ent=1.81]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.0393, ret=-0.00192, glen=107, tlen=267, kl=0.000883, act_lr=1e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.00902, ret=0.000127, glen=118, tlen=278, kl=0.000901, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.00902, ret=0.000127, glen=118, tlen=278, kl=0.000901, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.0243, ret=-0.00238, glen=96.6, tlen=256, kl=0.000892, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=0.0243, ret=-0.00238, glen=96.6, tlen=256, kl=0.000892, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=0.248, ret=-0.00191, glen=118, tlen=278, kl=0.000877, act_lr=1e-7, ent=2.01]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=0.248, ret=-0.00191, glen=118, tlen=278, kl=0.000877, act_lr=1e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=0.0458, ret=-0.00112, glen=140, tlen=301, kl=0.000861, act_lr=1e-7, ent=1.6]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.13it/s, pg=0.0458, ret=-0.00112, glen=140, tlen=301, kl=0.000861, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.13it/s, pg=0.038, ret=0.00013, glen=108, tlen=269, kl=0.000864, act_lr=1e-7, ent=1.7]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.14it/s, pg=0.038, ret=0.00013, glen=108, tlen=269, kl=0.000864, act_lr=1e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.14it/s, pg=-0.133, ret=0.00173, glen=121, tlen=281, kl=0.00088, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=-0.133, ret=0.00173, glen=121, tlen=281, kl=0.00088, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=-0.173, ret=0.00139, glen=111, tlen=272, kl=0.000902, act_lr=1e-7, ent=1.72]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.173, ret=0.00139, glen=111, tlen=272, kl=0.000902, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.075, ret=-0.000734, glen=111, tlen=271, kl=0.000881, act_lr=1e-7, ent=1.8]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.16it/s, pg=0.075, ret=-0.000734, glen=111, tlen=271, kl=0.000881, act_lr=1e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.16it/s, pg=0.0364, ret=-0.0013, glen=121, tlen=281, kl=0.000901, act_lr=1e-7, ent=1.63]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=0.0364, ret=-0.0013, glen=121, tlen=281, kl=0.000901, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.0686, ret=-2.51e-6, glen=116, tlen=277, kl=0.000928, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.15it/s, pg=-0.0686, ret=-2.51e-6, glen=116, tlen=277, kl=0.000928, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.15it/s, pg=-0.0676, ret=0.00107, glen=108, tlen=269, kl=0.000891, act_lr=1e-7, ent=1.68] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=-0.0676, ret=0.00107, glen=108, tlen=269, kl=0.000891, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.082, ret=0.000271, glen=126, tlen=286, kl=0.000856, act_lr=1e-7, ent=2.07]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.082, ret=0.000271, glen=126, tlen=286, kl=0.000856, act_lr=1e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.00671, ret=-0.000421, glen=94.4, tlen=255, kl=0.000852, act_lr=1e-7, ent=1.55]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=-0.00671, ret=-0.000421, glen=94.4, tlen=255, kl=0.000852, act_lr=1e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=-0.25, ret=0.000238, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.68]     Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.25, ret=0.000238, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=0.338, ret=4.2e-5, glen=353, tlen=514, kl=0.000729, act_lr=1e-7, ent=2.82]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.13it/s, pg=0.338, ret=4.2e-5, glen=353, tlen=514, kl=0.000729, act_lr=1e-7, ent=2.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.13it/s, pg=0.0494, ret=0.000201, glen=119, tlen=279, kl=0.000907, act_lr=1e-7, ent=1.92]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:35,  1.14it/s, pg=0.0494, ret=0.000201, glen=119, tlen=279, kl=0.000907, act_lr=1e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.122, ret=0.000526, glen=113, tlen=273, kl=0.000918, act_lr=1e-7, ent=1.61]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.122, ret=0.000526, glen=113, tlen=273, kl=0.000918, act_lr=1e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=0.101, ret=-0.00188, glen=114, tlen=275, kl=0.000894, act_lr=1e-7, ent=1.69] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.101, ret=-0.00188, glen=114, tlen=275, kl=0.000894, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0151, ret=0.00018, glen=125, tlen=285, kl=0.000873, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0151, ret=0.00018, glen=125, tlen=285, kl=0.000873, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.234, ret=0.000414, glen=113, tlen=273, kl=0.000904, act_lr=1e-7, ent=1.72]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.234, ret=0.000414, glen=113, tlen=273, kl=0.000904, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.049, ret=-0.000156, glen=106, tlen=266, kl=0.000912, act_lr=1e-7, ent=1.77]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.049, ret=-0.000156, glen=106, tlen=266, kl=0.000912, act_lr=1e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.0792, ret=0.000538, glen=106, tlen=267, kl=0.000945, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0792, ret=0.000538, glen=106, tlen=267, kl=0.000945, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0269, ret=7.95e-5, glen=110, tlen=271, kl=0.000875, act_lr=1e-7, ent=1.59]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0269, ret=7.95e-5, glen=110, tlen=271, kl=0.000875, act_lr=1e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.135, ret=-1.89e-5, glen=123, tlen=284, kl=0.000855, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.135, ret=-1.89e-5, glen=123, tlen=284, kl=0.000855, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0774, ret=0.00101, glen=103, tlen=263, kl=0.000931, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0774, ret=0.00101, glen=103, tlen=263, kl=0.000931, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0476, ret=-0.00123, glen=121, tlen=282, kl=0.000851, act_lr=1e-7, ent=1.85]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.0476, ret=-0.00123, glen=121, tlen=282, kl=0.000851, act_lr=1e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0303, ret=0.000846, glen=127, tlen=288, kl=0.000811, act_lr=1e-7, ent=2.13]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0303, ret=0.000846, glen=127, tlen=288, kl=0.000811, act_lr=1e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0804, ret=-0.00016, glen=98.8, tlen=259, kl=0.000901, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=-0.0804, ret=-0.00016, glen=98.8, tlen=259, kl=0.000901, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.0238, ret=-0.000203, glen=98.5, tlen=259, kl=0.000941, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=-0.0238, ret=-0.000203, glen=98.5, tlen=259, kl=0.000941, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.0546, ret=0.0011, glen=115, tlen=275, kl=0.000786, act_lr=1e-7, ent=1.43]     Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0546, ret=0.0011, glen=115, tlen=275, kl=0.000786, act_lr=1e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0159, ret=0.000236, glen=102, tlen=262, kl=0.00088, act_lr=1e-7, ent=1.59]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.14it/s, pg=0.0159, ret=0.000236, glen=102, tlen=262, kl=0.00088, act_lr=1e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.14it/s, pg=0.125, ret=-6.43e-5, glen=131, tlen=291, kl=0.0009, act_lr=1e-7, ent=1.81]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.14it/s, pg=0.125, ret=-6.43e-5, glen=131, tlen=291, kl=0.0009, act_lr=1e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.23, ret=0.0289, glen=93.6, tlen=255, kl=0.000889, act_lr=1e-7, ent=1.73]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.23, ret=0.0289, glen=93.6, tlen=255, kl=0.000889, act_lr=1e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=-0.0799, ret=0.000608, glen=109, tlen=269, kl=0.000916, act_lr=1e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.0799, ret=0.000608, glen=109, tlen=269, kl=0.000916, act_lr=1e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.0713, ret=0.000816, glen=120, tlen=281, kl=0.0009, act_lr=1e-7, ent=2.03]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0713, ret=0.000816, glen=120, tlen=281, kl=0.0009, act_lr=1e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=0.0663, ret=-0.00037, glen=112, tlen=273, kl=0.000896, act_lr=1e-7, ent=1.58]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=0.0663, ret=-0.00037, glen=112, tlen=273, kl=0.000896, act_lr=1e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.211, ret=0.00184, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.63] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.211, ret=0.00184, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.0505, ret=-0.00124, glen=115, tlen=276, kl=0.000849, act_lr=1e-7, ent=1.64]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=0.0505, ret=-0.00124, glen=115, tlen=276, kl=0.000849, act_lr=1e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0694, ret=0.00014, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.6] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0694, ret=0.00014, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0265, ret=-0.000634, glen=101, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.68]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0265, ret=-0.000634, glen=101, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.104, ret=0.000431, glen=117, tlen=277, kl=0.000908, act_lr=1e-7, ent=1.76] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.104, ret=0.000431, glen=117, tlen=277, kl=0.000908, act_lr=1e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=-0.0182, ret=-0.000597, glen=120, tlen=281, kl=0.000882, act_lr=1e-7, ent=1.94]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=-0.0182, ret=-0.000597, glen=120, tlen=281, kl=0.000882, act_lr=1e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.167, ret=0.000342, glen=101, tlen=261, kl=0.000909, act_lr=1e-7, ent=1.71]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=-0.167, ret=0.000342, glen=101, tlen=261, kl=0.000909, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=-0.149, ret=0.00203, glen=112, tlen=272, kl=0.000911, act_lr=1e-7, ent=1.67] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=-0.149, ret=0.00203, glen=112, tlen=272, kl=0.000911, act_lr=1e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.00372, ret=-0.00123, glen=104, tlen=264, kl=0.000896, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.18it/s, pg=0.00372, ret=-0.00123, glen=104, tlen=264, kl=0.000896, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.195, ret=0.0016, glen=101, tlen=261, kl=0.00093, act_lr=1e-7, ent=1.66]    Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.195, ret=0.0016, glen=101, tlen=261, kl=0.00093, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=0.214, ret=-0.00248, glen=154, tlen=314, kl=0.000893, act_lr=1e-7, ent=2.17]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.214, ret=-0.00248, glen=154, tlen=314, kl=0.000893, act_lr=1e-7, ent=2.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.195, ret=-0.00141, glen=123, tlen=284, kl=0.000898, act_lr=1e-7, ent=1.95]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.195, ret=-0.00141, glen=123, tlen=284, kl=0.000898, act_lr=1e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.000458, ret=-0.000291, glen=120, tlen=281, kl=0.000905, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.000458, ret=-0.000291, glen=120, tlen=281, kl=0.000905, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.224, ret=0.00203, glen=104, tlen=264, kl=0.000823, act_lr=1e-7, ent=1.89]     Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.224, ret=0.00203, glen=104, tlen=264, kl=0.000823, act_lr=1e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.218, ret=0.00191, glen=102, tlen=262, kl=0.000894, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.218, ret=0.00191, glen=102, tlen=262, kl=0.000894, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.105, ret=-0.0017, glen=103, tlen=263, kl=0.000896, act_lr=1e-7, ent=1.5]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.18it/s, pg=0.105, ret=-0.0017, glen=103, tlen=263, kl=0.000896, act_lr=1e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=-0.0441, ret=0.000576, glen=115, tlen=276, kl=0.000901, act_lr=1e-7, ent=1.71]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.18it/s, pg=-0.0441, ret=0.000576, glen=115, tlen=276, kl=0.000901, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.18it/s, pg=-0.0565, ret=-0.000726, glen=106, tlen=266, kl=0.000917, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.18it/s, pg=-0.0565, ret=-0.000726, glen=106, tlen=266, kl=0.000917, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.18it/s, pg=0.0109, ret=0.000953, glen=103, tlen=263, kl=0.000872, act_lr=1e-7, ent=1.69]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0109, ret=0.000953, glen=103, tlen=263, kl=0.000872, act_lr=1e-7, ent=1.69]
2025-07-24 16:28:31.524 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.39s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.123, ret=-0.000452, glen=125, tlen=286, kl=0.000888, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=0.123, ret=-0.000452, glen=125, tlen=286, kl=0.000888, act_lr=1.2e-7, ent=1.83]
2025-07-24 16:28:32.376 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 16:28:34.954 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 16:28:35.298 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.29s
2025-07-24 16:28:35.309 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01867935113739549, 'actor_lr': 1.0035087826596729e-07, 'clip_ratio': 0.0, 'entropy': 1.7527947906862225, 'kl': 0.0008854698716548451, 'response_length': 116.7514379400956, 'total_length': 277.13355670058934, 'teacher_total_length': 289.843635425233, 'return': 0.0004855551092636265, 'policy_update_steps': 1.0}
Episode [1/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:43<27:11, 233.03s/it]2025-07-24 16:28:35.353 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:29:59.237 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:29:59.416 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:29:59.417 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 84.06s
2025-07-24 16:30:01.723 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 108.2264,std_num_tokens: 106.4321,avg_correct_num_tokens: 101.3536,std_correct_num_tokens: 80.9237,avg_incorrect_num_tokens: 119.4747,std_incorrect_num_tokens: 137.6432
2025-07-24 16:30:02.195 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.78s
2025-07-24 16:30:05.160 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.96s
2025-07-24 16:30:33.561 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 16:30:33.562 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.40s
2025-07-24 16:30:35.031 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.00s
2025-07-24 16:30:35.032 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.00016510206292437665, avg_kl: 0.000943119750429162, avg_response_length: 109.35092505211253, avg_orm_score: 0.0, avg_custom_rewards: -0.00016510206292437665
2025-07-24 16:30:35.065 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter6_replay_buffer.jsonl
2025-07-24 16:30:36.896 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.83s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0025, ret=0.000914, glen=100, tlen=261, kl=0.000953, act_lr=1.2e-7, ent=1.87]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.0025, ret=0.000914, glen=100, tlen=261, kl=0.000953, act_lr=1.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.00173, ret=-0.00164, glen=118, tlen=279, kl=0.000913, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=-0.00173, ret=-0.00164, glen=118, tlen=279, kl=0.000913, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=0.0443, ret=-0.000604, glen=117, tlen=277, kl=0.000939, act_lr=1.2e-7, ent=1.75] Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=0.0443, ret=-0.000604, glen=117, tlen=277, kl=0.000939, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=0.102, ret=-0.000442, glen=107, tlen=267, kl=0.000919, act_lr=1.2e-7, ent=1.63] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=0.102, ret=-0.000442, glen=107, tlen=267, kl=0.000919, act_lr=1.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=0.00345, ret=-0.000125, glen=101, tlen=261, kl=0.000919, act_lr=1.2e-7, ent=1.92]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.00345, ret=-0.000125, glen=101, tlen=261, kl=0.000919, act_lr=1.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=-0.0453, ret=0.00017, glen=106, tlen=266, kl=0.000981, act_lr=1.2e-7, ent=1.55]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=-0.0453, ret=0.00017, glen=106, tlen=266, kl=0.000981, act_lr=1.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.0955, ret=0.000365, glen=99.6, tlen=260, kl=0.000933, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=-0.0955, ret=0.000365, glen=99.6, tlen=260, kl=0.000933, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.041, ret=0.000494, glen=132, tlen=292, kl=0.000901, act_lr=1.2e-7, ent=1.75]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.041, ret=0.000494, glen=132, tlen=292, kl=0.000901, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0201, ret=-0.000161, glen=107, tlen=267, kl=0.000984, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.0201, ret=-0.000161, glen=107, tlen=267, kl=0.000984, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0221, ret=0.000481, glen=104, tlen=265, kl=0.000927, act_lr=1.2e-7, ent=1.68] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0221, ret=0.000481, glen=104, tlen=265, kl=0.000927, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=-0.184, ret=0.000986, glen=105, tlen=265, kl=0.000944, act_lr=1.2e-7, ent=1.77] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=-0.184, ret=0.000986, glen=105, tlen=265, kl=0.000944, act_lr=1.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0591, ret=7.01e-5, glen=109, tlen=270, kl=0.000937, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0591, ret=7.01e-5, glen=109, tlen=270, kl=0.000937, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0573, ret=0.000754, glen=124, tlen=285, kl=0.000931, act_lr=1.2e-7, ent=2.08]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0573, ret=0.000754, glen=124, tlen=285, kl=0.000931, act_lr=1.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.116, ret=0.000816, glen=99.4, tlen=260, kl=0.000991, act_lr=1.2e-7, ent=1.6]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.116, ret=0.000816, glen=99.4, tlen=260, kl=0.000991, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=0.14, ret=-0.00184, glen=110, tlen=271, kl=0.000939, act_lr=1.2e-7, ent=1.68]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=0.14, ret=-0.00184, glen=110, tlen=271, kl=0.000939, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.05, ret=0.000554, glen=99.9, tlen=260, kl=0.000954, act_lr=1.2e-7, ent=1.71]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=-0.05, ret=0.000554, glen=99.9, tlen=260, kl=0.000954, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.129, ret=0.00148, glen=129, tlen=289, kl=0.000969, act_lr=1.2e-7, ent=1.93] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.129, ret=0.00148, glen=129, tlen=289, kl=0.000969, act_lr=1.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=-0.117, ret=0.00104, glen=98.2, tlen=259, kl=0.000943, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.117, ret=0.00104, glen=98.2, tlen=259, kl=0.000943, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.0033, ret=9.83e-5, glen=108, tlen=269, kl=0.00092, act_lr=1.2e-7, ent=1.76]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0033, ret=9.83e-5, glen=108, tlen=269, kl=0.00092, act_lr=1.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.135, ret=0.000641, glen=104, tlen=265, kl=0.00101, act_lr=1.2e-7, ent=1.59]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.16it/s, pg=-0.135, ret=0.000641, glen=104, tlen=265, kl=0.00101, act_lr=1.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.16it/s, pg=0.11, ret=-0.000124, glen=110, tlen=270, kl=0.000933, act_lr=1.2e-7, ent=1.82]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=0.11, ret=-0.000124, glen=110, tlen=270, kl=0.000933, act_lr=1.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=0.0835, ret=-0.0011, glen=101, tlen=262, kl=0.00097, act_lr=1.2e-7, ent=1.6]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=0.0835, ret=-0.0011, glen=101, tlen=262, kl=0.00097, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.173, ret=0.00124, glen=101, tlen=262, kl=0.000942, act_lr=1.2e-7, ent=1.59]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.173, ret=0.00124, glen=101, tlen=262, kl=0.000942, act_lr=1.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.173, ret=0.000238, glen=101, tlen=261, kl=0.000931, act_lr=1.2e-7, ent=1.73]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.173, ret=0.000238, glen=101, tlen=261, kl=0.000931, act_lr=1.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.00574, ret=0.000516, glen=120, tlen=281, kl=0.000922, act_lr=1.2e-7, ent=1.95]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.00574, ret=0.000516, glen=120, tlen=281, kl=0.000922, act_lr=1.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.105, ret=6.01e-5, glen=134, tlen=295, kl=0.00092, act_lr=1.2e-7, ent=1.94]    Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.105, ret=6.01e-5, glen=134, tlen=295, kl=0.00092, act_lr=1.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.205, ret=-0.00114, glen=117, tlen=278, kl=0.000961, act_lr=1.2e-7, ent=1.79]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.205, ret=-0.00114, glen=117, tlen=278, kl=0.000961, act_lr=1.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.115, ret=0.000597, glen=95.7, tlen=256, kl=0.000937, act_lr=1.2e-7, ent=1.72]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:24,  1.15it/s, pg=-0.115, ret=0.000597, glen=95.7, tlen=256, kl=0.000937, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:24,  1.15it/s, pg=-0.0523, ret=-0.000468, glen=110, tlen=271, kl=0.000978, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.06it/s, pg=-0.0523, ret=-0.000468, glen=110, tlen=271, kl=0.000978, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.06it/s, pg=0.0282, ret=-0.000835, glen=99.3, tlen=259, kl=0.000975, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.09it/s, pg=0.0282, ret=-0.000835, glen=99.3, tlen=259, kl=0.000975, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.09it/s, pg=-0.0337, ret=0.00046, glen=108, tlen=269, kl=0.00094, act_lr=1.2e-7, ent=1.71]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.09it/s, pg=-0.0337, ret=0.00046, glen=108, tlen=269, kl=0.00094, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.09it/s, pg=0.122, ret=-0.0013, glen=128, tlen=289, kl=0.00095, act_lr=1.2e-7, ent=2.01]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.12it/s, pg=0.122, ret=-0.0013, glen=128, tlen=289, kl=0.00095, act_lr=1.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=0.0417, ret=-0.000591, glen=112, tlen=273, kl=0.000932, act_lr=1.2e-7, ent=1.78]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.13it/s, pg=0.0417, ret=-0.000591, glen=112, tlen=273, kl=0.000932, act_lr=1.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.13it/s, pg=0.0226, ret=-6.05e-5, glen=104, tlen=264, kl=0.000957, act_lr=1.2e-7, ent=1.69] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.14it/s, pg=0.0226, ret=-6.05e-5, glen=104, tlen=264, kl=0.000957, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.14it/s, pg=0.0665, ret=-0.00216, glen=110, tlen=270, kl=0.000896, act_lr=1.2e-7, ent=1.61]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.15it/s, pg=0.0665, ret=-0.00216, glen=110, tlen=270, kl=0.000896, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.15it/s, pg=0.175, ret=-0.000352, glen=123, tlen=283, kl=0.00091, act_lr=1.2e-7, ent=1.54] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.175, ret=-0.000352, glen=123, tlen=283, kl=0.00091, act_lr=1.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.103, ret=0.000979, glen=102, tlen=262, kl=0.000901, act_lr=1.2e-7, ent=1.64]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.103, ret=0.000979, glen=102, tlen=262, kl=0.000901, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.0288, ret=0.00044, glen=104, tlen=264, kl=0.000969, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.0288, ret=0.00044, glen=104, tlen=264, kl=0.000969, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.277, ret=-0.000879, glen=118, tlen=279, kl=0.000906, act_lr=1.2e-7, ent=2.15]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.277, ret=-0.000879, glen=118, tlen=279, kl=0.000906, act_lr=1.2e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0346, ret=0.000369, glen=104, tlen=265, kl=0.000946, act_lr=1.2e-7, ent=1.62]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0346, ret=0.000369, glen=104, tlen=265, kl=0.000946, act_lr=1.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0407, ret=0.00145, glen=103, tlen=263, kl=0.000937, act_lr=1.2e-7, ent=1.61] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.0407, ret=0.00145, glen=103, tlen=263, kl=0.000937, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.0691, ret=1.65e-5, glen=105, tlen=265, kl=0.000992, act_lr=1.2e-7, ent=1.66] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.0691, ret=1.65e-5, glen=105, tlen=265, kl=0.000992, act_lr=1.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.215, ret=0.00128, glen=114, tlen=274, kl=0.000937, act_lr=1.2e-7, ent=1.64]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.18it/s, pg=-0.215, ret=0.00128, glen=114, tlen=274, kl=0.000937, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.18it/s, pg=0.158, ret=-0.0014, glen=125, tlen=285, kl=0.000907, act_lr=1.2e-7, ent=1.84] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.18it/s, pg=0.158, ret=-0.0014, glen=125, tlen=285, kl=0.000907, act_lr=1.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.18it/s, pg=-0.0244, ret=-0.000285, glen=101, tlen=261, kl=0.000997, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.18it/s, pg=-0.0244, ret=-0.000285, glen=101, tlen=261, kl=0.000997, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.101, ret=-0.000606, glen=109, tlen=270, kl=0.000928, act_lr=1.2e-7, ent=1.69] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.18it/s, pg=-0.101, ret=-0.000606, glen=109, tlen=270, kl=0.000928, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.18it/s, pg=-0.025, ret=0.00108, glen=101, tlen=261, kl=0.000947, act_lr=1.2e-7, ent=1.64]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.18it/s, pg=-0.025, ret=0.00108, glen=101, tlen=261, kl=0.000947, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.18it/s, pg=-0.0969, ret=0.00216, glen=98.2, tlen=259, kl=0.000931, act_lr=1.2e-7, ent=1.8]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=-0.0969, ret=0.00216, glen=98.2, tlen=259, kl=0.000931, act_lr=1.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.138, ret=0.000337, glen=114, tlen=274, kl=0.000929, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.138, ret=0.000337, glen=114, tlen=274, kl=0.000929, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=0.0835, ret=-0.00118, glen=117, tlen=277, kl=0.000963, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.18it/s, pg=0.0835, ret=-0.00118, glen=117, tlen=277, kl=0.000963, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.18it/s, pg=-0.0207, ret=0.000118, glen=108, tlen=268, kl=0.000925, act_lr=1.2e-7, ent=1.61]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.0207, ret=0.000118, glen=108, tlen=268, kl=0.000925, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=0.123, ret=-0.000666, glen=109, tlen=269, kl=0.000966, act_lr=1.2e-7, ent=1.71] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.18it/s, pg=0.123, ret=-0.000666, glen=109, tlen=269, kl=0.000966, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.18it/s, pg=-0.0159, ret=-0.00061, glen=115, tlen=276, kl=0.000943, act_lr=1.2e-7, ent=1.78]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.0159, ret=-0.00061, glen=115, tlen=276, kl=0.000943, act_lr=1.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0294, ret=0.000539, glen=99.7, tlen=260, kl=0.000939, act_lr=1.2e-7, ent=1.55]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0294, ret=0.000539, glen=99.7, tlen=260, kl=0.000939, act_lr=1.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0861, ret=-0.00142, glen=118, tlen=278, kl=0.000899, act_lr=1.2e-7, ent=1.7]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.0861, ret=-0.00142, glen=118, tlen=278, kl=0.000899, act_lr=1.2e-7, ent=1.7]
2025-07-24 16:31:25.535 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.45s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.0397, ret=-0.000993, glen=106, tlen=267, kl=0.000998, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.0397, ret=-0.000993, glen=106, tlen=267, kl=0.000998, act_lr=1.4e-7, ent=1.82]
2025-07-24 16:31:26.394 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 16:31:28.969 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 16:31:29.294 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.33s
2025-07-24 16:31:29.301 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.006455455507550921, 'actor_lr': 1.203571386046828e-07, 'clip_ratio': 0.0, 'entropy': 1.7342252433300018, 'kl': 0.0009432690484183175, 'response_length': 109.31775610787528, 'total_length': 269.7792137690953, 'teacher_total_length': 281.8311091831752, 'return': -4.152664457381304e-06, 'policy_update_steps': 1.0}
Episode [1/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [25:37<21:22, 213.73s/it]2025-07-24 16:31:29.328 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:33:05.119 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:33:05.300 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:33:05.301 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 95.97s
2025-07-24 16:33:07.358 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0112,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6497,std_num_tokens: 116.1839,avg_correct_num_tokens: 105.9321,std_correct_num_tokens: 86.2368,avg_incorrect_num_tokens: 128.7628,std_incorrect_num_tokens: 151.6254
2025-07-24 16:33:07.688 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.39s
2025-07-24 16:33:10.874 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 16:33:39.737 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 228
2025-07-24 16:33:39.738 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.86s
2025-07-24 16:33:41.246 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.07s
2025-07-24 16:33:41.246 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008591915036065569, avg_kl: 0.0009288536874871505, avg_response_length: 116.01782658225612, avg_orm_score: 0.0, avg_custom_rewards: -0.0008591915036065569
2025-07-24 16:33:41.288 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter7_replay_buffer.jsonl
2025-07-24 16:33:43.242 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.96s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.0303, ret=-0.000445, glen=108, tlen=269, kl=0.000888, act_lr=1.4e-7, ent=1.61]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.0303, ret=-0.000445, glen=108, tlen=269, kl=0.000888, act_lr=1.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.156, ret=-0.000947, glen=115, tlen=275, kl=0.000926, act_lr=1.4e-7, ent=1.67]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.156, ret=-0.000947, glen=115, tlen=275, kl=0.000926, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.00891, ret=-3.33e-5, glen=106, tlen=266, kl=0.000901, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=0.00891, ret=-3.33e-5, glen=106, tlen=266, kl=0.000901, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.209, ret=0.0018, glen=109, tlen=269, kl=0.000939, act_lr=1.4e-7, ent=1.79]   Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.209, ret=0.0018, glen=109, tlen=269, kl=0.000939, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=0.0459, ret=-0.00124, glen=108, tlen=269, kl=0.000956, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.12it/s, pg=0.0459, ret=-0.00124, glen=108, tlen=269, kl=0.000956, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.12it/s, pg=0.0549, ret=-0.00106, glen=119, tlen=279, kl=0.000936, act_lr=1.4e-7, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.13it/s, pg=0.0549, ret=-0.00106, glen=119, tlen=279, kl=0.000936, act_lr=1.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.13it/s, pg=0.0494, ret=-0.00189, glen=124, tlen=285, kl=0.000936, act_lr=1.4e-7, ent=1.8] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=0.0494, ret=-0.00189, glen=124, tlen=285, kl=0.000936, act_lr=1.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=0.0083, ret=-0.0017, glen=126, tlen=287, kl=0.000961, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=0.0083, ret=-0.0017, glen=126, tlen=287, kl=0.000961, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=-0.00143, ret=0.000171, glen=107, tlen=267, kl=0.000928, act_lr=1.4e-7, ent=1.58]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.16it/s, pg=-0.00143, ret=0.000171, glen=107, tlen=267, kl=0.000928, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.16it/s, pg=-0.0471, ret=0.000764, glen=114, tlen=274, kl=0.000972, act_lr=1.4e-7, ent=1.78] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.0471, ret=0.000764, glen=114, tlen=274, kl=0.000972, act_lr=1.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.0348, ret=0.000309, glen=107, tlen=268, kl=0.000876, act_lr=1.4e-7, ent=1.65] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.0348, ret=0.000309, glen=107, tlen=268, kl=0.000876, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=0.0833, ret=-0.00116, glen=119, tlen=279, kl=0.000929, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=0.0833, ret=-0.00116, glen=119, tlen=279, kl=0.000929, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.152, ret=0.00148, glen=117, tlen=277, kl=0.00094, act_lr=1.4e-7, ent=1.74]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=-0.152, ret=0.00148, glen=117, tlen=277, kl=0.00094, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=0.209, ret=-0.00144, glen=142, tlen=302, kl=0.000803, act_lr=1.4e-7, ent=1.46]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=0.209, ret=-0.00144, glen=142, tlen=302, kl=0.000803, act_lr=1.4e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0901, ret=-0.00105, glen=126, tlen=286, kl=0.00094, act_lr=1.4e-7, ent=1.84]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=0.0901, ret=-0.00105, glen=126, tlen=286, kl=0.00094, act_lr=1.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=-0.0391, ret=-0.000464, glen=104, tlen=264, kl=0.000998, act_lr=1.4e-7, ent=1.7]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0391, ret=-0.000464, glen=104, tlen=264, kl=0.000998, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.1, ret=0.000556, glen=120, tlen=280, kl=0.000887, act_lr=1.4e-7, ent=1.77]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.1, ret=0.000556, glen=120, tlen=280, kl=0.000887, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.175, ret=0.00107, glen=104, tlen=265, kl=0.000903, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.175, ret=0.00107, glen=104, tlen=265, kl=0.000903, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.156, ret=0.000737, glen=113, tlen=274, kl=0.00091, act_lr=1.4e-7, ent=1.6] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.156, ret=0.000737, glen=113, tlen=274, kl=0.00091, act_lr=1.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.22, ret=-0.00184, glen=136, tlen=296, kl=0.000958, act_lr=1.4e-7, ent=2.18]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.22, ret=-0.00184, glen=136, tlen=296, kl=0.000958, act_lr=1.4e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.124, ret=0.00102, glen=121, tlen=282, kl=0.000892, act_lr=1.4e-7, ent=1.89]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.124, ret=0.00102, glen=121, tlen=282, kl=0.000892, act_lr=1.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.00397, ret=-0.00198, glen=112, tlen=272, kl=0.000965, act_lr=1.4e-7, ent=1.64]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.00397, ret=-0.00198, glen=112, tlen=272, kl=0.000965, act_lr=1.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0125, ret=-0.000189, glen=117, tlen=278, kl=0.000898, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.17it/s, pg=0.0125, ret=-0.000189, glen=117, tlen=278, kl=0.000898, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.17it/s, pg=-0.171, ret=0.000803, glen=116, tlen=276, kl=0.000919, act_lr=1.4e-7, ent=1.77] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.171, ret=0.000803, glen=116, tlen=276, kl=0.000919, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.241, ret=-0.00267, glen=115, tlen=275, kl=0.00094, act_lr=1.4e-7, ent=1.69]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.241, ret=-0.00267, glen=115, tlen=275, kl=0.00094, act_lr=1.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.0515, ret=-5.73e-5, glen=112, tlen=272, kl=0.000903, act_lr=1.4e-7, ent=1.63]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.0515, ret=-5.73e-5, glen=112, tlen=272, kl=0.000903, act_lr=1.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.071, ret=-0.000173, glen=114, tlen=274, kl=0.000935, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=-0.071, ret=-0.000173, glen=114, tlen=274, kl=0.000935, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=0.0312, ret=-0.000752, glen=123, tlen=283, kl=0.000926, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0312, ret=-0.000752, glen=123, tlen=283, kl=0.000926, act_lr=1.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0782, ret=0.000848, glen=106, tlen=267, kl=0.000965, act_lr=1.4e-7, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=-0.0782, ret=0.000848, glen=106, tlen=267, kl=0.000965, act_lr=1.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=0.0155, ret=-0.000318, glen=110, tlen=270, kl=0.000912, act_lr=1.4e-7, ent=1.62]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=0.0155, ret=-0.000318, glen=110, tlen=270, kl=0.000912, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.0761, ret=-0.000297, glen=118, tlen=278, kl=0.000931, act_lr=1.4e-7, ent=1.64]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0761, ret=-0.000297, glen=118, tlen=278, kl=0.000931, act_lr=1.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.162, ret=-0.00244, glen=114, tlen=274, kl=0.000969, act_lr=1.4e-7, ent=1.62]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.14it/s, pg=0.162, ret=-0.00244, glen=114, tlen=274, kl=0.000969, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.14it/s, pg=-0.00757, ret=-0.000327, glen=125, tlen=286, kl=0.000869, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=-0.00757, ret=-0.000327, glen=125, tlen=286, kl=0.000869, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=0.212, ret=-0.00124, glen=109, tlen=270, kl=0.000921, act_lr=1.4e-7, ent=1.72]    Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=0.212, ret=-0.00124, glen=109, tlen=270, kl=0.000921, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=-0.09, ret=0.00143, glen=117, tlen=277, kl=0.000926, act_lr=1.4e-7, ent=1.74] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.09, ret=0.00143, glen=117, tlen=277, kl=0.000926, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.131, ret=5.03e-5, glen=147, tlen=308, kl=0.000883, act_lr=1.4e-7, ent=2.02]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=0.131, ret=5.03e-5, glen=147, tlen=308, kl=0.000883, act_lr=1.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.179, ret=0.00152, glen=106, tlen=266, kl=0.00092, act_lr=1.4e-7, ent=1.67]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.179, ret=0.00152, glen=106, tlen=266, kl=0.00092, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.000244, ret=8.56e-5, glen=113, tlen=274, kl=0.000953, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:32<00:16,  1.17it/s, pg=-0.000244, ret=8.56e-5, glen=113, tlen=274, kl=0.000953, act_lr=1.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.149, ret=0.0015, glen=101, tlen=261, kl=0.000969, act_lr=1.4e-7, ent=1.58]    Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.149, ret=0.0015, glen=101, tlen=261, kl=0.000969, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.22, ret=0.00216, glen=124, tlen=283, kl=0.000974, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.22, ret=0.00216, glen=124, tlen=283, kl=0.000974, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0248, ret=0.000524, glen=118, tlen=279, kl=0.000932, act_lr=1.4e-7, ent=1.71]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0248, ret=0.000524, glen=118, tlen=279, kl=0.000932, act_lr=1.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0811, ret=0.000216, glen=124, tlen=285, kl=0.000938, act_lr=1.4e-7, ent=1.58]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0811, ret=0.000216, glen=124, tlen=285, kl=0.000938, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.084, ret=0.000508, glen=105, tlen=266, kl=0.000934, act_lr=1.4e-7, ent=1.7] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.084, ret=0.000508, glen=105, tlen=266, kl=0.000934, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.201, ret=0.00257, glen=104, tlen=264, kl=0.000941, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.201, ret=0.00257, glen=104, tlen=264, kl=0.000941, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.161, ret=-0.00166, glen=114, tlen=275, kl=0.000942, act_lr=1.4e-7, ent=1.75]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=0.161, ret=-0.00166, glen=114, tlen=275, kl=0.000942, act_lr=1.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=0.017, ret=-0.000343, glen=118, tlen=279, kl=0.000933, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=0.017, ret=-0.000343, glen=118, tlen=279, kl=0.000933, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.105, ret=-0.0001, glen=131, tlen=292, kl=0.000915, act_lr=1.4e-7, ent=1.76]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.105, ret=-0.0001, glen=131, tlen=292, kl=0.000915, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.0308, ret=0.000512, glen=113, tlen=274, kl=0.000984, act_lr=1.4e-7, ent=1.83]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.0308, ret=0.000512, glen=113, tlen=274, kl=0.000984, act_lr=1.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.318, ret=0.00119, glen=98.4, tlen=259, kl=0.000909, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.318, ret=0.00119, glen=98.4, tlen=259, kl=0.000909, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=0.162, ret=-0.00163, glen=125, tlen=286, kl=0.000945, act_lr=1.4e-7, ent=2.02] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.18it/s, pg=0.162, ret=-0.00163, glen=125, tlen=286, kl=0.000945, act_lr=1.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.18it/s, pg=-0.0259, ret=0.00063, glen=115, tlen=275, kl=0.000906, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.0259, ret=0.00063, glen=115, tlen=275, kl=0.000906, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.106, ret=-0.000578, glen=122, tlen=282, kl=0.000878, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.17it/s, pg=0.106, ret=-0.000578, glen=122, tlen=282, kl=0.000878, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0309, ret=-0.000328, glen=104, tlen=264, kl=0.000916, act_lr=1.4e-7, ent=1.51]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0309, ret=-0.000328, glen=104, tlen=264, kl=0.000916, act_lr=1.4e-7, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.046, ret=-0.000683, glen=110, tlen=271, kl=0.000943, act_lr=1.4e-7, ent=1.79]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.15it/s, pg=0.046, ret=-0.000683, glen=110, tlen=271, kl=0.000943, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.15it/s, pg=-0.169, ret=0.000447, glen=126, tlen=286, kl=0.000958, act_lr=1.4e-7, ent=1.87]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.16it/s, pg=-0.169, ret=0.000447, glen=126, tlen=286, kl=0.000958, act_lr=1.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.16it/s, pg=-0.0964, ret=0.000802, glen=107, tlen=268, kl=0.000969, act_lr=1.4e-7, ent=1.74]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.16it/s, pg=-0.0964, ret=0.000802, glen=107, tlen=268, kl=0.000969, act_lr=1.4e-7, ent=1.74]
2025-07-24 16:34:32.772 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.34s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.16it/s, pg=-0.107, ret=0.00246, glen=134, tlen=294, kl=0.000942, act_lr=1.6e-7, ent=1.87]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=-0.107, ret=0.00246, glen=134, tlen=294, kl=0.000942, act_lr=1.6e-7, ent=1.87]
2025-07-24 16:34:33.586 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.74s
2025-07-24 16:34:36.209 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.62s
2025-07-24 16:34:36.542 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.24s
2025-07-24 16:34:36.549 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.006998446949741296, 'actor_lr': 1.403508745948114e-07, 'clip_ratio': 0.0, 'entropy': 1.7287561015078896, 'kl': 0.0009288536874871505, 'response_length': 116.01782654879386, 'total_length': 276.4504688999109, 'teacher_total_length': 288.38180488452576, 'return': -5.060130525523339e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [28:44<17:06, 205.30s/it]2025-07-24 16:34:36.594 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:37:20.213 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:37:20.385 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:37:20.386 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 163.79s
2025-07-24 16:37:22.438 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 117.0878,std_num_tokens: 178.9064,avg_correct_num_tokens: 106.5291,std_correct_num_tokens: 138.5686,avg_incorrect_num_tokens: 132.9484,std_incorrect_num_tokens: 225.4444
2025-07-24 16:37:22.886 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.50s
2025-07-24 16:37:26.296 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.41s

[2025-07-24 16:04:20,314] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-07-24 16:04:25.876 | INFO     | __main__:<module>:136 - --------- config key ---------        ------ value ------
seed                                  42
ref_num_nodes                         4
ref_num_gpus_per_node                 1
reward_num_nodes                      1
reward_num_gpus_per_node              2
actor_num_nodes                       4
actor_num_gpus_per_node               1
critic_num_nodes                      4
critic_num_gpus_per_node              1
colocate_critic_reward                True
colocate_actor_ref                    True
colocate_all                          True
vllm_num_engines                      4
vllm_tensor_parallel_size             1
vllm_sync_backend                     nccl
local_rank                            -1
pretrain                              /home/a/anokhin/links/scratch/Qwen2.5-1.5B
critic_pretrain
reward_pretrain                       <class 'NoneType'>
ckpt_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
save_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
tensorboard_log_dir                   /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
prompt_data                           <class 'omegaconf.listconfig.ListConfig'>
load_checkpoint                       False
zero_stage                            3
bf16                                  True
zpg                                   1
adam_offload                          False
flash_attn                            True
grad_accum_dtype                      <class 'NoneType'>
disable_trace_cache                   False
gradient_checkpointing                True
gradient_checkpointing_use_reentrant  False
disable_fast_tokenizer                False
target_modules                        all-linear
enable_prefix_caching                 True
enable_chunked_prefill                False
max_num_batched_tokens                2048
enforce_eager                         False
gpu_memory_utilization                0.25
eval_steps                            -1
save_steps                            -1
save_interval                         50
actor_learning_rate                   1e-06
critic_learning_rate                  5e-06
num_episodes                          20
max_epochs                            1
prompt_max_len                        2048
generate_max_len                      8000
train_batch_size                      256
micro_train_batch_size                1
rollout_batch_size                    128
micro_rollout_batch_size              128
micro_forward_batch_size              1
policy_update_steps                   1
critic_update_steps                   12
max_len                               8192
max_norm                              1.0
num_warmup_steps                      50
l2                                    0.0
eps_clip                              0.2
value_clip                            0.2
lambd                                 1.0
gamma                                 1.0
normalize_reward                      True
top_p                                 1.0
temperature                           1.0
freezing_actor_steps                  -1
n_samples_per_prompt                  64
kl_target                             <class 'NoneType'>
init_kl_coef                          0
use_kl_estimator_k3                   True
use_abs_kl                            False
use_kl_loss                           True
kl_loss_coef                          0.0
adam_betas                            (0.9, 0.95)
reward_clip_range                     (-10, 10)
use_compute_reward_fn                 True
advantage_normalize                   True
value_head_prefix                     value_head
ref_reward_offload                    False
enable_eval                           True
eval_interval                         10
update_ref_every_epoch                True
use_orm_score                         False
total_num_nodes                       4
exp_name                              binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
eval_prompt_data                      <class 'omegaconf.listconfig.ListConfig'>
prompt_data_probs                     <class 'omegaconf.listconfig.ListConfig'>
packing_max_len                       10048
top_k                                 -1
stop                                  <class 'omegaconf.listconfig.ListConfig'>
use_grpo                              True
wandb: Currently logged in as: avecplezir (irina-rish). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.21.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/wandb/run-20250724_160427-8405gkw1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/irina-rish/open-reasoner-zero
wandb: üöÄ View run at https://wandb.ai/irina-rish/open-reasoner-zero/runs/8405gkw1
2025-07-24 16:04:30,542	INFO worker.py:1841 -- Started a local Ray instance.
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it]
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it]
[36m(LLMActor pid=1435060)[0m 
[36m(LLMActor pid=1435061)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m 
[36m(LLMActor pid=1435062)[0m 
[36m(LLMActor pid=1435061)[0m 
2025-07-24 16:05:08.022 | INFO     | orz.ppo.utils:create_vllm_engines:452 - Offloaded all vLLM engines to CPU
2025-07-24 16:05:08.244 | INFO     | playground.orz_7b_ppo:train_dataset:580 - Start processing 1603 dialogues
2025-07-24 16:05:10.312 | INFO     | playground.orz_7b_ppo:train_dataset:589 - Finished processing 1603 dialogues
2025-07-24 16:05:10.316 | INFO     | playground.orz_7b_ppo:eval_dataset:603 - Start processing 687 dialogues
2025-07-24 16:05:10.753 | INFO     | playground.orz_7b_ppo:eval_dataset:612 - Finished processing 687 dialogues
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
[36m(RefRayActorBase pid=1436501)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(LLMActor pid=1435061)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it][32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 4x across cluster][0m
[36m(PolicyRayActorBase pid=1435816)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...
[36m(PolicyRayActorBase pid=1435815)[0m Detected CUDA files, patching ldflags
[36m(PolicyRayActorBase pid=1435815)[0m Emitting ninja build file /home/a/anokhin/.cache/torch_extensions/py312_cu122/fused_adam/build.ninja...
[36m(PolicyRayActorBase pid=1435815)[0m /home/a/anokhin/envs/reasoner3.12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(PolicyRayActorBase pid=1435815)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(PolicyRayActorBase pid=1435815)[0m   warnings.warn(
[36m(PolicyRayActorBase pid=1435815)[0m Building extension module fused_adam...
[36m(PolicyRayActorBase pid=1435815)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(PolicyRayActorBase pid=1435815)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435815)[0m Loading extension module fused_adam...
2025-07-24 16:05:48.652 | INFO     | orz.ppo.trainer:build_models:815 - init policy/ref/critic/reward models done
2025-07-24 16:05:49.316 | INFO     | orz.ppo.trainer:train:74 - Create vllm engine gourps done.
2025-07-24 16:05:51.913 | INFO     | orz.ppo.trainer:train:76 - Sync actor weights to vllm engines, time cost: 2.60s
2025-07-24 16:05:52.208 | INFO     | orz.ppo.trainer:train:80 - Offload policy model to cpu, time cost: 0.29s
math_verify is not installed in this environment
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:38 __init__.py:207] Automatically detected platform cuda.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435061)[0m WARNING 07-24 16:04:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=1435061)[0m WARNING 07-24 16:04:54 config.py:685] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=44, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:39 __init__.py:207] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(LLMActor pid=1435060)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435062)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:55 cuda.py:229] Using Flash Attention backend.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:56 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...
[36m(LLMActor pid=1435062)[0m INFO 07-24 16:05:04 model_runner.py:1115] Loading model weights took 2.9105 GB
[36m(LLMActor pid=1435059)[0m WARNING 07-24 16:04:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m WARNING 07-24 16:04:54 config.py:685] Async output processing is not supported on the current platform type cuda.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=43, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, [32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:55 cuda.py:229] Using Flash Attention backend.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:56 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] Memory profiling takes 0.51 seconds
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:05 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:05 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:07 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.60 seconds
[36m(pid=1435640)[0m [2025-07-24 16:05:13,702] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 model_runner.py:1115] Loading model weights took 2.9105 GB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] Memory profiling takes 0.49 seconds[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:05 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:05 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:07 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.58 seconds[32m [repeated 3x across cluster][0m
[36m(pid=1435816)[0m [2025-07-24 16:05:21,478] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=1435818)[0m [2025-07-24 16:05:21,675] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:25,874] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:25,874] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(pid=1436500)[0m [2025-07-24 16:05:29,104] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 3x across cluster][0m
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:33,170] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:33,280] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 16:05:33,199] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:36,755] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(pid=1436499)[0m [2025-07-24 16:05:29,310] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 2x across cluster][0m
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:38,261] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 4x across cluster][0m
[36m(RefRayActorBase pid=1436499)[0m [2025-07-24 16:05:33,311] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,358] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,359] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,367] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,368] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,568] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,569] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.71 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,569] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.38 GB, percent = 10.4%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,709] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,710] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,710] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.38 GB, percent = 10.4%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,711] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7e4375e80>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:38,832] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:38,833] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:42,327] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(PolicyRayActorBase pid=1435815)[0m ninja: no work to do.
[36m(PolicyRayActorBase pid=1435815)[0m Time to load fused_adam op: 1.1178452968597412 seconds
[36m(PolicyRayActorBase pid=1435815)[0m [2025-07-24 16:05:44,887] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435815)[0m [2025-07-24 16:05:38,841] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,958] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,959] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,981] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,984] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,984] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,242] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,243] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.6 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,243] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,244] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,244] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,417] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,418] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,418] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,567] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,567] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,568] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,697] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,697] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,698] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.73 GB         CA 0.72 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.72 GB         Max_CA 1 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.88 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,922] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,923] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,923] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,054] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 3.59 GB         CA 4.32 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,286] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,286] [INFO] [utils.py:782:see_memory_usage] MA 4.53 GB         Max_MA 5.39 GB         CA 5.76 GB         Max_CA 6 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.46 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7e72ecb7a690>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(PolicyRayActorBase pid=1435640)[0m     "partition_activations": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "contiguous_memory_optimization": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "cpu_checkpointing": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "number_checkpoints": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "synchronize_checkpoint_boundary": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "profile": false
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "start_step": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "end_step": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "metric_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "arg_mappings": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "metric": "throughput", 
[36m(PolicyRayActorBase pid=1435640)[0m     "model_info": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "results_dir": "autotuning_results", 
[36m(PolicyRayActorBase pid=1435640)[0m     "exps_dir": "autotuning_exps", 
[36m(PolicyRayActorBase pid=1435640)[0m     "overwrite": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "fast": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "start_profile_step": 3, 
[36m(PolicyRayActorBase pid=1435640)[0m     "end_profile_step": 5, 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_type": "gridsearch", 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_early_stopping": 5, 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_num_trials": 50, 
[36m(PolicyRayActorBase pid=1435640)[0m     "model_info_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "mp_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "max_train_batch_size": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "min_train_batch_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(PolicyRayActorBase pid=1435640)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "num_tuning_micro_batch_sizes": 3
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7e72ecb79a90>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "recompute_fwd_factor": 0.0, 
[36m(PolicyRayActorBase pid=1435640)[0m     "profile_step": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "module_depth": -1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "top_modules": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "detailed": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "output_file": null
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "persistent_storage_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "persistent_time_interval": 100, 
[36m(PolicyRayActorBase pid=1435640)[0m     "num_of_version_in_retention": 2, 
[36m(PolicyRayActorBase pid=1435640)[0m     "enable_nebula_load": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "load_path": null
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:989:print_user_config]   json = {
[36m(PolicyRayActorBase pid=1435640)[0m     "steps_per_print": 100, 
[36m(PolicyRayActorBase pid=1435640)[0m     "zero_optimization": {
[36m(PolicyRayActorBase pid=1435640)[0m         "stage": 3, 
[36m(PolicyRayActorBase pid=1435640)[0m         "offload_param": {
[36m(PolicyRayActorBase pid=1435640)[0m             "device": "none"
[36m(PolicyRayActorBase pid=1435640)[0m         }, 
[36m(PolicyRayActorBase pid=1435640)[0m         "offload_optimizer": {
[36m(PolicyRayActorBase pid=1435640)[0m             "device": "none", 
[36m(PolicyRayActorBase pid=1435640)[0m             "pin_memory": true
[36m(PolicyRayActorBase pid=1435640)[0m         }, 
[36m(PolicyRayActorBase pid=1435640)[0m         "sub_group_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_max_live_parameters": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_max_reuse_distance": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "reduce_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_hpz_partition_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_quantized_weights": false, 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_quantized_gradients": false
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "bf16": {
[36m(PolicyRayActorBase pid=1435640)[0m         "enabled": true
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "gradient_clipping": 1.0, 
[36m(PolicyRayActorBase pid=1435640)[0m     "prescale_gradients": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "wall_clock_breakdown": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "data_types": {
[36m(PolicyRayActorBase pid=1435640)[0m         "grad_accum_dtype": "fp32"
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(LLMActor pid=1435061)[0m init_process_group: master_address=10.224.3.58, master_port=41739,  rank=3, world_size=5, group_name=openrlhf
[36m(PolicyRayActorBase pid=1435640)[0m WARNING:using --vllm_sync_backend=gloo for vLLM version > 0.4.2 (or export NCCL_P2P_DISABLE=1)
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435818)[0m Time to load fused_adam op: 1.216843605041504 seconds[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435818)[0m [2025-07-24 16:05:44,986] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 3x across cluster][0m
Episode [1/20]:   0%|          | 0/13 [00:00<?, ?it/s]2025-07-24 16:05:52.312 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(PolicyRayActorBase pid=1435640)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435818)[0m Loading extension module fused_adam...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<00:59,  2.87it/s, est. speed input: 512.95 toks/s, output: 20.06 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 2/172 [00:00<00:36,  4.64it/s, est. speed input: 767.99 toks/s, output: 40.31 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:04<00:00, 20.63it/s, est. speed input: 6303.86 toks/s, output: 2760.14 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:05<00:01, 12.39it/s, est. speed input: 5149.97 toks/s, output: 2432.96 toks/s][32m [repeated 110x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00,  1.88it/s, est. speed input: 3136.79 toks/s, output: 1923.58 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00, 17.30it/s, est. speed input: 3136.79 toks/s, output: 1923.58 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:08<00:00,  2.74it/s, est. speed input: 3468.63 toks/s, output: 1831.23 toks/s][32m [repeated 30x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:05<00:01,  8.87it/s, est. speed input: 5068.50 toks/s, output: 2273.45 toks/s]
2025-07-24 16:06:07.922 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 494.2853,strategyqa_test/accuracy: 0.3115,eval_accuracy: 0.3115
2025-07-24 16:06:08.418 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:07:55.866 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:07:56.045 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:07:56.046 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 107.63s
2025-07-24 16:08:14.553 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0094,avg_pass_at_n: 1.0000,avg_num_tokens: 114.4545,std_num_tokens: 132.9845,avg_correct_num_tokens: 105.4537,std_correct_num_tokens: 84.9028,avg_incorrect_num_tokens: 129.5815,std_incorrect_num_tokens: 186.8873
2025-07-24 16:08:14.903 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 18.86s
2025-07-24 16:08:18.212 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.31s
2025-07-24 16:08:47.873 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:08:47.874 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.66s
2025-07-24 16:08:49.264 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 16:08:49.265 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0006267745350284185, avg_kl: 0.0, avg_response_length: 116.57365384164335, avg_orm_score: 0.0, avg_custom_rewards: 0.0006267745350284185
2025-07-24 16:08:49.328 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter0_replay_buffer.jsonl
2025-07-24 16:08:51.234 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00,  1.23s/it, est. speed input: 2194.44 toks/s, output: 1255.84 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00, 12.11it/s, est. speed input: 2194.44 toks/s, output: 1255.84 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:12<00:00,  1.09it/s, est. speed input: 2445.90 toks/s, output: 1474.87 toks/s][32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.14, ret=-0.00164, glen=103, tlen=264, kl=0, act_lr=0, ent=1.73]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<01:24,  1.49s/it, pg=0.14, ret=-0.00164, glen=103, tlen=264, kl=0, act_lr=0, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:02<01:24,  1.49s/it, pg=0.057, ret=-0.000267, glen=104, tlen=264, kl=0, act_lr=0, ent=1.67]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<01:09,  1.24s/it, pg=0.057, ret=-0.000267, glen=104, tlen=264, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:04<01:09,  1.24s/it, pg=0.069, ret=-0.001, glen=139, tlen=299, kl=0, act_lr=0, ent=1.95]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:04<01:15,  1.37s/it, pg=0.069, ret=-0.001, glen=139, tlen=299, kl=0, act_lr=0, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:04<01:15,  1.37s/it, pg=0.287, ret=-0.000979, glen=126, tlen=287, kl=0, act_lr=0, ent=1.76]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<01:03,  1.17s/it, pg=0.287, ret=-0.000979, glen=126, tlen=287, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:05<01:03,  1.17s/it, pg=-0.055, ret=-0.000206, glen=105, tlen=265, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:56,  1.07s/it, pg=-0.055, ret=-0.000206, glen=105, tlen=265, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:06<00:56,  1.07s/it, pg=-0.107, ret=-0.000409, glen=115, tlen=275, kl=0, act_lr=0, ent=1.63]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:52,  1.01s/it, pg=-0.107, ret=-0.000409, glen=115, tlen=275, kl=0, act_lr=0, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:07<00:52,  1.01s/it, pg=-0.0383, ret=-0.000243, glen=112, tlen=272, kl=0, act_lr=0, ent=1.64]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:49,  1.04it/s, pg=-0.0383, ret=-0.000243, glen=112, tlen=272, kl=0, act_lr=0, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:08<00:49,  1.04it/s, pg=-0.0483, ret=0.00091, glen=104, tlen=264, kl=0, act_lr=0, ent=1.78]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:46,  1.08it/s, pg=-0.0483, ret=0.00091, glen=104, tlen=264, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:09<00:46,  1.08it/s, pg=-0.0175, ret=-0.000894, glen=106, tlen=266, kl=0, act_lr=0, ent=1.59]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:09<00:44,  1.11it/s, pg=-0.0175, ret=-0.000894, glen=106, tlen=266, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:10<00:44,  1.11it/s, pg=0.0853, ret=-0.000294, glen=117, tlen=277, kl=0, act_lr=0, ent=1.73] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:10<00:43,  1.10it/s, pg=0.0853, ret=-0.000294, glen=117, tlen=277, kl=0, act_lr=0, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:11<00:43,  1.10it/s, pg=0.118, ret=-0.000719, glen=116, tlen=276, kl=0, act_lr=0, ent=1.75] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:11<00:41,  1.12it/s, pg=0.118, ret=-0.000719, glen=116, tlen=276, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:11<00:41,  1.12it/s, pg=0.00751, ret=0.000226, glen=112, tlen=273, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.14it/s, pg=0.00751, ret=0.000226, glen=112, tlen=273, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:12<00:40,  1.14it/s, pg=0.126, ret=-0.00022, glen=108, tlen=268, kl=0, act_lr=0, ent=1.69]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:39,  1.15it/s, pg=0.126, ret=-0.00022, glen=108, tlen=268, kl=0, act_lr=0, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:13<00:39,  1.15it/s, pg=-0.179, ret=0.00049, glen=98.3, tlen=259, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.16it/s, pg=-0.179, ret=0.00049, glen=98.3, tlen=259, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:14<00:37,  1.16it/s, pg=-0.0616, ret=3.93e-5, glen=109, tlen=269, kl=0, act_lr=0, ent=1.77]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:36,  1.17it/s, pg=-0.0616, ret=3.93e-5, glen=109, tlen=269, kl=0, act_lr=0, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:15<00:36,  1.17it/s, pg=0.0941, ret=-0.000905, glen=123, tlen=284, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:15<00:35,  1.17it/s, pg=0.0941, ret=-0.000905, glen=123, tlen=284, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:16<00:35,  1.17it/s, pg=-0.103, ret=-0.000127, glen=94.4, tlen=255, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:16<00:34,  1.17it/s, pg=-0.103, ret=-0.000127, glen=94.4, tlen=255, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:16<00:34,  1.17it/s, pg=0.0759, ret=0.000215, glen=123, tlen=284, kl=0, act_lr=0, ent=1.76]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=0.0759, ret=0.000215, glen=123, tlen=284, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:17<00:34,  1.17it/s, pg=-0.154, ret=0.00225, glen=121, tlen=282, kl=0, act_lr=0, ent=1.62] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.18it/s, pg=-0.154, ret=0.00225, glen=121, tlen=282, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:18<00:33,  1.18it/s, pg=0.0566, ret=-0.000261, glen=116, tlen=276, kl=0, act_lr=0, ent=1.72]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.18it/s, pg=0.0566, ret=-0.000261, glen=116, tlen=276, kl=0, act_lr=0, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:19<00:32,  1.18it/s, pg=-0.0662, ret=0.000428, glen=101, tlen=262, kl=0, act_lr=0, ent=1.59]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.18it/s, pg=-0.0662, ret=0.000428, glen=101, tlen=262, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:20<00:31,  1.18it/s, pg=-0.102, ret=-0.000148, glen=124, tlen=284, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.18it/s, pg=-0.102, ret=-0.000148, glen=124, tlen=284, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:21<00:30,  1.18it/s, pg=0.0445, ret=-0.000576, glen=91.7, tlen=252, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:21<00:29,  1.18it/s, pg=0.0445, ret=-0.000576, glen=91.7, tlen=252, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:22<00:29,  1.18it/s, pg=-0.0389, ret=-0.000265, glen=112, tlen=273, kl=0, act_lr=0, ent=1.82]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:22<00:28,  1.18it/s, pg=-0.0389, ret=-0.000265, glen=112, tlen=273, kl=0, act_lr=0, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:22<00:28,  1.18it/s, pg=-0.00873, ret=0.00307, glen=179, tlen=339, kl=0, act_lr=0, ent=2.23] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.17it/s, pg=-0.00873, ret=0.00307, glen=179, tlen=339, kl=0, act_lr=0, ent=2.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:23<00:28,  1.17it/s, pg=-0.203, ret=0.00135, glen=116, tlen=277, kl=0, act_lr=0, ent=1.68]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.17it/s, pg=-0.203, ret=0.00135, glen=116, tlen=277, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:24<00:27,  1.17it/s, pg=0.111, ret=-0.00221, glen=118, tlen=278, kl=0, act_lr=0, ent=1.84]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.111, ret=-0.00221, glen=118, tlen=278, kl=0, act_lr=0, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:25<00:26,  1.17it/s, pg=-0.00208, ret=-0.000165, glen=115, tlen=276, kl=0, act_lr=0, ent=1.54]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.18it/s, pg=-0.00208, ret=-0.000165, glen=115, tlen=276, kl=0, act_lr=0, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:26<00:25,  1.18it/s, pg=-0.113, ret=0.000108, glen=107, tlen=267, kl=0, act_lr=0, ent=1.67]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.113, ret=0.000108, glen=107, tlen=267, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:27<00:27,  1.07it/s, pg=-0.0209, ret=-0.000352, glen=108, tlen=269, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.10it/s, pg=-0.0209, ret=-0.000352, glen=108, tlen=269, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:28<00:25,  1.10it/s, pg=-0.00781, ret=-0.00215, glen=91.8, tlen=252, kl=0, act_lr=0, ent=1.54]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.12it/s, pg=-0.00781, ret=-0.00215, glen=91.8, tlen=252, kl=0, act_lr=0, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:29<00:24,  1.12it/s, pg=0.244, ret=-0.000828, glen=137, tlen=298, kl=0, act_lr=0, ent=2.15]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:22,  1.14it/s, pg=0.244, ret=-0.000828, glen=137, tlen=298, kl=0, act_lr=0, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:30<00:22,  1.14it/s, pg=-0.0299, ret=0.000116, glen=122, tlen=282, kl=0, act_lr=0, ent=2.02]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:30<00:21,  1.15it/s, pg=-0.0299, ret=0.000116, glen=122, tlen=282, kl=0, act_lr=0, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:30<00:21,  1.15it/s, pg=0.0894, ret=-0.000191, glen=108, tlen=268, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.16it/s, pg=0.0894, ret=-0.000191, glen=108, tlen=268, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:31<00:20,  1.16it/s, pg=0.102, ret=0.00018, glen=116, tlen=276, kl=0, act_lr=0, ent=1.83]   Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=0.102, ret=0.00018, glen=116, tlen=276, kl=0, act_lr=0, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:32<00:19,  1.16it/s, pg=-0.0702, ret=-0.000848, glen=108, tlen=269, kl=0, act_lr=0, ent=1.78]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=-0.0702, ret=-0.000848, glen=108, tlen=269, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:33<00:18,  1.17it/s, pg=-0.0553, ret=-0.000724, glen=101, tlen=261, kl=0, act_lr=0, ent=1.8] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=-0.0553, ret=-0.000724, glen=101, tlen=261, kl=0, act_lr=0, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:34<00:17,  1.17it/s, pg=-0.0718, ret=-0.000542, glen=102, tlen=263, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.17it/s, pg=-0.0718, ret=-0.000542, glen=102, tlen=263, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:35<00:17,  1.17it/s, pg=-0.136, ret=0.00152, glen=128, tlen=288, kl=0, act_lr=0, ent=1.91]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.17it/s, pg=-0.136, ret=0.00152, glen=128, tlen=288, kl=0, act_lr=0, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.17it/s, pg=0.176, ret=-0.000703, glen=119, tlen=280, kl=0, act_lr=0, ent=1.81]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.18it/s, pg=0.176, ret=-0.000703, glen=119, tlen=280, kl=0, act_lr=0, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:36<00:15,  1.18it/s, pg=0.217, ret=0.000893, glen=163, tlen=324, kl=0, act_lr=0, ent=2.15] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.217, ret=0.000893, glen=163, tlen=324, kl=0, act_lr=0, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:37<00:14,  1.16it/s, pg=-0.0223, ret=-0.00191, glen=116, tlen=277, kl=0, act_lr=0, ent=1.77]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.15it/s, pg=-0.0223, ret=-0.00191, glen=116, tlen=277, kl=0, act_lr=0, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:38<00:13,  1.15it/s, pg=-0.11, ret=0.00112, glen=116, tlen=277, kl=0, act_lr=0, ent=1.66]   Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:13,  1.15it/s, pg=-0.11, ret=0.00112, glen=116, tlen=277, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:39<00:13,  1.15it/s, pg=-0.045, ret=0.0016, glen=125, tlen=286, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.16it/s, pg=-0.045, ret=0.0016, glen=125, tlen=286, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:40<00:12,  1.16it/s, pg=-0.22, ret=0.0033, glen=127, tlen=288, kl=0, act_lr=0, ent=2.1]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=-0.22, ret=0.0033, glen=127, tlen=288, kl=0, act_lr=0, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:41<00:11,  1.17it/s, pg=-0.105, ret=0.000793, glen=140, tlen=300, kl=0, act_lr=0, ent=1.99]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.17it/s, pg=-0.105, ret=0.000793, glen=140, tlen=300, kl=0, act_lr=0, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:42<00:10,  1.17it/s, pg=-0.178, ret=0.00118, glen=128, tlen=288, kl=0, act_lr=0, ent=1.88] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:42<00:09,  1.17it/s, pg=-0.178, ret=0.00118, glen=128, tlen=288, kl=0, act_lr=0, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:42<00:09,  1.17it/s, pg=0.188, ret=-0.00214, glen=113, tlen=274, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=0.188, ret=-0.00214, glen=113, tlen=274, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:43<00:08,  1.17it/s, pg=0.219, ret=-0.0027, glen=122, tlen=283, kl=0, act_lr=0, ent=1.52] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=0.219, ret=-0.0027, glen=122, tlen=283, kl=0, act_lr=0, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:44<00:07,  1.17it/s, pg=0.0356, ret=0.00127, glen=114, tlen=274, kl=0, act_lr=0, ent=1.93]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=0.0356, ret=0.00127, glen=114, tlen=274, kl=0, act_lr=0, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:45<00:06,  1.17it/s, pg=-0.0473, ret=0.001, glen=108, tlen=268, kl=0, act_lr=0, ent=1.62] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=-0.0473, ret=0.001, glen=108, tlen=268, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:46<00:05,  1.17it/s, pg=-0.137, ret=0.00144, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.18it/s, pg=-0.137, ret=0.00144, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:47<00:05,  1.18it/s, pg=-0.071, ret=-8.45e-5, glen=137, tlen=297, kl=0, act_lr=0, ent=1.98]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.18it/s, pg=-0.071, ret=-8.45e-5, glen=137, tlen=297, kl=0, act_lr=0, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.18it/s, pg=-0.0575, ret=0.000681, glen=113, tlen=274, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.18it/s, pg=-0.0575, ret=0.000681, glen=113, tlen=274, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:48<00:03,  1.18it/s, pg=0.00867, ret=-0.000882, glen=122, tlen=282, kl=0, act_lr=0, ent=1.91]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.18it/s, pg=0.00867, ret=-0.000882, glen=122, tlen=282, kl=0, act_lr=0, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:49<00:02,  1.18it/s, pg=-0.179, ret=0.00125, glen=111, tlen=271, kl=0, act_lr=0, ent=1.59]   Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.18it/s, pg=-0.179, ret=0.00125, glen=111, tlen=271, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:50<00:01,  1.18it/s, pg=-0.203, ret=0.00141, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.18it/s, pg=-0.203, ret=0.00141, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]
2025-07-24 16:09:42.917 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 51.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:51<00:00,  1.18it/s, pg=0.0981, ret=-0.00132, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:51<00:00,  1.11it/s, pg=0.0981, ret=-0.00132, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.77]
2025-07-24 16:09:43.796 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 16:09:46.357 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 16:09:48.828 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 57.52s
2025-07-24 16:09:48.836 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.007191941655915359, 'actor_lr': 3.448275841112169e-10, 'clip_ratio': 0.0, 'entropy': 1.7657057540170078, 'kl': 0.0, 'response_length': 116.28467454581425, 'total_length': 276.7488821621599, 'teacher_total_length': 289.0060219600283, 'return': -1.278092644622967e-06, 'policy_update_steps': 1.0}
Episode [1/20]:   8%|‚ñä         | 1/13 [03:56<47:19, 236.63s/it]2025-07-24 16:09:48.895 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:11:20.756 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:11:20.934 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:11:20.934 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 92.04s
2025-07-24 16:11:23.095 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 109.7526,std_num_tokens: 111.6655,avg_correct_num_tokens: 102.7106,std_correct_num_tokens: 88.0398,avg_incorrect_num_tokens: 121.4100,std_incorrect_num_tokens: 141.6397
2025-07-24 16:11:23.425 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.49s
2025-07-24 16:11:26.606 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 16:11:55.557 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 225
2025-07-24 16:11:55.558 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 16:11:56.918 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.97s
2025-07-24 16:11:56.918 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0004019819732962383, avg_kl: 0.0, avg_response_length: 111.20896928575304, avg_orm_score: 0.0, avg_custom_rewards: -0.0004019819732962383
2025-07-24 16:11:56.953 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter1_replay_buffer.jsonl
2025-07-24 16:11:58.810 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.86s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.122, ret=0.00139, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.62]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.122, ret=0.00139, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.229, ret=0.00258, glen=99.7, tlen=261, kl=0, act_lr=2e-8, ent=1.64]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.08it/s, pg=-0.229, ret=0.00258, glen=99.7, tlen=261, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.08it/s, pg=-0.00248, ret=-0.000607, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.54]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=-0.00248, ret=-0.000607, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=-0.146, ret=0.00126, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.64]    Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=-0.146, ret=0.00126, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=-0.136, ret=-1.2e-5, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.136, ret=-1.2e-5, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.0627, ret=-0.00105, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0627, ret=-0.00105, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.0963, ret=0.000658, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.14it/s, pg=-0.0963, ret=0.000658, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.14it/s, pg=0.046, ret=0.00101, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.98]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=0.046, ret=0.00101, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=-0.0628, ret=-0.000412, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.0628, ret=-0.000412, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.134, ret=-0.000758, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.88]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=0.134, ret=-0.000758, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=0.0674, ret=-0.00141, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.74]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.17it/s, pg=0.0674, ret=-0.00141, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.17it/s, pg=0.0914, ret=0.00054, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.62] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=0.0914, ret=0.00054, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=-0.0233, ret=-0.000625, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.75]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.0233, ret=-0.000625, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.256, ret=0.00166, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.59]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=-0.256, ret=0.00166, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0753, ret=-0.000334, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.59]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0753, ret=-0.000334, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.101, ret=0.000645, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.57] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.101, ret=0.000645, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.1, ret=0.00133, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.1, ret=0.00133, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.0258, ret=-0.000727, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=0.0258, ret=-0.000727, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=0.107, ret=-0.00128, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.65]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.18it/s, pg=0.107, ret=-0.00128, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.18it/s, pg=-0.0837, ret=0.000457, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.18it/s, pg=-0.0837, ret=0.000457, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.18it/s, pg=-0.184, ret=0.00276, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.69]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.18it/s, pg=-0.184, ret=0.00276, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.18it/s, pg=0.0781, ret=5.91e-5, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.82]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.18it/s, pg=0.0781, ret=5.91e-5, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.18it/s, pg=0.253, ret=-0.00223, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.81]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.18it/s, pg=0.253, ret=-0.00223, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.18it/s, pg=0.0104, ret=-0.00124, glen=110, tlen=271, kl=0, act_lr=2e-8, ent=1.8]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.18it/s, pg=0.0104, ret=-0.00124, glen=110, tlen=271, kl=0, act_lr=2e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.18it/s, pg=0.171, ret=-0.00157, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=2.06]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.18it/s, pg=0.171, ret=-0.00157, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.18it/s, pg=-0.0759, ret=0.00144, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.78]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.18it/s, pg=-0.0759, ret=0.00144, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.18it/s, pg=0.0108, ret=1.68e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.86] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=0.0108, ret=1.68e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=0.277, ret=-0.0011, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.86] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.18it/s, pg=0.277, ret=-0.0011, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.18it/s, pg=0.035, ret=-0.000798, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:27,  1.02it/s, pg=0.035, ret=-0.000798, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:27,  1.02it/s, pg=-0.104, ret=0.00169, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.53] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.06it/s, pg=-0.104, ret=0.00169, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.06it/s, pg=0.151, ret=-0.00124, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=1.89]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=0.151, ret=-0.00124, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.0665, ret=0.000756, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.11it/s, pg=-0.0665, ret=0.000756, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.00211, ret=-0.00148, glen=125, tlen=285, kl=0, act_lr=2e-8, ent=1.78]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.00211, ret=-0.00148, glen=125, tlen=285, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=0.0208, ret=-0.000786, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.66]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.13it/s, pg=0.0208, ret=-0.000786, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.13it/s, pg=-0.198, ret=0.000905, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=2.12] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=-0.198, ret=0.000905, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=-0.00299, ret=0.000586, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.59]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=-0.00299, ret=0.000586, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.163, ret=0.00107, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.58]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.163, ret=0.00107, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0769, ret=-2.25e-5, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0769, ret=-2.25e-5, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.194, ret=-0.000224, glen=114, tlen=275, kl=0, act_lr=2e-8, ent=1.74]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.194, ret=-0.000224, glen=114, tlen=275, kl=0, act_lr=2e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0107, ret=-0.000238, glen=96.3, tlen=257, kl=0, act_lr=2e-8, ent=1.57]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0107, ret=-0.000238, glen=96.3, tlen=257, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0047, ret=6.06e-5, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.6]     Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0047, ret=6.06e-5, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.00222, ret=0.000656, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.00222, ret=0.000656, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=0.0798, ret=-0.000233, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.58] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=0.0798, ret=-0.000233, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.0158, ret=-0.0013, glen=99.7, tlen=260, kl=0, act_lr=2e-8, ent=1.58]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=-0.0158, ret=-0.0013, glen=99.7, tlen=260, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=0.202, ret=-0.00102, glen=116, tlen=278, kl=0, act_lr=2e-8, ent=2.01]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.202, ret=-0.00102, glen=116, tlen=278, kl=0, act_lr=2e-8, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.136, ret=-0.00071, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.64]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.18it/s, pg=0.136, ret=-0.00071, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.0179, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.0179, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=-0.0343, ret=0.000563, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.6] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=-0.0343, ret=0.000563, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=-0.125, ret=0.000842, glen=103, tlen=263, kl=0, act_lr=2e-8, ent=1.97]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.125, ret=0.000842, glen=103, tlen=263, kl=0, act_lr=2e-8, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=0.0212, ret=-0.00376, glen=120, tlen=282, kl=0, act_lr=2e-8, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.0212, ret=-0.00376, glen=120, tlen=282, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.0927, ret=0.000311, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.18it/s, pg=-0.0927, ret=0.000311, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.18it/s, pg=0.163, ret=-0.000887, glen=129, tlen=290, kl=0, act_lr=2e-8, ent=2.04]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.163, ret=-0.000887, glen=129, tlen=290, kl=0, act_lr=2e-8, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.057, ret=0.000675, glen=98.6, tlen=259, kl=0, act_lr=2e-8, ent=1.57]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.16it/s, pg=-0.057, ret=0.000675, glen=98.6, tlen=259, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.16it/s, pg=0.0166, ret=-0.00109, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.67] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.16it/s, pg=0.0166, ret=-0.00109, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=-0.0638, ret=0.00119, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0638, ret=0.00119, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.16, ret=-0.00194, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.16, ret=-0.00194, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.73]
2025-07-24 16:12:48.426 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.43s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.181, ret=0.00102, glen=143, tlen=304, kl=0, act_lr=4e-8, ent=1.55]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.181, ret=0.00102, glen=143, tlen=304, kl=0, act_lr=4e-8, ent=1.55]
2025-07-24 16:12:49.303 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:12:51.663 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 16:12:52.008 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.13s
2025-07-24 16:12:52.015 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0045301872387267, 'actor_lr': 2.035087706930059e-08, 'clip_ratio': 0.0, 'entropy': 1.7246172030766804, 'kl': 0.0, 'response_length': 111.14298368755139, 'total_length': 271.7544030975877, 'teacher_total_length': 283.5741668165776, 'return': -7.169973717904405e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  15%|‚ñà‚ñå        | 2/13 [06:59<37:37, 205.19s/it]2025-07-24 16:12:52.058 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:15:32.344 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:15:32.523 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:15:32.524 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 160.47s
2025-07-24 16:15:34.999 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 112.6627,std_num_tokens: 145.2815,avg_correct_num_tokens: 103.4203,std_correct_num_tokens: 86.0646,avg_incorrect_num_tokens: 127.2372,std_incorrect_num_tokens: 205.8219
2025-07-24 16:15:35.377 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.85s
2025-07-24 16:15:38.311 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.93s
2025-07-24 16:16:07.259 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 226
2025-07-24 16:16:07.260 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 16:16:08.665 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 16:16:08.665 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0024170190746488, avg_kl: 0.000875793727098313, avg_response_length: 118.18322855181398, avg_orm_score: 0.0, avg_custom_rewards: -0.0024170190746488
2025-07-24 16:16:08.703 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter2_replay_buffer.jsonl
2025-07-24 16:16:10.603 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.0745, ret=-8.99e-5, glen=129, tlen=290, kl=0.000856, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0745, ret=-8.99e-5, glen=129, tlen=290, kl=0.000856, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0279, ret=1.71e-5, glen=111, tlen=271, kl=0.000875, act_lr=4e-8, ent=1.76] Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=-0.0279, ret=1.71e-5, glen=111, tlen=271, kl=0.000875, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=-0.117, ret=-0.000587, glen=116, tlen=276, kl=0.000868, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=-0.117, ret=-0.000587, glen=116, tlen=276, kl=0.000868, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=0.0988, ret=-0.0011, glen=118, tlen=278, kl=0.000878, act_lr=4e-8, ent=1.68]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=0.0988, ret=-0.0011, glen=118, tlen=278, kl=0.000878, act_lr=4e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=0.0905, ret=0.000838, glen=136, tlen=296, kl=0.000888, act_lr=4e-8, ent=1.94]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=0.0905, ret=0.000838, glen=136, tlen=296, kl=0.000888, act_lr=4e-8, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=-0.0579, ret=-0.000396, glen=104, tlen=264, kl=0.000914, act_lr=4e-8, ent=1.84]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.16it/s, pg=-0.0579, ret=-0.000396, glen=104, tlen=264, kl=0.000914, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.16it/s, pg=-0.195, ret=0.00119, glen=120, tlen=281, kl=0.000845, act_lr=4e-8, ent=1.85]   Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=-0.195, ret=0.00119, glen=120, tlen=281, kl=0.000845, act_lr=4e-8, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=-0.14, ret=0.000105, glen=103, tlen=264, kl=0.000918, act_lr=4e-8, ent=1.6] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:41,  1.17it/s, pg=-0.14, ret=0.000105, glen=103, tlen=264, kl=0.000918, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:41,  1.17it/s, pg=-0.183, ret=0.000671, glen=104, tlen=264, kl=0.00087, act_lr=4e-8, ent=1.6]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.183, ret=0.000671, glen=104, tlen=264, kl=0.00087, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.0739, ret=-0.000439, glen=112, tlen=273, kl=0.000913, act_lr=4e-8, ent=1.9]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=-0.0739, ret=-0.000439, glen=112, tlen=273, kl=0.000913, act_lr=4e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.139, ret=0.00231, glen=121, tlen=281, kl=0.000895, act_lr=4e-8, ent=1.6]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.15it/s, pg=-0.139, ret=0.00231, glen=121, tlen=281, kl=0.000895, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.15it/s, pg=-0.012, ret=0.000149, glen=110, tlen=270, kl=0.000875, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.012, ret=0.000149, glen=110, tlen=270, kl=0.000875, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.26, ret=-0.00287, glen=114, tlen=273, kl=0.000817, act_lr=4e-8, ent=1.48]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=0.26, ret=-0.00287, glen=114, tlen=273, kl=0.000817, act_lr=4e-8, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=0.00775, ret=0.000437, glen=112, tlen=273, kl=0.000891, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.16it/s, pg=0.00775, ret=0.000437, glen=112, tlen=273, kl=0.000891, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.16it/s, pg=-0.13, ret=0.000705, glen=103, tlen=263, kl=0.000855, act_lr=4e-8, ent=1.8]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.13, ret=0.000705, glen=103, tlen=263, kl=0.000855, act_lr=4e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.0493, ret=0.00138, glen=137, tlen=297, kl=0.000841, act_lr=4e-8, ent=2.22]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0493, ret=0.00138, glen=137, tlen=297, kl=0.000841, act_lr=4e-8, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.276, ret=-0.000972, glen=120, tlen=280, kl=0.000904, act_lr=4e-8, ent=1.86]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.276, ret=-0.000972, glen=120, tlen=280, kl=0.000904, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.086, ret=0.000553, glen=103, tlen=263, kl=0.00088, act_lr=4e-8, ent=1.61] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.086, ret=0.000553, glen=103, tlen=263, kl=0.00088, act_lr=4e-8, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.195, ret=0.00143, glen=118, tlen=278, kl=0.000857, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.195, ret=0.00143, glen=118, tlen=278, kl=0.000857, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.188, ret=0.00116, glen=116, tlen=277, kl=0.000867, act_lr=4e-8, ent=1.73]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.188, ret=0.00116, glen=116, tlen=277, kl=0.000867, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.00564, ret=0.00012, glen=113, tlen=274, kl=0.000859, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.00564, ret=0.00012, glen=113, tlen=274, kl=0.000859, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.113, ret=-0.00015, glen=131, tlen=291, kl=0.000892, act_lr=4e-8, ent=1.78]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.113, ret=-0.00015, glen=131, tlen=291, kl=0.000892, act_lr=4e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0288, ret=-0.0014, glen=108, tlen=267, kl=0.000916, act_lr=4e-8, ent=1.64]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.17it/s, pg=0.0288, ret=-0.0014, glen=108, tlen=267, kl=0.000916, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0328, ret=-0.000227, glen=130, tlen=290, kl=0.000848, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0328, ret=-0.000227, glen=130, tlen=290, kl=0.000848, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0914, ret=-0.00149, glen=113, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.73] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0914, ret=-0.00149, glen=113, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0418, ret=0.000939, glen=111, tlen=271, kl=0.000896, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0418, ret=0.000939, glen=111, tlen=271, kl=0.000896, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.108, ret=-0.00106, glen=103, tlen=263, kl=0.000921, act_lr=4e-8, ent=1.7]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.108, ret=-0.00106, glen=103, tlen=263, kl=0.000921, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=-0.0739, ret=0.00138, glen=116, tlen=277, kl=0.000908, act_lr=4e-8, ent=1.85]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=-0.0739, ret=0.00138, glen=116, tlen=277, kl=0.000908, act_lr=4e-8, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0544, ret=-0.000121, glen=108, tlen=269, kl=0.00084, act_lr=4e-8, ent=1.63]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:27,  1.01it/s, pg=-0.0544, ret=-0.000121, glen=108, tlen=269, kl=0.00084, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:27,  1.01it/s, pg=-0.159, ret=0.00108, glen=114, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.65]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.05it/s, pg=-0.159, ret=0.00108, glen=114, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.05it/s, pg=-0.207, ret=0.00114, glen=99, tlen=260, kl=0.00088, act_lr=4e-8, ent=1.63]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.207, ret=0.00114, glen=99, tlen=260, kl=0.00088, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.09it/s, pg=-0.171, ret=0.00131, glen=108, tlen=269, kl=0.000911, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=-0.171, ret=0.00131, glen=108, tlen=269, kl=0.000911, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.233, ret=-0.00321, glen=108, tlen=268, kl=0.000928, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.233, ret=-0.00321, glen=108, tlen=268, kl=0.000928, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=-0.00635, ret=0.00115, glen=124, tlen=284, kl=0.000877, act_lr=4e-8, ent=1.86]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.14it/s, pg=-0.00635, ret=0.00115, glen=124, tlen=284, kl=0.000877, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.14it/s, pg=-0.00122, ret=0.000491, glen=102, tlen=262, kl=0.000856, act_lr=4e-8, ent=1.66]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.15it/s, pg=-0.00122, ret=0.000491, glen=102, tlen=262, kl=0.000856, act_lr=4e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.15it/s, pg=-0.0938, ret=0.000208, glen=112, tlen=273, kl=0.000917, act_lr=4e-8, ent=1.84] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0938, ret=0.000208, glen=112, tlen=273, kl=0.000917, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.0966, ret=-0.000281, glen=93.6, tlen=254, kl=0.000879, act_lr=4e-8, ent=1.52]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.0966, ret=-0.000281, glen=93.6, tlen=254, kl=0.000879, act_lr=4e-8, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0652, ret=-0.000276, glen=96.6, tlen=257, kl=0.000896, act_lr=4e-8, ent=1.67]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0652, ret=-0.000276, glen=96.6, tlen=257, kl=0.000896, act_lr=4e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=-0.0344, ret=0.00203, glen=109, tlen=270, kl=0.000886, act_lr=4e-8, ent=1.98]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0344, ret=0.00203, glen=109, tlen=270, kl=0.000886, act_lr=4e-8, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.11, ret=0.00118, glen=114, tlen=274, kl=0.000892, act_lr=4e-8, ent=1.7]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.11, ret=0.00118, glen=114, tlen=274, kl=0.000892, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.242, ret=-0.00316, glen=115, tlen=275, kl=0.000875, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.242, ret=-0.00316, glen=115, tlen=275, kl=0.000875, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0181, ret=3.29e-5, glen=107, tlen=266, kl=0.000891, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0181, ret=3.29e-5, glen=107, tlen=266, kl=0.000891, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.154, ret=0.000425, glen=101, tlen=261, kl=0.000907, act_lr=4e-8, ent=1.64]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.154, ret=0.000425, glen=101, tlen=261, kl=0.000907, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.00129, glen=109, tlen=269, kl=0.00086, act_lr=4e-8, ent=1.84]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.00129, glen=109, tlen=269, kl=0.00086, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.0341, ret=0.000607, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.77]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0341, ret=0.000607, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.218, ret=0.00251, glen=115, tlen=276, kl=0.000878, act_lr=4e-8, ent=1.86]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.218, ret=0.00251, glen=115, tlen=276, kl=0.000878, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.244, ret=-0.00315, glen=125, tlen=285, kl=0.000886, act_lr=4e-8, ent=1.7] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.244, ret=-0.00315, glen=125, tlen=285, kl=0.000886, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.149, ret=-0.00317, glen=126, tlen=286, kl=0.000907, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.149, ret=-0.00317, glen=126, tlen=286, kl=0.000907, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.431, ret=-0.0183, glen=410, tlen=570, kl=0.000707, act_lr=4e-8, ent=2.71] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:07,  1.14it/s, pg=0.431, ret=-0.0183, glen=410, tlen=570, kl=0.000707, act_lr=4e-8, ent=2.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:07,  1.14it/s, pg=0.14, ret=-0.00188, glen=118, tlen=278, kl=0.000865, act_lr=4e-8, ent=1.57]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.15it/s, pg=0.14, ret=-0.00188, glen=118, tlen=278, kl=0.000865, act_lr=4e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.15it/s, pg=-0.0471, ret=-0.000547, glen=108, tlen=269, kl=0.000831, act_lr=4e-8, ent=1.8]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.0471, ret=-0.000547, glen=108, tlen=269, kl=0.000831, act_lr=4e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=-0.0851, ret=0.00087, glen=112, tlen=272, kl=0.00089, act_lr=4e-8, ent=1.74]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=-0.0851, ret=0.00087, glen=112, tlen=272, kl=0.00089, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.151, ret=0.000737, glen=110, tlen=270, kl=0.000897, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.151, ret=0.000737, glen=110, tlen=270, kl=0.000897, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.00233, ret=0.000999, glen=112, tlen=272, kl=0.000839, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.00233, ret=0.000999, glen=112, tlen=272, kl=0.000839, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0515, ret=-0.000648, glen=108, tlen=269, kl=0.000858, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0515, ret=-0.000648, glen=108, tlen=269, kl=0.000858, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.148, ret=-0.000656, glen=128, tlen=289, kl=0.000846, act_lr=4e-8, ent=1.94]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.148, ret=-0.000656, glen=128, tlen=289, kl=0.000846, act_lr=4e-8, ent=1.94]
2025-07-24 16:17:00.357 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.57s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.00983, ret=0.000313, glen=110, tlen=271, kl=0.00084, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.00983, ret=0.000313, glen=110, tlen=271, kl=0.00084, act_lr=6e-8, ent=1.69]
2025-07-24 16:17:01.248 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:17:03.773 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 16:17:04.101 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.43s
2025-07-24 16:17:04.151 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01652477080361885, 'actor_lr': 4.0350876916587013e-08, 'clip_ratio': 0.0, 'entropy': 1.7695354474218268, 'kl': 0.0008754730224609375, 'response_length': 118.31446369907312, 'total_length': 278.56398947197096, 'teacher_total_length': 290.05270921138293, 'return': -0.0002886613505675964, 'policy_update_steps': 1.0}
Episode [1/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:11<37:46, 226.63s/it]2025-07-24 16:17:04.191 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:18:50.122 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:18:50.301 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:18:50.302 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 106.11s
2025-07-24 16:18:52.281 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 109.1659,std_num_tokens: 128.0053,avg_correct_num_tokens: 101.0823,std_correct_num_tokens: 86.4976,avg_incorrect_num_tokens: 121.6989,std_incorrect_num_tokens: 173.0066
2025-07-24 16:18:52.733 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.43s
2025-07-24 16:18:55.681 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.95s
2025-07-24 16:19:24.217 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 224
2025-07-24 16:19:24.218 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.53s
2025-07-24 16:19:25.583 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 16:19:25.583 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0010229884017982321, avg_kl: 0.0009026101657322475, avg_response_length: 111.71290295464652, avg_orm_score: 0.0, avg_custom_rewards: -0.0010229884017982321
2025-07-24 16:19:25.637 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter3_replay_buffer.jsonl
2025-07-24 16:19:27.483 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.169, ret=-0.00226, glen=187, tlen=347, kl=0.000835, act_lr=6e-8, ent=2.19]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:59,  1.08s/it, pg=0.169, ret=-0.00226, glen=187, tlen=347, kl=0.000835, act_lr=6e-8, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:59,  1.08s/it, pg=-0.114, ret=0.00101, glen=100, tlen=260, kl=0.000943, act_lr=6e-8, ent=1.74]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:51,  1.06it/s, pg=-0.114, ret=0.00101, glen=100, tlen=260, kl=0.000943, act_lr=6e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:51,  1.06it/s, pg=0.0869, ret=0.000845, glen=111, tlen=272, kl=0.000916, act_lr=6e-8, ent=1.79]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.10it/s, pg=0.0869, ret=0.000845, glen=111, tlen=272, kl=0.000916, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.10it/s, pg=-0.111, ret=0.0013, glen=106, tlen=267, kl=0.000919, act_lr=6e-8, ent=1.65]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.13it/s, pg=-0.111, ret=0.0013, glen=106, tlen=267, kl=0.000919, act_lr=6e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.13it/s, pg=0.12, ret=-0.00134, glen=120, tlen=280, kl=0.000899, act_lr=6e-8, ent=2.09]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.12, ret=-0.00134, glen=120, tlen=280, kl=0.000899, act_lr=6e-8, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=0.0226, ret=-0.000796, glen=97.6, tlen=258, kl=0.000943, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=0.0226, ret=-0.000796, glen=97.6, tlen=258, kl=0.000943, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.102, ret=0.0021, glen=108, tlen=269, kl=0.000917, act_lr=6e-8, ent=1.66]    Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=-0.102, ret=0.0021, glen=108, tlen=269, kl=0.000917, act_lr=6e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.0961, ret=-0.000353, glen=94.6, tlen=255, kl=0.000871, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0961, ret=-0.000353, glen=94.6, tlen=255, kl=0.000871, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.157, ret=0.00116, glen=108, tlen=269, kl=0.000896, act_lr=6e-8, ent=1.64]    Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.157, ret=0.00116, glen=108, tlen=269, kl=0.000896, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0488, ret=0.000599, glen=103, tlen=264, kl=0.000929, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0488, ret=0.000599, glen=103, tlen=264, kl=0.000929, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=0.173, ret=-0.00187, glen=118, tlen=278, kl=0.00083, act_lr=6e-8, ent=1.93]   Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=0.173, ret=-0.00187, glen=118, tlen=278, kl=0.00083, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=-0.108, ret=0.000618, glen=91.4, tlen=252, kl=0.000933, act_lr=6e-8, ent=1.7]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.16it/s, pg=-0.108, ret=0.000618, glen=91.4, tlen=252, kl=0.000933, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.16it/s, pg=0.0664, ret=-0.000441, glen=116, tlen=277, kl=0.000853, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.16it/s, pg=0.0664, ret=-0.000441, glen=116, tlen=277, kl=0.000853, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.16it/s, pg=-0.137, ret=0.000305, glen=146, tlen=306, kl=0.000909, act_lr=6e-8, ent=2.09] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.17it/s, pg=-0.137, ret=0.000305, glen=146, tlen=306, kl=0.000909, act_lr=6e-8, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.17it/s, pg=-0.0267, ret=-0.00147, glen=109, tlen=269, kl=0.000905, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.0267, ret=-0.00147, glen=109, tlen=269, kl=0.000905, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.00988, ret=-0.000825, glen=96.3, tlen=257, kl=0.000897, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.16it/s, pg=-0.00988, ret=-0.000825, glen=96.3, tlen=257, kl=0.000897, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.00235, ret=-0.000202, glen=91.3, tlen=252, kl=0.000913, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.00235, ret=-0.000202, glen=91.3, tlen=252, kl=0.000913, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.0131, ret=-0.000867, glen=109, tlen=269, kl=0.000899, act_lr=6e-8, ent=1.79]   Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.0131, ret=-0.000867, glen=109, tlen=269, kl=0.000899, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0363, ret=-0.000524, glen=108, tlen=269, kl=0.000918, act_lr=6e-8, ent=1.61]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0363, ret=-0.000524, glen=108, tlen=269, kl=0.000918, act_lr=6e-8, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.086, ret=-0.000445, glen=158, tlen=318, kl=0.000839, act_lr=6e-8, ent=2.21]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.086, ret=-0.000445, glen=158, tlen=318, kl=0.000839, act_lr=6e-8, ent=2.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.201, ret=0.00272, glen=115, tlen=275, kl=0.000888, act_lr=6e-8, ent=1.93] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.201, ret=0.00272, glen=115, tlen=275, kl=0.000888, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=0.148, ret=-0.0012, glen=127, tlen=287, kl=0.000909, act_lr=6e-8, ent=1.76] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.148, ret=-0.0012, glen=127, tlen=287, kl=0.000909, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.021, ret=0.00115, glen=105, tlen=266, kl=0.000918, act_lr=6e-8, ent=1.78]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=0.021, ret=0.00115, glen=105, tlen=266, kl=0.000918, act_lr=6e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.104, ret=-0.000369, glen=98.4, tlen=259, kl=0.000913, act_lr=6e-8, ent=1.53]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.104, ret=-0.000369, glen=98.4, tlen=259, kl=0.000913, act_lr=6e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.125, ret=-0.000261, glen=137, tlen=297, kl=0.000834, act_lr=6e-8, ent=2.22]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.125, ret=-0.000261, glen=137, tlen=297, kl=0.000834, act_lr=6e-8, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.0552, ret=0.00125, glen=109, tlen=268, kl=0.00092, act_lr=6e-8, ent=1.68] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.0552, ret=0.00125, glen=109, tlen=268, kl=0.00092, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.045, ret=-0.00055, glen=97.8, tlen=258, kl=0.000957, act_lr=6e-8, ent=1.59]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.045, ret=-0.00055, glen=97.8, tlen=258, kl=0.000957, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.00842, ret=0.000603, glen=119, tlen=279, kl=0.00087, act_lr=6e-8, ent=1.87]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.00842, ret=0.000603, glen=119, tlen=279, kl=0.00087, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.0585, ret=0.000429, glen=98, tlen=258, kl=0.000882, act_lr=6e-8, ent=1.65] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.0585, ret=0.000429, glen=98, tlen=258, kl=0.000882, act_lr=6e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=0.256, ret=-0.00329, glen=105, tlen=266, kl=0.00092, act_lr=6e-8, ent=1.7]   Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.256, ret=-0.00329, glen=105, tlen=266, kl=0.00092, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=0.00259, ret=-0.000603, glen=113, tlen=274, kl=0.000949, act_lr=6e-8, ent=1.77]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.00259, ret=-0.000603, glen=113, tlen=274, kl=0.000949, act_lr=6e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.115, ret=0.000115, glen=112, tlen=272, kl=0.000887, act_lr=6e-8, ent=1.8]   Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.13it/s, pg=-0.115, ret=0.000115, glen=112, tlen=272, kl=0.000887, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.0431, ret=0.00139, glen=113, tlen=273, kl=0.000855, act_lr=6e-8, ent=1.82]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=-0.0431, ret=0.00139, glen=113, tlen=273, kl=0.000855, act_lr=6e-8, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=0.0608, ret=-0.000599, glen=102, tlen=263, kl=0.000956, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=0.0608, ret=-0.000599, glen=102, tlen=263, kl=0.000956, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=-0.0305, ret=-0.000302, glen=98.9, tlen=259, kl=0.000947, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.0305, ret=-0.000302, glen=98.9, tlen=259, kl=0.000947, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=-0.0984, ret=0.00109, glen=118, tlen=279, kl=0.000922, act_lr=6e-8, ent=1.87]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=-0.0984, ret=0.00109, glen=118, tlen=279, kl=0.000922, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=0.0214, ret=0.000823, glen=123, tlen=284, kl=0.000901, act_lr=6e-8, ent=1.78]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=0.0214, ret=0.000823, glen=123, tlen=284, kl=0.000901, act_lr=6e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.234, ret=0.00201, glen=112, tlen=272, kl=0.000901, act_lr=6e-8, ent=1.79] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:32<00:15,  1.17it/s, pg=-0.234, ret=0.00201, glen=112, tlen=272, kl=0.000901, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0181, ret=-0.00049, glen=109, tlen=269, kl=0.000919, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.16it/s, pg=0.0181, ret=-0.00049, glen=109, tlen=269, kl=0.000919, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=0.115, ret=-0.000841, glen=89.2, tlen=249, kl=0.000929, act_lr=6e-8, ent=1.64]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.16it/s, pg=0.115, ret=-0.000841, glen=89.2, tlen=249, kl=0.000929, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.16it/s, pg=0.253, ret=-0.00188, glen=136, tlen=296, kl=0.000793, act_lr=6e-8, ent=1.51]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.16it/s, pg=0.253, ret=-0.00188, glen=136, tlen=296, kl=0.000793, act_lr=6e-8, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=0.0101, ret=-0.0004, glen=110, tlen=270, kl=0.000902, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.16it/s, pg=0.0101, ret=-0.0004, glen=110, tlen=270, kl=0.000902, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.16it/s, pg=-0.148, ret=3.85e-5, glen=97.2, tlen=257, kl=0.000912, act_lr=6e-8, ent=1.58]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=3.85e-5, glen=97.2, tlen=257, kl=0.000912, act_lr=6e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0665, ret=-0.00255, glen=96, tlen=256, kl=0.000932, act_lr=6e-8, ent=1.59] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0665, ret=-0.00255, glen=96, tlen=256, kl=0.000932, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.19, ret=-0.00182, glen=113, tlen=273, kl=0.000876, act_lr=6e-8, ent=1.84] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.17it/s, pg=0.19, ret=-0.00182, glen=113, tlen=273, kl=0.000876, act_lr=6e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.079, ret=0.000555, glen=107, tlen=268, kl=0.000908, act_lr=6e-8, ent=1.62]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.079, ret=0.000555, glen=107, tlen=268, kl=0.000908, act_lr=6e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.113, ret=0.00129, glen=130, tlen=291, kl=0.000882, act_lr=6e-8, ent=1.73] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.18it/s, pg=-0.113, ret=0.00129, glen=130, tlen=291, kl=0.000882, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.18it/s, pg=0.0896, ret=-0.000923, glen=97.5, tlen=258, kl=0.000916, act_lr=6e-8, ent=1.54]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=0.0896, ret=-0.000923, glen=97.5, tlen=258, kl=0.000916, act_lr=6e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.0272, ret=0.00111, glen=114, tlen=274, kl=0.00092, act_lr=6e-8, ent=1.67]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.0272, ret=0.00111, glen=114, tlen=274, kl=0.00092, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=-0.0159, ret=-0.000171, glen=120, tlen=280, kl=0.00091, act_lr=6e-8, ent=2.06]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.0159, ret=-0.000171, glen=120, tlen=280, kl=0.00091, act_lr=6e-8, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0125, ret=-0.000249, glen=101, tlen=261, kl=0.000925, act_lr=6e-8, ent=1.59]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0125, ret=-0.000249, glen=101, tlen=261, kl=0.000925, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0162, ret=0.000158, glen=114, tlen=274, kl=0.000901, act_lr=6e-8, ent=1.88]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=-0.0162, ret=0.000158, glen=114, tlen=274, kl=0.000901, act_lr=6e-8, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.0827, ret=0.000486, glen=119, tlen=280, kl=0.000891, act_lr=6e-8, ent=1.73]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.0827, ret=0.000486, glen=119, tlen=280, kl=0.000891, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0355, ret=-0.000383, glen=96.4, tlen=257, kl=0.000915, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0355, ret=-0.000383, glen=96.4, tlen=257, kl=0.000915, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.0361, ret=-0.000192, glen=115, tlen=275, kl=0.000886, act_lr=6e-8, ent=1.7]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=-0.0361, ret=-0.000192, glen=115, tlen=275, kl=0.000886, act_lr=6e-8, ent=1.7]
2025-07-24 16:20:16.148 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=-0.194, ret=0.000859, glen=109, tlen=270, kl=0.000936, act_lr=8e-8, ent=1.97] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.194, ret=0.000859, glen=109, tlen=270, kl=0.000936, act_lr=8e-8, ent=1.97]
2025-07-24 16:20:17.016 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:20:19.425 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.41s
2025-07-24 16:20:19.750 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.20s
2025-07-24 16:20:19.758 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.010056640420641218, 'actor_lr': 6.03571407456879e-08, 'clip_ratio': 0.0, 'entropy': 1.7597153910568781, 'kl': 0.0009026101657322475, 'response_length': 111.71290343148368, 'total_length': 272.0588773999895, 'teacher_total_length': 284.70700345720564, 'return': -7.95320956967771e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:27<32:09, 214.38s/it]2025-07-24 16:20:19.806 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:23:00.801 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:23:00.983 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:23:00.984 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 161.18s
2025-07-24 16:23:03.047 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0100,avg_pass_at_n: 1.0000,avg_num_tokens: 113.3406,std_num_tokens: 152.2395,avg_correct_num_tokens: 103.6449,std_correct_num_tokens: 88.5879,avg_incorrect_num_tokens: 128.1518,std_incorrect_num_tokens: 215.0134
2025-07-24 16:23:03.500 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.52s
2025-07-24 16:23:06.581 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.08s
2025-07-24 16:23:36.209 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:23:36.209 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.63s
2025-07-24 16:23:37.920 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.24s
2025-07-24 16:23:37.920 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0005573580287067488, avg_kl: 0.0008828129830839332, avg_response_length: 118.33821242553178, avg_orm_score: 0.0, avg_custom_rewards: -0.0005573580287067488
2025-07-24 16:23:37.956 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter4_replay_buffer.jsonl
2025-07-24 16:23:39.880 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.93s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.198, ret=0.00151, glen=102, tlen=263, kl=0.000929, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.198, ret=0.00151, glen=102, tlen=263, kl=0.000929, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.00903, ret=-0.00139, glen=109, tlen=270, kl=0.000844, act_lr=8e-8, ent=1.6]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.06it/s, pg=-0.00903, ret=-0.00139, glen=109, tlen=270, kl=0.000844, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.06it/s, pg=-0.138, ret=0.000485, glen=110, tlen=270, kl=0.000888, act_lr=8e-8, ent=1.75] Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.11it/s, pg=-0.138, ret=0.000485, glen=110, tlen=270, kl=0.000888, act_lr=8e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.11it/s, pg=0.0961, ret=-0.00124, glen=117, tlen=278, kl=0.000882, act_lr=8e-8, ent=1.58]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.13it/s, pg=0.0961, ret=-0.00124, glen=117, tlen=278, kl=0.000882, act_lr=8e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.13it/s, pg=-0.0509, ret=-0.000247, glen=103, tlen=264, kl=0.000858, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:46,  1.15it/s, pg=-0.0509, ret=-0.000247, glen=103, tlen=264, kl=0.000858, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:46,  1.15it/s, pg=-0.0633, ret=0.00111, glen=111, tlen=272, kl=0.000883, act_lr=8e-8, ent=1.76]  Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:44,  1.16it/s, pg=-0.0633, ret=0.00111, glen=111, tlen=272, kl=0.000883, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:44,  1.16it/s, pg=0.114, ret=-0.00224, glen=356, tlen=517, kl=0.000704, act_lr=8e-8, ent=1.33] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.13it/s, pg=0.114, ret=-0.00224, glen=356, tlen=517, kl=0.000704, act_lr=8e-8, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.13it/s, pg=-0.0398, ret=-0.000177, glen=102, tlen=262, kl=0.000905, act_lr=8e-8, ent=1.64]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=-0.0398, ret=-0.000177, glen=102, tlen=262, kl=0.000905, act_lr=8e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=-0.0136, ret=-0.000271, glen=111, tlen=272, kl=0.000905, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.14it/s, pg=-0.0136, ret=-0.000271, glen=111, tlen=272, kl=0.000905, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.14it/s, pg=0.171, ret=-0.00211, glen=106, tlen=266, kl=0.000896, act_lr=8e-8, ent=1.54]   Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.15it/s, pg=0.171, ret=-0.00211, glen=106, tlen=266, kl=0.000896, act_lr=8e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.15it/s, pg=0.0506, ret=-0.000404, glen=111, tlen=272, kl=0.000831, act_lr=8e-8, ent=1.95]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.16it/s, pg=0.0506, ret=-0.000404, glen=111, tlen=272, kl=0.000831, act_lr=8e-8, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.16it/s, pg=0.0435, ret=-0.00103, glen=105, tlen=265, kl=0.000893, act_lr=8e-8, ent=1.59] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.16it/s, pg=0.0435, ret=-0.00103, glen=105, tlen=265, kl=0.000893, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.16it/s, pg=0.0495, ret=-0.00215, glen=108, tlen=269, kl=0.000892, act_lr=8e-8, ent=1.71]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=0.0495, ret=-0.00215, glen=108, tlen=269, kl=0.000892, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=0.0609, ret=-0.00107, glen=106, tlen=267, kl=0.000899, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=0.0609, ret=-0.00107, glen=106, tlen=267, kl=0.000899, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=-0.142, ret=0.00198, glen=118, tlen=278, kl=0.000916, act_lr=8e-8, ent=1.74] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:37,  1.15it/s, pg=-0.142, ret=0.00198, glen=118, tlen=278, kl=0.000916, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:37,  1.15it/s, pg=-0.0829, ret=-0.000248, glen=108, tlen=269, kl=0.000873, act_lr=8e-8, ent=1.57]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.0829, ret=-0.000248, glen=108, tlen=269, kl=0.000873, act_lr=8e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.051, ret=0.000987, glen=112, tlen=273, kl=0.000872, act_lr=8e-8, ent=1.72]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.051, ret=0.000987, glen=112, tlen=273, kl=0.000872, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.0227, ret=-0.00132, glen=101, tlen=262, kl=0.000894, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=0.0227, ret=-0.00132, glen=101, tlen=262, kl=0.000894, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=0.153, ret=-0.00147, glen=116, tlen=276, kl=0.000903, act_lr=8e-8, ent=1.78] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=0.153, ret=-0.00147, glen=116, tlen=276, kl=0.000903, act_lr=8e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.00244, ret=0.000159, glen=126, tlen=287, kl=0.000901, act_lr=8e-8, ent=1.92]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.00244, ret=0.000159, glen=126, tlen=287, kl=0.000901, act_lr=8e-8, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.105, ret=0.000885, glen=113, tlen=274, kl=0.000881, act_lr=8e-8, ent=1.66] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.105, ret=0.000885, glen=113, tlen=274, kl=0.000881, act_lr=8e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=-0.161, ret=0.00229, glen=122, tlen=282, kl=0.000855, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.161, ret=0.00229, glen=122, tlen=282, kl=0.000855, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0652, ret=0.000451, glen=111, tlen=272, kl=0.000863, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.17it/s, pg=-0.0652, ret=0.000451, glen=111, tlen=272, kl=0.000863, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.168, ret=0.00189, glen=129, tlen=290, kl=0.000857, act_lr=8e-8, ent=2.03]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:28,  1.17it/s, pg=-0.168, ret=0.00189, glen=129, tlen=290, kl=0.000857, act_lr=8e-8, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:28,  1.17it/s, pg=-0.166, ret=0.00144, glen=132, tlen=294, kl=0.000873, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.18it/s, pg=-0.166, ret=0.00144, glen=132, tlen=294, kl=0.000873, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.18it/s, pg=0.239, ret=-0.00231, glen=134, tlen=295, kl=0.000848, act_lr=8e-8, ent=1.9] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.18it/s, pg=0.239, ret=-0.00231, glen=134, tlen=295, kl=0.000848, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.18it/s, pg=-0.0405, ret=-3.71e-5, glen=132, tlen=293, kl=0.000905, act_lr=8e-8, ent=1.81]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.18it/s, pg=-0.0405, ret=-3.71e-5, glen=132, tlen=293, kl=0.000905, act_lr=8e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.18it/s, pg=-0.126, ret=-0.000624, glen=134, tlen=294, kl=0.000897, act_lr=8e-8, ent=1.87]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=-0.126, ret=-0.000624, glen=134, tlen=294, kl=0.000897, act_lr=8e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.102, ret=-0.000284, glen=106, tlen=267, kl=0.000874, act_lr=8e-8, ent=1.62]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.07it/s, pg=-0.102, ret=-0.000284, glen=106, tlen=267, kl=0.000874, act_lr=8e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.00371, ret=9.82e-5, glen=126, tlen=286, kl=0.000901, act_lr=8e-8, ent=1.91]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=-0.00371, ret=9.82e-5, glen=126, tlen=286, kl=0.000901, act_lr=8e-8, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.10it/s, pg=0.149, ret=-0.00141, glen=138, tlen=298, kl=0.000907, act_lr=8e-8, ent=2]     Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.149, ret=-0.00141, glen=138, tlen=298, kl=0.000907, act_lr=8e-8, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.0511, ret=2.9e-5, glen=123, tlen=284, kl=0.000866, act_lr=8e-8, ent=1.71]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:22,  1.14it/s, pg=0.0511, ret=2.9e-5, glen=123, tlen=284, kl=0.000866, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.14it/s, pg=0.0269, ret=-0.000232, glen=118, tlen=279, kl=0.000917, act_lr=8e-8, ent=1.78]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.15it/s, pg=0.0269, ret=-0.000232, glen=118, tlen=279, kl=0.000917, act_lr=8e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.15it/s, pg=0.081, ret=-0.000541, glen=105, tlen=266, kl=0.000872, act_lr=8e-8, ent=1.6]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.16it/s, pg=0.081, ret=-0.000541, glen=105, tlen=266, kl=0.000872, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.16it/s, pg=0.0626, ret=-4.4e-5, glen=96.2, tlen=257, kl=0.00088, act_lr=8e-8, ent=1.55]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=0.0626, ret=-4.4e-5, glen=96.2, tlen=257, kl=0.00088, act_lr=8e-8, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=-0.187, ret=0.000526, glen=114, tlen=275, kl=0.000891, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.17it/s, pg=-0.187, ret=0.000526, glen=114, tlen=275, kl=0.000891, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=0.107, ret=-0.000542, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:17,  1.17it/s, pg=0.107, ret=-0.000542, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=0.218, ret=-0.00381, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.218, ret=-0.00381, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=-0.301, ret=0.00221, glen=105, tlen=266, kl=0.000881, act_lr=8e-8, ent=1.59]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.17it/s, pg=-0.301, ret=0.00221, glen=105, tlen=266, kl=0.000881, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=-0.159, ret=0.00148, glen=117, tlen=278, kl=0.000934, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=-0.159, ret=0.00148, glen=117, tlen=278, kl=0.000934, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=-0.0958, ret=8.63e-5, glen=112, tlen=273, kl=0.000878, act_lr=8e-8, ent=1.49]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0958, ret=8.63e-5, glen=112, tlen=273, kl=0.000878, act_lr=8e-8, ent=1.49]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=-0.134, ret=0.00187, glen=111, tlen=273, kl=0.000899, act_lr=8e-8, ent=1.74] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.16it/s, pg=-0.134, ret=0.00187, glen=111, tlen=273, kl=0.000899, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.16it/s, pg=-0.0825, ret=-0.000351, glen=116, tlen=278, kl=0.000887, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=-0.0825, ret=-0.000351, glen=116, tlen=278, kl=0.000887, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=0.0884, ret=0.000805, glen=116, tlen=277, kl=0.000852, act_lr=8e-8, ent=1.9]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:12,  1.17it/s, pg=0.0884, ret=0.000805, glen=116, tlen=277, kl=0.000852, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.17it/s, pg=-0.0308, ret=6.35e-5, glen=95.1, tlen=256, kl=0.000897, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0308, ret=6.35e-5, glen=95.1, tlen=256, kl=0.000897, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.0798, ret=-0.000667, glen=120, tlen=281, kl=0.000873, act_lr=8e-8, ent=1.91]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.17it/s, pg=0.0798, ret=-0.000667, glen=120, tlen=281, kl=0.000873, act_lr=8e-8, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=-0.188, ret=0.000712, glen=97.3, tlen=258, kl=0.000892, act_lr=8e-8, ent=1.51]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.188, ret=0.000712, glen=97.3, tlen=258, kl=0.000892, act_lr=8e-8, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=-0.0296, ret=0.000783, glen=108, tlen=269, kl=0.000907, act_lr=8e-8, ent=1.62]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.17it/s, pg=-0.0296, ret=0.000783, glen=108, tlen=269, kl=0.000907, act_lr=8e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=0.0599, ret=-0.000454, glen=98.9, tlen=260, kl=0.000945, act_lr=8e-8, ent=1.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.17it/s, pg=0.0599, ret=-0.000454, glen=98.9, tlen=260, kl=0.000945, act_lr=8e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=-0.0551, ret=0.0086, glen=115, tlen=277, kl=0.000934, act_lr=8e-8, ent=2.12]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.18it/s, pg=-0.0551, ret=0.0086, glen=115, tlen=277, kl=0.000934, act_lr=8e-8, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.18it/s, pg=-0.0998, ret=0.000231, glen=108, tlen=268, kl=0.000911, act_lr=8e-8, ent=1.54]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.18it/s, pg=-0.0998, ret=0.000231, glen=108, tlen=268, kl=0.000911, act_lr=8e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.18it/s, pg=0.0715, ret=-0.00159, glen=108, tlen=269, kl=0.000924, act_lr=8e-8, ent=1.53] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:44<00:05,  1.18it/s, pg=0.0715, ret=-0.00159, glen=108, tlen=269, kl=0.000924, act_lr=8e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.18it/s, pg=-0.0443, ret=0.000486, glen=108, tlen=269, kl=0.00084, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.18it/s, pg=-0.0443, ret=0.000486, glen=108, tlen=269, kl=0.00084, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.18it/s, pg=-0.0603, ret=0.00058, glen=105, tlen=266, kl=0.000932, act_lr=8e-8, ent=1.64]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.18it/s, pg=-0.0603, ret=0.00058, glen=105, tlen=266, kl=0.000932, act_lr=8e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.18it/s, pg=0.134, ret=0.000693, glen=156, tlen=317, kl=0.000748, act_lr=8e-8, ent=2.34] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.16it/s, pg=0.134, ret=0.000693, glen=156, tlen=317, kl=0.000748, act_lr=8e-8, ent=2.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.16it/s, pg=0.141, ret=-0.000664, glen=113, tlen=273, kl=0.000916, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.16it/s, pg=0.141, ret=-0.000664, glen=113, tlen=273, kl=0.000916, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.16it/s, pg=0.0629, ret=-0.000506, glen=122, tlen=283, kl=0.000863, act_lr=8e-8, ent=1.75]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=0.0629, ret=-0.000506, glen=122, tlen=283, kl=0.000863, act_lr=8e-8, ent=1.75]
2025-07-24 16:24:30.303 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.0569, ret=0.0016, glen=116, tlen=277, kl=0.000896, act_lr=1e-7, ent=1.78]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.14it/s, pg=-0.0569, ret=0.0016, glen=116, tlen=277, kl=0.000896, act_lr=1e-7, ent=1.78]
2025-07-24 16:24:31.175 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-24 16:24:33.619 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.44s
2025-07-24 16:24:33.946 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.99s
2025-07-24 16:24:33.953 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.015775351688779634, 'actor_lr': 8.034482712854038e-08, 'clip_ratio': 0.0, 'entropy': 1.7241236361963996, 'kl': 0.0008834004402160645, 'response_length': 118.16882981925175, 'total_length': 278.961176115891, 'teacher_total_length': 291.66929363382275, 'return': 7.941008265459396e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:41<30:29, 228.74s/it]2025-07-24 16:24:33.997 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:27:03.390 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:27:03.580 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 16:27:03.581 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 149.58s
2025-07-24 16:27:05.690 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0079,avg_pass_at_n: 1.0000,avg_num_tokens: 112.2317,std_num_tokens: 149.0196,avg_correct_num_tokens: 101.6415,std_correct_num_tokens: 81.0229,avg_incorrect_num_tokens: 129.6903,std_incorrect_num_tokens: 217.9545
2025-07-24 16:27:06.131 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.55s
2025-07-24 16:27:09.389 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.25s
2025-07-24 16:27:38.368 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 16:27:38.369 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.98s
2025-07-24 16:27:39.785 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.01s
2025-07-24 16:27:39.786 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0008574851802940017, avg_kl: 0.0008850013632081153, avg_response_length: 116.88514386819848, avg_orm_score: 0.0, avg_custom_rewards: 0.0008574851802940017
2025-07-24 16:27:39.818 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter5_replay_buffer.jsonl
2025-07-24 16:27:41.721 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0393, ret=-0.00192, glen=107, tlen=267, kl=0.000883, act_lr=1e-7, ent=1.81]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.0393, ret=-0.00192, glen=107, tlen=267, kl=0.000883, act_lr=1e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.00902, ret=0.000127, glen=118, tlen=278, kl=0.000901, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.00902, ret=0.000127, glen=118, tlen=278, kl=0.000901, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.0243, ret=-0.00238, glen=96.6, tlen=256, kl=0.000892, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=0.0243, ret=-0.00238, glen=96.6, tlen=256, kl=0.000892, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=0.248, ret=-0.00191, glen=118, tlen=278, kl=0.000877, act_lr=1e-7, ent=2.01]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=0.248, ret=-0.00191, glen=118, tlen=278, kl=0.000877, act_lr=1e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=0.0458, ret=-0.00112, glen=140, tlen=301, kl=0.000861, act_lr=1e-7, ent=1.6]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.13it/s, pg=0.0458, ret=-0.00112, glen=140, tlen=301, kl=0.000861, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.13it/s, pg=0.038, ret=0.00013, glen=108, tlen=269, kl=0.000864, act_lr=1e-7, ent=1.7]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.14it/s, pg=0.038, ret=0.00013, glen=108, tlen=269, kl=0.000864, act_lr=1e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.14it/s, pg=-0.133, ret=0.00173, glen=121, tlen=281, kl=0.00088, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=-0.133, ret=0.00173, glen=121, tlen=281, kl=0.00088, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=-0.173, ret=0.00139, glen=111, tlen=272, kl=0.000902, act_lr=1e-7, ent=1.72]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.173, ret=0.00139, glen=111, tlen=272, kl=0.000902, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.075, ret=-0.000734, glen=111, tlen=271, kl=0.000881, act_lr=1e-7, ent=1.8]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.16it/s, pg=0.075, ret=-0.000734, glen=111, tlen=271, kl=0.000881, act_lr=1e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.16it/s, pg=0.0364, ret=-0.0013, glen=121, tlen=281, kl=0.000901, act_lr=1e-7, ent=1.63]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=0.0364, ret=-0.0013, glen=121, tlen=281, kl=0.000901, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.0686, ret=-2.51e-6, glen=116, tlen=277, kl=0.000928, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.15it/s, pg=-0.0686, ret=-2.51e-6, glen=116, tlen=277, kl=0.000928, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.15it/s, pg=-0.0676, ret=0.00107, glen=108, tlen=269, kl=0.000891, act_lr=1e-7, ent=1.68] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=-0.0676, ret=0.00107, glen=108, tlen=269, kl=0.000891, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.082, ret=0.000271, glen=126, tlen=286, kl=0.000856, act_lr=1e-7, ent=2.07]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.082, ret=0.000271, glen=126, tlen=286, kl=0.000856, act_lr=1e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.00671, ret=-0.000421, glen=94.4, tlen=255, kl=0.000852, act_lr=1e-7, ent=1.55]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=-0.00671, ret=-0.000421, glen=94.4, tlen=255, kl=0.000852, act_lr=1e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=-0.25, ret=0.000238, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.68]     Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.25, ret=0.000238, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=0.338, ret=4.2e-5, glen=353, tlen=514, kl=0.000729, act_lr=1e-7, ent=2.82]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.13it/s, pg=0.338, ret=4.2e-5, glen=353, tlen=514, kl=0.000729, act_lr=1e-7, ent=2.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.13it/s, pg=0.0494, ret=0.000201, glen=119, tlen=279, kl=0.000907, act_lr=1e-7, ent=1.92]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:35,  1.14it/s, pg=0.0494, ret=0.000201, glen=119, tlen=279, kl=0.000907, act_lr=1e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.122, ret=0.000526, glen=113, tlen=273, kl=0.000918, act_lr=1e-7, ent=1.61]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.122, ret=0.000526, glen=113, tlen=273, kl=0.000918, act_lr=1e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=0.101, ret=-0.00188, glen=114, tlen=275, kl=0.000894, act_lr=1e-7, ent=1.69] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.101, ret=-0.00188, glen=114, tlen=275, kl=0.000894, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0151, ret=0.00018, glen=125, tlen=285, kl=0.000873, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0151, ret=0.00018, glen=125, tlen=285, kl=0.000873, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.234, ret=0.000414, glen=113, tlen=273, kl=0.000904, act_lr=1e-7, ent=1.72]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.234, ret=0.000414, glen=113, tlen=273, kl=0.000904, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.049, ret=-0.000156, glen=106, tlen=266, kl=0.000912, act_lr=1e-7, ent=1.77]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.049, ret=-0.000156, glen=106, tlen=266, kl=0.000912, act_lr=1e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.0792, ret=0.000538, glen=106, tlen=267, kl=0.000945, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0792, ret=0.000538, glen=106, tlen=267, kl=0.000945, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0269, ret=7.95e-5, glen=110, tlen=271, kl=0.000875, act_lr=1e-7, ent=1.59]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0269, ret=7.95e-5, glen=110, tlen=271, kl=0.000875, act_lr=1e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.135, ret=-1.89e-5, glen=123, tlen=284, kl=0.000855, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.135, ret=-1.89e-5, glen=123, tlen=284, kl=0.000855, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0774, ret=0.00101, glen=103, tlen=263, kl=0.000931, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0774, ret=0.00101, glen=103, tlen=263, kl=0.000931, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0476, ret=-0.00123, glen=121, tlen=282, kl=0.000851, act_lr=1e-7, ent=1.85]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.0476, ret=-0.00123, glen=121, tlen=282, kl=0.000851, act_lr=1e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0303, ret=0.000846, glen=127, tlen=288, kl=0.000811, act_lr=1e-7, ent=2.13]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0303, ret=0.000846, glen=127, tlen=288, kl=0.000811, act_lr=1e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0804, ret=-0.00016, glen=98.8, tlen=259, kl=0.000901, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=-0.0804, ret=-0.00016, glen=98.8, tlen=259, kl=0.000901, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.0238, ret=-0.000203, glen=98.5, tlen=259, kl=0.000941, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=-0.0238, ret=-0.000203, glen=98.5, tlen=259, kl=0.000941, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.0546, ret=0.0011, glen=115, tlen=275, kl=0.000786, act_lr=1e-7, ent=1.43]     Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0546, ret=0.0011, glen=115, tlen=275, kl=0.000786, act_lr=1e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0159, ret=0.000236, glen=102, tlen=262, kl=0.00088, act_lr=1e-7, ent=1.59]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.14it/s, pg=0.0159, ret=0.000236, glen=102, tlen=262, kl=0.00088, act_lr=1e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.14it/s, pg=0.125, ret=-6.43e-5, glen=131, tlen=291, kl=0.0009, act_lr=1e-7, ent=1.81]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.14it/s, pg=0.125, ret=-6.43e-5, glen=131, tlen=291, kl=0.0009, act_lr=1e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.23, ret=0.0289, glen=93.6, tlen=255, kl=0.000889, act_lr=1e-7, ent=1.73]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.23, ret=0.0289, glen=93.6, tlen=255, kl=0.000889, act_lr=1e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=-0.0799, ret=0.000608, glen=109, tlen=269, kl=0.000916, act_lr=1e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.0799, ret=0.000608, glen=109, tlen=269, kl=0.000916, act_lr=1e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.0713, ret=0.000816, glen=120, tlen=281, kl=0.0009, act_lr=1e-7, ent=2.03]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0713, ret=0.000816, glen=120, tlen=281, kl=0.0009, act_lr=1e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=0.0663, ret=-0.00037, glen=112, tlen=273, kl=0.000896, act_lr=1e-7, ent=1.58]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=0.0663, ret=-0.00037, glen=112, tlen=273, kl=0.000896, act_lr=1e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.211, ret=0.00184, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.63] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.211, ret=0.00184, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.0505, ret=-0.00124, glen=115, tlen=276, kl=0.000849, act_lr=1e-7, ent=1.64]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=0.0505, ret=-0.00124, glen=115, tlen=276, kl=0.000849, act_lr=1e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0694, ret=0.00014, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.6] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0694, ret=0.00014, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0265, ret=-0.000634, glen=101, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.68]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0265, ret=-0.000634, glen=101, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.104, ret=0.000431, glen=117, tlen=277, kl=0.000908, act_lr=1e-7, ent=1.76] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.104, ret=0.000431, glen=117, tlen=277, kl=0.000908, act_lr=1e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=-0.0182, ret=-0.000597, glen=120, tlen=281, kl=0.000882, act_lr=1e-7, ent=1.94]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=-0.0182, ret=-0.000597, glen=120, tlen=281, kl=0.000882, act_lr=1e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.167, ret=0.000342, glen=101, tlen=261, kl=0.000909, act_lr=1e-7, ent=1.71]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=-0.167, ret=0.000342, glen=101, tlen=261, kl=0.000909, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=-0.149, ret=0.00203, glen=112, tlen=272, kl=0.000911, act_lr=1e-7, ent=1.67] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=-0.149, ret=0.00203, glen=112, tlen=272, kl=0.000911, act_lr=1e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.00372, ret=-0.00123, glen=104, tlen=264, kl=0.000896, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.18it/s, pg=0.00372, ret=-0.00123, glen=104, tlen=264, kl=0.000896, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.195, ret=0.0016, glen=101, tlen=261, kl=0.00093, act_lr=1e-7, ent=1.66]    Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.195, ret=0.0016, glen=101, tlen=261, kl=0.00093, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=0.214, ret=-0.00248, glen=154, tlen=314, kl=0.000893, act_lr=1e-7, ent=2.17]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.214, ret=-0.00248, glen=154, tlen=314, kl=0.000893, act_lr=1e-7, ent=2.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.195, ret=-0.00141, glen=123, tlen=284, kl=0.000898, act_lr=1e-7, ent=1.95]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.195, ret=-0.00141, glen=123, tlen=284, kl=0.000898, act_lr=1e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.000458, ret=-0.000291, glen=120, tlen=281, kl=0.000905, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.000458, ret=-0.000291, glen=120, tlen=281, kl=0.000905, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.224, ret=0.00203, glen=104, tlen=264, kl=0.000823, act_lr=1e-7, ent=1.89]     Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.224, ret=0.00203, glen=104, tlen=264, kl=0.000823, act_lr=1e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.218, ret=0.00191, glen=102, tlen=262, kl=0.000894, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.218, ret=0.00191, glen=102, tlen=262, kl=0.000894, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.105, ret=-0.0017, glen=103, tlen=263, kl=0.000896, act_lr=1e-7, ent=1.5]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.18it/s, pg=0.105, ret=-0.0017, glen=103, tlen=263, kl=0.000896, act_lr=1e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=-0.0441, ret=0.000576, glen=115, tlen=276, kl=0.000901, act_lr=1e-7, ent=1.71]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.18it/s, pg=-0.0441, ret=0.000576, glen=115, tlen=276, kl=0.000901, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.18it/s, pg=-0.0565, ret=-0.000726, glen=106, tlen=266, kl=0.000917, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.18it/s, pg=-0.0565, ret=-0.000726, glen=106, tlen=266, kl=0.000917, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.18it/s, pg=0.0109, ret=0.000953, glen=103, tlen=263, kl=0.000872, act_lr=1e-7, ent=1.69]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0109, ret=0.000953, glen=103, tlen=263, kl=0.000872, act_lr=1e-7, ent=1.69]
2025-07-24 16:28:31.524 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.39s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.123, ret=-0.000452, glen=125, tlen=286, kl=0.000888, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=0.123, ret=-0.000452, glen=125, tlen=286, kl=0.000888, act_lr=1.2e-7, ent=1.83]
2025-07-24 16:28:32.376 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 16:28:34.954 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 16:28:35.298 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.29s
2025-07-24 16:28:35.309 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01867935113739549, 'actor_lr': 1.0035087826596729e-07, 'clip_ratio': 0.0, 'entropy': 1.7527947906862225, 'kl': 0.0008854698716548451, 'response_length': 116.7514379400956, 'total_length': 277.13355670058934, 'teacher_total_length': 289.843635425233, 'return': 0.0004855551092636265, 'policy_update_steps': 1.0}
Episode [1/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:43<27:11, 233.03s/it]2025-07-24 16:28:35.353 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:29:59.237 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:29:59.416 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:29:59.417 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 84.06s
2025-07-24 16:30:01.723 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 108.2264,std_num_tokens: 106.4321,avg_correct_num_tokens: 101.3536,std_correct_num_tokens: 80.9237,avg_incorrect_num_tokens: 119.4747,std_incorrect_num_tokens: 137.6432
2025-07-24 16:30:02.195 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.78s
2025-07-24 16:30:05.160 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.96s
2025-07-24 16:30:33.561 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 16:30:33.562 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.40s
2025-07-24 16:30:35.031 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.00s
2025-07-24 16:30:35.032 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.00016510206292437665, avg_kl: 0.000943119750429162, avg_response_length: 109.35092505211253, avg_orm_score: 0.0, avg_custom_rewards: -0.00016510206292437665
2025-07-24 16:30:35.065 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter6_replay_buffer.jsonl
2025-07-24 16:30:36.896 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.83s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0025, ret=0.000914, glen=100, tlen=261, kl=0.000953, act_lr=1.2e-7, ent=1.87]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.0025, ret=0.000914, glen=100, tlen=261, kl=0.000953, act_lr=1.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.00173, ret=-0.00164, glen=118, tlen=279, kl=0.000913, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=-0.00173, ret=-0.00164, glen=118, tlen=279, kl=0.000913, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=0.0443, ret=-0.000604, glen=117, tlen=277, kl=0.000939, act_lr=1.2e-7, ent=1.75] Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=0.0443, ret=-0.000604, glen=117, tlen=277, kl=0.000939, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=0.102, ret=-0.000442, glen=107, tlen=267, kl=0.000919, act_lr=1.2e-7, ent=1.63] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=0.102, ret=-0.000442, glen=107, tlen=267, kl=0.000919, act_lr=1.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=0.00345, ret=-0.000125, glen=101, tlen=261, kl=0.000919, act_lr=1.2e-7, ent=1.92]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.00345, ret=-0.000125, glen=101, tlen=261, kl=0.000919, act_lr=1.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=-0.0453, ret=0.00017, glen=106, tlen=266, kl=0.000981, act_lr=1.2e-7, ent=1.55]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=-0.0453, ret=0.00017, glen=106, tlen=266, kl=0.000981, act_lr=1.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.0955, ret=0.000365, glen=99.6, tlen=260, kl=0.000933, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=-0.0955, ret=0.000365, glen=99.6, tlen=260, kl=0.000933, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.041, ret=0.000494, glen=132, tlen=292, kl=0.000901, act_lr=1.2e-7, ent=1.75]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.041, ret=0.000494, glen=132, tlen=292, kl=0.000901, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0201, ret=-0.000161, glen=107, tlen=267, kl=0.000984, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.0201, ret=-0.000161, glen=107, tlen=267, kl=0.000984, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0221, ret=0.000481, glen=104, tlen=265, kl=0.000927, act_lr=1.2e-7, ent=1.68] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0221, ret=0.000481, glen=104, tlen=265, kl=0.000927, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=-0.184, ret=0.000986, glen=105, tlen=265, kl=0.000944, act_lr=1.2e-7, ent=1.77] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=-0.184, ret=0.000986, glen=105, tlen=265, kl=0.000944, act_lr=1.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0591, ret=7.01e-5, glen=109, tlen=270, kl=0.000937, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0591, ret=7.01e-5, glen=109, tlen=270, kl=0.000937, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0573, ret=0.000754, glen=124, tlen=285, kl=0.000931, act_lr=1.2e-7, ent=2.08]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0573, ret=0.000754, glen=124, tlen=285, kl=0.000931, act_lr=1.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.116, ret=0.000816, glen=99.4, tlen=260, kl=0.000991, act_lr=1.2e-7, ent=1.6]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.116, ret=0.000816, glen=99.4, tlen=260, kl=0.000991, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=0.14, ret=-0.00184, glen=110, tlen=271, kl=0.000939, act_lr=1.2e-7, ent=1.68]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=0.14, ret=-0.00184, glen=110, tlen=271, kl=0.000939, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.05, ret=0.000554, glen=99.9, tlen=260, kl=0.000954, act_lr=1.2e-7, ent=1.71]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=-0.05, ret=0.000554, glen=99.9, tlen=260, kl=0.000954, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.129, ret=0.00148, glen=129, tlen=289, kl=0.000969, act_lr=1.2e-7, ent=1.93] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.129, ret=0.00148, glen=129, tlen=289, kl=0.000969, act_lr=1.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=-0.117, ret=0.00104, glen=98.2, tlen=259, kl=0.000943, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.117, ret=0.00104, glen=98.2, tlen=259, kl=0.000943, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.0033, ret=9.83e-5, glen=108, tlen=269, kl=0.00092, act_lr=1.2e-7, ent=1.76]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0033, ret=9.83e-5, glen=108, tlen=269, kl=0.00092, act_lr=1.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.135, ret=0.000641, glen=104, tlen=265, kl=0.00101, act_lr=1.2e-7, ent=1.59]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.16it/s, pg=-0.135, ret=0.000641, glen=104, tlen=265, kl=0.00101, act_lr=1.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.16it/s, pg=0.11, ret=-0.000124, glen=110, tlen=270, kl=0.000933, act_lr=1.2e-7, ent=1.82]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=0.11, ret=-0.000124, glen=110, tlen=270, kl=0.000933, act_lr=1.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=0.0835, ret=-0.0011, glen=101, tlen=262, kl=0.00097, act_lr=1.2e-7, ent=1.6]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=0.0835, ret=-0.0011, glen=101, tlen=262, kl=0.00097, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.173, ret=0.00124, glen=101, tlen=262, kl=0.000942, act_lr=1.2e-7, ent=1.59]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.173, ret=0.00124, glen=101, tlen=262, kl=0.000942, act_lr=1.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.173, ret=0.000238, glen=101, tlen=261, kl=0.000931, act_lr=1.2e-7, ent=1.73]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.173, ret=0.000238, glen=101, tlen=261, kl=0.000931, act_lr=1.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.00574, ret=0.000516, glen=120, tlen=281, kl=0.000922, act_lr=1.2e-7, ent=1.95]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.00574, ret=0.000516, glen=120, tlen=281, kl=0.000922, act_lr=1.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.105, ret=6.01e-5, glen=134, tlen=295, kl=0.00092, act_lr=1.2e-7, ent=1.94]    Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.105, ret=6.01e-5, glen=134, tlen=295, kl=0.00092, act_lr=1.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.205, ret=-0.00114, glen=117, tlen=278, kl=0.000961, act_lr=1.2e-7, ent=1.79]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.205, ret=-0.00114, glen=117, tlen=278, kl=0.000961, act_lr=1.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.115, ret=0.000597, glen=95.7, tlen=256, kl=0.000937, act_lr=1.2e-7, ent=1.72]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:24,  1.15it/s, pg=-0.115, ret=0.000597, glen=95.7, tlen=256, kl=0.000937, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:24,  1.15it/s, pg=-0.0523, ret=-0.000468, glen=110, tlen=271, kl=0.000978, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.06it/s, pg=-0.0523, ret=-0.000468, glen=110, tlen=271, kl=0.000978, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.06it/s, pg=0.0282, ret=-0.000835, glen=99.3, tlen=259, kl=0.000975, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.09it/s, pg=0.0282, ret=-0.000835, glen=99.3, tlen=259, kl=0.000975, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.09it/s, pg=-0.0337, ret=0.00046, glen=108, tlen=269, kl=0.00094, act_lr=1.2e-7, ent=1.71]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.09it/s, pg=-0.0337, ret=0.00046, glen=108, tlen=269, kl=0.00094, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.09it/s, pg=0.122, ret=-0.0013, glen=128, tlen=289, kl=0.00095, act_lr=1.2e-7, ent=2.01]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.12it/s, pg=0.122, ret=-0.0013, glen=128, tlen=289, kl=0.00095, act_lr=1.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=0.0417, ret=-0.000591, glen=112, tlen=273, kl=0.000932, act_lr=1.2e-7, ent=1.78]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.13it/s, pg=0.0417, ret=-0.000591, glen=112, tlen=273, kl=0.000932, act_lr=1.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.13it/s, pg=0.0226, ret=-6.05e-5, glen=104, tlen=264, kl=0.000957, act_lr=1.2e-7, ent=1.69] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.14it/s, pg=0.0226, ret=-6.05e-5, glen=104, tlen=264, kl=0.000957, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.14it/s, pg=0.0665, ret=-0.00216, glen=110, tlen=270, kl=0.000896, act_lr=1.2e-7, ent=1.61]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.15it/s, pg=0.0665, ret=-0.00216, glen=110, tlen=270, kl=0.000896, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.15it/s, pg=0.175, ret=-0.000352, glen=123, tlen=283, kl=0.00091, act_lr=1.2e-7, ent=1.54] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.175, ret=-0.000352, glen=123, tlen=283, kl=0.00091, act_lr=1.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.103, ret=0.000979, glen=102, tlen=262, kl=0.000901, act_lr=1.2e-7, ent=1.64]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.103, ret=0.000979, glen=102, tlen=262, kl=0.000901, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.0288, ret=0.00044, glen=104, tlen=264, kl=0.000969, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.0288, ret=0.00044, glen=104, tlen=264, kl=0.000969, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.277, ret=-0.000879, glen=118, tlen=279, kl=0.000906, act_lr=1.2e-7, ent=2.15]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.277, ret=-0.000879, glen=118, tlen=279, kl=0.000906, act_lr=1.2e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0346, ret=0.000369, glen=104, tlen=265, kl=0.000946, act_lr=1.2e-7, ent=1.62]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0346, ret=0.000369, glen=104, tlen=265, kl=0.000946, act_lr=1.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0407, ret=0.00145, glen=103, tlen=263, kl=0.000937, act_lr=1.2e-7, ent=1.61] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.0407, ret=0.00145, glen=103, tlen=263, kl=0.000937, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.0691, ret=1.65e-5, glen=105, tlen=265, kl=0.000992, act_lr=1.2e-7, ent=1.66] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.0691, ret=1.65e-5, glen=105, tlen=265, kl=0.000992, act_lr=1.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.215, ret=0.00128, glen=114, tlen=274, kl=0.000937, act_lr=1.2e-7, ent=1.64]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.18it/s, pg=-0.215, ret=0.00128, glen=114, tlen=274, kl=0.000937, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.18it/s, pg=0.158, ret=-0.0014, glen=125, tlen=285, kl=0.000907, act_lr=1.2e-7, ent=1.84] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.18it/s, pg=0.158, ret=-0.0014, glen=125, tlen=285, kl=0.000907, act_lr=1.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.18it/s, pg=-0.0244, ret=-0.000285, glen=101, tlen=261, kl=0.000997, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.18it/s, pg=-0.0244, ret=-0.000285, glen=101, tlen=261, kl=0.000997, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.101, ret=-0.000606, glen=109, tlen=270, kl=0.000928, act_lr=1.2e-7, ent=1.69] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.18it/s, pg=-0.101, ret=-0.000606, glen=109, tlen=270, kl=0.000928, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.18it/s, pg=-0.025, ret=0.00108, glen=101, tlen=261, kl=0.000947, act_lr=1.2e-7, ent=1.64]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.18it/s, pg=-0.025, ret=0.00108, glen=101, tlen=261, kl=0.000947, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.18it/s, pg=-0.0969, ret=0.00216, glen=98.2, tlen=259, kl=0.000931, act_lr=1.2e-7, ent=1.8]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=-0.0969, ret=0.00216, glen=98.2, tlen=259, kl=0.000931, act_lr=1.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.138, ret=0.000337, glen=114, tlen=274, kl=0.000929, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.138, ret=0.000337, glen=114, tlen=274, kl=0.000929, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=0.0835, ret=-0.00118, glen=117, tlen=277, kl=0.000963, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.18it/s, pg=0.0835, ret=-0.00118, glen=117, tlen=277, kl=0.000963, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.18it/s, pg=-0.0207, ret=0.000118, glen=108, tlen=268, kl=0.000925, act_lr=1.2e-7, ent=1.61]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.0207, ret=0.000118, glen=108, tlen=268, kl=0.000925, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=0.123, ret=-0.000666, glen=109, tlen=269, kl=0.000966, act_lr=1.2e-7, ent=1.71] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.18it/s, pg=0.123, ret=-0.000666, glen=109, tlen=269, kl=0.000966, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.18it/s, pg=-0.0159, ret=-0.00061, glen=115, tlen=276, kl=0.000943, act_lr=1.2e-7, ent=1.78]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.0159, ret=-0.00061, glen=115, tlen=276, kl=0.000943, act_lr=1.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0294, ret=0.000539, glen=99.7, tlen=260, kl=0.000939, act_lr=1.2e-7, ent=1.55]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0294, ret=0.000539, glen=99.7, tlen=260, kl=0.000939, act_lr=1.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0861, ret=-0.00142, glen=118, tlen=278, kl=0.000899, act_lr=1.2e-7, ent=1.7]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.0861, ret=-0.00142, glen=118, tlen=278, kl=0.000899, act_lr=1.2e-7, ent=1.7]
2025-07-24 16:31:25.535 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.45s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.0397, ret=-0.000993, glen=106, tlen=267, kl=0.000998, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.0397, ret=-0.000993, glen=106, tlen=267, kl=0.000998, act_lr=1.4e-7, ent=1.82]
2025-07-24 16:31:26.394 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 16:31:28.969 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 16:31:29.294 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.33s
2025-07-24 16:31:29.301 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.006455455507550921, 'actor_lr': 1.203571386046828e-07, 'clip_ratio': 0.0, 'entropy': 1.7342252433300018, 'kl': 0.0009432690484183175, 'response_length': 109.31775610787528, 'total_length': 269.7792137690953, 'teacher_total_length': 281.8311091831752, 'return': -4.152664457381304e-06, 'policy_update_steps': 1.0}
Episode [1/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [25:37<21:22, 213.73s/it]2025-07-24 16:31:29.328 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:33:05.119 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:33:05.300 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:33:05.301 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 95.97s
2025-07-24 16:33:07.358 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0112,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6497,std_num_tokens: 116.1839,avg_correct_num_tokens: 105.9321,std_correct_num_tokens: 86.2368,avg_incorrect_num_tokens: 128.7628,std_incorrect_num_tokens: 151.6254
2025-07-24 16:33:07.688 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.39s
2025-07-24 16:33:10.874 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 16:33:39.737 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 228
2025-07-24 16:33:39.738 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.86s
2025-07-24 16:33:41.246 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.07s
2025-07-24 16:33:41.246 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008591915036065569, avg_kl: 0.0009288536874871505, avg_response_length: 116.01782658225612, avg_orm_score: 0.0, avg_custom_rewards: -0.0008591915036065569
2025-07-24 16:33:41.288 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter7_replay_buffer.jsonl
2025-07-24 16:33:43.242 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.96s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.0303, ret=-0.000445, glen=108, tlen=269, kl=0.000888, act_lr=1.4e-7, ent=1.61]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.0303, ret=-0.000445, glen=108, tlen=269, kl=0.000888, act_lr=1.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.156, ret=-0.000947, glen=115, tlen=275, kl=0.000926, act_lr=1.4e-7, ent=1.67]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.156, ret=-0.000947, glen=115, tlen=275, kl=0.000926, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.00891, ret=-3.33e-5, glen=106, tlen=266, kl=0.000901, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=0.00891, ret=-3.33e-5, glen=106, tlen=266, kl=0.000901, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.209, ret=0.0018, glen=109, tlen=269, kl=0.000939, act_lr=1.4e-7, ent=1.79]   Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.209, ret=0.0018, glen=109, tlen=269, kl=0.000939, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=0.0459, ret=-0.00124, glen=108, tlen=269, kl=0.000956, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.12it/s, pg=0.0459, ret=-0.00124, glen=108, tlen=269, kl=0.000956, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.12it/s, pg=0.0549, ret=-0.00106, glen=119, tlen=279, kl=0.000936, act_lr=1.4e-7, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.13it/s, pg=0.0549, ret=-0.00106, glen=119, tlen=279, kl=0.000936, act_lr=1.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.13it/s, pg=0.0494, ret=-0.00189, glen=124, tlen=285, kl=0.000936, act_lr=1.4e-7, ent=1.8] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=0.0494, ret=-0.00189, glen=124, tlen=285, kl=0.000936, act_lr=1.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=0.0083, ret=-0.0017, glen=126, tlen=287, kl=0.000961, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=0.0083, ret=-0.0017, glen=126, tlen=287, kl=0.000961, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=-0.00143, ret=0.000171, glen=107, tlen=267, kl=0.000928, act_lr=1.4e-7, ent=1.58]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.16it/s, pg=-0.00143, ret=0.000171, glen=107, tlen=267, kl=0.000928, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.16it/s, pg=-0.0471, ret=0.000764, glen=114, tlen=274, kl=0.000972, act_lr=1.4e-7, ent=1.78] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.0471, ret=0.000764, glen=114, tlen=274, kl=0.000972, act_lr=1.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.0348, ret=0.000309, glen=107, tlen=268, kl=0.000876, act_lr=1.4e-7, ent=1.65] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.0348, ret=0.000309, glen=107, tlen=268, kl=0.000876, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=0.0833, ret=-0.00116, glen=119, tlen=279, kl=0.000929, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=0.0833, ret=-0.00116, glen=119, tlen=279, kl=0.000929, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.152, ret=0.00148, glen=117, tlen=277, kl=0.00094, act_lr=1.4e-7, ent=1.74]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=-0.152, ret=0.00148, glen=117, tlen=277, kl=0.00094, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=0.209, ret=-0.00144, glen=142, tlen=302, kl=0.000803, act_lr=1.4e-7, ent=1.46]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=0.209, ret=-0.00144, glen=142, tlen=302, kl=0.000803, act_lr=1.4e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0901, ret=-0.00105, glen=126, tlen=286, kl=0.00094, act_lr=1.4e-7, ent=1.84]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=0.0901, ret=-0.00105, glen=126, tlen=286, kl=0.00094, act_lr=1.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=-0.0391, ret=-0.000464, glen=104, tlen=264, kl=0.000998, act_lr=1.4e-7, ent=1.7]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0391, ret=-0.000464, glen=104, tlen=264, kl=0.000998, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.1, ret=0.000556, glen=120, tlen=280, kl=0.000887, act_lr=1.4e-7, ent=1.77]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.1, ret=0.000556, glen=120, tlen=280, kl=0.000887, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.175, ret=0.00107, glen=104, tlen=265, kl=0.000903, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.175, ret=0.00107, glen=104, tlen=265, kl=0.000903, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.156, ret=0.000737, glen=113, tlen=274, kl=0.00091, act_lr=1.4e-7, ent=1.6] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.156, ret=0.000737, glen=113, tlen=274, kl=0.00091, act_lr=1.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.22, ret=-0.00184, glen=136, tlen=296, kl=0.000958, act_lr=1.4e-7, ent=2.18]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.22, ret=-0.00184, glen=136, tlen=296, kl=0.000958, act_lr=1.4e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.124, ret=0.00102, glen=121, tlen=282, kl=0.000892, act_lr=1.4e-7, ent=1.89]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.124, ret=0.00102, glen=121, tlen=282, kl=0.000892, act_lr=1.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.00397, ret=-0.00198, glen=112, tlen=272, kl=0.000965, act_lr=1.4e-7, ent=1.64]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.00397, ret=-0.00198, glen=112, tlen=272, kl=0.000965, act_lr=1.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0125, ret=-0.000189, glen=117, tlen=278, kl=0.000898, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.17it/s, pg=0.0125, ret=-0.000189, glen=117, tlen=278, kl=0.000898, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.17it/s, pg=-0.171, ret=0.000803, glen=116, tlen=276, kl=0.000919, act_lr=1.4e-7, ent=1.77] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.171, ret=0.000803, glen=116, tlen=276, kl=0.000919, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.241, ret=-0.00267, glen=115, tlen=275, kl=0.00094, act_lr=1.4e-7, ent=1.69]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.241, ret=-0.00267, glen=115, tlen=275, kl=0.00094, act_lr=1.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.0515, ret=-5.73e-5, glen=112, tlen=272, kl=0.000903, act_lr=1.4e-7, ent=1.63]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.0515, ret=-5.73e-5, glen=112, tlen=272, kl=0.000903, act_lr=1.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.071, ret=-0.000173, glen=114, tlen=274, kl=0.000935, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=-0.071, ret=-0.000173, glen=114, tlen=274, kl=0.000935, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=0.0312, ret=-0.000752, glen=123, tlen=283, kl=0.000926, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0312, ret=-0.000752, glen=123, tlen=283, kl=0.000926, act_lr=1.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0782, ret=0.000848, glen=106, tlen=267, kl=0.000965, act_lr=1.4e-7, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=-0.0782, ret=0.000848, glen=106, tlen=267, kl=0.000965, act_lr=1.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=0.0155, ret=-0.000318, glen=110, tlen=270, kl=0.000912, act_lr=1.4e-7, ent=1.62]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=0.0155, ret=-0.000318, glen=110, tlen=270, kl=0.000912, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.0761, ret=-0.000297, glen=118, tlen=278, kl=0.000931, act_lr=1.4e-7, ent=1.64]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0761, ret=-0.000297, glen=118, tlen=278, kl=0.000931, act_lr=1.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.162, ret=-0.00244, glen=114, tlen=274, kl=0.000969, act_lr=1.4e-7, ent=1.62]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.14it/s, pg=0.162, ret=-0.00244, glen=114, tlen=274, kl=0.000969, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.14it/s, pg=-0.00757, ret=-0.000327, glen=125, tlen=286, kl=0.000869, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=-0.00757, ret=-0.000327, glen=125, tlen=286, kl=0.000869, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=0.212, ret=-0.00124, glen=109, tlen=270, kl=0.000921, act_lr=1.4e-7, ent=1.72]    Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=0.212, ret=-0.00124, glen=109, tlen=270, kl=0.000921, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=-0.09, ret=0.00143, glen=117, tlen=277, kl=0.000926, act_lr=1.4e-7, ent=1.74] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.09, ret=0.00143, glen=117, tlen=277, kl=0.000926, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.131, ret=5.03e-5, glen=147, tlen=308, kl=0.000883, act_lr=1.4e-7, ent=2.02]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=0.131, ret=5.03e-5, glen=147, tlen=308, kl=0.000883, act_lr=1.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.179, ret=0.00152, glen=106, tlen=266, kl=0.00092, act_lr=1.4e-7, ent=1.67]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.179, ret=0.00152, glen=106, tlen=266, kl=0.00092, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.000244, ret=8.56e-5, glen=113, tlen=274, kl=0.000953, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:32<00:16,  1.17it/s, pg=-0.000244, ret=8.56e-5, glen=113, tlen=274, kl=0.000953, act_lr=1.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.149, ret=0.0015, glen=101, tlen=261, kl=0.000969, act_lr=1.4e-7, ent=1.58]    Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.149, ret=0.0015, glen=101, tlen=261, kl=0.000969, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.22, ret=0.00216, glen=124, tlen=283, kl=0.000974, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.22, ret=0.00216, glen=124, tlen=283, kl=0.000974, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0248, ret=0.000524, glen=118, tlen=279, kl=0.000932, act_lr=1.4e-7, ent=1.71]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0248, ret=0.000524, glen=118, tlen=279, kl=0.000932, act_lr=1.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0811, ret=0.000216, glen=124, tlen=285, kl=0.000938, act_lr=1.4e-7, ent=1.58]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0811, ret=0.000216, glen=124, tlen=285, kl=0.000938, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.084, ret=0.000508, glen=105, tlen=266, kl=0.000934, act_lr=1.4e-7, ent=1.7] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.084, ret=0.000508, glen=105, tlen=266, kl=0.000934, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.201, ret=0.00257, glen=104, tlen=264, kl=0.000941, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.201, ret=0.00257, glen=104, tlen=264, kl=0.000941, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.161, ret=-0.00166, glen=114, tlen=275, kl=0.000942, act_lr=1.4e-7, ent=1.75]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=0.161, ret=-0.00166, glen=114, tlen=275, kl=0.000942, act_lr=1.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=0.017, ret=-0.000343, glen=118, tlen=279, kl=0.000933, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=0.017, ret=-0.000343, glen=118, tlen=279, kl=0.000933, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.105, ret=-0.0001, glen=131, tlen=292, kl=0.000915, act_lr=1.4e-7, ent=1.76]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.105, ret=-0.0001, glen=131, tlen=292, kl=0.000915, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.0308, ret=0.000512, glen=113, tlen=274, kl=0.000984, act_lr=1.4e-7, ent=1.83]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.0308, ret=0.000512, glen=113, tlen=274, kl=0.000984, act_lr=1.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.318, ret=0.00119, glen=98.4, tlen=259, kl=0.000909, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.318, ret=0.00119, glen=98.4, tlen=259, kl=0.000909, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=0.162, ret=-0.00163, glen=125, tlen=286, kl=0.000945, act_lr=1.4e-7, ent=2.02] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.18it/s, pg=0.162, ret=-0.00163, glen=125, tlen=286, kl=0.000945, act_lr=1.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.18it/s, pg=-0.0259, ret=0.00063, glen=115, tlen=275, kl=0.000906, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.0259, ret=0.00063, glen=115, tlen=275, kl=0.000906, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.106, ret=-0.000578, glen=122, tlen=282, kl=0.000878, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.17it/s, pg=0.106, ret=-0.000578, glen=122, tlen=282, kl=0.000878, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0309, ret=-0.000328, glen=104, tlen=264, kl=0.000916, act_lr=1.4e-7, ent=1.51]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0309, ret=-0.000328, glen=104, tlen=264, kl=0.000916, act_lr=1.4e-7, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.046, ret=-0.000683, glen=110, tlen=271, kl=0.000943, act_lr=1.4e-7, ent=1.79]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.15it/s, pg=0.046, ret=-0.000683, glen=110, tlen=271, kl=0.000943, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.15it/s, pg=-0.169, ret=0.000447, glen=126, tlen=286, kl=0.000958, act_lr=1.4e-7, ent=1.87]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.16it/s, pg=-0.169, ret=0.000447, glen=126, tlen=286, kl=0.000958, act_lr=1.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.16it/s, pg=-0.0964, ret=0.000802, glen=107, tlen=268, kl=0.000969, act_lr=1.4e-7, ent=1.74]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.16it/s, pg=-0.0964, ret=0.000802, glen=107, tlen=268, kl=0.000969, act_lr=1.4e-7, ent=1.74]
2025-07-24 16:34:32.772 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.34s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.16it/s, pg=-0.107, ret=0.00246, glen=134, tlen=294, kl=0.000942, act_lr=1.6e-7, ent=1.87]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=-0.107, ret=0.00246, glen=134, tlen=294, kl=0.000942, act_lr=1.6e-7, ent=1.87]
2025-07-24 16:34:33.586 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.74s
2025-07-24 16:34:36.209 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.62s
2025-07-24 16:34:36.542 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.24s
2025-07-24 16:34:36.549 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.006998446949741296, 'actor_lr': 1.403508745948114e-07, 'clip_ratio': 0.0, 'entropy': 1.7287561015078896, 'kl': 0.0009288536874871505, 'response_length': 116.01782654879386, 'total_length': 276.4504688999109, 'teacher_total_length': 288.38180488452576, 'return': -5.060130525523339e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [28:44<17:06, 205.30s/it]2025-07-24 16:34:36.594 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:37:20.213 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:37:20.385 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:37:20.386 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 163.79s
2025-07-24 16:37:22.438 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 117.0878,std_num_tokens: 178.9064,avg_correct_num_tokens: 106.5291,std_correct_num_tokens: 138.5686,avg_incorrect_num_tokens: 132.9484,std_incorrect_num_tokens: 225.4444
2025-07-24 16:37:22.886 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.50s
2025-07-24 16:37:26.296 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.41s
2025-07-24 16:37:55.841 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 231
2025-07-24 16:37:55.843 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.54s
2025-07-24 16:37:57.408 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.17s
2025-07-24 16:37:57.409 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0001567989220899163, avg_kl: 0.0009046550436969444, avg_response_length: 124.27245621454148, avg_orm_score: 0.0, avg_custom_rewards: 0.0001567989220899163
2025-07-24 16:37:57.468 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter8_replay_buffer.jsonl
2025-07-24 16:37:59.484 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 2.02s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.0107, ret=0.000161, glen=104, tlen=264, kl=0.000959, act_lr=1.6e-7, ent=1.62]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.0107, ret=0.000161, glen=104, tlen=264, kl=0.000959, act_lr=1.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.22, ret=0.00211, glen=106, tlen=266, kl=0.000923, act_lr=1.6e-7, ent=1.69]   Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:51,  1.08it/s, pg=-0.22, ret=0.00211, glen=106, tlen=266, kl=0.000923, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:51,  1.08it/s, pg=-0.16, ret=0.000603, glen=115, tlen=274, kl=0.000951, act_lr=1.6e-7, ent=1.87]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.12it/s, pg=-0.16, ret=0.000603, glen=115, tlen=274, kl=0.000951, act_lr=1.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.12it/s, pg=-0.183, ret=2.14e-6, glen=108, tlen=268, kl=0.000919, act_lr=1.6e-7, ent=1.76]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:48,  1.12it/s, pg=-0.183, ret=2.14e-6, glen=108, tlen=268, kl=0.000919, act_lr=1.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:48,  1.12it/s, pg=0.0267, ret=0.00127, glen=134, tlen=295, kl=0.0009, act_lr=1.6e-7, ent=2.15]  Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:47,  1.11it/s, pg=0.0267, ret=0.00127, glen=134, tlen=295, kl=0.0009, act_lr=1.6e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:47,  1.11it/s, pg=0.195, ret=-0.00178, glen=119, tlen=279, kl=0.000922, act_lr=1.6e-7, ent=1.67]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:46,  1.13it/s, pg=0.195, ret=-0.00178, glen=119, tlen=279, kl=0.000922, act_lr=1.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:46,  1.13it/s, pg=0.00659, ret=-5.91e-5, glen=125, tlen=285, kl=0.000896, act_lr=1.6e-7, ent=1.76]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:44,  1.14it/s, pg=0.00659, ret=-5.91e-5, glen=125, tlen=285, kl=0.000896, act_lr=1.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:44,  1.14it/s, pg=0.0305, ret=-0.000431, glen=105, tlen=265, kl=0.00092, act_lr=1.6e-7, ent=1.81] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=0.0305, ret=-0.000431, glen=105, tlen=265, kl=0.00092, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:44,  1.13it/s, pg=0.234, ret=-0.000956, glen=175, tlen=335, kl=0.00068, act_lr=1.6e-7, ent=1.43] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.13it/s, pg=0.234, ret=-0.000956, glen=175, tlen=335, kl=0.00068, act_lr=1.6e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.13it/s, pg=-0.129, ret=0.0026, glen=262, tlen=422, kl=0.00078, act_lr=1.6e-7, ent=2.04]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:43,  1.12it/s, pg=-0.129, ret=0.0026, glen=262, tlen=422, kl=0.00078, act_lr=1.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:43,  1.12it/s, pg=-0.149, ret=0.000181, glen=114, tlen=275, kl=0.000896, act_lr=1.6e-7, ent=1.66]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:41,  1.13it/s, pg=-0.149, ret=0.000181, glen=114, tlen=275, kl=0.000896, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:41,  1.13it/s, pg=0.0181, ret=-0.000662, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.69]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:40,  1.14it/s, pg=0.0181, ret=-0.000662, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.14it/s, pg=-0.0212, ret=-0.000789, glen=131, tlen=291, kl=0.000903, act_lr=1.6e-7, ent=1.92]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:39,  1.15it/s, pg=-0.0212, ret=-0.000789, glen=131, tlen=291, kl=0.000903, act_lr=1.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:39,  1.15it/s, pg=-0.0698, ret=0.000806, glen=121, tlen=281, kl=0.000921, act_lr=1.6e-7, ent=1.83] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.16it/s, pg=-0.0698, ret=0.000806, glen=121, tlen=281, kl=0.000921, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.16it/s, pg=0.114, ret=-0.00101, glen=124, tlen=285, kl=0.000932, act_lr=1.6e-7, ent=1.77]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.16it/s, pg=0.114, ret=-0.00101, glen=124, tlen=285, kl=0.000932, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:36,  1.16it/s, pg=-0.0747, ret=0.00157, glen=139, tlen=299, kl=0.000863, act_lr=1.6e-7, ent=2.13]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.0747, ret=0.00157, glen=139, tlen=299, kl=0.000863, act_lr=1.6e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.141, ret=0.00161, glen=127, tlen=287, kl=0.000916, act_lr=1.6e-7, ent=1.78] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.141, ret=0.00161, glen=127, tlen=287, kl=0.000916, act_lr=1.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=-0.164, ret=0.00207, glen=92.1, tlen=252, kl=0.000936, act_lr=1.6e-7, ent=1.63]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=-0.164, ret=0.00207, glen=92.1, tlen=252, kl=0.000936, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.143, ret=0.00126, glen=121, tlen=281, kl=0.000937, act_lr=1.6e-7, ent=1.65] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=-0.143, ret=0.00126, glen=121, tlen=281, kl=0.000937, act_lr=1.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.17, ret=-0.00114, glen=130, tlen=291, kl=0.000927, act_lr=1.6e-7, ent=1.9]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.17, ret=-0.00114, glen=130, tlen=291, kl=0.000927, act_lr=1.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=0.0278, ret=-0.00047, glen=107, tlen=267, kl=0.000916, act_lr=1.6e-7, ent=1.63]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=0.0278, ret=-0.00047, glen=107, tlen=267, kl=0.000916, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=0.0596, ret=-0.000442, glen=102, tlen=262, kl=0.000951, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=0.0596, ret=-0.000442, glen=102, tlen=262, kl=0.000951, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.17it/s, pg=-0.108, ret=-0.000339, glen=107, tlen=267, kl=0.000957, act_lr=1.6e-7, ent=1.68]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:30,  1.16it/s, pg=-0.108, ret=-0.000339, glen=107, tlen=267, kl=0.000957, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:30,  1.16it/s, pg=0.0728, ret=1.22e-5, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.79]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.16it/s, pg=0.0728, ret=1.22e-5, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.16it/s, pg=-0.234, ret=0.00178, glen=114, tlen=274, kl=0.000922, act_lr=1.6e-7, ent=1.68]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.17it/s, pg=-0.234, ret=0.00178, glen=114, tlen=274, kl=0.000922, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.17it/s, pg=0.0123, ret=-0.000921, glen=103, tlen=263, kl=0.000914, act_lr=1.6e-7, ent=1.62]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.17it/s, pg=0.0123, ret=-0.000921, glen=103, tlen=263, kl=0.000914, act_lr=1.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.17it/s, pg=0.0591, ret=0.000343, glen=128, tlen=289, kl=0.000909, act_lr=1.6e-7, ent=1.81] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.17it/s, pg=0.0591, ret=0.000343, glen=128, tlen=289, kl=0.000909, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.304, ret=-0.00378, glen=352, tlen=512, kl=0.00068, act_lr=1.6e-7, ent=1.55]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:26,  1.14it/s, pg=0.304, ret=-0.00378, glen=352, tlen=512, kl=0.00068, act_lr=1.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:26,  1.14it/s, pg=-0.114, ret=0.000583, glen=98.5, tlen=258, kl=0.00094, act_lr=1.6e-7, ent=1.75]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.05it/s, pg=-0.114, ret=0.000583, glen=98.5, tlen=258, kl=0.00094, act_lr=1.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.05it/s, pg=-0.0344, ret=0.00102, glen=126, tlen=286, kl=0.000917, act_lr=1.6e-7, ent=1.69]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:26,  1.07it/s, pg=-0.0344, ret=0.00102, glen=126, tlen=286, kl=0.000917, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:26,  1.07it/s, pg=0.0116, ret=-0.000544, glen=144, tlen=304, kl=0.000899, act_lr=1.6e-7, ent=1.64]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.10it/s, pg=0.0116, ret=-0.000544, glen=144, tlen=304, kl=0.000899, act_lr=1.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.10it/s, pg=0.128, ret=-0.000817, glen=154, tlen=314, kl=0.00088, act_lr=1.6e-7, ent=2.21]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:23,  1.11it/s, pg=0.128, ret=-0.000817, glen=154, tlen=314, kl=0.00088, act_lr=1.6e-7, ent=2.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:23,  1.11it/s, pg=-0.212, ret=0.00158, glen=93.9, tlen=254, kl=0.000908, act_lr=1.6e-7, ent=1.57]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:22,  1.13it/s, pg=-0.212, ret=0.00158, glen=93.9, tlen=254, kl=0.000908, act_lr=1.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:22,  1.13it/s, pg=-0.028, ret=-0.00102, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.85]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.14it/s, pg=-0.028, ret=-0.00102, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.14it/s, pg=0.116, ret=-0.00131, glen=109, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.68] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.15it/s, pg=0.116, ret=-0.00131, glen=109, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.15it/s, pg=0.0752, ret=-9.92e-5, glen=120, tlen=280, kl=0.000905, act_lr=1.6e-7, ent=1.71]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:19,  1.16it/s, pg=0.0752, ret=-9.92e-5, glen=120, tlen=280, kl=0.000905, act_lr=1.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:19,  1.16it/s, pg=-0.105, ret=-0.000179, glen=118, tlen=278, kl=0.000948, act_lr=1.6e-7, ent=1.72]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.14it/s, pg=-0.105, ret=-0.000179, glen=118, tlen=278, kl=0.000948, act_lr=1.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.14it/s, pg=-0.00171, ret=0.000523, glen=112, tlen=273, kl=0.00096, act_lr=1.6e-7, ent=1.91]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.15it/s, pg=-0.00171, ret=0.000523, glen=112, tlen=273, kl=0.00096, act_lr=1.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.15it/s, pg=-0.19, ret=0.000197, glen=114, tlen=274, kl=0.000903, act_lr=1.6e-7, ent=1.81]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.16it/s, pg=-0.19, ret=0.000197, glen=114, tlen=274, kl=0.000903, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.16it/s, pg=0.0858, ret=-0.000674, glen=115, tlen=275, kl=0.000908, act_lr=1.6e-7, ent=1.88]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=0.0858, ret=-0.000674, glen=115, tlen=275, kl=0.000908, act_lr=1.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=-0.0663, ret=0.000468, glen=97.9, tlen=258, kl=0.000859, act_lr=1.6e-7, ent=1.55]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0663, ret=0.000468, glen=97.9, tlen=258, kl=0.000859, act_lr=1.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.0156, ret=4.56e-5, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.98]   Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=0.0156, ret=4.56e-5, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=-0.0187, ret=-0.00026, glen=128, tlen=289, kl=0.000949, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=-0.0187, ret=-0.00026, glen=128, tlen=289, kl=0.000949, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=-0.119, ret=0.000105, glen=115, tlen=275, kl=0.000877, act_lr=1.6e-7, ent=1.66] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=-0.119, ret=0.000105, glen=115, tlen=275, kl=0.000877, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.107, ret=0.000863, glen=148, tlen=308, kl=0.00092, act_lr=1.6e-7, ent=1.95]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.107, ret=0.000863, glen=148, tlen=308, kl=0.00092, act_lr=1.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=0.171, ret=-0.00138, glen=116, tlen=276, kl=0.000943, act_lr=1.6e-7, ent=1.96]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=0.171, ret=-0.00138, glen=116, tlen=276, kl=0.000943, act_lr=1.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.17it/s, pg=0.0599, ret=-0.000645, glen=97.8, tlen=258, kl=0.000915, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.0599, ret=-0.000645, glen=97.8, tlen=258, kl=0.000915, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.0975, ret=-0.00115, glen=116, tlen=276, kl=0.000877, act_lr=1.6e-7, ent=1.81]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.17it/s, pg=0.0975, ret=-0.00115, glen=116, tlen=276, kl=0.000877, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=-0.0564, ret=0.000432, glen=96.9, tlen=257, kl=0.000917, act_lr=1.6e-7, ent=1.66]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.17it/s, pg=-0.0564, ret=0.000432, glen=96.9, tlen=257, kl=0.000917, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=-0.114, ret=0.000657, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.96]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.17it/s, pg=-0.114, ret=0.000657, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=-0.154, ret=0.00122, glen=113, tlen=273, kl=0.000901, act_lr=1.6e-7, ent=1.75] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.17it/s, pg=-0.154, ret=0.00122, glen=113, tlen=273, kl=0.000901, act_lr=1.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=0.0724, ret=-0.000144, glen=118, tlen=277, kl=0.000937, act_lr=1.6e-7, ent=1.77]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.0724, ret=-0.000144, glen=118, tlen=277, kl=0.000937, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.17it/s, pg=0.0834, ret=-0.00107, glen=108, tlen=268, kl=0.000936, act_lr=1.6e-7, ent=1.54] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.17it/s, pg=0.0834, ret=-0.00107, glen=108, tlen=268, kl=0.000936, act_lr=1.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.17it/s, pg=0.0294, ret=-0.00163, glen=99.9, tlen=260, kl=0.000909, act_lr=1.6e-7, ent=1.65]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=0.0294, ret=-0.00163, glen=99.9, tlen=260, kl=0.000909, act_lr=1.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=-0.0415, ret=0.000174, glen=118, tlen=277, kl=0.000902, act_lr=1.6e-7, ent=1.82]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=-0.0415, ret=0.000174, glen=118, tlen=277, kl=0.000902, act_lr=1.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=0.162, ret=-0.00143, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.77]  Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=0.162, ret=-0.00143, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=0.0326, ret=-0.000676, glen=121, tlen=281, kl=0.00092, act_lr=1.6e-7, ent=1.71]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=0.0326, ret=-0.000676, glen=121, tlen=281, kl=0.00092, act_lr=1.6e-7, ent=1.71]
2025-07-24 16:38:50.254 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.58s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.178, ret=0.00199, glen=162, tlen=322, kl=0.000824, act_lr=1.8e-7, ent=2.21] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.178, ret=0.00199, glen=162, tlen=322, kl=0.000824, act_lr=1.8e-7, ent=2.21]
2025-07-24 16:38:50.919 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 16:38:53.281 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 16:38:53.609 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 54.06s
2025-07-24 16:38:53.617 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011384635136045259, 'actor_lr': 1.603448266423427e-07, 'clip_ratio': 0.0, 'entropy': 1.779008370021294, 'kl': 0.0009049485469686574, 'response_length': 124.24242677359746, 'total_length': 284.38584926210603, 'teacher_total_length': 297.4944489577721, 'return': 7.362090651315219e-06, 'policy_update_steps': 1.0}
Episode [1/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [33:01<14:45, 221.48s/it]2025-07-24 16:38:53.661 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:41:35.895 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:41:36.080 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:41:36.081 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 162.42s
2025-07-24 16:41:38.141 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0114,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6687,std_num_tokens: 156.2734,avg_correct_num_tokens: 103.5393,std_correct_num_tokens: 85.3857,avg_incorrect_num_tokens: 132.2278,std_incorrect_num_tokens: 225.7101
2025-07-24 16:41:38.489 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.41s
2025-07-24 16:41:41.718 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.23s
2025-07-24 16:42:11.152 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:42:11.152 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.43s
2025-07-24 16:42:12.802 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.23s
2025-07-24 16:42:12.802 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.002387216028591667, avg_kl: 0.000913978143550423, avg_response_length: 120.19191138921346, avg_orm_score: 0.0, avg_custom_rewards: -0.002387216028591667
2025-07-24 16:42:12.881 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter9_replay_buffer.jsonl
2025-07-24 16:42:14.834 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.95s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.167, ret=0.00094, glen=112, tlen=272, kl=0.000929, act_lr=1.8e-7, ent=1.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.04s/it, pg=-0.167, ret=0.00094, glen=112, tlen=272, kl=0.000929, act_lr=1.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.04s/it, pg=-0.0608, ret=-0.0012, glen=110, tlen=270, kl=0.000912, act_lr=1.8e-7, ent=1.69]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.07it/s, pg=-0.0608, ret=-0.0012, glen=110, tlen=270, kl=0.000912, act_lr=1.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.07it/s, pg=0.0568, ret=-0.00149, glen=124, tlen=284, kl=0.000892, act_lr=1.8e-7, ent=1.75]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.11it/s, pg=0.0568, ret=-0.00149, glen=124, tlen=284, kl=0.000892, act_lr=1.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.11it/s, pg=-0.03, ret=-0.000764, glen=103, tlen=263, kl=0.000978, act_lr=1.8e-7, ent=1.78]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:48,  1.11it/s, pg=-0.03, ret=-0.000764, glen=103, tlen=263, kl=0.000978, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:48,  1.11it/s, pg=0.0298, ret=2.33e-5, glen=110, tlen=270, kl=0.000919, act_lr=1.8e-7, ent=1.68] Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:47,  1.11it/s, pg=0.0298, ret=2.33e-5, glen=110, tlen=270, kl=0.000919, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:47,  1.11it/s, pg=-0.0338, ret=0.00151, glen=121, tlen=281, kl=0.000902, act_lr=1.8e-7, ent=1.93]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:46,  1.12it/s, pg=-0.0338, ret=0.00151, glen=121, tlen=281, kl=0.000902, act_lr=1.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:46,  1.12it/s, pg=0.0105, ret=-0.00113, glen=109, tlen=269, kl=0.000882, act_lr=1.8e-7, ent=1.78]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.12it/s, pg=0.0105, ret=-0.00113, glen=109, tlen=269, kl=0.000882, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.12it/s, pg=0.0318, ret=0.000172, glen=113, tlen=274, kl=0.000943, act_lr=1.8e-7, ent=1.82]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=0.0318, ret=0.000172, glen=113, tlen=274, kl=0.000943, act_lr=1.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:44,  1.13it/s, pg=0.043, ret=-0.00067, glen=101, tlen=261, kl=0.000973, act_lr=1.8e-7, ent=1.63] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.12it/s, pg=0.043, ret=-0.00067, glen=101, tlen=261, kl=0.000973, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.12it/s, pg=-0.114, ret=0.000238, glen=103, tlen=264, kl=0.000927, act_lr=1.8e-7, ent=1.57]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:42,  1.13it/s, pg=-0.114, ret=0.000238, glen=103, tlen=264, kl=0.000927, act_lr=1.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:42,  1.13it/s, pg=-0.113, ret=0.000905, glen=113, tlen=273, kl=0.000938, act_lr=1.8e-7, ent=1.82]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:41,  1.14it/s, pg=-0.113, ret=0.000905, glen=113, tlen=273, kl=0.000938, act_lr=1.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:41,  1.14it/s, pg=-0.0438, ret=0.0011, glen=118, tlen=279, kl=0.000937, act_lr=1.8e-7, ent=1.74] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.15it/s, pg=-0.0438, ret=0.0011, glen=118, tlen=279, kl=0.000937, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.15it/s, pg=-0.117, ret=-0.000279, glen=125, tlen=286, kl=0.000884, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.16it/s, pg=-0.117, ret=-0.000279, glen=125, tlen=286, kl=0.000884, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.16it/s, pg=-0.0399, ret=0.00097, glen=107, tlen=268, kl=0.000889, act_lr=1.8e-7, ent=1.74] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:38,  1.14it/s, pg=-0.0399, ret=0.00097, glen=107, tlen=268, kl=0.000889, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:38,  1.14it/s, pg=-0.061, ret=0.00064, glen=112, tlen=272, kl=0.000845, act_lr=1.8e-7, ent=1.87] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:37,  1.15it/s, pg=-0.061, ret=0.00064, glen=112, tlen=272, kl=0.000845, act_lr=1.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:37,  1.15it/s, pg=-0.0618, ret=1.3e-5, glen=112, tlen=272, kl=0.000928, act_lr=1.8e-7, ent=1.81]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.15it/s, pg=-0.0618, ret=1.3e-5, glen=112, tlen=272, kl=0.000928, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:15<00:36,  1.15it/s, pg=0.0719, ret=-0.000909, glen=106, tlen=266, kl=0.000894, act_lr=1.8e-7, ent=1.63]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.0719, ret=-0.000909, glen=106, tlen=266, kl=0.000894, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.119, ret=-0.000221, glen=128, tlen=288, kl=0.000912, act_lr=1.8e-7, ent=1.81] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.16it/s, pg=0.119, ret=-0.000221, glen=128, tlen=288, kl=0.000912, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.16it/s, pg=0.0474, ret=0.00186, glen=120, tlen=279, kl=0.000905, act_lr=1.8e-7, ent=1.87] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=0.0474, ret=0.00186, glen=120, tlen=279, kl=0.000905, act_lr=1.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=-0.0794, ret=-0.0006, glen=102, tlen=263, kl=0.000916, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=-0.0794, ret=-0.0006, glen=102, tlen=263, kl=0.000916, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.182, ret=0.000858, glen=138, tlen=299, kl=0.000923, act_lr=1.8e-7, ent=1.91]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.182, ret=0.000858, glen=138, tlen=299, kl=0.000923, act_lr=1.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=0.174, ret=-0.00192, glen=104, tlen=264, kl=0.000955, act_lr=1.8e-7, ent=1.59] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=0.174, ret=-0.00192, glen=104, tlen=264, kl=0.000955, act_lr=1.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.17it/s, pg=0.0674, ret=3e-5, glen=117, tlen=277, kl=0.000915, act_lr=1.8e-7, ent=1.85]   Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=0.0674, ret=3e-5, glen=117, tlen=277, kl=0.000915, act_lr=1.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.232, ret=0.00165, glen=107, tlen=267, kl=0.000911, act_lr=1.8e-7, ent=1.75]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.17it/s, pg=-0.232, ret=0.00165, glen=107, tlen=267, kl=0.000911, act_lr=1.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.17it/s, pg=-0.00453, ret=-0.000212, glen=98.1, tlen=258, kl=0.000957, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.16it/s, pg=-0.00453, ret=-0.000212, glen=98.1, tlen=258, kl=0.000957, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.16it/s, pg=-0.0513, ret=-0.000126, glen=100, tlen=261, kl=0.000911, act_lr=1.8e-7, ent=1.64]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.16it/s, pg=-0.0513, ret=-0.000126, glen=100, tlen=261, kl=0.000911, act_lr=1.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.16it/s, pg=-0.0331, ret=-0.000246, glen=123, tlen=283, kl=0.000939, act_lr=1.8e-7, ent=1.72]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.16it/s, pg=-0.0331, ret=-0.000246, glen=123, tlen=283, kl=0.000939, act_lr=1.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.16it/s, pg=0.0574, ret=0.000944, glen=173, tlen=334, kl=0.000851, act_lr=1.8e-7, ent=2.42]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:26,  1.15it/s, pg=0.0574, ret=0.000944, glen=173, tlen=334, kl=0.000851, act_lr=1.8e-7, ent=2.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:26,  1.15it/s, pg=-0.1, ret=0.000148, glen=97.4, tlen=257, kl=0.000938, act_lr=1.8e-7, ent=1.71] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.06it/s, pg=-0.1, ret=0.000148, glen=97.4, tlen=257, kl=0.000938, act_lr=1.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.06it/s, pg=0.298, ret=-0.00334, glen=142, tlen=302, kl=0.000829, act_lr=1.8e-7, ent=1.68]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.09it/s, pg=0.298, ret=-0.00334, glen=142, tlen=302, kl=0.000829, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.09it/s, pg=-0.0555, ret=0.00156, glen=118, tlen=278, kl=0.00091, act_lr=1.8e-7, ent=1.81]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.11it/s, pg=-0.0555, ret=0.00156, glen=118, tlen=278, kl=0.00091, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.11it/s, pg=0.0816, ret=-0.000133, glen=112, tlen=272, kl=0.000849, act_lr=1.8e-7, ent=2.1]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:23,  1.13it/s, pg=0.0816, ret=-0.000133, glen=112, tlen=272, kl=0.000849, act_lr=1.8e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:23,  1.13it/s, pg=0.108, ret=-0.00123, glen=108, tlen=268, kl=0.000927, act_lr=1.8e-7, ent=1.57] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.14it/s, pg=0.108, ret=-0.00123, glen=108, tlen=268, kl=0.000927, act_lr=1.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.14it/s, pg=-0.104, ret=0.00119, glen=122, tlen=282, kl=0.000923, act_lr=1.8e-7, ent=1.76]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.15it/s, pg=-0.104, ret=0.00119, glen=122, tlen=282, kl=0.000923, act_lr=1.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.15it/s, pg=-0.355, ret=0.00279, glen=114, tlen=274, kl=0.000935, act_lr=1.8e-7, ent=1.73]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=-0.355, ret=0.00279, glen=114, tlen=274, kl=0.000935, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=0.105, ret=-0.00129, glen=104, tlen=264, kl=0.000938, act_lr=1.8e-7, ent=1.59]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.16it/s, pg=0.105, ret=-0.00129, glen=104, tlen=264, kl=0.000938, act_lr=1.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.16it/s, pg=-0.0505, ret=-0.000461, glen=111, tlen=271, kl=0.000882, act_lr=1.8e-7, ent=1.73]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.17it/s, pg=-0.0505, ret=-0.000461, glen=111, tlen=271, kl=0.000882, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.17it/s, pg=0.0753, ret=-0.000717, glen=117, tlen=277, kl=0.000923, act_lr=1.8e-7, ent=1.73] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.0753, ret=-0.000717, glen=117, tlen=277, kl=0.000923, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.17it/s, pg=0.0708, ret=-0.00128, glen=113, tlen=273, kl=0.000932, act_lr=1.8e-7, ent=1.81] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=0.0708, ret=-0.00128, glen=113, tlen=273, kl=0.000932, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=0.0461, ret=-5.98e-5, glen=128, tlen=288, kl=0.000878, act_lr=1.8e-7, ent=1.92]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=0.0461, ret=-5.98e-5, glen=128, tlen=288, kl=0.000878, act_lr=1.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=0.203, ret=-0.000602, glen=172, tlen=332, kl=0.000859, act_lr=1.8e-7, ent=2.6] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=0.203, ret=-0.000602, glen=172, tlen=332, kl=0.000859, act_lr=1.8e-7, ent=2.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.0457, ret=-0.00189, glen=108, tlen=268, kl=0.000929, act_lr=1.8e-7, ent=1.64]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.16it/s, pg=0.0457, ret=-0.00189, glen=108, tlen=268, kl=0.000929, act_lr=1.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.16it/s, pg=-0.143, ret=-0.000868, glen=120, tlen=280, kl=0.000915, act_lr=1.8e-7, ent=1.75]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.16it/s, pg=-0.143, ret=-0.000868, glen=120, tlen=280, kl=0.000915, act_lr=1.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.16it/s, pg=-0.0614, ret=0.000739, glen=108, tlen=268, kl=0.000954, act_lr=1.8e-7, ent=1.72]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:12,  1.17it/s, pg=-0.0614, ret=0.000739, glen=108, tlen=268, kl=0.000954, act_lr=1.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.17it/s, pg=-0.0588, ret=0.000803, glen=115, tlen=275, kl=0.00093, act_lr=1.8e-7, ent=1.88] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0588, ret=0.000803, glen=115, tlen=275, kl=0.00093, act_lr=1.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=0.169, ret=-0.0013, glen=170, tlen=331, kl=0.000835, act_lr=1.8e-7, ent=2.43]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.16it/s, pg=0.169, ret=-0.0013, glen=170, tlen=331, kl=0.000835, act_lr=1.8e-7, ent=2.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.16it/s, pg=0.0458, ret=-0.000339, glen=117, tlen=278, kl=0.000936, act_lr=1.8e-7, ent=1.74]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.16it/s, pg=0.0458, ret=-0.000339, glen=117, tlen=278, kl=0.000936, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.16it/s, pg=0.176, ret=-0.00183, glen=120, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.94]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.16it/s, pg=0.176, ret=-0.00183, glen=120, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.16it/s, pg=-0.0997, ret=0.00109, glen=115, tlen=276, kl=0.000915, act_lr=1.8e-7, ent=1.66]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.16it/s, pg=-0.0997, ret=0.00109, glen=115, tlen=276, kl=0.000915, act_lr=1.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.16it/s, pg=-0.058, ret=0.00078, glen=119, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.71] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.17it/s, pg=-0.058, ret=0.00078, glen=119, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=0.112, ret=-0.0111, glen=354, tlen=515, kl=0.000912, act_lr=1.8e-7, ent=2.26] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:06,  1.14it/s, pg=0.112, ret=-0.0111, glen=354, tlen=515, kl=0.000912, act_lr=1.8e-7, ent=2.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:06,  1.14it/s, pg=-0.0525, ret=0.000687, glen=115, tlen=275, kl=0.000942, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.15it/s, pg=-0.0525, ret=0.000687, glen=115, tlen=275, kl=0.000942, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.15it/s, pg=0.0139, ret=0.000187, glen=114, tlen=274, kl=0.000918, act_lr=1.8e-7, ent=1.78] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.15it/s, pg=0.0139, ret=0.000187, glen=114, tlen=274, kl=0.000918, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.15it/s, pg=-0.0474, ret=-0.000178, glen=114, tlen=274, kl=0.000901, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.16it/s, pg=-0.0474, ret=-0.000178, glen=114, tlen=274, kl=0.000901, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.16it/s, pg=-0.161, ret=0.00186, glen=115, tlen=275, kl=0.000966, act_lr=1.8e-7, ent=1.79]   Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.16it/s, pg=-0.161, ret=0.00186, glen=115, tlen=275, kl=0.000966, act_lr=1.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.16it/s, pg=-0.0821, ret=-0.000283, glen=118, tlen=278, kl=0.000918, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=-0.0821, ret=-0.000283, glen=118, tlen=278, kl=0.000918, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=-0.0858, ret=0.000249, glen=115, tlen=275, kl=0.00095, act_lr=1.8e-7, ent=1.68]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=-0.0858, ret=0.000249, glen=115, tlen=275, kl=0.00095, act_lr=1.8e-7, ent=1.68]
2025-07-24 16:43:05.612 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.60s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=0.167, ret=-0.0059, glen=122, tlen=282, kl=0.000868, act_lr=2e-7, ent=1.83]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=0.167, ret=-0.0059, glen=122, tlen=282, kl=0.000868, act_lr=2e-7, ent=1.83]
2025-07-24 16:43:06.421 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 16:43:08.950 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-24 16:43:09.295 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 54.40s
2025-07-24 16:43:09.306 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.008862774947593952, 'actor_lr': 1.8034482829715012e-07, 'clip_ratio': 0.0, 'entropy': 1.796474444455114, 'kl': 0.000913324027225889, 'response_length': 121.13656826676994, 'total_length': 281.3766242717874, 'teacher_total_length': 292.80887893150594, 'return': -0.00032191864930014005, 'policy_update_steps': 1.0}
Episode [1/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [37:17<11:36, 232.04s/it]2025-07-24 16:43:09.311 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   1%|          | 1/172 [00:00<01:06,  2.57it/s, est. speed input: 456.75 toks/s, output: 28.22 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:05<00:00, 14.31it/s, est. speed input: 5341.39 toks/s, output: 2771.51 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:00, 19.94it/s, est. speed input: 5616.74 toks/s, output: 2821.74 toks/s][32m [repeated 119x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:10<00:00,  2.38it/s, est. speed input: 2889.48 toks/s, output: 1905.21 toks/s][32m [repeated 27x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:01,  8.16it/s, est. speed input: 4712.16 toks/s, output: 2455.32 toks/s][32m [repeated 2x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00,  1.86it/s, est. speed input: 2619.69 toks/s, output: 1697.83 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00, 14.45it/s, est. speed input: 2619.69 toks/s, output: 1697.83 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:12<00:00,  1.71it/s, est. speed input: 2551.21 toks/s, output: 1730.78 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:23<00:00,  2.48s/it, est. speed input: 1348.37 toks/s, output: 975.15 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:23<00:00,  7.44it/s, est. speed input: 1348.37 toks/s, output: 975.15 toks/s][32m [repeated 3x across cluster][0m
2025-07-24 16:43:34.003 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 553.0451,strategyqa_test/accuracy: 0.3100,eval_accuracy: 0.3100
2025-07-24 16:43:34.277 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:45:19.372 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:45:19.541 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:45:19.541 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 105.26s
2025-07-24 16:45:21.827 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0083,avg_pass_at_n: 1.0000,avg_num_tokens: 110.7008,std_num_tokens: 112.9670,avg_correct_num_tokens: 103.3926,std_correct_num_tokens: 85.9129,avg_incorrect_num_tokens: 122.1896,std_incorrect_num_tokens: 144.9279
2025-07-24 16:45:22.278 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.74s
2025-07-24 16:45:25.395 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.11s
2025-07-24 16:45:54.301 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 225
2025-07-24 16:45:54.301 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.90s
2025-07-24 16:45:55.984 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.20s
2025-07-24 16:45:55.985 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008869843830406252, avg_kl: 0.0009025743272569445, avg_response_length: 112.13507792154948, avg_orm_score: 0.0, avg_custom_rewards: -0.0008869843830406252
2025-07-24 16:45:56.042 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter10_replay_buffer.jsonl
2025-07-24 16:45:57.965 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.92s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0414, ret=0.00175, glen=122, tlen=282, kl=0.000906, act_lr=2e-7, ent=1.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.0414, ret=0.00175, glen=122, tlen=282, kl=0.000906, act_lr=2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.107, ret=-0.00114, glen=108, tlen=268, kl=0.000941, act_lr=2e-7, ent=1.81]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.107, ret=-0.00114, glen=108, tlen=268, kl=0.000941, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.0516, ret=-0.000559, glen=123, tlen=284, kl=0.000924, act_lr=2e-7, ent=1.88]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=0.0516, ret=-0.000559, glen=123, tlen=284, kl=0.000924, act_lr=2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.0151, ret=-0.000345, glen=113, tlen=273, kl=0.000877, act_lr=2e-7, ent=1.63]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.0151, ret=-0.000345, glen=113, tlen=273, kl=0.000877, act_lr=2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=-0.0871, ret=0.000792, glen=111, tlen=271, kl=0.000837, act_lr=2e-7, ent=1.59] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.0871, ret=0.000792, glen=111, tlen=271, kl=0.000837, act_lr=2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.0622, ret=-0.00134, glen=107, tlen=267, kl=0.000955, act_lr=2e-7, ent=1.68] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0622, ret=-0.00134, glen=107, tlen=267, kl=0.000955, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=0.095, ret=-0.000418, glen=129, tlen=289, kl=0.000929, act_lr=2e-7, ent=1.81]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=0.095, ret=-0.000418, glen=129, tlen=289, kl=0.000929, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=0.133, ret=-0.00107, glen=108, tlen=268, kl=0.000939, act_lr=2e-7, ent=1.7]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.133, ret=-0.00107, glen=108, tlen=268, kl=0.000939, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.218, ret=0.0014, glen=94.6, tlen=256, kl=0.000877, act_lr=2e-7, ent=1.65]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:42,  1.14it/s, pg=-0.218, ret=0.0014, glen=94.6, tlen=256, kl=0.000877, act_lr=2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=-0.0128, ret=-0.000764, glen=109, tlen=269, kl=0.000888, act_lr=2e-7, ent=1.78]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.0128, ret=-0.000764, glen=109, tlen=269, kl=0.000888, act_lr=2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.132, ret=-0.00123, glen=134, tlen=294, kl=0.000905, act_lr=2e-7, ent=2.43]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.132, ret=-0.00123, glen=134, tlen=294, kl=0.000905, act_lr=2e-7, ent=2.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0379, ret=-0.000525, glen=98.7, tlen=259, kl=0.000902, act_lr=2e-7, ent=1.8]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.0379, ret=-0.000525, glen=98.7, tlen=259, kl=0.000902, act_lr=2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.0292, ret=0.000107, glen=118, tlen=279, kl=0.000882, act_lr=2e-7, ent=1.82]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=0.0292, ret=0.000107, glen=118, tlen=279, kl=0.000882, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=0.106, ret=-0.000375, glen=109, tlen=269, kl=0.000896, act_lr=2e-7, ent=1.75]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=0.106, ret=-0.000375, glen=109, tlen=269, kl=0.000896, act_lr=2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=0.168, ret=-0.000278, glen=101, tlen=261, kl=0.0009, act_lr=2e-7, ent=1.71]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.168, ret=-0.000278, glen=101, tlen=261, kl=0.0009, act_lr=2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0755, ret=-0.00168, glen=115, tlen=276, kl=0.000895, act_lr=2e-7, ent=1.82]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=0.0755, ret=-0.00168, glen=115, tlen=276, kl=0.000895, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.14, ret=-0.000119, glen=110, tlen=271, kl=0.000889, act_lr=2e-7, ent=1.71]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.14, ret=-0.000119, glen=110, tlen=271, kl=0.000889, act_lr=2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.17, ret=-0.000759, glen=128, tlen=288, kl=0.000891, act_lr=2e-7, ent=1.84] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=0.17, ret=-0.000759, glen=128, tlen=288, kl=0.000891, act_lr=2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.168, ret=0.000796, glen=102, tlen=262, kl=0.000935, act_lr=2e-7, ent=1.67]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=-0.168, ret=0.000796, glen=102, tlen=262, kl=0.000935, act_lr=2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0351, ret=0.000656, glen=108, tlen=268, kl=0.000875, act_lr=2e-7, ent=1.54]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0351, ret=0.000656, glen=108, tlen=268, kl=0.000875, act_lr=2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.0476, ret=0.000906, glen=110, tlen=270, kl=0.00092, act_lr=2e-7, ent=1.69] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.0476, ret=0.000906, glen=110, tlen=270, kl=0.00092, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.217, ret=0.00145, glen=105, tlen=266, kl=0.000929, act_lr=2e-7, ent=1.82] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.217, ret=0.00145, glen=105, tlen=266, kl=0.000929, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.162, ret=0.000478, glen=97, tlen=258, kl=0.000879, act_lr=2e-7, ent=1.63]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.17it/s, pg=-0.162, ret=0.000478, glen=97, tlen=258, kl=0.000879, act_lr=2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.193, ret=0.00145, glen=97.3, tlen=258, kl=0.000923, act_lr=2e-7, ent=1.7]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.193, ret=0.00145, glen=97.3, tlen=258, kl=0.000923, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0632, ret=-0.000921, glen=110, tlen=270, kl=0.000922, act_lr=2e-7, ent=1.93]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0632, ret=-0.000921, glen=110, tlen=270, kl=0.000922, act_lr=2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.189, ret=0.000405, glen=113, tlen=273, kl=0.000966, act_lr=2e-7, ent=1.82] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.189, ret=0.000405, glen=113, tlen=273, kl=0.000966, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.118, ret=-0.00109, glen=118, tlen=279, kl=0.000927, act_lr=2e-7, ent=1.89] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.118, ret=-0.00109, glen=118, tlen=279, kl=0.000927, act_lr=2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0112, ret=-0.000626, glen=117, tlen=277, kl=0.000939, act_lr=2e-7, ent=1.73]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0112, ret=-0.000626, glen=117, tlen=277, kl=0.000939, act_lr=2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=0.0182, ret=0.000292, glen=103, tlen=263, kl=0.000883, act_lr=2e-7, ent=1.69] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=0.0182, ret=0.000292, glen=103, tlen=263, kl=0.000883, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=0.0637, ret=0.000526, glen=121, tlen=281, kl=0.0009, act_lr=2e-7, ent=1.85]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=0.0637, ret=0.000526, glen=121, tlen=281, kl=0.0009, act_lr=2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=-0.0696, ret=0.00077, glen=115, tlen=276, kl=0.000937, act_lr=2e-7, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.0696, ret=0.00077, glen=115, tlen=276, kl=0.000937, act_lr=2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.0938, ret=0.0018, glen=105, tlen=265, kl=0.000905, act_lr=2e-7, ent=1.58] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.14it/s, pg=-0.0938, ret=0.0018, glen=105, tlen=265, kl=0.000905, act_lr=2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=0.232, ret=-0.00113, glen=127, tlen=287, kl=0.000861, act_lr=2e-7, ent=1.72]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.14it/s, pg=0.232, ret=-0.00113, glen=127, tlen=287, kl=0.000861, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.0125, ret=-0.000442, glen=98.9, tlen=259, kl=0.000895, act_lr=2e-7, ent=1.57]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.0125, ret=-0.000442, glen=98.9, tlen=259, kl=0.000895, act_lr=2e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.069, ret=-0.00163, glen=118, tlen=278, kl=0.000866, act_lr=2e-7, ent=1.71]    Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.069, ret=-0.00163, glen=118, tlen=278, kl=0.000866, act_lr=2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.15, ret=0.00151, glen=103, tlen=264, kl=0.000925, act_lr=2e-7, ent=1.7]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.15, ret=0.00151, glen=103, tlen=264, kl=0.000925, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=0.0533, ret=-0.000197, glen=107, tlen=268, kl=0.000916, act_lr=2e-7, ent=1.73]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=0.0533, ret=-0.000197, glen=107, tlen=268, kl=0.000916, act_lr=2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.239, ret=0.00188, glen=107, tlen=268, kl=0.000933, act_lr=2e-7, ent=1.84]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:32<00:16,  1.17it/s, pg=-0.239, ret=0.00188, glen=107, tlen=268, kl=0.000933, act_lr=2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0502, ret=-0.000372, glen=108, tlen=268, kl=0.000896, act_lr=2e-7, ent=1.68]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.0502, ret=-0.000372, glen=108, tlen=268, kl=0.000896, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0953, ret=-0.000232, glen=108, tlen=269, kl=0.000927, act_lr=2e-7, ent=1.76]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0953, ret=-0.000232, glen=108, tlen=269, kl=0.000927, act_lr=2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.135, ret=-0.000272, glen=99.9, tlen=260, kl=0.000921, act_lr=2e-7, ent=1.73] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.135, ret=-0.000272, glen=99.9, tlen=260, kl=0.000921, act_lr=2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.145, ret=-0.000124, glen=100, tlen=260, kl=0.000901, act_lr=2e-7, ent=1.69]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.145, ret=-0.000124, glen=100, tlen=260, kl=0.000901, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.0598, ret=3.61e-5, glen=110, tlen=271, kl=0.000921, act_lr=2e-7, ent=1.59] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.0598, ret=3.61e-5, glen=110, tlen=271, kl=0.000921, act_lr=2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=0.139, ret=-0.00268, glen=150, tlen=310, kl=0.000731, act_lr=2e-7, ent=1.56] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.139, ret=-0.00268, glen=150, tlen=310, kl=0.000731, act_lr=2e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.171, ret=-0.00168, glen=113, tlen=273, kl=0.0009, act_lr=2e-7, ent=1.75]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=0.171, ret=-0.00168, glen=113, tlen=273, kl=0.0009, act_lr=2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=6.1e-5, ret=-0.000907, glen=117, tlen=277, kl=0.000917, act_lr=2e-7, ent=1.72]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=6.1e-5, ret=-0.000907, glen=117, tlen=277, kl=0.000917, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.0767, ret=0.000404, glen=110, tlen=270, kl=0.000897, act_lr=2e-7, ent=1.74]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.0767, ret=0.000404, glen=110, tlen=270, kl=0.000897, act_lr=2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.0562, ret=4.53e-5, glen=127, tlen=287, kl=0.000876, act_lr=2e-7, ent=2.16] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=-0.0562, ret=4.53e-5, glen=127, tlen=287, kl=0.000876, act_lr=2e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.0868, ret=0.00218, glen=115, tlen=275, kl=0.000899, act_lr=2e-7, ent=1.68]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=-0.0868, ret=0.00218, glen=115, tlen=275, kl=0.000899, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=0.132, ret=-0.00134, glen=118, tlen=279, kl=0.000875, act_lr=2e-7, ent=1.89] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.132, ret=-0.00134, glen=118, tlen=279, kl=0.000875, act_lr=2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=0.0215, ret=-0.00107, glen=119, tlen=280, kl=0.000937, act_lr=2e-7, ent=1.77]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.0215, ret=-0.00107, glen=119, tlen=280, kl=0.000937, act_lr=2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.00391, ret=-0.000157, glen=105, tlen=265, kl=0.000881, act_lr=2e-7, ent=1.76]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.17it/s, pg=-0.00391, ret=-0.000157, glen=105, tlen=265, kl=0.000881, act_lr=2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.151, ret=0.000789, glen=105, tlen=265, kl=0.000913, act_lr=2e-7, ent=1.68]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.151, ret=0.000789, glen=105, tlen=265, kl=0.000913, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.101, ret=-0.00112, glen=114, tlen=275, kl=0.000936, act_lr=2e-7, ent=1.81] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.101, ret=-0.00112, glen=114, tlen=275, kl=0.000936, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0388, ret=0.000642, glen=125, tlen=286, kl=0.000809, act_lr=2e-7, ent=1.62]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0388, ret=0.000642, glen=125, tlen=286, kl=0.000809, act_lr=2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=-0.115, ret=0.000798, glen=111, tlen=271, kl=0.000903, act_lr=2e-7, ent=1.65] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=-0.115, ret=0.000798, glen=111, tlen=271, kl=0.000903, act_lr=2e-7, ent=1.65]
2025-07-24 16:46:47.480 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.31s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.1, ret=0.00206, glen=110, tlen=271, kl=0.000926, act_lr=2.2e-7, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=-0.1, ret=0.00206, glen=110, tlen=271, kl=0.000926, act_lr=2.2e-7, ent=1.73]
2025-07-24 16:46:48.308 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 16:46:50.839 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-24 16:46:51.163 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.12s
2025-07-24 16:46:51.170 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.009948998166803728, 'actor_lr': 2.0035087955923365e-07, 'clip_ratio': 0.0, 'entropy': 1.7507029165301407, 'kl': 0.0009024268702456826, 'response_length': 111.99827147366707, 'total_length': 272.4189592327988, 'teacher_total_length': 284.16753775613347, 'return': -4.6558342434939715e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [40:58<07:37, 228.93s/it]2025-07-24 16:46:51.217 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:48:47.349 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:48:47.532 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:48:47.532 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 116.32s
2025-07-24 16:48:49.726 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 114.9939,std_num_tokens: 149.9138,avg_correct_num_tokens: 102.9947,std_correct_num_tokens: 81.3509,avg_incorrect_num_tokens: 134.7240,std_incorrect_num_tokens: 218.9069
2025-07-24 16:48:50.164 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.63s
2025-07-24 16:48:53.390 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.22s
2025-07-24 16:49:22.835 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:49:22.836 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.44s
2025-07-24 16:49:24.415 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.17s
2025-07-24 16:49:24.416 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0012488026775858939, avg_kl: 0.0009306695263458652, avg_response_length: 117.93109827166562, avg_orm_score: 0.0, avg_custom_rewards: 0.0012488026775858939
2025-07-24 16:49:24.482 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter11_replay_buffer.jsonl
2025-07-24 16:49:26.461 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.98s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.0697, ret=-0.000943, glen=101, tlen=262, kl=0.00094, act_lr=2.2e-7, ent=1.68]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.05s/it, pg=0.0697, ret=-0.000943, glen=101, tlen=262, kl=0.00094, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.05s/it, pg=-0.27, ret=0.012, glen=122, tlen=282, kl=0.000855, act_lr=2.2e-7, ent=1.76]    Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.06it/s, pg=-0.27, ret=0.012, glen=122, tlen=282, kl=0.000855, act_lr=2.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.06it/s, pg=-0.146, ret=0.000722, glen=114, tlen=275, kl=0.000962, act_lr=2.2e-7, ent=1.82]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.10it/s, pg=-0.146, ret=0.000722, glen=114, tlen=275, kl=0.000962, act_lr=2.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.10it/s, pg=-0.0287, ret=-0.000316, glen=126, tlen=287, kl=0.000924, act_lr=2.2e-7, ent=1.9]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.13it/s, pg=-0.0287, ret=-0.000316, glen=126, tlen=287, kl=0.000924, act_lr=2.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.13it/s, pg=0.13, ret=-0.00114, glen=117, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.72]   Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:46,  1.14it/s, pg=0.13, ret=-0.00114, glen=117, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:46,  1.14it/s, pg=-0.139, ret=0.00188, glen=112, tlen=272, kl=0.000925, act_lr=2.2e-7, ent=1.66]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:45,  1.13it/s, pg=-0.139, ret=0.00188, glen=112, tlen=272, kl=0.000925, act_lr=2.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:45,  1.13it/s, pg=0.2, ret=-0.000557, glen=211, tlen=372, kl=0.00091, act_lr=2.2e-7, ent=2.19]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.13it/s, pg=0.2, ret=-0.000557, glen=211, tlen=372, kl=0.00091, act_lr=2.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.13it/s, pg=-0.0331, ret=5.92e-5, glen=116, tlen=277, kl=0.000958, act_lr=2.2e-7, ent=1.78]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.14it/s, pg=-0.0331, ret=5.92e-5, glen=116, tlen=277, kl=0.000958, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.14it/s, pg=0.0508, ret=-0.00177, glen=100, tlen=260, kl=0.000978, act_lr=2.2e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.15it/s, pg=0.0508, ret=-0.00177, glen=100, tlen=260, kl=0.000978, act_lr=2.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.15it/s, pg=-0.127, ret=0.00028, glen=93.1, tlen=253, kl=0.000921, act_lr=2.2e-7, ent=1.59]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.16it/s, pg=-0.127, ret=0.00028, glen=93.1, tlen=253, kl=0.000921, act_lr=2.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.16it/s, pg=-0.0802, ret=-0.000248, glen=114, tlen=274, kl=0.000965, act_lr=2.2e-7, ent=1.87]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.16it/s, pg=-0.0802, ret=-0.000248, glen=114, tlen=274, kl=0.000965, act_lr=2.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.16it/s, pg=-0.012, ret=0.000218, glen=117, tlen=277, kl=0.000941, act_lr=2.2e-7, ent=1.72]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.16it/s, pg=-0.012, ret=0.000218, glen=117, tlen=277, kl=0.000941, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.16it/s, pg=-0.29, ret=0.000731, glen=98.3, tlen=259, kl=0.000941, act_lr=2.2e-7, ent=1.64]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=-0.29, ret=0.000731, glen=98.3, tlen=259, kl=0.000941, act_lr=2.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=0.0443, ret=0.000299, glen=127, tlen=288, kl=0.000971, act_lr=2.2e-7, ent=1.65]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=0.0443, ret=0.000299, glen=127, tlen=288, kl=0.000971, act_lr=2.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=0.111, ret=-0.00198, glen=114, tlen=275, kl=0.00098, act_lr=2.2e-7, ent=1.7]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=0.111, ret=-0.00198, glen=114, tlen=275, kl=0.00098, act_lr=2.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=-0.336, ret=0.00294, glen=130, tlen=290, kl=0.000918, act_lr=2.2e-7, ent=1.76]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:13<00:35,  1.17it/s, pg=-0.336, ret=0.00294, glen=130, tlen=290, kl=0.000918, act_lr=2.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:35,  1.17it/s, pg=-0.125, ret=0.00119, glen=112, tlen=273, kl=0.000945, act_lr=2.2e-7, ent=1.6] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.17it/s, pg=-0.125, ret=0.00119, glen=112, tlen=273, kl=0.000945, act_lr=2.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.17it/s, pg=-0.0329, ret=0.000899, glen=125, tlen=285, kl=0.000925, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=-0.0329, ret=0.000899, glen=125, tlen=285, kl=0.000925, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.0841, ret=0.000657, glen=95.8, tlen=256, kl=0.000859, act_lr=2.2e-7, ent=1.55]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.18it/s, pg=-0.0841, ret=0.000657, glen=95.8, tlen=256, kl=0.000859, act_lr=2.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.18it/s, pg=-0.042, ret=-7.61e-5, glen=101, tlen=262, kl=0.000952, act_lr=2.2e-7, ent=1.63]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.16it/s, pg=-0.042, ret=-7.61e-5, glen=101, tlen=262, kl=0.000952, act_lr=2.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.16it/s, pg=-0.0566, ret=-0.00022, glen=93.8, tlen=254, kl=0.000958, act_lr=2.2e-7, ent=1.64]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.16it/s, pg=-0.0566, ret=-0.00022, glen=93.8, tlen=254, kl=0.000958, act_lr=2.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.16it/s, pg=-0.152, ret=0.000598, glen=102, tlen=262, kl=0.00098, act_lr=2.2e-7, ent=1.68]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.152, ret=0.000598, glen=102, tlen=262, kl=0.00098, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=0.193, ret=-0.00152, glen=105, tlen=266, kl=0.000911, act_lr=2.2e-7, ent=1.71]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.17it/s, pg=0.193, ret=-0.00152, glen=105, tlen=266, kl=0.000911, act_lr=2.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.0391, ret=0.00202, glen=135, tlen=295, kl=0.000947, act_lr=2.2e-7, ent=2.08]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.17it/s, pg=-0.0391, ret=0.00202, glen=135, tlen=295, kl=0.000947, act_lr=2.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.17it/s, pg=0.0673, ret=-0.00154, glen=112, tlen=273, kl=0.000904, act_lr=2.2e-7, ent=1.9] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.15it/s, pg=0.0673, ret=-0.00154, glen=112, tlen=273, kl=0.000904, act_lr=2.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.15it/s, pg=0.113, ret=-0.00118, glen=126, tlen=286, kl=0.000923, act_lr=2.2e-7, ent=1.86]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.16it/s, pg=0.113, ret=-0.00118, glen=126, tlen=286, kl=0.000923, act_lr=2.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.16it/s, pg=-0.189, ret=0.000218, glen=112, tlen=272, kl=0.000908, act_lr=2.2e-7, ent=1.66]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.17it/s, pg=-0.189, ret=0.000218, glen=112, tlen=272, kl=0.000908, act_lr=2.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.00732, ret=-0.000992, glen=114, tlen=275, kl=0.000945, act_lr=2.2e-7, ent=1.82]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=0.00732, ret=-0.000992, glen=114, tlen=275, kl=0.000945, act_lr=2.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.0691, ret=0.00184, glen=137, tlen=299, kl=0.000942, act_lr=2.2e-7, ent=1.91]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.06it/s, pg=-0.0691, ret=0.00184, glen=137, tlen=299, kl=0.000942, act_lr=2.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.06it/s, pg=0.0711, ret=0.00011, glen=125, tlen=286, kl=0.000912, act_lr=2.2e-7, ent=1.79] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.09it/s, pg=0.0711, ret=0.00011, glen=125, tlen=286, kl=0.000912, act_lr=2.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.09it/s, pg=0.0966, ret=0.000325, glen=125, tlen=285, kl=0.000935, act_lr=2.2e-7, ent=1.99]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.0966, ret=0.000325, glen=125, tlen=285, kl=0.000935, act_lr=2.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.245, ret=-0.00271, glen=124, tlen=284, kl=0.000927, act_lr=2.2e-7, ent=1.91] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:22,  1.13it/s, pg=0.245, ret=-0.00271, glen=124, tlen=284, kl=0.000927, act_lr=2.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.13it/s, pg=0.123, ret=-0.00078, glen=126, tlen=286, kl=0.000937, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.14it/s, pg=0.123, ret=-0.00078, glen=126, tlen=286, kl=0.000937, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.14it/s, pg=0.103, ret=-0.000709, glen=125, tlen=286, kl=0.000891, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.15it/s, pg=0.103, ret=-0.000709, glen=125, tlen=286, kl=0.000891, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.15it/s, pg=0.0511, ret=0.00163, glen=159, tlen=319, kl=0.000774, act_lr=2.2e-7, ent=1.45] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:20,  1.15it/s, pg=0.0511, ret=0.00163, glen=159, tlen=319, kl=0.000774, act_lr=2.2e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:20,  1.15it/s, pg=-0.108, ret=0.000659, glen=113, tlen=275, kl=0.000937, act_lr=2.2e-7, ent=1.76]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:19,  1.16it/s, pg=-0.108, ret=0.000659, glen=113, tlen=275, kl=0.000937, act_lr=2.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:19,  1.16it/s, pg=0.136, ret=-0.00108, glen=107, tlen=267, kl=0.000952, act_lr=2.2e-7, ent=1.58] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.16it/s, pg=0.136, ret=-0.00108, glen=107, tlen=267, kl=0.000952, act_lr=2.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.16it/s, pg=0.098, ret=-0.00133, glen=166, tlen=326, kl=0.000774, act_lr=2.2e-7, ent=2.52]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.15it/s, pg=0.098, ret=-0.00133, glen=166, tlen=326, kl=0.000774, act_lr=2.2e-7, ent=2.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.15it/s, pg=-0.00522, ret=-8.73e-5, glen=116, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.73]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.15it/s, pg=-0.00522, ret=-8.73e-5, glen=116, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.15it/s, pg=0.0577, ret=-0.00109, glen=107, tlen=267, kl=0.000923, act_lr=2.2e-7, ent=1.75]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.16it/s, pg=0.0577, ret=-0.00109, glen=107, tlen=267, kl=0.000923, act_lr=2.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=-0.0135, ret=-0.000361, glen=117, tlen=277, kl=0.000985, act_lr=2.2e-7, ent=1.84]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0135, ret=-0.000361, glen=117, tlen=277, kl=0.000985, act_lr=2.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=-0.104, ret=0.000264, glen=107, tlen=267, kl=0.000985, act_lr=2.2e-7, ent=1.81]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=-0.104, ret=0.000264, glen=107, tlen=267, kl=0.000985, act_lr=2.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=0.0083, ret=-0.000608, glen=113, tlen=273, kl=0.000937, act_lr=2.2e-7, ent=1.72]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=0.0083, ret=-0.000608, glen=113, tlen=273, kl=0.000937, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=-0.0266, ret=0.000425, glen=102, tlen=263, kl=0.000929, act_lr=2.2e-7, ent=1.54]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=-0.0266, ret=0.000425, glen=102, tlen=263, kl=0.000929, act_lr=2.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.204, ret=-0.0019, glen=129, tlen=289, kl=0.000926, act_lr=2.2e-7, ent=2.19]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.204, ret=-0.0019, glen=129, tlen=289, kl=0.000926, act_lr=2.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.019, ret=0.000675, glen=109, tlen=270, kl=0.000972, act_lr=2.2e-7, ent=1.81]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.17it/s, pg=-0.019, ret=0.000675, glen=109, tlen=270, kl=0.000972, act_lr=2.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=-0.0184, ret=-9.74e-5, glen=111, tlen=272, kl=0.000907, act_lr=2.2e-7, ent=1.47]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.0184, ret=-9.74e-5, glen=111, tlen=272, kl=0.000907, act_lr=2.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.116, ret=-0.000519, glen=138, tlen=299, kl=0.000902, act_lr=2.2e-7, ent=2.31] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.15it/s, pg=0.116, ret=-0.000519, glen=138, tlen=299, kl=0.000902, act_lr=2.2e-7, ent=2.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.15it/s, pg=-0.0499, ret=0.000151, glen=110, tlen=270, kl=0.000923, act_lr=2.2e-7, ent=1.62]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.16it/s, pg=-0.0499, ret=0.000151, glen=110, tlen=270, kl=0.000923, act_lr=2.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.16it/s, pg=-0.0669, ret=-0.000424, glen=102, tlen=262, kl=0.000954, act_lr=2.2e-7, ent=1.58]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.16it/s, pg=-0.0669, ret=-0.000424, glen=102, tlen=262, kl=0.000954, act_lr=2.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.16it/s, pg=0.0181, ret=0.000197, glen=120, tlen=281, kl=0.000949, act_lr=2.2e-7, ent=1.78]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.17it/s, pg=0.0181, ret=0.000197, glen=120, tlen=281, kl=0.000949, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=-0.0667, ret=-0.00023, glen=113, tlen=273, kl=0.000981, act_lr=2.2e-7, ent=1.68]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=-0.0667, ret=-0.00023, glen=113, tlen=273, kl=0.000981, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=-0.132, ret=0.000294, glen=97.7, tlen=258, kl=0.000955, act_lr=2.2e-7, ent=1.59]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.17it/s, pg=-0.132, ret=0.000294, glen=97.7, tlen=258, kl=0.000955, act_lr=2.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.17it/s, pg=-0.128, ret=-9.17e-5, glen=104, tlen=265, kl=0.00097, act_lr=2.2e-7, ent=1.83]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.17it/s, pg=-0.128, ret=-9.17e-5, glen=104, tlen=265, kl=0.00097, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=0.0692, ret=-0.000887, glen=117, tlen=278, kl=0.000918, act_lr=2.2e-7, ent=1.6]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=0.0692, ret=-0.000887, glen=117, tlen=278, kl=0.000918, act_lr=2.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=-0.0249, ret=-3.51e-5, glen=112, tlen=273, kl=0.000934, act_lr=2.2e-7, ent=1.72]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=-0.0249, ret=-3.51e-5, glen=112, tlen=273, kl=0.000934, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=-0.237, ret=0.00239, glen=139, tlen=300, kl=0.000922, act_lr=2.2e-7, ent=1.98]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=-0.237, ret=0.00239, glen=139, tlen=300, kl=0.000922, act_lr=2.2e-7, ent=1.98]
2025-07-24 16:50:17.071 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.39s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.13, ret=0.00169, glen=115, tlen=275, kl=0.000914, act_lr=2.4e-7, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.13, ret=0.00169, glen=115, tlen=275, kl=0.000914, act_lr=2.4e-7, ent=1.73]
2025-07-24 16:50:17.735 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 16:50:20.111 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.38s
2025-07-24 16:50:20.442 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.89s
2025-07-24 16:50:20.452 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.017218655553357356, 'actor_lr': 2.2034483136175022e-07, 'clip_ratio': 0.0, 'entropy': 1.7747623344947552, 'kl': 0.0009304408379428988, 'response_length': 117.81621248968716, 'total_length': 278.36411364325164, 'teacher_total_length': 290.11740059688174, 'return': 0.00017178226247277302, 'policy_update_steps': 1.0}
Episode [1/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [44:28<03:42, 222.95s/it]2025-07-24 16:50:20.458 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:06,  2.55it/s, est. speed input: 459.36 toks/s, output: 28.07 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/172 [00:05<00:00, 16.83it/s, est. speed input: 5649.25 toks/s, output: 2822.09 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:01, 14.44it/s, est. speed input: 5320.78 toks/s, output: 2624.76 toks/s][32m [repeated 116x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:10<00:03,  1.79it/s, est. speed input: 2919.33 toks/s, output: 1755.87 toks/s][32m [repeated 27x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:05<00:01, 13.02it/s, est. speed input: 4765.75 toks/s, output: 2392.88 toks/s][32m [repeated 4x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:10<00:00,  1.69it/s, est. speed input: 2883.94 toks/s, output: 1736.57 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:10<00:00, 15.89it/s, est. speed input: 2883.94 toks/s, output: 1736.57 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:15<00:00,  1.03it/s, est. speed input: 1964.50 toks/s, output: 1389.90 toks/s][32m [repeated 10x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:16<00:00,  1.16it/s, est. speed input: 1908.70 toks/s, output: 1402.62 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:16<00:00, 10.53it/s, est. speed input: 1908.70 toks/s, output: 1402.62 toks/s][32m [repeated 3x across cluster][0m
2025-07-24 16:50:38.129 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 563.0422,strategyqa_test/accuracy: 0.3217,eval_accuracy: 0.3217
2025-07-24 16:50:38.383 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:51:31.029 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:51:31.203 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:51:31.204 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 52.82s
2025-07-24 16:51:32.319 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0091,avg_pass_at_n: 1.0000,avg_num_tokens: 104.6768,std_num_tokens: 104.4267,avg_correct_num_tokens: 99.6748,std_correct_num_tokens: 83.4959,avg_incorrect_num_tokens: 112.9474,std_incorrect_num_tokens: 131.5251
2025-07-24 16:51:32.649 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.44s
2025-07-24 16:51:34.214 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.56s
2025-07-24 16:51:49.298 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 115
2025-07-24 16:51:49.299 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 15.08s
2025-07-24 16:51:50.428 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.71s
2025-07-24 16:51:50.429 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00014084085477920978, avg_kl: 0.0009495610776154891, avg_response_length: 105.95155016028363, avg_orm_score: 0.0, avg_custom_rewards: 0.00014084085477920978
2025-07-24 16:51:50.461 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter12_replay_buffer.jsonl
2025-07-24 16:51:51.402 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.94s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s, pg=0.105, ret=-0.000926, glen=115, tlen=275, kl=0.000928, act_lr=2.4e-7, ent=1.84]Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:00<00:27,  1.00it/s, pg=0.105, ret=-0.000926, glen=115, tlen=275, kl=0.000928, act_lr=2.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:01<00:27,  1.00it/s, pg=-0.172, ret=0.000433, glen=90.8, tlen=250, kl=0.00099, act_lr=2.4e-7, ent=1.65]Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:01<00:24,  1.09it/s, pg=-0.172, ret=0.000433, glen=90.8, tlen=250, kl=0.00099, act_lr=2.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:02<00:24,  1.09it/s, pg=0.125, ret=0.000343, glen=124, tlen=283, kl=0.000853, act_lr=2.4e-7, ent=1.91] Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:02<00:23,  1.12it/s, pg=0.125, ret=0.000343, glen=124, tlen=283, kl=0.000853, act_lr=2.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:03<00:23,  1.12it/s, pg=0.0442, ret=-0.00187, glen=101, tlen=260, kl=0.000957, act_lr=2.4e-7, ent=1.8]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:03<00:22,  1.12it/s, pg=0.0442, ret=-0.00187, glen=101, tlen=260, kl=0.000957, act_lr=2.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:04<00:22,  1.12it/s, pg=0.108, ret=-0.000293, glen=98.1, tlen=258, kl=0.000941, act_lr=2.4e-7, ent=1.82]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:04<00:21,  1.14it/s, pg=0.108, ret=-0.000293, glen=98.1, tlen=258, kl=0.000941, act_lr=2.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:05<00:21,  1.14it/s, pg=0.0654, ret=0.000206, glen=122, tlen=281, kl=0.000998, act_lr=2.4e-7, ent=2.08] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:05<00:20,  1.15it/s, pg=0.0654, ret=0.000206, glen=122, tlen=281, kl=0.000998, act_lr=2.4e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:06<00:20,  1.15it/s, pg=-0.167, ret=0.0018, glen=106, tlen=266, kl=0.000976, act_lr=2.4e-7, ent=1.7]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:06<00:19,  1.15it/s, pg=-0.167, ret=0.0018, glen=106, tlen=266, kl=0.000976, act_lr=2.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:07<00:19,  1.15it/s, pg=-0.181, ret=0.00129, glen=108, tlen=267, kl=0.000966, act_lr=2.4e-7, ent=1.84]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.16it/s, pg=-0.181, ret=0.00129, glen=108, tlen=267, kl=0.000966, act_lr=2.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.16it/s, pg=0.033, ret=0.000164, glen=96.6, tlen=257, kl=0.000953, act_lr=2.4e-7, ent=1.74]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:07<00:17,  1.16it/s, pg=0.033, ret=0.000164, glen=96.6, tlen=257, kl=0.000953, act_lr=2.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:08<00:17,  1.16it/s, pg=-0.0825, ret=-7.81e-5, glen=103, tlen=263, kl=0.000933, act_lr=2.4e-7, ent=1.63]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:08<00:16,  1.17it/s, pg=-0.0825, ret=-7.81e-5, glen=103, tlen=263, kl=0.000933, act_lr=2.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:09<00:16,  1.17it/s, pg=0.0226, ret=-0.00105, glen=97.7, tlen=257, kl=0.000984, act_lr=2.4e-7, ent=1.71]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:09<00:15,  1.17it/s, pg=0.0226, ret=-0.00105, glen=97.7, tlen=257, kl=0.000984, act_lr=2.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:10<00:15,  1.17it/s, pg=0.0446, ret=8.37e-5, glen=112, tlen=272, kl=0.00094, act_lr=2.4e-7, ent=1.66]   Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:10<00:14,  1.17it/s, pg=0.0446, ret=8.37e-5, glen=112, tlen=272, kl=0.00094, act_lr=2.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:11<00:14,  1.17it/s, pg=-0.185, ret=0.00143, glen=113, tlen=272, kl=0.000949, act_lr=2.4e-7, ent=1.59]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:11<00:13,  1.17it/s, pg=-0.185, ret=0.00143, glen=113, tlen=272, kl=0.000949, act_lr=2.4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:12<00:13,  1.17it/s, pg=0.168, ret=-0.00153, glen=98.4, tlen=258, kl=0.00095, act_lr=2.4e-7, ent=1.52]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:12<00:12,  1.17it/s, pg=0.168, ret=-0.00153, glen=98.4, tlen=258, kl=0.00095, act_lr=2.4e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:13<00:12,  1.17it/s, pg=0.0481, ret=-0.00205, glen=98.3, tlen=258, kl=0.000963, act_lr=2.4e-7, ent=1.61]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.14it/s, pg=0.0481, ret=-0.00205, glen=98.3, tlen=258, kl=0.000963, act_lr=2.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.14it/s, pg=-0.0548, ret=-0.000386, glen=97.3, tlen=257, kl=0.000949, act_lr=2.4e-7, ent=1.54]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:13<00:11,  1.15it/s, pg=-0.0548, ret=-0.000386, glen=97.3, tlen=257, kl=0.000949, act_lr=2.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:14<00:11,  1.15it/s, pg=0.132, ret=-0.000671, glen=116, tlen=276, kl=0.000998, act_lr=2.4e-7, ent=1.84]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:14<00:10,  1.16it/s, pg=0.132, ret=-0.000671, glen=116, tlen=276, kl=0.000998, act_lr=2.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:15<00:10,  1.16it/s, pg=0.0545, ret=-0.000663, glen=96.6, tlen=256, kl=0.000903, act_lr=2.4e-7, ent=1.62]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:15<00:09,  1.17it/s, pg=0.0545, ret=-0.000663, glen=96.6, tlen=256, kl=0.000903, act_lr=2.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:16<00:09,  1.17it/s, pg=0.0808, ret=-0.00105, glen=107, tlen=266, kl=0.000937, act_lr=2.4e-7, ent=1.6]   Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:16<00:08,  1.17it/s, pg=0.0808, ret=-0.00105, glen=107, tlen=266, kl=0.000937, act_lr=2.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:17<00:08,  1.17it/s, pg=-0.141, ret=0.00102, glen=93, tlen=253, kl=0.000953, act_lr=2.4e-7, ent=1.66] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:17<00:07,  1.17it/s, pg=-0.141, ret=0.00102, glen=93, tlen=253, kl=0.000953, act_lr=2.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:18<00:07,  1.17it/s, pg=0.00894, ret=-2e-5, glen=122, tlen=282, kl=0.000944, act_lr=2.4e-7, ent=1.67]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:18<00:06,  1.17it/s, pg=0.00894, ret=-2e-5, glen=122, tlen=282, kl=0.000944, act_lr=2.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:19<00:06,  1.17it/s, pg=0.107, ret=0.000842, glen=113, tlen=273, kl=0.000957, act_lr=2.4e-7, ent=1.87]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=0.107, ret=0.000842, glen=113, tlen=273, kl=0.000957, act_lr=2.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=0.0717, ret=-0.000238, glen=105, tlen=265, kl=0.000992, act_lr=2.4e-7, ent=1.79]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:19<00:05,  1.17it/s, pg=0.0717, ret=-0.000238, glen=105, tlen=265, kl=0.000992, act_lr=2.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:20<00:05,  1.17it/s, pg=0.0761, ret=-0.000323, glen=108, tlen=268, kl=0.000914, act_lr=2.4e-7, ent=1.73]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:20<00:04,  1.17it/s, pg=0.0761, ret=-0.000323, glen=108, tlen=268, kl=0.000914, act_lr=2.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:21<00:04,  1.17it/s, pg=0.191, ret=-0.00147, glen=122, tlen=281, kl=0.000879, act_lr=2.4e-7, ent=1.75]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:21<00:03,  1.17it/s, pg=0.191, ret=-0.00147, glen=122, tlen=281, kl=0.000879, act_lr=2.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:22<00:03,  1.17it/s, pg=-0.161, ret=0.000976, glen=101, tlen=261, kl=0.000937, act_lr=2.4e-7, ent=1.71]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:22<00:02,  1.17it/s, pg=-0.161, ret=0.000976, glen=101, tlen=261, kl=0.000937, act_lr=2.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:23<00:02,  1.17it/s, pg=-0.223, ret=0.00107, glen=99, tlen=259, kl=0.000966, act_lr=2.4e-7, ent=1.64]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:23<00:01,  1.17it/s, pg=-0.223, ret=0.00107, glen=99, tlen=259, kl=0.000966, act_lr=2.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:24<00:01,  1.17it/s, pg=-0.134, ret=0.00128, glen=103, tlen=263, kl=0.000971, act_lr=2.4e-7, ent=1.69]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:24<00:00,  1.17it/s, pg=-0.134, ret=0.00128, glen=103, tlen=263, kl=0.000971, act_lr=2.4e-7, ent=1.69]
2025-07-24 16:52:16.921 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 25.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.17it/s, pg=-0.168, ret=0.00159, glen=109, tlen=269, kl=0.000997, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.11it/s, pg=-0.168, ret=0.00159, glen=109, tlen=269, kl=0.000997, act_lr=2.6e-7, ent=1.67]
2025-07-24 16:52:17.603 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 16:52:19.717 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.11s
2025-07-24 16:52:20.042 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 28.60s
2025-07-24 16:52:20.046 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0063823173786031785, 'actor_lr': 2.406896471554852e-07, 'clip_ratio': 0.0, 'entropy': 1.720232079769003, 'kl': 0.0009509119494207974, 'response_length': 106.02787728145205, 'total_length': 265.74897818729795, 'teacher_total_length': 278.236981622104, 'return': -2.9182686783566044e-06, 'policy_update_steps': 1.0}
Episode [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [46:27<00:00, 191.64s/it]2025-07-24 16:52:28.712 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(extract_final_answers_batch pid=1439602)[0m [2025-07-24 16:07:59,012] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[36m(LLMActor pid=1435059)[0m init_process_group: master_address=10.224.3.58, master_port=41739,  rank=2, world_size=5, group_name=openrlhf[32m [repeated 3x across cluster][0m
[36m(extract_final_answers_batch pid=1439602)[0m [2025-07-24 16:07:59,017] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[36m(pid=1440883)[0m [2025-07-24 16:08:05,597] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 15x across cluster][0m
[36m(pid=1440883)[0m [2025-07-24 16:08:05,602] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 15x across cluster][0m
[36m(get_reflection_pattern_score pid=1437884)[0m math_verify is not installed in this environment
[36m(pid=1440893)[0m [2025-07-24 16:08:09,954] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 28x across cluster][0m
[36m(pid=1440893)[0m [2025-07-24 16:08:09,957] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 28x across cluster][0m
[36m(pid=1440889)[0m math_verify is not installed in this environment[32m [repeated 15x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Invalidate trace cache @ step 370 and module 0: cache has only 370 modules
[36m(pid=1440894)[0m math_verify is not installed in this environment[32m [repeated 21x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:09:42,907] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(extract_final_answers_batch pid=1440886)[0m math_verify is not installed in this environment
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:12:48,419] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(get_reflection_pattern_score pid=1440891)[0m math_verify is not installed in this environment[32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:17:00,346] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:20:16,141] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:24:30,296] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:28:31,517] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:31:25,527] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:34:32,765] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:38:50,247] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:43:05,605] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:46:47,473] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:50:17,064] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:26,602] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:26,801] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 678, num_elems = 3.55B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,297] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,297] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,304] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,306] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,548] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,549] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 6.45 GB         CA 2.51 GB         Max_CA 38 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,549] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.88 GB, percent = 21.6%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [2/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [46:36<00:00, 215.12s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,706] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,707] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 2.51 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,707] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.88 GB, percent = 21.6%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  Falsehuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7d9736b10>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   nebula_config ................ {huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 16:52:28.966 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:55:06.949 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:55:07.121 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:55:07.121 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 158.16s
2025-07-24 16:55:09.423 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0088,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6980,std_num_tokens: 158.8815,avg_correct_num_tokens: 104.4542,std_correct_num_tokens: 82.5497,avg_incorrect_num_tokens: 132.0131,std_incorrect_num_tokens: 236.4671
2025-07-24 16:55:09.866 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.74s
2025-07-24 16:55:12.897 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.03s
2025-07-24 16:55:42.940 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:55:42.940 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 30.04s
2025-07-24 16:55:44.550 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.20s
2025-07-24 16:55:44.551 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0010314403766741788, avg_kl: 0.0, avg_response_length: 120.66883367013723, avg_orm_score: 0.0, avg_custom_rewards: -0.0010314403766741788
2025-07-24 16:55:44.631 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter13_replay_buffer.jsonl
2025-07-24 16:55:46.571 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.94s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.0809, ret=-0.00196, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.59]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:57,  1.01s/it, pg=0.0809, ret=-0.00196, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:57,  1.01s/it, pg=0.161, ret=0.000202, glen=120, tlen=280, kl=0, act_lr=2.6e-7, ent=2.14] Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:51,  1.08it/s, pg=0.161, ret=0.000202, glen=120, tlen=280, kl=0, act_lr=2.6e-7, ent=2.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:51,  1.08it/s, pg=-0.141, ret=0.000303, glen=110, tlen=270, kl=0, act_lr=2.6e-7, ent=1.63]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.12it/s, pg=-0.141, ret=0.000303, glen=110, tlen=270, kl=0, act_lr=2.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.12it/s, pg=0.00488, ret=-1.7e-5, glen=100, tlen=261, kl=0, act_lr=2.6e-7, ent=1.77]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:48,  1.12it/s, pg=0.00488, ret=-1.7e-5, glen=100, tlen=261, kl=0, act_lr=2.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:48,  1.12it/s, pg=0.00537, ret=-0.00154, glen=99.8, tlen=260, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:47,  1.12it/s, pg=0.00537, ret=-0.00154, glen=99.8, tlen=260, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:47,  1.12it/s, pg=-0.0101, ret=-0.000664, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:45,  1.14it/s, pg=-0.0101, ret=-0.000664, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:45,  1.14it/s, pg=-0.255, ret=0.00309, glen=117, tlen=278, kl=0, act_lr=2.6e-7, ent=1.62]   Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:44,  1.15it/s, pg=-0.255, ret=0.00309, glen=117, tlen=278, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:44,  1.15it/s, pg=0.00128, ret=-0.000769, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.56]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.16it/s, pg=0.00128, ret=-0.000769, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.16it/s, pg=-0.241, ret=0.00179, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.72]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.16it/s, pg=-0.241, ret=0.00179, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.16it/s, pg=-0.129, ret=-2.23e-5, glen=114, tlen=274, kl=0, act_lr=2.6e-7, ent=1.54]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.17it/s, pg=-0.129, ret=-2.23e-5, glen=114, tlen=274, kl=0, act_lr=2.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.17it/s, pg=0.177, ret=-0.00184, glen=122, tlen=282, kl=0, act_lr=2.6e-7, ent=2.06] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.17it/s, pg=0.177, ret=-0.00184, glen=122, tlen=282, kl=0, act_lr=2.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.17it/s, pg=0.231, ret=-0.00109, glen=138, tlen=299, kl=0, act_lr=2.6e-7, ent=1.6] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.17it/s, pg=0.231, ret=-0.00109, glen=138, tlen=299, kl=0, act_lr=2.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.17it/s, pg=0.0111, ret=-0.000123, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.99]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=0.0111, ret=-0.000123, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=-0.26, ret=0.00117, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.7]    Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=-0.26, ret=0.00117, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=-0.138, ret=0.000469, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.55]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=-0.138, ret=0.000469, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=0.0195, ret=-0.000199, glen=107, tlen=268, kl=0, act_lr=2.6e-7, ent=1.58]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:13<00:36,  1.15it/s, pg=0.0195, ret=-0.000199, glen=107, tlen=268, kl=0, act_lr=2.6e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.15it/s, pg=-0.0808, ret=-0.000727, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.79]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.0808, ret=-0.000727, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=-0.0922, ret=-0.000559, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.64]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=-0.0922, ret=-0.000559, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.143, ret=0.00049, glen=97.7, tlen=258, kl=0, act_lr=2.6e-7, ent=1.66]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=-0.143, ret=0.00049, glen=97.7, tlen=258, kl=0, act_lr=2.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.227, ret=-0.00104, glen=126, tlen=286, kl=0, act_lr=2.6e-7, ent=1.64] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.227, ret=-0.00104, glen=126, tlen=286, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.104, ret=0.00078, glen=125, tlen=285, kl=0, act_lr=2.6e-7, ent=1.85]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.104, ret=0.00078, glen=125, tlen=285, kl=0, act_lr=2.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=-0.0652, ret=0.000275, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.57]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0652, ret=0.000275, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0134, ret=0.000357, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.62]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.18it/s, pg=-0.0134, ret=0.000357, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.18it/s, pg=0.133, ret=-0.000775, glen=112, tlen=272, kl=0, act_lr=2.6e-7, ent=1.84] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:28,  1.18it/s, pg=0.133, ret=-0.000775, glen=112, tlen=272, kl=0, act_lr=2.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:28,  1.18it/s, pg=-0.0751, ret=9.99e-5, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.64]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.16it/s, pg=-0.0751, ret=9.99e-5, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.16it/s, pg=0.0591, ret=-0.000209, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.87]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.16it/s, pg=0.0591, ret=-0.000209, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.16it/s, pg=-0.0345, ret=2.93e-5, glen=96.8, tlen=257, kl=0, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.17it/s, pg=-0.0345, ret=2.93e-5, glen=96.8, tlen=257, kl=0, act_lr=2.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=-0.0227, ret=-0.000372, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.71]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=-0.0227, ret=-0.000372, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.0903, ret=0.00146, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.69]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.07it/s, pg=-0.0903, ret=0.00146, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.0934, ret=-0.000907, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=-0.0934, ret=-0.000907, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=0.0637, ret=-0.00047, glen=118, tlen=279, kl=0, act_lr=2.6e-7, ent=1.76]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:26<00:24,  1.12it/s, pg=0.0637, ret=-0.00047, glen=118, tlen=279, kl=0, act_lr=2.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.182, ret=-0.00114, glen=391, tlen=553, kl=0, act_lr=2.6e-7, ent=2.26] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:23,  1.11it/s, pg=0.182, ret=-0.00114, glen=391, tlen=553, kl=0, act_lr=2.6e-7, ent=2.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:23,  1.11it/s, pg=-0.00452, ret=0.000591, glen=130, tlen=291, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:22,  1.12it/s, pg=-0.00452, ret=0.000591, glen=130, tlen=291, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:22,  1.12it/s, pg=-0.36, ret=0.00104, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.69]    Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:21,  1.14it/s, pg=-0.36, ret=0.00104, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:21,  1.14it/s, pg=-0.0426, ret=-0.000918, glen=114, tlen=275, kl=0, act_lr=2.6e-7, ent=1.72]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.15it/s, pg=-0.0426, ret=-0.000918, glen=114, tlen=275, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.15it/s, pg=-0.122, ret=0.00126, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.64]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:19,  1.16it/s, pg=-0.122, ret=0.00126, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:19,  1.16it/s, pg=0.0905, ret=-0.000472, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.92]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.16it/s, pg=0.0905, ret=-0.000472, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.16it/s, pg=-0.225, ret=0.000221, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.63] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.16it/s, pg=-0.225, ret=0.000221, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.16it/s, pg=-0.0738, ret=0.000694, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.69]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.16it/s, pg=-0.0738, ret=0.000694, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.16it/s, pg=0.154, ret=0.000223, glen=124, tlen=284, kl=0, act_lr=2.6e-7, ent=1.97]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.16it/s, pg=0.154, ret=0.000223, glen=124, tlen=284, kl=0, act_lr=2.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=-0.122, ret=0.000834, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.72]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.17it/s, pg=-0.122, ret=0.000834, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.17it/s, pg=-0.164, ret=0.000858, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.54]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=-0.164, ret=0.000858, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=0.316, ret=0.000223, glen=151, tlen=312, kl=0, act_lr=2.6e-7, ent=2.16] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=0.316, ret=0.000223, glen=151, tlen=312, kl=0, act_lr=2.6e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=0.0946, ret=-0.00134, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.66]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=0.0946, ret=-0.00134, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.0197, ret=-0.000257, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.88]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.0197, ret=-0.000257, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0845, ret=0.000793, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.71]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.14it/s, pg=-0.0845, ret=0.000793, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.14it/s, pg=0.139, ret=-0.00178, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.87]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.15it/s, pg=0.139, ret=-0.00178, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.15it/s, pg=0.125, ret=-0.00188, glen=186, tlen=346, kl=0, act_lr=2.6e-7, ent=1.42]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.14it/s, pg=0.125, ret=-0.00188, glen=186, tlen=346, kl=0, act_lr=2.6e-7, ent=1.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.14it/s, pg=0.028, ret=6.72e-5, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.83] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.15it/s, pg=0.028, ret=6.72e-5, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.15it/s, pg=0.022, ret=-0.000282, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=2.34]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.16it/s, pg=0.022, ret=-0.000282, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=2.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.16it/s, pg=0.0121, ret=0.000783, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.87]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:06,  1.16it/s, pg=0.0121, ret=0.000783, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:06,  1.16it/s, pg=0.101, ret=-0.000986, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.101, ret=-0.000986, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.0901, ret=-0.00111, glen=123, tlen=284, kl=0, act_lr=2.6e-7, ent=1.75]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.17it/s, pg=0.0901, ret=-0.00111, glen=123, tlen=284, kl=0, act_lr=2.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.17it/s, pg=-0.032, ret=-0.000552, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.79]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.17it/s, pg=-0.032, ret=-0.000552, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=-0.117, ret=0.00115, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.63]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=-0.117, ret=0.00115, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=-0.0551, ret=-0.000221, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.76]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=-0.0551, ret=-0.000221, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=-0.133, ret=0.00125, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.56]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=-0.133, ret=0.00125, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.56]
2025-07-24 16:56:37.139 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.35s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.0843, ret=0.000149, glen=108, tlen=269, kl=0, act_lr=2.8e-7, ent=1.7]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.0843, ret=0.000149, glen=108, tlen=269, kl=0, act_lr=2.8e-7, ent=1.7]
2025-07-24 16:56:37.974 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 16:56:40.579 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 16:56:40.908 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 54.24s
2025-07-24 16:56:40.915 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.018242244062752558, 'actor_lr': 2.6034483467136506e-07, 'clip_ratio': 0.0, 'entropy': 1.752143958519245, 'kl': 0.0, 'response_length': 120.72242802587049, 'total_length': 281.317540662042, 'teacher_total_length': 292.39703947922277, 'return': -6.165954067094797e-05, 'policy_update_steps': 1.0}

Episode [2/20]:   8%|‚ñä         | 1/13 [04:12<50:26, 252.20s/it][A2025-07-24 16:56:40.963 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:58:48.154 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:58:48.328 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:58:48.328 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 127.37s
2025-07-24 16:58:50.256 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0093,avg_pass_at_n: 1.0000,avg_num_tokens: 110.9402,std_num_tokens: 136.6484,avg_correct_num_tokens: 102.8409,std_correct_num_tokens: 80.3895,avg_incorrect_num_tokens: 123.4079,std_incorrect_num_tokens: 192.9054
2025-07-24 16:58:50.708 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 16:58:53.627 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.92s
2025-07-24 16:59:22.448 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 16:59:22.448 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.82s
2025-07-24 16:59:24.284 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.34s
2025-07-24 16:59:24.284 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0006905885425853719, avg_kl: 0.0009041336664544329, avg_response_length: 113.81047716854953, avg_orm_score: 0.0, avg_custom_rewards: -0.0006905885425853719
2025-07-24 16:59:24.324 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter14_replay_buffer.jsonl
2025-07-24 16:59:26.237 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.92s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0193, ret=-0.00029, glen=130, tlen=291, kl=0.000899, act_lr=2.8e-7, ent=1.82]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.02s/it, pg=0.0193, ret=-0.00029, glen=130, tlen=291, kl=0.000899, act_lr=2.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.02s/it, pg=0.156, ret=-0.00196, glen=93.7, tlen=254, kl=0.000931, act_lr=2.8e-7, ent=1.62]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.08it/s, pg=0.156, ret=-0.00196, glen=93.7, tlen=254, kl=0.000931, act_lr=2.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.08it/s, pg=-0.124, ret=9.82e-6, glen=120, tlen=280, kl=0.000926, act_lr=2.8e-7, ent=1.93] Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.10it/s, pg=-0.124, ret=9.82e-6, glen=120, tlen=280, kl=0.000926, act_lr=2.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.10it/s, pg=-0.147, ret=0.000743, glen=104, tlen=265, kl=0.000936, act_lr=2.8e-7, ent=1.71]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:48,  1.10it/s, pg=-0.147, ret=0.000743, glen=104, tlen=265, kl=0.000936, act_lr=2.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:48,  1.10it/s, pg=-0.114, ret=0.000342, glen=95.3, tlen=256, kl=0.000895, act_lr=2.8e-7, ent=1.59]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.12it/s, pg=-0.114, ret=0.000342, glen=95.3, tlen=256, kl=0.000895, act_lr=2.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.12it/s, pg=-0.121, ret=-0.00448, glen=98.3, tlen=260, kl=0.000918, act_lr=2.8e-7, ent=1.7] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:46,  1.10it/s, pg=-0.121, ret=-0.00448, glen=98.3, tlen=260, kl=0.000918, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:46,  1.10it/s, pg=-0.188, ret=0.00118, glen=103, tlen=264, kl=0.000904, act_lr=2.8e-7, ent=1.69] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.12it/s, pg=-0.188, ret=0.00118, glen=103, tlen=264, kl=0.000904, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.12it/s, pg=-0.0248, ret=0.000862, glen=121, tlen=282, kl=0.000915, act_lr=2.8e-7, ent=2.06]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.14it/s, pg=-0.0248, ret=0.000862, glen=121, tlen=282, kl=0.000915, act_lr=2.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.14it/s, pg=0.0918, ret=-0.000369, glen=102, tlen=262, kl=0.000957, act_lr=2.8e-7, ent=1.7] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.0918, ret=-0.000369, glen=102, tlen=262, kl=0.000957, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.142, ret=0.000965, glen=99.6, tlen=260, kl=0.000934, act_lr=2.8e-7, ent=1.59]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=-0.142, ret=0.000965, glen=99.6, tlen=260, kl=0.000934, act_lr=2.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=0.12, ret=-0.00182, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.61]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.12, ret=-0.00182, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0225, ret=-0.00157, glen=100, tlen=261, kl=0.000946, act_lr=2.8e-7, ent=1.69]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=-0.0225, ret=-0.00157, glen=100, tlen=261, kl=0.000946, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.092, ret=-0.000109, glen=94.5, tlen=255, kl=0.000902, act_lr=2.8e-7, ent=1.64]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.092, ret=-0.000109, glen=94.5, tlen=255, kl=0.000902, act_lr=2.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.0952, ret=0.000228, glen=104, tlen=264, kl=0.000929, act_lr=2.8e-7, ent=1.67] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.15it/s, pg=-0.0952, ret=0.000228, glen=104, tlen=264, kl=0.000929, act_lr=2.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.15it/s, pg=-0.159, ret=0.00125, glen=116, tlen=277, kl=0.00096, act_lr=2.8e-7, ent=1.88]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.159, ret=0.00125, glen=116, tlen=277, kl=0.00096, act_lr=2.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=-0.222, ret=0.000263, glen=115, tlen=276, kl=0.000889, act_lr=2.8e-7, ent=1.75]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=-0.222, ret=0.000263, glen=115, tlen=276, kl=0.000889, act_lr=2.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=0.308, ret=-0.00168, glen=140, tlen=301, kl=0.000789, act_lr=2.8e-7, ent=2.5]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.16it/s, pg=0.308, ret=-0.00168, glen=140, tlen=301, kl=0.000789, act_lr=2.8e-7, ent=2.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.16it/s, pg=0.152, ret=-0.00189, glen=116, tlen=276, kl=0.000853, act_lr=2.8e-7, ent=1.88]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=0.152, ret=-0.00189, glen=116, tlen=276, kl=0.000853, act_lr=2.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=0.198, ret=-0.00172, glen=102, tlen=262, kl=0.000895, act_lr=2.8e-7, ent=1.72]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=0.198, ret=-0.00172, glen=102, tlen=262, kl=0.000895, act_lr=2.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.117, ret=0.000665, glen=125, tlen=285, kl=0.000928, act_lr=2.8e-7, ent=1.9]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.117, ret=0.000665, glen=125, tlen=285, kl=0.000928, act_lr=2.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=0.132, ret=-0.000176, glen=113, tlen=274, kl=0.000902, act_lr=2.8e-7, ent=1.94]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.132, ret=-0.000176, glen=113, tlen=274, kl=0.000902, act_lr=2.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.269, ret=0.00174, glen=110, tlen=270, kl=0.000903, act_lr=2.8e-7, ent=1.58] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.269, ret=0.00174, glen=110, tlen=270, kl=0.000903, act_lr=2.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=0.000916, ret=0.000205, glen=119, tlen=280, kl=0.000903, act_lr=2.8e-7, ent=1.61]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.000916, ret=0.000205, glen=119, tlen=280, kl=0.000903, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.117, ret=-0.0026, glen=115, tlen=277, kl=0.00093, act_lr=2.8e-7, ent=1.74]     Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.117, ret=-0.0026, glen=115, tlen=277, kl=0.00093, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.025, ret=0.00111, glen=110, tlen=270, kl=0.000908, act_lr=2.8e-7, ent=1.65]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.025, ret=0.00111, glen=110, tlen=270, kl=0.000908, act_lr=2.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0581, ret=0.000316, glen=120, tlen=281, kl=0.000909, act_lr=2.8e-7, ent=1.89]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0581, ret=0.000316, glen=120, tlen=281, kl=0.000909, act_lr=2.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.179, ret=0.00196, glen=105, tlen=266, kl=0.000912, act_lr=2.8e-7, ent=1.79]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=-0.179, ret=0.00196, glen=105, tlen=266, kl=0.000912, act_lr=2.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=-0.0406, ret=0.000538, glen=103, tlen=263, kl=0.000919, act_lr=2.8e-7, ent=1.57]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.18it/s, pg=-0.0406, ret=0.000538, glen=103, tlen=263, kl=0.000919, act_lr=2.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.18it/s, pg=-0.0606, ret=0.000437, glen=98.1, tlen=259, kl=0.00091, act_lr=2.8e-7, ent=1.63]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=-0.0606, ret=0.000437, glen=98.1, tlen=259, kl=0.00091, act_lr=2.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=-0.087, ret=9.55e-5, glen=102, tlen=263, kl=0.000868, act_lr=2.8e-7, ent=1.58]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=-0.087, ret=9.55e-5, glen=102, tlen=263, kl=0.000868, act_lr=2.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.2, ret=-0.00238, glen=113, tlen=274, kl=0.000948, act_lr=2.8e-7, ent=1.78]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.2, ret=-0.00238, glen=113, tlen=274, kl=0.000948, act_lr=2.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.00183, ret=-0.000425, glen=109, tlen=269, kl=0.000917, act_lr=2.8e-7, ent=1.6]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.14it/s, pg=-0.00183, ret=-0.000425, glen=109, tlen=269, kl=0.000917, act_lr=2.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=0.0249, ret=0.000216, glen=132, tlen=293, kl=0.000913, act_lr=2.8e-7, ent=1.97]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=0.0249, ret=0.000216, glen=132, tlen=293, kl=0.000913, act_lr=2.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=-0.0125, ret=0.000189, glen=101, tlen=262, kl=0.000877, act_lr=2.8e-7, ent=1.59]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.13it/s, pg=-0.0125, ret=0.000189, glen=101, tlen=262, kl=0.000877, act_lr=2.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.13it/s, pg=-0.0887, ret=-0.000223, glen=104, tlen=265, kl=0.000878, act_lr=2.8e-7, ent=1.76]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=-0.0887, ret=-0.000223, glen=104, tlen=265, kl=0.000878, act_lr=2.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=0.0115, ret=-0.00103, glen=104, tlen=265, kl=0.000889, act_lr=2.8e-7, ent=1.67]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=0.0115, ret=-0.00103, glen=104, tlen=265, kl=0.000889, act_lr=2.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.0695, ret=0.00129, glen=121, tlen=282, kl=0.000887, act_lr=2.8e-7, ent=1.74]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.0695, ret=0.00129, glen=121, tlen=282, kl=0.000887, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.152, ret=0.0011, glen=112, tlen=273, kl=0.000863, act_lr=2.8e-7, ent=1.94]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=-0.152, ret=0.0011, glen=112, tlen=273, kl=0.000863, act_lr=2.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.16it/s, pg=0.239, ret=-0.00133, glen=110, tlen=271, kl=0.000948, act_lr=2.8e-7, ent=1.7]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.239, ret=-0.00133, glen=110, tlen=271, kl=0.000948, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.00172, ret=4.96e-5, glen=104, tlen=264, kl=0.000927, act_lr=2.8e-7, ent=1.71]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.00172, ret=4.96e-5, glen=104, tlen=264, kl=0.000927, act_lr=2.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.092, ret=0.00148, glen=110, tlen=271, kl=0.000925, act_lr=2.8e-7, ent=1.76]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.092, ret=0.00148, glen=110, tlen=271, kl=0.000925, act_lr=2.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.21, ret=-0.00164, glen=120, tlen=281, kl=0.000881, act_lr=2.8e-7, ent=1.63] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.21, ret=-0.00164, glen=120, tlen=281, kl=0.000881, act_lr=2.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.061, ret=-0.000327, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.82]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.061, ret=-0.000327, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.104, ret=0.000491, glen=113, tlen=274, kl=0.000908, act_lr=2.8e-7, ent=1.61] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.104, ret=0.000491, glen=113, tlen=274, kl=0.000908, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.111, ret=0.00087, glen=115, tlen=276, kl=0.000896, act_lr=2.8e-7, ent=1.94] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.111, ret=0.00087, glen=115, tlen=276, kl=0.000896, act_lr=2.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=6.1e-5, ret=-0.00167, glen=114, tlen=275, kl=0.000909, act_lr=2.8e-7, ent=1.57]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=6.1e-5, ret=-0.00167, glen=114, tlen=275, kl=0.000909, act_lr=2.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.352, ret=0.00033, glen=169, tlen=330, kl=0.000752, act_lr=2.8e-7, ent=2.41]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.352, ret=0.00033, glen=169, tlen=330, kl=0.000752, act_lr=2.8e-7, ent=2.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.229, ret=0.000519, glen=227, tlen=387, kl=0.00079, act_lr=2.8e-7, ent=2.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.15it/s, pg=0.229, ret=0.000519, glen=227, tlen=387, kl=0.00079, act_lr=2.8e-7, ent=2.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.15it/s, pg=0.077, ret=0.00179, glen=138, tlen=299, kl=0.000861, act_lr=2.8e-7, ent=2.07]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.16it/s, pg=0.077, ret=0.00179, glen=138, tlen=299, kl=0.000861, act_lr=2.8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.16it/s, pg=0.129, ret=-0.00103, glen=125, tlen=286, kl=0.000898, act_lr=2.8e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.16it/s, pg=0.129, ret=-0.00103, glen=125, tlen=286, kl=0.000898, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.16it/s, pg=-0.144, ret=0.00121, glen=120, tlen=281, kl=0.000937, act_lr=2.8e-7, ent=1.7] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.144, ret=0.00121, glen=120, tlen=281, kl=0.000937, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=-0.0637, ret=0.000135, glen=114, tlen=275, kl=0.000937, act_lr=2.8e-7, ent=1.78]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0637, ret=0.000135, glen=114, tlen=275, kl=0.000937, act_lr=2.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=-0.205, ret=0.00113, glen=105, tlen=266, kl=0.000887, act_lr=2.8e-7, ent=1.69]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.205, ret=0.00113, glen=105, tlen=266, kl=0.000887, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.0141, ret=-0.000547, glen=108, tlen=269, kl=0.000937, act_lr=2.8e-7, ent=1.74]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=-0.0141, ret=-0.000547, glen=108, tlen=269, kl=0.000937, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.102, ret=0.000407, glen=101, tlen=262, kl=0.000902, act_lr=2.8e-7, ent=1.6]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.102, ret=0.000407, glen=101, tlen=262, kl=0.000902, act_lr=2.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0684, ret=-0.000756, glen=109, tlen=269, kl=0.000924, act_lr=2.8e-7, ent=1.8]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0684, ret=-0.000756, glen=109, tlen=269, kl=0.000924, act_lr=2.8e-7, ent=1.8]
2025-07-24 17:00:16.021 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.0094, ret=-0.000802, glen=93.7, tlen=255, kl=0.000897, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.0094, ret=-0.000802, glen=93.7, tlen=255, kl=0.000897, act_lr=3e-7, ent=1.54]
2025-07-24 17:00:16.879 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:00:19.398 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 17:00:19.724 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.38s
2025-07-24 17:00:19.732 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.012011912831088952, 'actor_lr': 2.8035087221692183e-07, 'clip_ratio': 0.0, 'entropy': 1.7733097954800254, 'kl': 0.000904317487750137, 'response_length': 113.70133142304002, 'total_length': 274.47614649722453, 'teacher_total_length': 287.0512256287692, 'return': -0.00011782957128097973, 'policy_update_steps': 1.0}

Episode [2/20]:  15%|‚ñà‚ñå        | 2/13 [07:51<42:38, 232.57s/it][A2025-07-24 17:00:19.776 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:01:53.455 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:01:53.639 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:01:53.639 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 93.86s
2025-07-24 17:01:55.582 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0083,avg_pass_at_n: 1.0000,avg_num_tokens: 108.7333,std_num_tokens: 114.1218,avg_correct_num_tokens: 100.7200,std_correct_num_tokens: 84.5683,avg_incorrect_num_tokens: 123.2784,std_incorrect_num_tokens: 152.8202
2025-07-24 17:01:55.880 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.24s
2025-07-24 17:01:58.820 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.94s
2025-07-24 17:02:27.188 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:02:27.189 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.37s
2025-07-24 17:02:28.798 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.14s
2025-07-24 17:02:28.799 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0009074466998089572, avg_kl: 0.0010004257407423627, avg_response_length: 110.29346431851921, avg_orm_score: 0.0, avg_custom_rewards: -0.0009074466998089572
2025-07-24 17:02:28.857 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter15_replay_buffer.jsonl
2025-07-24 17:02:30.761 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0857, ret=-8.2e-5, glen=121, tlen=281, kl=0.000998, act_lr=3e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:57,  1.04s/it, pg=0.0857, ret=-8.2e-5, glen=121, tlen=281, kl=0.000998, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:57,  1.04s/it, pg=0.00806, ret=-0.00193, glen=107, tlen=268, kl=0.001, act_lr=3e-7, ent=1.74] Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.07it/s, pg=0.00806, ret=-0.00193, glen=107, tlen=268, kl=0.001, act_lr=3e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.07it/s, pg=0.153, ret=-0.000369, glen=118, tlen=278, kl=0.000999, act_lr=3e-7, ent=1.99]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.11it/s, pg=0.153, ret=-0.000369, glen=118, tlen=278, kl=0.000999, act_lr=3e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.11it/s, pg=0.145, ret=0.000795, glen=124, tlen=285, kl=0.000955, act_lr=3e-7, ent=1.66] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:45,  1.13it/s, pg=0.145, ret=0.000795, glen=124, tlen=285, kl=0.000955, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:45,  1.13it/s, pg=0.108, ret=-0.00145, glen=94.3, tlen=254, kl=0.000965, act_lr=3e-7, ent=1.63]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=0.108, ret=-0.00145, glen=94.3, tlen=254, kl=0.000965, act_lr=3e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=0.13, ret=-0.00181, glen=126, tlen=287, kl=0.000984, act_lr=3e-7, ent=1.89]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.14it/s, pg=0.13, ret=-0.00181, glen=126, tlen=287, kl=0.000984, act_lr=3e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.14it/s, pg=-0.0962, ret=-0.00144, glen=112, tlen=272, kl=0.00101, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=-0.0962, ret=-0.00144, glen=112, tlen=272, kl=0.00101, act_lr=3e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=0.0673, ret=-0.00165, glen=98.8, tlen=259, kl=0.00108, act_lr=3e-7, ent=1.66]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.0673, ret=-0.00165, glen=98.8, tlen=259, kl=0.00108, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.167, ret=-0.00193, glen=113, tlen=273, kl=0.000993, act_lr=3e-7, ent=1.7]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.16it/s, pg=0.167, ret=-0.00193, glen=113, tlen=273, kl=0.000993, act_lr=3e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=0.152, ret=-0.000931, glen=125, tlen=285, kl=0.00093, act_lr=3e-7, ent=1.94]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=0.152, ret=-0.000931, glen=125, tlen=285, kl=0.00093, act_lr=3e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=0.0105, ret=-0.000132, glen=104, tlen=264, kl=0.00108, act_lr=3e-7, ent=1.74]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=0.0105, ret=-0.000132, glen=104, tlen=264, kl=0.00108, act_lr=3e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0103, ret=0.00046, glen=108, tlen=269, kl=0.000982, act_lr=3e-7, ent=1.69]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0103, ret=0.00046, glen=108, tlen=269, kl=0.000982, act_lr=3e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0554, ret=0.00057, glen=117, tlen=278, kl=0.000978, act_lr=3e-7, ent=1.71] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0554, ret=0.00057, glen=117, tlen=278, kl=0.000978, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.0176, ret=-0.00144, glen=105, tlen=265, kl=0.00102, act_lr=3e-7, ent=1.71]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.0176, ret=-0.00144, glen=105, tlen=265, kl=0.00102, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.199, ret=-1.44e-5, glen=123, tlen=284, kl=0.000886, act_lr=3e-7, ent=1.98]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.199, ret=-1.44e-5, glen=123, tlen=284, kl=0.000886, act_lr=3e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.0967, ret=0.0017, glen=138, tlen=298, kl=0.000977, act_lr=3e-7, ent=2.16] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.16it/s, pg=-0.0967, ret=0.0017, glen=138, tlen=298, kl=0.000977, act_lr=3e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.063, ret=0.000135, glen=120, tlen=280, kl=0.00105, act_lr=3e-7, ent=1.82]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.16it/s, pg=-0.063, ret=0.000135, glen=120, tlen=280, kl=0.00105, act_lr=3e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.156, ret=0.000398, glen=110, tlen=270, kl=0.000978, act_lr=3e-7, ent=1.69]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.156, ret=0.000398, glen=110, tlen=270, kl=0.000978, act_lr=3e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.201, ret=0.00138, glen=109, tlen=270, kl=0.00101, act_lr=3e-7, ent=1.76]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.201, ret=0.00138, glen=109, tlen=270, kl=0.00101, act_lr=3e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.0329, ret=0.00167, glen=125, tlen=286, kl=0.001, act_lr=3e-7, ent=1.99] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=-0.0329, ret=0.00167, glen=125, tlen=286, kl=0.001, act_lr=3e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.223, ret=0.000759, glen=104, tlen=264, kl=0.00103, act_lr=3e-7, ent=1.86]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.223, ret=0.000759, glen=104, tlen=264, kl=0.00103, act_lr=3e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.0745, ret=0.00113, glen=106, tlen=266, kl=0.00102, act_lr=3e-7, ent=1.66]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=-0.0745, ret=0.00113, glen=106, tlen=266, kl=0.00102, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=-0.068, ret=0.000485, glen=103, tlen=263, kl=0.00102, act_lr=3e-7, ent=1.6] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.18it/s, pg=-0.068, ret=0.000485, glen=103, tlen=263, kl=0.00102, act_lr=3e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.18it/s, pg=-0.333, ret=0.00254, glen=97.8, tlen=258, kl=0.00106, act_lr=3e-7, ent=1.75]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.333, ret=0.00254, glen=97.8, tlen=258, kl=0.00106, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.0176, ret=0.0004, glen=110, tlen=270, kl=0.00103, act_lr=3e-7, ent=1.7]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0176, ret=0.0004, glen=110, tlen=270, kl=0.00103, act_lr=3e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.082, ret=-0.00287, glen=109, tlen=269, kl=0.001, act_lr=3e-7, ent=1.71] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.082, ret=-0.00287, glen=109, tlen=269, kl=0.001, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.105, ret=-0.000728, glen=94.4, tlen=255, kl=0.001, act_lr=3e-7, ent=1.58]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.105, ret=-0.000728, glen=94.4, tlen=255, kl=0.001, act_lr=3e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0679, ret=0.000288, glen=106, tlen=266, kl=0.001, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.18it/s, pg=-0.0679, ret=0.000288, glen=106, tlen=266, kl=0.001, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.18it/s, pg=0.0962, ret=0.000213, glen=114, tlen=274, kl=0.000996, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=0.0962, ret=0.000213, glen=114, tlen=274, kl=0.000996, act_lr=3e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.0581, ret=0.000558, glen=97.7, tlen=258, kl=0.000989, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.0581, ret=0.000558, glen=97.7, tlen=258, kl=0.000989, act_lr=3e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.0338, ret=-0.000136, glen=108, tlen=268, kl=0.000981, act_lr=3e-7, ent=1.67] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:26<00:22,  1.12it/s, pg=0.0338, ret=-0.000136, glen=108, tlen=268, kl=0.000981, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.0797, ret=-0.000401, glen=103, tlen=263, kl=0.000977, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.12it/s, pg=0.0797, ret=-0.000401, glen=103, tlen=263, kl=0.000977, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=-0.00641, ret=0.00123, glen=110, tlen=270, kl=0.001, act_lr=3e-7, ent=1.67]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.12it/s, pg=-0.00641, ret=0.00123, glen=110, tlen=270, kl=0.001, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.12it/s, pg=0.00464, ret=-0.000319, glen=118, tlen=278, kl=0.001, act_lr=3e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.12it/s, pg=0.00464, ret=-0.000319, glen=118, tlen=278, kl=0.001, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.12it/s, pg=0.0905, ret=-0.000405, glen=119, tlen=279, kl=0.00101, act_lr=3e-7, ent=1.78]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.14it/s, pg=0.0905, ret=-0.000405, glen=119, tlen=279, kl=0.00101, act_lr=3e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.14it/s, pg=-0.136, ret=0.000559, glen=97.3, tlen=258, kl=0.00101, act_lr=3e-7, ent=1.53]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=-0.136, ret=0.000559, glen=97.3, tlen=258, kl=0.00101, act_lr=3e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=0.049, ret=0.000669, glen=136, tlen=296, kl=0.000963, act_lr=3e-7, ent=1.87] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.15it/s, pg=0.049, ret=0.000669, glen=136, tlen=296, kl=0.000963, act_lr=3e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.15it/s, pg=0.112, ret=-0.000923, glen=96.2, tlen=256, kl=0.00101, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.112, ret=-0.000923, glen=96.2, tlen=256, kl=0.00101, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0203, ret=-0.00122, glen=108, tlen=268, kl=0.000957, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0203, ret=-0.00122, glen=108, tlen=268, kl=0.000957, act_lr=3e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=0.0566, ret=-0.00011, glen=111, tlen=270, kl=0.001, act_lr=3e-7, ent=1.62]   Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=0.0566, ret=-0.00011, glen=111, tlen=270, kl=0.001, act_lr=3e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.103, ret=-0.000102, glen=114, tlen=275, kl=0.00105, act_lr=3e-7, ent=1.79]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.103, ret=-0.000102, glen=114, tlen=275, kl=0.00105, act_lr=3e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.103, ret=0.00104, glen=97.4, tlen=258, kl=0.000996, act_lr=3e-7, ent=1.65]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.103, ret=0.00104, glen=97.4, tlen=258, kl=0.000996, act_lr=3e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.111, ret=-4.74e-5, glen=98.2, tlen=258, kl=0.00102, act_lr=3e-7, ent=1.72]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.111, ret=-4.74e-5, glen=98.2, tlen=258, kl=0.00102, act_lr=3e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.0845, ret=0.000954, glen=121, tlen=281, kl=0.00103, act_lr=3e-7, ent=1.94]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0845, ret=0.000954, glen=121, tlen=281, kl=0.00103, act_lr=3e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0912, ret=-7.66e-6, glen=100, tlen=261, kl=0.00105, act_lr=3e-7, ent=1.63]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.0912, ret=-7.66e-6, glen=100, tlen=261, kl=0.00105, act_lr=3e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.369, ret=0.00192, glen=94.5, tlen=255, kl=0.00104, act_lr=3e-7, ent=1.55] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.18it/s, pg=-0.369, ret=0.00192, glen=94.5, tlen=255, kl=0.00104, act_lr=3e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.18it/s, pg=-0.102, ret=-0.00056, glen=93.6, tlen=253, kl=0.00102, act_lr=3e-7, ent=1.6]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.102, ret=-0.00056, glen=93.6, tlen=253, kl=0.00102, act_lr=3e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.109, ret=-0.00215, glen=112, tlen=273, kl=0.00096, act_lr=3e-7, ent=1.55] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.109, ret=-0.00215, glen=112, tlen=273, kl=0.00096, act_lr=3e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.00894, ret=0.00136, glen=108, tlen=268, kl=0.00101, act_lr=3e-7, ent=1.82]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.00894, ret=0.00136, glen=108, tlen=268, kl=0.00101, act_lr=3e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.132, ret=0.000867, glen=120, tlen=280, kl=0.000981, act_lr=3e-7, ent=1.6] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.132, ret=0.000867, glen=120, tlen=280, kl=0.000981, act_lr=3e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0342, ret=-0.000824, glen=109, tlen=269, kl=0.00101, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0342, ret=-0.000824, glen=109, tlen=269, kl=0.00101, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0789, ret=-0.000533, glen=120, tlen=281, kl=0.000966, act_lr=3e-7, ent=1.69]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=-0.0789, ret=-0.000533, glen=120, tlen=281, kl=0.000966, act_lr=3e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.157, ret=2.5e-5, glen=103, tlen=263, kl=0.00104, act_lr=3e-7, ent=1.76]     Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.157, ret=2.5e-5, glen=103, tlen=263, kl=0.00104, act_lr=3e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.11, ret=-0.000137, glen=109, tlen=269, kl=0.000966, act_lr=3e-7, ent=1.64]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=0.11, ret=-0.000137, glen=109, tlen=269, kl=0.000966, act_lr=3e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.000244, ret=0.00159, glen=119, tlen=280, kl=0.00101, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=-0.000244, ret=0.00159, glen=119, tlen=280, kl=0.00101, act_lr=3e-7, ent=1.81]
2025-07-24 17:03:19.615 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.0649, ret=-0.00145, glen=106, tlen=266, kl=0.000973, act_lr=3.2e-7, ent=1.76]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=0.0649, ret=-0.00145, glen=106, tlen=266, kl=0.000973, act_lr=3.2e-7, ent=1.76]
2025-07-24 17:03:20.652 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.97s
2025-07-24 17:03:23.253 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 17:03:23.580 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.56s
2025-07-24 17:03:23.587 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011712959834507533, 'actor_lr': 3.003571532441843e-07, 'clip_ratio': 0.0, 'entropy': 1.7289215901068278, 'kl': 0.001000523567199707, 'response_length': 110.19897638048444, 'total_length': 270.40987750462125, 'teacher_total_length': 282.2843802315848, 'return': -4.291131816509213e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [10:54<35:03, 210.32s/it][A2025-07-24 17:03:23.630 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:05:53.580 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:05:53.764 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:05:53.765 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 150.14s
2025-07-24 17:05:55.755 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0074,avg_pass_at_n: 1.0000,avg_num_tokens: 111.1066,std_num_tokens: 174.6282,avg_correct_num_tokens: 102.2503,std_correct_num_tokens: 93.7130,avg_incorrect_num_tokens: 127.2418,std_incorrect_num_tokens: 263.9140
2025-07-24 17:05:56.201 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.44s
2025-07-24 17:05:59.618 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.42s
2025-07-24 17:06:28.701 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 226
2025-07-24 17:06:28.706 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.08s
2025-07-24 17:06:30.151 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.02s
2025-07-24 17:06:30.152 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0028858509378825044, avg_kl: 0.001027183195131015, avg_response_length: 119.2440990009139, avg_orm_score: 0.0, avg_custom_rewards: -0.0028858509378825044
2025-07-24 17:06:30.193 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter16_replay_buffer.jsonl
2025-07-24 17:06:32.094 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.059, ret=-0.00195, glen=110, tlen=270, kl=0.00101, act_lr=3.2e-7, ent=1.63]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.059, ret=-0.00195, glen=110, tlen=270, kl=0.00101, act_lr=3.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.159, ret=-0.00141, glen=119, tlen=280, kl=0.00102, act_lr=3.2e-7, ent=2.12]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.159, ret=-0.00141, glen=119, tlen=280, kl=0.00102, act_lr=3.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=-0.0616, ret=-0.000482, glen=103, tlen=264, kl=0.00101, act_lr=3.2e-7, ent=1.8]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=-0.0616, ret=-0.000482, glen=103, tlen=264, kl=0.00101, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=0.137, ret=-0.00139, glen=107, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.71]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=0.137, ret=-0.00139, glen=107, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=-0.052, ret=-0.00256, glen=106, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.68]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.052, ret=-0.00256, glen=106, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=-0.0311, ret=-0.0021, glen=109, tlen=269, kl=0.00104, act_lr=3.2e-7, ent=1.66]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=-0.0311, ret=-0.0021, glen=109, tlen=269, kl=0.00104, act_lr=3.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.168, ret=0.00126, glen=113, tlen=273, kl=0.00106, act_lr=3.2e-7, ent=1.7]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.14it/s, pg=-0.168, ret=0.00126, glen=113, tlen=273, kl=0.00106, act_lr=3.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.14it/s, pg=-0.267, ret=0.00125, glen=119, tlen=280, kl=0.00104, act_lr=3.2e-7, ent=2.01]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=-0.267, ret=0.00125, glen=119, tlen=280, kl=0.00104, act_lr=3.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:42,  1.15it/s, pg=-0.123, ret=0.000276, glen=96.3, tlen=257, kl=0.00105, act_lr=3.2e-7, ent=1.64]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.12it/s, pg=-0.123, ret=0.000276, glen=96.3, tlen=257, kl=0.00105, act_lr=3.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.12it/s, pg=-0.0805, ret=0.000641, glen=110, tlen=271, kl=0.00106, act_lr=3.2e-7, ent=1.78]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.13it/s, pg=-0.0805, ret=0.000641, glen=110, tlen=271, kl=0.00106, act_lr=3.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.13it/s, pg=-0.0139, ret=0.000428, glen=118, tlen=279, kl=0.00102, act_lr=3.2e-7, ent=1.69]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.14it/s, pg=-0.0139, ret=0.000428, glen=118, tlen=279, kl=0.00102, act_lr=3.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.14it/s, pg=0.011, ret=-6.8e-5, glen=111, tlen=272, kl=0.00102, act_lr=3.2e-7, ent=1.75]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=0.011, ret=-6.8e-5, glen=111, tlen=272, kl=0.00102, act_lr=3.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=0.148, ret=-0.000256, glen=201, tlen=360, kl=0.00102, act_lr=3.2e-7, ent=2.25]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.14it/s, pg=0.148, ret=-0.000256, glen=201, tlen=360, kl=0.00102, act_lr=3.2e-7, ent=2.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.14it/s, pg=-0.133, ret=0.000837, glen=106, tlen=266, kl=0.00108, act_lr=3.2e-7, ent=1.73]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=-0.133, ret=0.000837, glen=106, tlen=266, kl=0.00108, act_lr=3.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=-0.196, ret=0.000923, glen=90.1, tlen=250, kl=0.00106, act_lr=3.2e-7, ent=1.57]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.196, ret=0.000923, glen=90.1, tlen=250, kl=0.00106, act_lr=3.2e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=-0.173, ret=0.0012, glen=118, tlen=278, kl=0.00103, act_lr=3.2e-7, ent=1.84]   Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.12it/s, pg=-0.173, ret=0.0012, glen=118, tlen=278, kl=0.00103, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:15<00:36,  1.12it/s, pg=-0.0416, ret=-0.000252, glen=103, tlen=263, kl=0.00104, act_lr=3.2e-7, ent=1.61]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.0416, ret=-0.000252, glen=103, tlen=263, kl=0.00104, act_lr=3.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.083, ret=-0.000116, glen=110, tlen=270, kl=0.00102, act_lr=3.2e-7, ent=1.87] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.083, ret=-0.000116, glen=110, tlen=270, kl=0.00102, act_lr=3.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=0.121, ret=-0.000823, glen=124, tlen=285, kl=0.00106, act_lr=3.2e-7, ent=2.04] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.121, ret=-0.000823, glen=124, tlen=285, kl=0.00106, act_lr=3.2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.072, ret=0.000149, glen=104, tlen=265, kl=0.00102, act_lr=3.2e-7, ent=1.83]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.072, ret=0.000149, glen=104, tlen=265, kl=0.00102, act_lr=3.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.168, ret=0.00113, glen=104, tlen=265, kl=0.00107, act_lr=3.2e-7, ent=1.74] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.16it/s, pg=-0.168, ret=0.00113, glen=104, tlen=265, kl=0.00107, act_lr=3.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.16it/s, pg=-0.114, ret=0.000862, glen=103, tlen=263, kl=0.00101, act_lr=3.2e-7, ent=1.65]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.114, ret=0.000862, glen=103, tlen=263, kl=0.00101, act_lr=3.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.147, ret=0.00235, glen=116, tlen=276, kl=0.00103, act_lr=3.2e-7, ent=2.07] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.147, ret=0.00235, glen=116, tlen=276, kl=0.00103, act_lr=3.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:21<00:29,  1.17it/s, pg=0.12, ret=-0.00126, glen=101, tlen=261, kl=0.00108, act_lr=3.2e-7, ent=1.78] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.12, ret=-0.00126, glen=101, tlen=261, kl=0.00108, act_lr=3.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0222, ret=5.42e-5, glen=112, tlen=272, kl=0.00106, act_lr=3.2e-7, ent=1.81]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0222, ret=5.42e-5, glen=112, tlen=272, kl=0.00106, act_lr=3.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.192, ret=-0.00131, glen=115, tlen=275, kl=0.000993, act_lr=3.2e-7, ent=1.9] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.192, ret=-0.00131, glen=115, tlen=275, kl=0.000993, act_lr=3.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.0276, ret=0.00153, glen=111, tlen=271, kl=0.00101, act_lr=3.2e-7, ent=2.1]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=-0.0276, ret=0.00153, glen=111, tlen=271, kl=0.00101, act_lr=3.2e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0979, ret=0.000566, glen=145, tlen=305, kl=0.000959, act_lr=3.2e-7, ent=2.67]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.16it/s, pg=0.0979, ret=0.000566, glen=145, tlen=305, kl=0.000959, act_lr=3.2e-7, ent=2.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.16it/s, pg=0.00751, ret=-0.00134, glen=115, tlen=275, kl=0.00103, act_lr=3.2e-7, ent=1.82]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=0.00751, ret=-0.00134, glen=115, tlen=275, kl=0.00103, act_lr=3.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.105, ret=0.000243, glen=109, tlen=269, kl=0.00109, act_lr=3.2e-7, ent=1.84] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.09it/s, pg=-0.105, ret=0.000243, glen=109, tlen=269, kl=0.00109, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.09it/s, pg=-0.251, ret=0.00212, glen=104, tlen=264, kl=0.00108, act_lr=3.2e-7, ent=1.62] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.11it/s, pg=-0.251, ret=0.00212, glen=104, tlen=264, kl=0.00108, act_lr=3.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.11it/s, pg=-0.0129, ret=-0.000268, glen=111, tlen=271, kl=0.00102, act_lr=3.2e-7, ent=1.98]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=-0.0129, ret=-0.000268, glen=111, tlen=271, kl=0.00102, act_lr=3.2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:29<00:22,  1.13it/s, pg=0.00995, ret=-0.00117, glen=108, tlen=268, kl=0.00101, act_lr=3.2e-7, ent=1.84] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=0.00995, ret=-0.00117, glen=108, tlen=268, kl=0.00101, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.14, ret=-0.000362, glen=102, tlen=262, kl=0.00111, act_lr=3.2e-7, ent=1.8]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.14, ret=-0.000362, glen=102, tlen=262, kl=0.00111, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.0603, ret=-8.67e-5, glen=97, tlen=257, kl=0.00108, act_lr=3.2e-7, ent=1.64]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.0603, ret=-8.67e-5, glen=97, tlen=257, kl=0.00108, act_lr=3.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.0744, ret=-0.000316, glen=121, tlen=282, kl=0.00103, act_lr=3.2e-7, ent=1.85]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0744, ret=-0.000316, glen=121, tlen=282, kl=0.00103, act_lr=3.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.113, ret=-0.000538, glen=100, tlen=260, kl=0.00106, act_lr=3.2e-7, ent=1.64] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.113, ret=-0.000538, glen=100, tlen=260, kl=0.00106, act_lr=3.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.0381, ret=-9.95e-5, glen=110, tlen=270, kl=0.001, act_lr=3.2e-7, ent=1.84]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0381, ret=-9.95e-5, glen=110, tlen=270, kl=0.001, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=0.287, ret=-0.0167, glen=388, tlen=549, kl=0.000831, act_lr=3.2e-7, ent=1.3] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.14it/s, pg=0.287, ret=-0.0167, glen=388, tlen=549, kl=0.000831, act_lr=3.2e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:35<00:15,  1.14it/s, pg=0.308, ret=-0.00248, glen=282, tlen=443, kl=0.000843, act_lr=3.2e-7, ent=2.77]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:15,  1.12it/s, pg=0.308, ret=-0.00248, glen=282, tlen=443, kl=0.000843, act_lr=3.2e-7, ent=2.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:15,  1.12it/s, pg=-0.101, ret=0.000376, glen=107, tlen=268, kl=0.00103, act_lr=3.2e-7, ent=1.63]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:14,  1.14it/s, pg=-0.101, ret=0.000376, glen=107, tlen=268, kl=0.00103, act_lr=3.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:14,  1.14it/s, pg=0.0177, ret=-0.000163, glen=126, tlen=286, kl=0.00101, act_lr=3.2e-7, ent=1.91]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:13,  1.15it/s, pg=0.0177, ret=-0.000163, glen=126, tlen=286, kl=0.00101, act_lr=3.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:13,  1.15it/s, pg=0.00317, ret=-0.000533, glen=120, tlen=281, kl=0.00101, act_lr=3.2e-7, ent=1.92]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.16it/s, pg=0.00317, ret=-0.000533, glen=120, tlen=281, kl=0.00101, act_lr=3.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=0.0264, ret=0.000651, glen=111, tlen=272, kl=0.00105, act_lr=3.2e-7, ent=1.81]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.0264, ret=0.000651, glen=111, tlen=272, kl=0.00105, act_lr=3.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.16it/s, pg=-0.0828, ret=-0.000698, glen=117, tlen=278, kl=0.00101, act_lr=3.2e-7, ent=1.8]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0828, ret=-0.000698, glen=117, tlen=278, kl=0.00101, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.17it/s, pg=0.0955, ret=0.00085, glen=113, tlen=274, kl=0.00106, act_lr=3.2e-7, ent=1.89]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0955, ret=0.00085, glen=113, tlen=274, kl=0.00106, act_lr=3.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:41<00:09,  1.17it/s, pg=0.324, ret=-0.00223, glen=117, tlen=277, kl=0.00102, act_lr=3.2e-7, ent=1.92]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.324, ret=-0.00223, glen=117, tlen=277, kl=0.00102, act_lr=3.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.234, ret=0.000626, glen=128, tlen=288, kl=0.000995, act_lr=3.2e-7, ent=2.13]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.15it/s, pg=-0.234, ret=0.000626, glen=128, tlen=288, kl=0.000995, act_lr=3.2e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.15it/s, pg=0.00909, ret=0.000214, glen=110, tlen=270, kl=0.00104, act_lr=3.2e-7, ent=1.79]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.15it/s, pg=0.00909, ret=0.000214, glen=110, tlen=270, kl=0.00104, act_lr=3.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.15it/s, pg=-0.137, ret=0.000394, glen=93.6, tlen=254, kl=0.00102, act_lr=3.2e-7, ent=1.8] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.16it/s, pg=-0.137, ret=0.000394, glen=93.6, tlen=254, kl=0.00102, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.16it/s, pg=-0.131, ret=0.000537, glen=103, tlen=263, kl=0.00102, act_lr=3.2e-7, ent=1.66]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.131, ret=0.000537, glen=103, tlen=263, kl=0.00102, act_lr=3.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=0.00946, ret=-0.000251, glen=108, tlen=269, kl=0.000979, act_lr=3.2e-7, ent=1.8]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.00946, ret=-0.000251, glen=108, tlen=269, kl=0.000979, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=-0.0494, ret=5.67e-6, glen=104, tlen=264, kl=0.00105, act_lr=3.2e-7, ent=1.61]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.0494, ret=5.67e-6, glen=104, tlen=264, kl=0.00105, act_lr=3.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.17it/s, pg=0.0171, ret=-0.000318, glen=93.6, tlen=254, kl=0.00105, act_lr=3.2e-7, ent=1.52]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=0.0171, ret=-0.000318, glen=93.6, tlen=254, kl=0.00105, act_lr=3.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0541, ret=0.000874, glen=105, tlen=266, kl=0.00103, act_lr=3.2e-7, ent=1.67] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0541, ret=0.000874, glen=105, tlen=266, kl=0.00103, act_lr=3.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0355, ret=-0.000357, glen=101, tlen=262, kl=0.00101, act_lr=3.2e-7, ent=1.8] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0355, ret=-0.000357, glen=101, tlen=262, kl=0.00101, act_lr=3.2e-7, ent=1.8]
2025-07-24 17:07:22.068 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.80s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.113, ret=-0.0006, glen=197, tlen=356, kl=0.00099, act_lr=3.4e-7, ent=2.42]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.113, ret=-0.0006, glen=197, tlen=356, kl=0.00099, act_lr=3.4e-7, ent=2.42]
2025-07-24 17:07:22.862 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.74s
2025-07-24 17:07:25.404 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 17:07:25.733 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.58s
2025-07-24 17:07:25.740 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.019771274767423932, 'actor_lr': 3.203508755265367e-07, 'clip_ratio': 0.0, 'entropy': 1.8358874362811708, 'kl': 0.001025835673014323, 'response_length': 120.7800596806041, 'total_length': 281.1828126070792, 'teacher_total_length': 292.6850355717174, 'return': -0.0003888742726861924, 'policy_update_steps': 1.0}

Episode [2/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:57<33:26, 222.89s/it][A2025-07-24 17:07:25.783 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:09:27.937 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:09:28.119 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:09:28.120 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 122.34s
2025-07-24 17:09:30.128 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0110,avg_pass_at_n: 1.0000,avg_num_tokens: 109.3993,std_num_tokens: 118.2373,avg_correct_num_tokens: 104.0115,std_correct_num_tokens: 85.2126,avg_incorrect_num_tokens: 119.8881,std_incorrect_num_tokens: 163.9925
2025-07-24 17:09:30.504 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 17:09:33.880 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.37s
2025-07-24 17:10:02.385 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:10:02.386 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.50s
2025-07-24 17:10:04.101 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.28s
2025-07-24 17:10:04.102 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008023398442323332, avg_kl: 0.0010367166835630955, avg_response_length: 111.21581179357965, avg_orm_score: 0.0, avg_custom_rewards: -0.0008023398442323332
2025-07-24 17:10:04.157 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter17_replay_buffer.jsonl
2025-07-24 17:10:06.061 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.123, ret=-0.00186, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.03s/it, pg=0.123, ret=-0.00186, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.03s/it, pg=-0.0354, ret=-0.000521, glen=108, tlen=268, kl=0.00107, act_lr=3.4e-7, ent=1.66]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.08it/s, pg=-0.0354, ret=-0.000521, glen=108, tlen=268, kl=0.00107, act_lr=3.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.08it/s, pg=-0.232, ret=0.00168, glen=109, tlen=269, kl=0.00106, act_lr=3.4e-7, ent=1.74]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.08it/s, pg=-0.232, ret=0.00168, glen=109, tlen=269, kl=0.00106, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.08it/s, pg=0.079, ret=0.000748, glen=104, tlen=264, kl=0.00104, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:47,  1.10it/s, pg=0.079, ret=0.000748, glen=104, tlen=264, kl=0.00104, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:47,  1.10it/s, pg=-0.179, ret=0.0017, glen=106, tlen=267, kl=0.001, act_lr=3.4e-7, ent=1.63]   Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.12it/s, pg=-0.179, ret=0.0017, glen=106, tlen=267, kl=0.001, act_lr=3.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.12it/s, pg=-0.019, ret=0.000826, glen=107, tlen=268, kl=0.00105, act_lr=3.4e-7, ent=1.72]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.11it/s, pg=-0.019, ret=0.000826, glen=107, tlen=268, kl=0.00105, act_lr=3.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.11it/s, pg=0.0769, ret=-0.00104, glen=98.4, tlen=258, kl=0.0011, act_lr=3.4e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.12it/s, pg=0.0769, ret=-0.00104, glen=98.4, tlen=258, kl=0.0011, act_lr=3.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.12it/s, pg=0.0399, ret=-1.22e-5, glen=115, tlen=275, kl=0.00102, act_lr=3.4e-7, ent=1.81]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.14it/s, pg=0.0399, ret=-1.22e-5, glen=115, tlen=275, kl=0.00102, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:42,  1.14it/s, pg=0.0515, ret=-0.000699, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.76]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=0.0515, ret=-0.000699, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=0.375, ret=-0.00287, glen=193, tlen=353, kl=0.000826, act_lr=3.4e-7, ent=1.44] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:40,  1.14it/s, pg=0.375, ret=-0.00287, glen=193, tlen=353, kl=0.000826, act_lr=3.4e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.14it/s, pg=0.0195, ret=-0.000691, glen=109, tlen=269, kl=0.00101, act_lr=3.4e-7, ent=1.96]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.15it/s, pg=0.0195, ret=-0.000691, glen=109, tlen=269, kl=0.00101, act_lr=3.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.15it/s, pg=0.0538, ret=-0.0011, glen=102, tlen=262, kl=0.00107, act_lr=3.4e-7, ent=1.68]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.16it/s, pg=0.0538, ret=-0.0011, glen=102, tlen=262, kl=0.00107, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.16it/s, pg=-0.0036, ret=-0.000255, glen=112, tlen=272, kl=0.00102, act_lr=3.4e-7, ent=1.67]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.16it/s, pg=-0.0036, ret=-0.000255, glen=112, tlen=272, kl=0.00102, act_lr=3.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.16it/s, pg=-0.031, ret=0.000163, glen=97.5, tlen=257, kl=0.00109, act_lr=3.4e-7, ent=1.75] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.17it/s, pg=-0.031, ret=0.000163, glen=97.5, tlen=257, kl=0.00109, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.17it/s, pg=-0.12, ret=0.000565, glen=100, tlen=260, kl=0.00106, act_lr=3.4e-7, ent=1.67]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.12, ret=0.000565, glen=100, tlen=260, kl=0.00106, act_lr=3.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.17it/s, pg=-0.0933, ret=0.000558, glen=125, tlen=286, kl=0.00101, act_lr=3.4e-7, ent=1.97]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.0933, ret=0.000558, glen=125, tlen=286, kl=0.00101, act_lr=3.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=0.0725, ret=0.000399, glen=113, tlen=273, kl=0.00101, act_lr=3.4e-7, ent=2.01] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=0.0725, ret=0.000399, glen=113, tlen=273, kl=0.00101, act_lr=3.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.19, ret=-0.000996, glen=112, tlen=272, kl=0.00111, act_lr=3.4e-7, ent=1.74] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.19, ret=-0.000996, glen=112, tlen=272, kl=0.00111, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.0269, ret=-0.00028, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.74]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0269, ret=-0.00028, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.197, ret=-0.00121, glen=125, tlen=285, kl=0.000987, act_lr=3.4e-7, ent=2.05]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.197, ret=-0.00121, glen=125, tlen=285, kl=0.000987, act_lr=3.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.0905, ret=-6.21e-5, glen=103, tlen=263, kl=0.00104, act_lr=3.4e-7, ent=1.7]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.0905, ret=-6.21e-5, glen=103, tlen=263, kl=0.00104, act_lr=3.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.046, ret=0.00048, glen=107, tlen=267, kl=0.00114, act_lr=3.4e-7, ent=1.99] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.046, ret=0.00048, glen=107, tlen=267, kl=0.00114, act_lr=3.4e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.17it/s, pg=-0.0386, ret=-0.000153, glen=125, tlen=285, kl=0.00107, act_lr=3.4e-7, ent=2.03]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.0386, ret=-0.000153, glen=125, tlen=285, kl=0.00107, act_lr=3.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.112, ret=0.00268, glen=124, tlen=284, kl=0.000963, act_lr=3.4e-7, ent=1.79]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.112, ret=0.00268, glen=124, tlen=284, kl=0.000963, act_lr=3.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.0514, ret=0.00116, glen=103, tlen=263, kl=0.00107, act_lr=3.4e-7, ent=1.74]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0514, ret=0.00116, glen=103, tlen=263, kl=0.00107, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.128, ret=0.000102, glen=114, tlen=274, kl=0.000995, act_lr=3.4e-7, ent=1.87]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.128, ret=0.000102, glen=114, tlen=274, kl=0.000995, act_lr=3.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.00299, ret=-0.000642, glen=107, tlen=267, kl=0.00101, act_lr=3.4e-7, ent=1.66]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.00299, ret=-0.000642, glen=107, tlen=267, kl=0.00101, act_lr=3.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.134, ret=0.000584, glen=113, tlen=273, kl=0.00102, act_lr=3.4e-7, ent=2.03]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.134, ret=0.000584, glen=113, tlen=273, kl=0.00102, act_lr=3.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.129, ret=0.000983, glen=96.6, tlen=256, kl=0.001, act_lr=3.4e-7, ent=1.63] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.129, ret=0.000983, glen=96.6, tlen=256, kl=0.001, act_lr=3.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.14, ret=0.000262, glen=109, tlen=269, kl=0.00104, act_lr=3.4e-7, ent=1.65]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.14, ret=0.000262, glen=109, tlen=269, kl=0.00104, act_lr=3.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=-0.108, ret=0.000256, glen=108, tlen=268, kl=0.00102, act_lr=3.4e-7, ent=1.76]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.108, ret=0.000256, glen=108, tlen=268, kl=0.00102, act_lr=3.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.0425, ret=-0.0015, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.64] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.14it/s, pg=0.0425, ret=-0.0015, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=0.111, ret=-0.00172, glen=102, tlen=262, kl=0.00109, act_lr=3.4e-7, ent=1.68]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=0.111, ret=-0.00172, glen=102, tlen=262, kl=0.00109, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=0.253, ret=-0.00118, glen=125, tlen=285, kl=0.000995, act_lr=3.4e-7, ent=1.83]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.16it/s, pg=0.253, ret=-0.00118, glen=125, tlen=285, kl=0.000995, act_lr=3.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.16it/s, pg=-0.0298, ret=-6.09e-5, glen=97.5, tlen=257, kl=0.00107, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.0298, ret=-6.09e-5, glen=97.5, tlen=257, kl=0.00107, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=-0.0617, ret=-0.000393, glen=95.3, tlen=255, kl=0.00105, act_lr=3.4e-7, ent=1.68]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.17it/s, pg=-0.0617, ret=-0.000393, glen=95.3, tlen=255, kl=0.00105, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.17it/s, pg=-0.222, ret=0.00161, glen=104, tlen=264, kl=0.00107, act_lr=3.4e-7, ent=1.75]    Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.222, ret=0.00161, glen=104, tlen=264, kl=0.00107, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.17it/s, pg=0.0331, ret=-6.7e-5, glen=119, tlen=278, kl=0.00101, act_lr=3.4e-7, ent=1.99]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0331, ret=-6.7e-5, glen=119, tlen=278, kl=0.00101, act_lr=3.4e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0375, ret=-0.0007, glen=117, tlen=277, kl=0.00108, act_lr=3.4e-7, ent=2.2] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0375, ret=-0.0007, glen=117, tlen=277, kl=0.00108, act_lr=3.4e-7, ent=2.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0994, ret=0.000623, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.72]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0994, ret=0.000623, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0241, ret=-0.000573, glen=102, tlen=262, kl=0.00106, act_lr=3.4e-7, ent=1.77]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.0241, ret=-0.000573, glen=102, tlen=262, kl=0.00106, act_lr=3.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.0948, ret=-0.000905, glen=101, tlen=261, kl=0.00104, act_lr=3.4e-7, ent=1.68] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.0948, ret=-0.000905, glen=101, tlen=261, kl=0.00104, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=0.000911, glen=94.2, tlen=254, kl=0.00104, act_lr=3.4e-7, ent=1.61]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=0.000911, glen=94.2, tlen=254, kl=0.00104, act_lr=3.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.138, ret=-0.00176, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.81]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.138, ret=-0.00176, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0524, ret=0.000207, glen=116, tlen=276, kl=0.00101, act_lr=3.4e-7, ent=1.79]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0524, ret=0.000207, glen=116, tlen=276, kl=0.00101, act_lr=3.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.0659, ret=0.000175, glen=119, tlen=279, kl=0.00105, act_lr=3.4e-7, ent=1.85] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.16it/s, pg=0.0659, ret=0.000175, glen=119, tlen=279, kl=0.00105, act_lr=3.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.16it/s, pg=-0.188, ret=0.00197, glen=115, tlen=275, kl=0.00107, act_lr=3.4e-7, ent=1.81] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.16it/s, pg=-0.188, ret=0.00197, glen=115, tlen=275, kl=0.00107, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.16it/s, pg=-0.104, ret=0.00198, glen=110, tlen=270, kl=0.00102, act_lr=3.4e-7, ent=1.71]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=-0.104, ret=0.00198, glen=110, tlen=270, kl=0.00102, act_lr=3.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=0.0485, ret=-0.000417, glen=114, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=0.0485, ret=-0.000417, glen=114, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=0.106, ret=-0.000378, glen=115, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.82] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=0.106, ret=-0.000378, glen=115, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.00607, ret=-0.000812, glen=124, tlen=285, kl=0.00101, act_lr=3.4e-7, ent=1.91]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.00607, ret=-0.000812, glen=124, tlen=285, kl=0.00101, act_lr=3.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.18it/s, pg=0.217, ret=-0.00246, glen=108, tlen=268, kl=0.000991, act_lr=3.4e-7, ent=2.08]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.217, ret=-0.00246, glen=108, tlen=268, kl=0.000991, act_lr=3.4e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.152, ret=0.00134, glen=116, tlen=276, kl=0.00104, act_lr=3.4e-7, ent=1.79] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.152, ret=0.00134, glen=116, tlen=276, kl=0.00104, act_lr=3.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0822, ret=0.000248, glen=110, tlen=270, kl=0.000991, act_lr=3.4e-7, ent=1.57]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.18it/s, pg=-0.0822, ret=0.000248, glen=110, tlen=270, kl=0.000991, act_lr=3.4e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.18it/s, pg=-0.233, ret=0.000735, glen=100, tlen=260, kl=0.00107, act_lr=3.4e-7, ent=1.64]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=-0.233, ret=0.000735, glen=100, tlen=260, kl=0.00107, act_lr=3.4e-7, ent=1.64]
2025-07-24 17:10:54.807 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=0.0127, ret=-0.000222, glen=102, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.76]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=0.0127, ret=-0.000222, glen=102, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.76]
2025-07-24 17:10:55.625 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 17:10:58.218 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 17:10:58.562 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.44s
2025-07-24 17:10:58.570 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011185143675122942, 'actor_lr': 3.4035715655379916e-07, 'clip_ratio': 0.0, 'entropy': 1.7742285962615694, 'kl': 0.0010373677526201522, 'response_length': 111.14685412815639, 'total_length': 271.05371638706754, 'teacher_total_length': 283.3528366088867, 'return': -4.607349507880697e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:29<29:14, 219.26s/it][A2025-07-24 17:10:58.612 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:13:25.363 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:13:25.550 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 17:13:25.550 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 146.94s
2025-07-24 17:13:27.545 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0122,avg_pass_at_n: 1.0000,avg_num_tokens: 112.0604,std_num_tokens: 152.0236,avg_correct_num_tokens: 100.6975,std_correct_num_tokens: 74.0874,avg_incorrect_num_tokens: 133.8238,std_incorrect_num_tokens: 236.9434
2025-07-24 17:13:27.997 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.45s
2025-07-24 17:13:31.208 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.21s
2025-07-24 17:14:00.264 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 17:14:00.264 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.05s
2025-07-24 17:14:01.742 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.97s
2025-07-24 17:14:01.743 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008560248386752812, avg_kl: 0.0010772150518610613, avg_response_length: 116.00188835917065, avg_orm_score: 0.0, avg_custom_rewards: -0.0008560248386752812
2025-07-24 17:14:01.775 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter18_replay_buffer.jsonl
2025-07-24 17:14:03.682 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.133, ret=0.00104, glen=129, tlen=290, kl=0.00101, act_lr=3.6e-7, ent=2.26]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<01:00,  1.08s/it, pg=0.133, ret=0.00104, glen=129, tlen=290, kl=0.00101, act_lr=3.6e-7, ent=2.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<01:00,  1.08s/it, pg=-0.0935, ret=-0.000242, glen=101, tlen=262, kl=0.00115, act_lr=3.6e-7, ent=1.63]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:53,  1.04it/s, pg=-0.0935, ret=-0.000242, glen=101, tlen=262, kl=0.00115, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:53,  1.04it/s, pg=0.0418, ret=0.00252, glen=130, tlen=291, kl=0.00106, act_lr=3.6e-7, ent=2.18]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:51,  1.05it/s, pg=0.0418, ret=0.00252, glen=130, tlen=291, kl=0.00106, act_lr=3.6e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:51,  1.05it/s, pg=0.0914, ret=0.000476, glen=133, tlen=293, kl=0.000974, act_lr=3.6e-7, ent=2.28]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:49,  1.06it/s, pg=0.0914, ret=0.000476, glen=133, tlen=293, kl=0.000974, act_lr=3.6e-7, ent=2.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:49,  1.06it/s, pg=0.00702, ret=-0.00127, glen=119, tlen=280, kl=0.00112, act_lr=3.6e-7, ent=1.79]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:47,  1.10it/s, pg=0.00702, ret=-0.00127, glen=119, tlen=280, kl=0.00112, act_lr=3.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:47,  1.10it/s, pg=-0.124, ret=0.000265, glen=122, tlen=283, kl=0.00112, act_lr=3.6e-7, ent=1.82] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.12it/s, pg=-0.124, ret=0.000265, glen=122, tlen=283, kl=0.00112, act_lr=3.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.12it/s, pg=0.174, ret=-0.000891, glen=118, tlen=279, kl=0.00101, act_lr=3.6e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.14it/s, pg=0.174, ret=-0.000891, glen=118, tlen=279, kl=0.00101, act_lr=3.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.14it/s, pg=-0.197, ret=0.000773, glen=99, tlen=259, kl=0.00113, act_lr=3.6e-7, ent=1.66] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.13it/s, pg=-0.197, ret=0.000773, glen=99, tlen=259, kl=0.00113, act_lr=3.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.13it/s, pg=0.0272, ret=-0.000583, glen=96.6, tlen=258, kl=0.00107, act_lr=3.6e-7, ent=1.53]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.14it/s, pg=0.0272, ret=-0.000583, glen=96.6, tlen=258, kl=0.00107, act_lr=3.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.14it/s, pg=-0.212, ret=0.000716, glen=110, tlen=271, kl=0.00114, act_lr=3.6e-7, ent=1.7]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.212, ret=0.000716, glen=110, tlen=271, kl=0.00114, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=-0.135, ret=0.00133, glen=107, tlen=267, kl=0.00107, act_lr=3.6e-7, ent=1.65]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=-0.135, ret=0.00133, glen=107, tlen=267, kl=0.00107, act_lr=3.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0614, ret=-0.000299, glen=122, tlen=282, kl=0.00103, act_lr=3.6e-7, ent=2.22]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.0614, ret=-0.000299, glen=122, tlen=282, kl=0.00103, act_lr=3.6e-7, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.0624, ret=0.000282, glen=137, tlen=298, kl=0.00102, act_lr=3.6e-7, ent=2.06]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=0.0624, ret=0.000282, glen=137, tlen=298, kl=0.00102, act_lr=3.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=-0.224, ret=0.000853, glen=103, tlen=264, kl=0.00109, act_lr=3.6e-7, ent=1.73]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=-0.224, ret=0.000853, glen=103, tlen=264, kl=0.00109, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=-0.0554, ret=-0.001, glen=104, tlen=265, kl=0.00112, act_lr=3.6e-7, ent=1.63] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.0554, ret=-0.001, glen=104, tlen=265, kl=0.00112, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:35,  1.17it/s, pg=-0.107, ret=5.2e-5, glen=113, tlen=274, kl=0.00109, act_lr=3.6e-7, ent=1.77] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.107, ret=5.2e-5, glen=113, tlen=274, kl=0.00109, act_lr=3.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.00143, ret=-0.00101, glen=107, tlen=268, kl=0.00114, act_lr=3.6e-7, ent=1.76]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.00143, ret=-0.00101, glen=107, tlen=268, kl=0.00114, act_lr=3.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.00244, ret=0.000598, glen=134, tlen=294, kl=0.00107, act_lr=3.6e-7, ent=2.24]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=-0.00244, ret=0.000598, glen=134, tlen=294, kl=0.00107, act_lr=3.6e-7, ent=2.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=0.121, ret=-0.000594, glen=133, tlen=294, kl=0.00108, act_lr=3.6e-7, ent=1.95]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.121, ret=-0.000594, glen=133, tlen=294, kl=0.00108, act_lr=3.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0337, ret=-0.00053, glen=104, tlen=264, kl=0.00112, act_lr=3.6e-7, ent=1.82]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0337, ret=-0.00053, glen=104, tlen=264, kl=0.00112, act_lr=3.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.127, ret=0.00152, glen=115, tlen=276, kl=0.0011, act_lr=3.6e-7, ent=1.74]   Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.16it/s, pg=-0.127, ret=0.00152, glen=115, tlen=276, kl=0.0011, act_lr=3.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.16it/s, pg=-0.0803, ret=0.00213, glen=130, tlen=291, kl=0.00103, act_lr=3.6e-7, ent=2.07]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.17it/s, pg=-0.0803, ret=0.00213, glen=130, tlen=291, kl=0.00103, act_lr=3.6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.17it/s, pg=-0.0977, ret=0.000646, glen=102, tlen=264, kl=0.0011, act_lr=3.6e-7, ent=1.75]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0977, ret=0.000646, glen=102, tlen=264, kl=0.0011, act_lr=3.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.19, ret=-0.00245, glen=142, tlen=303, kl=0.00102, act_lr=3.6e-7, ent=1.59]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.19, ret=-0.00245, glen=142, tlen=303, kl=0.00102, act_lr=3.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0645, ret=-0.000141, glen=125, tlen=286, kl=0.00101, act_lr=3.6e-7, ent=1.88]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0645, ret=-0.000141, glen=125, tlen=286, kl=0.00101, act_lr=3.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0251, ret=0.000188, glen=115, tlen=276, kl=0.00108, act_lr=3.6e-7, ent=1.71] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0251, ret=0.000188, glen=115, tlen=276, kl=0.00108, act_lr=3.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0215, ret=-0.000996, glen=122, tlen=283, kl=0.00104, act_lr=3.6e-7, ent=1.6] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.0215, ret=-0.000996, glen=122, tlen=283, kl=0.00104, act_lr=3.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=-0.079, ret=5e-5, glen=100, tlen=262, kl=0.0011, act_lr=3.6e-7, ent=1.69]     Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=-0.079, ret=5e-5, glen=100, tlen=262, kl=0.0011, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=0.145, ret=-0.00141, glen=116, tlen=277, kl=0.00105, act_lr=3.6e-7, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=0.145, ret=-0.00141, glen=116, tlen=277, kl=0.00105, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.107, ret=-0.000153, glen=116, tlen=277, kl=0.00113, act_lr=3.6e-7, ent=1.65]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.09it/s, pg=-0.107, ret=-0.000153, glen=116, tlen=277, kl=0.00113, act_lr=3.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.09it/s, pg=-0.0725, ret=-0.000543, glen=111, tlen=272, kl=0.00111, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.11it/s, pg=-0.0725, ret=-0.000543, glen=111, tlen=272, kl=0.00111, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.11it/s, pg=0.199, ret=-0.00168, glen=103, tlen=263, kl=0.0011, act_lr=3.6e-7, ent=1.73]    Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=0.199, ret=-0.00168, glen=103, tlen=263, kl=0.0011, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=0.168, ret=6.54e-5, glen=124, tlen=285, kl=0.00103, act_lr=3.6e-7, ent=1.9] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.14it/s, pg=0.168, ret=6.54e-5, glen=124, tlen=285, kl=0.00103, act_lr=3.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.14it/s, pg=0.0212, ret=-0.00105, glen=102, tlen=263, kl=0.00107, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=0.0212, ret=-0.00105, glen=102, tlen=263, kl=0.00107, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.0116, ret=0.000198, glen=123, tlen=284, kl=0.00109, act_lr=3.6e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.0116, ret=0.000198, glen=123, tlen=284, kl=0.00109, act_lr=3.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.179, ret=0.00332, glen=125, tlen=286, kl=0.00107, act_lr=3.6e-7, ent=1.81] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.179, ret=0.00332, glen=125, tlen=286, kl=0.00107, act_lr=3.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.188, ret=0.000264, glen=106, tlen=266, kl=0.00113, act_lr=3.6e-7, ent=1.64]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.188, ret=0.000264, glen=106, tlen=266, kl=0.00113, act_lr=3.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.0384, ret=0.00195, glen=116, tlen=277, kl=0.00103, act_lr=3.6e-7, ent=2.01]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0384, ret=0.00195, glen=116, tlen=277, kl=0.00103, act_lr=3.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=-0.0289, ret=-3.4e-5, glen=115, tlen=276, kl=0.00106, act_lr=3.6e-7, ent=1.68]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0289, ret=-3.4e-5, glen=115, tlen=276, kl=0.00106, act_lr=3.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0486, ret=-0.0019, glen=111, tlen=272, kl=0.00109, act_lr=3.6e-7, ent=1.72] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.0486, ret=-0.0019, glen=111, tlen=272, kl=0.00109, act_lr=3.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.0193, ret=0.00135, glen=119, tlen=279, kl=0.0011, act_lr=3.6e-7, ent=1.92]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.0193, ret=0.00135, glen=119, tlen=279, kl=0.0011, act_lr=3.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.259, ret=-0.0027, glen=241, tlen=402, kl=0.000964, act_lr=3.6e-7, ent=1.62]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:13,  1.14it/s, pg=0.259, ret=-0.0027, glen=241, tlen=402, kl=0.000964, act_lr=3.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:13,  1.14it/s, pg=-0.119, ret=0.000473, glen=92.3, tlen=253, kl=0.00111, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.15it/s, pg=-0.119, ret=0.000473, glen=92.3, tlen=253, kl=0.00111, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.15it/s, pg=0.0985, ret=-0.00133, glen=93.3, tlen=254, kl=0.00114, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.0985, ret=-0.00133, glen=93.3, tlen=254, kl=0.00114, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.16it/s, pg=-0.132, ret=0.0013, glen=113, tlen=274, kl=0.00107, act_lr=3.6e-7, ent=1.69]   Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.16it/s, pg=-0.132, ret=0.0013, glen=113, tlen=274, kl=0.00107, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.16it/s, pg=-0.0377, ret=-2.19e-5, glen=109, tlen=269, kl=0.00113, act_lr=3.6e-7, ent=1.78]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.12it/s, pg=-0.0377, ret=-2.19e-5, glen=109, tlen=269, kl=0.00113, act_lr=3.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:41<00:09,  1.12it/s, pg=0.0544, ret=-0.00123, glen=104, tlen=264, kl=0.00114, act_lr=3.6e-7, ent=1.63] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:09,  1.11it/s, pg=0.0544, ret=-0.00123, glen=104, tlen=264, kl=0.00114, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:42<00:09,  1.11it/s, pg=-0.104, ret=0.000615, glen=112, tlen=272, kl=0.00108, act_lr=3.6e-7, ent=1.7] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.13it/s, pg=-0.104, ret=0.000615, glen=112, tlen=272, kl=0.00108, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.13it/s, pg=0.165, ret=-0.00163, glen=128, tlen=289, kl=0.00103, act_lr=3.6e-7, ent=2.48]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:07,  1.14it/s, pg=0.165, ret=-0.00163, glen=128, tlen=289, kl=0.00103, act_lr=3.6e-7, ent=2.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:07,  1.14it/s, pg=-0.162, ret=0.000229, glen=98.4, tlen=259, kl=0.0011, act_lr=3.6e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.15it/s, pg=-0.162, ret=0.000229, glen=98.4, tlen=259, kl=0.0011, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.15it/s, pg=0.0644, ret=-0.000945, glen=102, tlen=263, kl=0.00108, act_lr=3.6e-7, ent=1.57]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=0.0644, ret=-0.000945, glen=102, tlen=263, kl=0.00108, act_lr=3.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=0.0893, ret=-0.00162, glen=115, tlen=276, kl=0.00107, act_lr=3.6e-7, ent=1.67] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=0.0893, ret=-0.00162, glen=115, tlen=276, kl=0.00107, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.146, ret=-0.000471, glen=105, tlen=265, kl=0.00108, act_lr=3.6e-7, ent=1.77]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.16it/s, pg=-0.146, ret=-0.000471, glen=105, tlen=265, kl=0.00108, act_lr=3.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.16it/s, pg=0.0989, ret=-0.00112, glen=117, tlen=278, kl=0.00107, act_lr=3.6e-7, ent=1.72] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=0.0989, ret=-0.00112, glen=117, tlen=278, kl=0.00107, act_lr=3.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=-0.0839, ret=0.000813, glen=104, tlen=265, kl=0.00113, act_lr=3.6e-7, ent=1.65]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0839, ret=0.000813, glen=104, tlen=265, kl=0.00113, act_lr=3.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.00679, ret=-0.00102, glen=100, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.57]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.00679, ret=-0.00102, glen=100, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.57]
2025-07-24 17:14:53.708 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.168, ret=0.00116, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=2]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.168, ret=0.00116, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=2]
2025-07-24 17:14:54.592 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:14:57.133 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 17:14:57.460 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.72s
2025-07-24 17:14:57.467 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01760175771880568, 'actor_lr': 3.6035087833752503e-07, 'clip_ratio': 0.0, 'entropy': 1.790599132839002, 'kl': 0.0010777439987450315, 'response_length': 115.91371034321033, 'total_length': 276.66269938151044, 'teacher_total_length': 288.8366179884526, 'return': -6.468255652866342e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:28<26:21, 225.94s/it][A2025-07-24 17:14:57.510 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:17:24.899 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:17:25.075 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:17:25.076 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 147.57s
2025-07-24 17:17:27.039 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 107.7162,std_num_tokens: 138.4706,avg_correct_num_tokens: 100.7582,std_correct_num_tokens: 80.4996,avg_incorrect_num_tokens: 121.6068,std_incorrect_num_tokens: 210.2995
2025-07-24 17:17:27.458 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 17:17:30.606 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.15s
2025-07-24 17:17:58.866 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:17:58.867 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.26s
2025-07-24 17:18:00.292 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 17:18:00.293 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008467278438578506, avg_kl: 0.0011273884452511912, avg_response_length: 112.22215236783562, avg_orm_score: 0.0, avg_custom_rewards: -0.0008467278438578506
2025-07-24 17:18:00.324 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter19_replay_buffer.jsonl
2025-07-24 17:18:02.165 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.139, ret=0.000396, glen=117, tlen=278, kl=0.00107, act_lr=3.8e-7, ent=1.9]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.139, ret=0.000396, glen=117, tlen=278, kl=0.00107, act_lr=3.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.00574, ret=0.000231, glen=109, tlen=269, kl=0.0011, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=0.00574, ret=0.000231, glen=109, tlen=269, kl=0.0011, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=-0.224, ret=0.00127, glen=99.7, tlen=260, kl=0.00113, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.224, ret=0.00127, glen=99.7, tlen=260, kl=0.00113, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=-0.0479, ret=0.00084, glen=125, tlen=285, kl=0.0011, act_lr=3.8e-7, ent=1.92] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.13it/s, pg=-0.0479, ret=0.00084, glen=125, tlen=285, kl=0.0011, act_lr=3.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.13it/s, pg=-0.0911, ret=0.000751, glen=108, tlen=268, kl=0.00114, act_lr=3.8e-7, ent=1.7]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=-0.0911, ret=0.000751, glen=108, tlen=268, kl=0.00114, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=-0.208, ret=0.00292, glen=109, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.73] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=-0.208, ret=0.00292, glen=109, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=0.0279, ret=-0.000519, glen=109, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=0.0279, ret=-0.000519, glen=109, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=0.0178, ret=0.000544, glen=102, tlen=262, kl=0.00116, act_lr=3.8e-7, ent=1.65] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0178, ret=0.000544, glen=102, tlen=262, kl=0.00116, act_lr=3.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0375, ret=-0.000419, glen=99.8, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.59]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.16it/s, pg=0.0375, ret=-0.000419, glen=99.8, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=0.0486, ret=0.000319, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.72]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=0.0486, ret=0.000319, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=0.116, ret=-0.000694, glen=118, tlen=278, kl=0.00108, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=0.116, ret=-0.000694, glen=118, tlen=278, kl=0.00108, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=0.0176, ret=-0.00114, glen=98.7, tlen=259, kl=0.00113, act_lr=3.8e-7, ent=1.6]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=0.0176, ret=-0.00114, glen=98.7, tlen=259, kl=0.00113, act_lr=3.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.046, ret=-0.000587, glen=113, tlen=273, kl=0.00114, act_lr=3.8e-7, ent=1.77]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.046, ret=-0.000587, glen=113, tlen=273, kl=0.00114, act_lr=3.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=0.0683, ret=0.000594, glen=116, tlen=276, kl=0.0011, act_lr=3.8e-7, ent=1.64] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=0.0683, ret=0.000594, glen=116, tlen=276, kl=0.0011, act_lr=3.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.0271, ret=-0.00186, glen=107, tlen=267, kl=0.0011, act_lr=3.8e-7, ent=1.85]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.0271, ret=-0.00186, glen=107, tlen=267, kl=0.0011, act_lr=3.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.0385, ret=0.000567, glen=100, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=-0.0385, ret=0.000567, glen=100, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.205, ret=0.00199, glen=111, tlen=272, kl=0.0011, act_lr=3.8e-7, ent=1.71]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.205, ret=0.00199, glen=111, tlen=272, kl=0.0011, act_lr=3.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=-0.0138, ret=0.000123, glen=112, tlen=273, kl=0.00115, act_lr=3.8e-7, ent=1.84]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.0138, ret=0.000123, glen=112, tlen=273, kl=0.00115, act_lr=3.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0432, ret=0.000324, glen=101, tlen=261, kl=0.00118, act_lr=3.8e-7, ent=1.8] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0432, ret=0.000324, glen=101, tlen=261, kl=0.00118, act_lr=3.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.0526, ret=-0.00154, glen=113, tlen=273, kl=0.00109, act_lr=3.8e-7, ent=1.79]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.0526, ret=-0.00154, glen=113, tlen=273, kl=0.00109, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.14, ret=0.000411, glen=102, tlen=262, kl=0.00119, act_lr=3.8e-7, ent=1.79] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.18it/s, pg=-0.14, ret=0.000411, glen=102, tlen=262, kl=0.00119, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.18it/s, pg=0.0447, ret=0.000533, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.0447, ret=0.000533, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=-0.179, ret=0.000435, glen=113, tlen=272, kl=0.00112, act_lr=3.8e-7, ent=1.75]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.179, ret=0.000435, glen=113, tlen=272, kl=0.00112, act_lr=3.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=0.0532, ret=-0.000346, glen=96.3, tlen=257, kl=0.00114, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.18it/s, pg=0.0532, ret=-0.000346, glen=96.3, tlen=257, kl=0.00114, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.18it/s, pg=-0.158, ret=0.00104, glen=105, tlen=265, kl=0.00114, act_lr=3.8e-7, ent=1.74]   Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.18it/s, pg=-0.158, ret=0.00104, glen=105, tlen=265, kl=0.00114, act_lr=3.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.18it/s, pg=0.13, ret=-0.000351, glen=108, tlen=269, kl=0.00111, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.13, ret=-0.000351, glen=108, tlen=269, kl=0.00111, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.168, ret=-0.000795, glen=110, tlen=270, kl=0.00117, act_lr=3.8e-7, ent=1.85]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.168, ret=-0.000795, glen=110, tlen=270, kl=0.00117, act_lr=3.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0799, ret=0.00053, glen=96.1, tlen=257, kl=0.00111, act_lr=3.8e-7, ent=1.7]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.0799, ret=0.00053, glen=96.1, tlen=257, kl=0.00111, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=0.105, ret=-0.00204, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=1.68] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=0.105, ret=-0.00204, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.0291, ret=-0.000474, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.59]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.0291, ret=-0.000474, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.213, ret=-0.00221, glen=95.8, tlen=256, kl=0.0012, act_lr=3.8e-7, ent=1.64]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:26<00:22,  1.12it/s, pg=0.213, ret=-0.00221, glen=95.8, tlen=256, kl=0.0012, act_lr=3.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.0385, ret=-0.00169, glen=111, tlen=271, kl=0.00114, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.14it/s, pg=0.0385, ret=-0.00169, glen=111, tlen=271, kl=0.00114, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=0.13, ret=0.00039, glen=344, tlen=504, kl=0.00105, act_lr=3.8e-7, ent=2.46]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.12it/s, pg=0.13, ret=0.00039, glen=344, tlen=504, kl=0.00105, act_lr=3.8e-7, ent=2.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.12it/s, pg=0.0659, ret=-0.00042, glen=98.6, tlen=259, kl=0.00115, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.13it/s, pg=0.0659, ret=-0.00042, glen=98.6, tlen=259, kl=0.00115, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.13it/s, pg=-0.104, ret=0.000536, glen=120, tlen=281, kl=0.00109, act_lr=3.8e-7, ent=1.61] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.14it/s, pg=-0.104, ret=0.000536, glen=120, tlen=281, kl=0.00109, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.14it/s, pg=-0.0788, ret=-0.000133, glen=103, tlen=263, kl=0.00114, act_lr=3.8e-7, ent=1.65]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=-0.0788, ret=-0.000133, glen=103, tlen=263, kl=0.00114, act_lr=3.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=-0.13, ret=0.00154, glen=96.9, tlen=257, kl=0.00117, act_lr=3.8e-7, ent=1.82]   Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.13, ret=0.00154, glen=96.9, tlen=257, kl=0.00117, act_lr=3.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=0.0087, ret=-0.000426, glen=127, tlen=288, kl=0.00113, act_lr=3.8e-7, ent=1.99]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:32<00:15,  1.16it/s, pg=0.0087, ret=-0.000426, glen=127, tlen=288, kl=0.00113, act_lr=3.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0921, ret=-0.000552, glen=117, tlen=277, kl=0.00115, act_lr=3.8e-7, ent=1.55]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0921, ret=-0.000552, glen=117, tlen=277, kl=0.00115, act_lr=3.8e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0944, ret=0.00084, glen=97.7, tlen=258, kl=0.00107, act_lr=3.8e-7, ent=1.92]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0944, ret=0.00084, glen=97.7, tlen=258, kl=0.00107, act_lr=3.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.132, ret=-0.00116, glen=127, tlen=288, kl=0.00112, act_lr=3.8e-7, ent=1.75]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.132, ret=-0.00116, glen=127, tlen=288, kl=0.00112, act_lr=3.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.154, ret=-0.00195, glen=123, tlen=284, kl=0.00114, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.17it/s, pg=0.154, ret=-0.00195, glen=123, tlen=284, kl=0.00114, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.17it/s, pg=-0.0782, ret=-0.000408, glen=94.9, tlen=255, kl=0.00114, act_lr=3.8e-7, ent=1.61]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.0782, ret=-0.000408, glen=94.9, tlen=255, kl=0.00114, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.0734, ret=0.000154, glen=99.1, tlen=259, kl=0.00119, act_lr=3.8e-7, ent=1.7]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0734, ret=0.000154, glen=99.1, tlen=259, kl=0.00119, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0541, ret=-0.000776, glen=97.2, tlen=258, kl=0.00117, act_lr=3.8e-7, ent=1.61]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.17it/s, pg=-0.0541, ret=-0.000776, glen=97.2, tlen=258, kl=0.00117, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.00745, ret=-0.000992, glen=108, tlen=269, kl=0.00113, act_lr=3.8e-7, ent=1.7]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=0.00745, ret=-0.000992, glen=108, tlen=269, kl=0.00113, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.169, ret=0.000857, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.62] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.169, ret=0.000857, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.0269, ret=0.00072, glen=99.8, tlen=260, kl=0.00117, act_lr=3.8e-7, ent=1.57]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.0269, ret=0.00072, glen=99.8, tlen=260, kl=0.00117, act_lr=3.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.034, ret=-0.000115, glen=110, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.034, ret=-0.000115, glen=110, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.231, ret=0.00129, glen=114, tlen=275, kl=0.00115, act_lr=3.8e-7, ent=1.87]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.231, ret=0.00129, glen=114, tlen=275, kl=0.00115, act_lr=3.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0255, ret=-0.000571, glen=108, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0255, ret=-0.000571, glen=108, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.211, ret=-0.00136, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.81]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=0.211, ret=-0.00136, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.136, ret=-0.000944, glen=94.8, tlen=255, kl=0.0012, act_lr=3.8e-7, ent=1.57]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=0.136, ret=-0.000944, glen=94.8, tlen=255, kl=0.0012, act_lr=3.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.198, ret=5.72e-6, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.7]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.198, ret=5.72e-6, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.259, ret=-0.00145, glen=148, tlen=309, kl=0.000987, act_lr=3.8e-7, ent=2.46]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.16it/s, pg=0.259, ret=-0.00145, glen=148, tlen=309, kl=0.000987, act_lr=3.8e-7, ent=2.46]
2025-07-24 17:18:50.823 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.16it/s, pg=-0.218, ret=0.00211, glen=105, tlen=265, kl=0.00117, act_lr=4e-7, ent=1.66]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.218, ret=0.00211, glen=105, tlen=265, kl=0.00117, act_lr=4e-7, ent=1.66]
2025-07-24 17:18:51.682 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:18:54.284 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 17:18:54.615 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.39s
2025-07-24 17:18:54.622 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011518278292247228, 'actor_lr': 3.8035713194923506e-07, 'clip_ratio': 0.0, 'entropy': 1.755924410053662, 'kl': 0.0011276347296578543, 'response_length': 112.190030506679, 'total_length': 272.52758407592773, 'teacher_total_length': 284.82583400181363, 'return': -6.525290284896203e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [26:25<22:57, 229.61s/it][A2025-07-24 17:18:54.629 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:09,  2.44it/s, est. speed input: 441.15 toks/s, output: 29.25 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 66/172 [00:02<00:02, 46.97it/s, est. speed input: 5620.92 toks/s, output: 1382.77 toks/s]Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 75/172 [00:02<00:01, 56.40it/s, est. speed input: 6089.27 toks/s, output: 1585.80 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:05<00:00, 14.19it/s, est. speed input: 5633.58 toks/s, output: 2582.87 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/172 [00:04<00:00, 20.03it/s, est. speed input: 5655.93 toks/s, output: 2647.15 toks/s][32m [repeated 111x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:10<00:01,  1.72it/s, est. speed input: 2943.60 toks/s, output: 1760.61 toks/s][32m [repeated 26x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00,  2.21it/s, est. speed input: 2723.51 toks/s, output: 1776.63 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00, 15.02it/s, est. speed input: 2723.51 toks/s, output: 1776.63 toks/s]
2025-07-24 17:19:09.888 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 505.7686,strategyqa_test/accuracy: 0.3493,eval_accuracy: 0.3493
2025-07-24 17:19:10.142 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:20:39.162 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:20:39.345 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:20:39.345 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 89.20s
2025-07-24 17:20:41.356 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0082,avg_pass_at_n: 1.0000,avg_num_tokens: 108.6934,std_num_tokens: 116.0161,avg_correct_num_tokens: 102.0378,std_correct_num_tokens: 81.0916,avg_incorrect_num_tokens: 122.3441,std_incorrect_num_tokens: 165.2319
2025-07-24 17:20:41.801 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.46s
2025-07-24 17:20:44.919 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.12s
2025-07-24 17:21:13.546 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 224
2025-07-24 17:21:13.550 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.63s
2025-07-24 17:21:15.023 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.97s
2025-07-24 17:21:15.024 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0034516584520523402, avg_kl: 0.0010991777692522322, avg_response_length: 111.61926296779087, avg_orm_score: 0.0, avg_custom_rewards: -0.0034516584520523402
2025-07-24 17:21:15.087 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter20_replay_buffer.jsonl
2025-07-24 17:21:16.946 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.86s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:13<00:01,  1.30s/it, est. speed input: 2333.97 toks/s, output: 1348.85 toks/s][32m [repeated 2x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:13<00:00,  1.11s/it, est. speed input: 2261.82 toks/s, output: 1363.14 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:13<00:00, 12.45it/s, est. speed input: 2261.82 toks/s, output: 1363.14 toks/s][32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s, pg=-0.171, ret=3.79e-5, glen=101, tlen=262, kl=0.00115, act_lr=4e-7, ent=1.67]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:00<00:54,  1.01it/s, pg=-0.171, ret=3.79e-5, glen=101, tlen=262, kl=0.00115, act_lr=4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:54,  1.01it/s, pg=-0.176, ret=0.0016, glen=111, tlen=271, kl=0.00105, act_lr=4e-7, ent=1.69] Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.09it/s, pg=-0.176, ret=0.0016, glen=111, tlen=271, kl=0.00105, act_lr=4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.09it/s, pg=-0.0822, ret=0.000507, glen=92, tlen=253, kl=0.00113, act_lr=4e-7, ent=1.53]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:46,  1.13it/s, pg=-0.0822, ret=0.000507, glen=92, tlen=253, kl=0.00113, act_lr=4e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:46,  1.13it/s, pg=-0.0607, ret=0.000538, glen=112, tlen=273, kl=0.00111, act_lr=4e-7, ent=1.72]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=-0.0607, ret=0.000538, glen=112, tlen=273, kl=0.00111, act_lr=4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=0.366, ret=-0.179, glen=198, tlen=358, kl=0.00103, act_lr=4e-7, ent=1.7]     Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=0.366, ret=-0.179, glen=198, tlen=358, kl=0.00103, act_lr=4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=0.0433, ret=-0.00124, glen=106, tlen=266, kl=0.00117, act_lr=4e-7, ent=1.67]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=0.0433, ret=-0.00124, glen=106, tlen=266, kl=0.00117, act_lr=4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.127, ret=0.000405, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.62]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.13it/s, pg=-0.127, ret=0.000405, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.13it/s, pg=-0.0811, ret=-0.000223, glen=106, tlen=266, kl=0.00112, act_lr=4e-7, ent=1.61]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=-0.0811, ret=-0.000223, glen=106, tlen=266, kl=0.00112, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0239, ret=-0.00058, glen=106, tlen=267, kl=0.00112, act_lr=4e-7, ent=1.73]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.15it/s, pg=0.0239, ret=-0.00058, glen=106, tlen=267, kl=0.00112, act_lr=4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=0.23, ret=-0.000434, glen=126, tlen=287, kl=0.00099, act_lr=4e-7, ent=1.78] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=0.23, ret=-0.000434, glen=126, tlen=287, kl=0.00099, act_lr=4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=-0.0474, ret=-0.00149, glen=121, tlen=281, kl=0.0011, act_lr=4e-7, ent=1.91]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=-0.0474, ret=-0.00149, glen=121, tlen=281, kl=0.0011, act_lr=4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=-0.172, ret=0.000637, glen=103, tlen=263, kl=0.00111, act_lr=4e-7, ent=1.81]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.172, ret=0.000637, glen=103, tlen=263, kl=0.00111, act_lr=4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=-0.0945, ret=0.000223, glen=119, tlen=280, kl=0.00113, act_lr=4e-7, ent=1.75]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=-0.0945, ret=0.000223, glen=119, tlen=280, kl=0.00113, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.0791, ret=0.00121, glen=101, tlen=261, kl=0.00115, act_lr=4e-7, ent=1.74] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.0791, ret=0.00121, glen=101, tlen=261, kl=0.00115, act_lr=4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.064, ret=0.000363, glen=110, tlen=270, kl=0.00108, act_lr=4e-7, ent=1.66]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.15it/s, pg=-0.064, ret=0.000363, glen=110, tlen=270, kl=0.00108, act_lr=4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.15it/s, pg=-0.147, ret=0.000127, glen=112, tlen=272, kl=0.00109, act_lr=4e-7, ent=1.75]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.16it/s, pg=-0.147, ret=0.000127, glen=112, tlen=272, kl=0.00109, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=0.139, ret=0.000384, glen=140, tlen=300, kl=0.000954, act_lr=4e-7, ent=1.42]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.15it/s, pg=0.139, ret=0.000384, glen=140, tlen=300, kl=0.000954, act_lr=4e-7, ent=1.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.15it/s, pg=0.0439, ret=-2.86e-5, glen=98.7, tlen=259, kl=0.00117, act_lr=4e-7, ent=1.71]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.16it/s, pg=0.0439, ret=-2.86e-5, glen=98.7, tlen=259, kl=0.00117, act_lr=4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.16it/s, pg=-0.00793, ret=0.00052, glen=120, tlen=281, kl=0.00105, act_lr=4e-7, ent=1.81]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.16it/s, pg=-0.00793, ret=0.00052, glen=120, tlen=281, kl=0.00105, act_lr=4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.16it/s, pg=0.054, ret=0.000511, glen=110, tlen=270, kl=0.00106, act_lr=4e-7, ent=1.82]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.14it/s, pg=0.054, ret=0.000511, glen=110, tlen=270, kl=0.00106, act_lr=4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.14it/s, pg=-0.0128, ret=-0.000115, glen=106, tlen=267, kl=0.00112, act_lr=4e-7, ent=1.68]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.15it/s, pg=-0.0128, ret=-0.000115, glen=106, tlen=267, kl=0.00112, act_lr=4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.15it/s, pg=-0.00323, ret=0.00029, glen=108, tlen=269, kl=0.00114, act_lr=4e-7, ent=1.68] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.00323, ret=0.00029, glen=108, tlen=269, kl=0.00114, act_lr=4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.16it/s, pg=-0.0271, ret=0.000809, glen=107, tlen=267, kl=0.0011, act_lr=4e-7, ent=1.65] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.0271, ret=0.000809, glen=107, tlen=267, kl=0.0011, act_lr=4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=0.0667, ret=0.000271, glen=107, tlen=268, kl=0.00109, act_lr=4e-7, ent=1.66]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=0.0667, ret=0.000271, glen=107, tlen=268, kl=0.00109, act_lr=4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.0618, ret=0.000431, glen=114, tlen=275, kl=0.00109, act_lr=4e-7, ent=1.79]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0618, ret=0.000431, glen=114, tlen=275, kl=0.00109, act_lr=4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.00208, ret=-0.000191, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.67]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.00208, ret=-0.000191, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.00674, ret=0.000487, glen=106, tlen=267, kl=0.00112, act_lr=4e-7, ent=1.71]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.00674, ret=0.000487, glen=106, tlen=267, kl=0.00112, act_lr=4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=0.0893, ret=-0.000867, glen=111, tlen=272, kl=0.00115, act_lr=4e-7, ent=1.7] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=0.0893, ret=-0.000867, glen=111, tlen=272, kl=0.00115, act_lr=4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=0.161, ret=-0.000467, glen=111, tlen=272, kl=0.00111, act_lr=4e-7, ent=1.8] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=0.161, ret=-0.000467, glen=111, tlen=272, kl=0.00111, act_lr=4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.13, ret=0.000367, glen=102, tlen=262, kl=0.00114, act_lr=4e-7, ent=1.66]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.13, ret=0.000367, glen=102, tlen=262, kl=0.00114, act_lr=4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=0.00781, ret=-0.000727, glen=108, tlen=269, kl=0.00112, act_lr=4e-7, ent=1.88]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.00781, ret=-0.000727, glen=108, tlen=269, kl=0.00112, act_lr=4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.126, ret=-0.00124, glen=105, tlen=266, kl=0.00114, act_lr=4e-7, ent=1.74]   Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.14it/s, pg=0.126, ret=-0.00124, glen=105, tlen=266, kl=0.00114, act_lr=4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=0.141, ret=0.00018, glen=139, tlen=300, kl=0.000957, act_lr=4e-7, ent=1.5] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=0.141, ret=0.00018, glen=139, tlen=300, kl=0.000957, act_lr=4e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=-0.0733, ret=0.000569, glen=104, tlen=265, kl=0.00109, act_lr=4e-7, ent=1.62]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=-0.0733, ret=0.000569, glen=104, tlen=265, kl=0.00109, act_lr=4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=0.0849, ret=-0.000602, glen=118, tlen=279, kl=0.00111, act_lr=4e-7, ent=1.68]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=0.0849, ret=-0.000602, glen=118, tlen=279, kl=0.00111, act_lr=4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.179, ret=-0.00116, glen=150, tlen=310, kl=0.000998, act_lr=4e-7, ent=1.44] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=0.179, ret=-0.00116, glen=150, tlen=310, kl=0.000998, act_lr=4e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=-0.0628, ret=-0.000245, glen=113, tlen=273, kl=0.00106, act_lr=4e-7, ent=1.59]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.0628, ret=-0.000245, glen=113, tlen=273, kl=0.00106, act_lr=4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=0.11, ret=0.000666, glen=102, tlen=263, kl=0.00114, act_lr=4e-7, ent=1.83]    Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.11, ret=0.000666, glen=102, tlen=263, kl=0.00114, act_lr=4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0175, ret=-0.00044, glen=111, tlen=271, kl=0.00109, act_lr=4e-7, ent=1.69]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0175, ret=-0.00044, glen=111, tlen=271, kl=0.00109, act_lr=4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.168, ret=0.000984, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.65]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.168, ret=0.000984, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.00578, ret=0.000122, glen=101, tlen=262, kl=0.00114, act_lr=4e-7, ent=1.6]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.00578, ret=0.000122, glen=101, tlen=262, kl=0.00114, act_lr=4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=-0.103, ret=-0.000468, glen=103, tlen=263, kl=0.00109, act_lr=4e-7, ent=1.57]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=-0.103, ret=-0.000468, glen=103, tlen=263, kl=0.00109, act_lr=4e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.205, ret=0.000724, glen=96.4, tlen=257, kl=0.00111, act_lr=4e-7, ent=1.56]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.205, ret=0.000724, glen=96.4, tlen=257, kl=0.00111, act_lr=4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.253, ret=-0.00188, glen=120, tlen=281, kl=0.00112, act_lr=4e-7, ent=1.93]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.253, ret=-0.00188, glen=120, tlen=281, kl=0.00112, act_lr=4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.174, ret=0.000619, glen=97.7, tlen=259, kl=0.00108, act_lr=4e-7, ent=1.67]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.174, ret=0.000619, glen=97.7, tlen=259, kl=0.00108, act_lr=4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.184, ret=-0.00127, glen=121, tlen=281, kl=0.00113, act_lr=4e-7, ent=2.11]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=0.184, ret=-0.00127, glen=121, tlen=281, kl=0.00113, act_lr=4e-7, ent=2.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.119, ret=0.000929, glen=96.7, tlen=257, kl=0.00117, act_lr=4e-7, ent=1.62]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.119, ret=0.000929, glen=96.7, tlen=257, kl=0.00117, act_lr=4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.0879, ret=-0.00045, glen=109, tlen=269, kl=0.00108, act_lr=4e-7, ent=1.77] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.0879, ret=-0.00045, glen=109, tlen=269, kl=0.00108, act_lr=4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.0801, ret=0.000348, glen=122, tlen=282, kl=0.00101, act_lr=4e-7, ent=1.9]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.0801, ret=0.000348, glen=122, tlen=282, kl=0.00101, act_lr=4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=0.141, ret=-0.00148, glen=101, tlen=262, kl=0.00109, act_lr=4e-7, ent=1.94] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=0.141, ret=-0.00148, glen=101, tlen=262, kl=0.00109, act_lr=4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.0719, ret=0.000516, glen=104, tlen=264, kl=0.00117, act_lr=4e-7, ent=1.77]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0719, ret=0.000516, glen=104, tlen=264, kl=0.00117, act_lr=4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=-0.197, ret=0.000563, glen=115, tlen=276, kl=0.00105, act_lr=4e-7, ent=1.69] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.197, ret=0.000563, glen=115, tlen=276, kl=0.00105, act_lr=4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.0718, ret=-0.0012, glen=106, tlen=266, kl=0.00114, act_lr=4e-7, ent=1.66] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=0.0718, ret=-0.0012, glen=106, tlen=266, kl=0.00114, act_lr=4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.0137, ret=0.0014, glen=119, tlen=279, kl=0.00108, act_lr=4e-7, ent=1.6]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=0.0137, ret=0.0014, glen=119, tlen=279, kl=0.00108, act_lr=4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0197, ret=-0.000533, glen=102, tlen=262, kl=0.00108, act_lr=4e-7, ent=1.54]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.0197, ret=-0.000533, glen=102, tlen=262, kl=0.00108, act_lr=4e-7, ent=1.54]
2025-07-24 17:22:05.659 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.53s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.0433, ret=-0.00129, glen=112, tlen=273, kl=0.00111, act_lr=4.2e-7, ent=1.63]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=0.0433, ret=-0.00129, glen=112, tlen=273, kl=0.00111, act_lr=4.2e-7, ent=1.63]
2025-07-24 17:22:06.486 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 17:22:09.069 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 17:22:09.414 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.41s
2025-07-24 17:22:09.421 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0018208367483956473, 'actor_lr': 4.003571473073667e-07, 'clip_ratio': 0.0, 'entropy': 1.7015411662203925, 'kl': 0.0010991777692522322, 'response_length': 111.61926296779087, 'total_length': 272.17945643833707, 'teacher_total_length': 282.65635626656666, 'return': -0.003224486442507311, 'policy_update_steps': 1.0}

Episode [2/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [29:40<18:12, 218.52s/it][A2025-07-24 17:22:09.464 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:24:39.797 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:24:39.978 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:24:39.978 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 150.51s
2025-07-24 17:24:41.982 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0150,avg_reflection_pattern_score: 0.0078,avg_pass_at_n: 1.0000,avg_num_tokens: 108.3983,std_num_tokens: 141.7982,avg_correct_num_tokens: 100.5236,std_correct_num_tokens: 83.5599,avg_incorrect_num_tokens: 130.8955,std_incorrect_num_tokens: 238.5759
2025-07-24 17:24:42.420 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.44s
2025-07-24 17:24:45.616 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.20s
2025-07-24 17:25:14.158 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 224
2025-07-24 17:25:14.159 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.54s
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
2025-07-24 17:25:51.768 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 17:25:51.769 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0016389060726526492, avg_kl: 0.0034644603729248047, avg_response_length: 112.85247571127755, avg_orm_score: 0.0, avg_custom_rewards: -0.0016389060726526492
2025-07-24 17:25:51.797 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter21_replay_buffer.jsonl
2025-07-24 17:25:53.643 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.18, ret=0.000524, glen=98.7, tlen=259, kl=0.00365, act_lr=4.2e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.03s/it, pg=-0.18, ret=0.000524, glen=98.7, tlen=259, kl=0.00365, act_lr=4.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.03s/it, pg=-0.00177, ret=-0.00136, glen=101, tlen=261, kl=0.0039, act_lr=4.2e-7, ent=1.66]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.07it/s, pg=-0.00177, ret=-0.00136, glen=101, tlen=261, kl=0.0039, act_lr=4.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.07it/s, pg=-0.134, ret=0.000427, glen=98.4, tlen=259, kl=0.00364, act_lr=4.2e-7, ent=1.8] Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.11it/s, pg=-0.134, ret=0.000427, glen=98.4, tlen=259, kl=0.00364, act_lr=4.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.11it/s, pg=-0.102, ret=0.00167, glen=127, tlen=288, kl=0.00304, act_lr=4.2e-7, ent=2.01] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.11it/s, pg=-0.102, ret=0.00167, glen=127, tlen=288, kl=0.00304, act_lr=4.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.11it/s, pg=-0.177, ret=0.000892, glen=109, tlen=269, kl=0.00313, act_lr=4.2e-7, ent=1.83]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=-0.177, ret=0.000892, glen=109, tlen=269, kl=0.00313, act_lr=4.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=0.157, ret=0.0002, glen=140, tlen=300, kl=0.00267, act_lr=4.2e-7, ent=2.15]   Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=0.157, ret=0.0002, glen=140, tlen=300, kl=0.00267, act_lr=4.2e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=-0.0825, ret=0.000591, glen=97.1, tlen=257, kl=0.00354, act_lr=4.2e-7, ent=1.69]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.11it/s, pg=-0.0825, ret=0.000591, glen=97.1, tlen=257, kl=0.00354, act_lr=4.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.11it/s, pg=0.152, ret=-0.00188, glen=105, tlen=265, kl=0.00378, act_lr=4.2e-7, ent=1.77]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.13it/s, pg=0.152, ret=-0.00188, glen=105, tlen=265, kl=0.00378, act_lr=4.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:42,  1.13it/s, pg=-0.155, ret=0.002, glen=118, tlen=279, kl=0.00316, act_lr=4.2e-7, ent=1.88]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.13it/s, pg=-0.155, ret=0.002, glen=118, tlen=279, kl=0.00316, act_lr=4.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.13it/s, pg=-0.222, ret=0.00127, glen=102, tlen=262, kl=0.00335, act_lr=4.2e-7, ent=1.84]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:41,  1.12it/s, pg=-0.222, ret=0.00127, glen=102, tlen=262, kl=0.00335, act_lr=4.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:41,  1.12it/s, pg=0.0859, ret=-0.000175, glen=122, tlen=282, kl=0.00307, act_lr=4.2e-7, ent=1.83]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:40,  1.12it/s, pg=0.0859, ret=-0.000175, glen=122, tlen=282, kl=0.00307, act_lr=4.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:40,  1.12it/s, pg=0.261, ret=-0.00216, glen=106, tlen=267, kl=0.0033, act_lr=4.2e-7, ent=1.89]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.13it/s, pg=0.261, ret=-0.00216, glen=106, tlen=267, kl=0.0033, act_lr=4.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.13it/s, pg=0.0613, ret=0.000444, glen=105, tlen=265, kl=0.00315, act_lr=4.2e-7, ent=1.94]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.14it/s, pg=0.0613, ret=0.000444, glen=105, tlen=265, kl=0.00315, act_lr=4.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.14it/s, pg=-0.0545, ret=-0.00011, glen=100, tlen=261, kl=0.00362, act_lr=4.2e-7, ent=1.81]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.15it/s, pg=-0.0545, ret=-0.00011, glen=100, tlen=261, kl=0.00362, act_lr=4.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.15it/s, pg=0.08, ret=-0.00163, glen=121, tlen=281, kl=0.00314, act_lr=4.2e-7, ent=1.91]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:36,  1.14it/s, pg=0.08, ret=-0.00163, glen=121, tlen=281, kl=0.00314, act_lr=4.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:36,  1.14it/s, pg=0.0445, ret=-0.00105, glen=106, tlen=267, kl=0.00359, act_lr=4.2e-7, ent=1.72]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.15it/s, pg=0.0445, ret=-0.00105, glen=106, tlen=267, kl=0.00359, act_lr=4.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:15<00:34,  1.15it/s, pg=-0.136, ret=-0.000141, glen=102, tlen=262, kl=0.0036, act_lr=4.2e-7, ent=1.86]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.136, ret=-0.000141, glen=102, tlen=262, kl=0.0036, act_lr=4.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=0.336, ret=-0.00394, glen=121, tlen=282, kl=0.0031, act_lr=4.2e-7, ent=1.88]  Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.16it/s, pg=0.336, ret=-0.00394, glen=121, tlen=282, kl=0.0031, act_lr=4.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.16it/s, pg=-0.142, ret=0.00103, glen=103, tlen=263, kl=0.00356, act_lr=4.2e-7, ent=1.75]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.16it/s, pg=-0.142, ret=0.00103, glen=103, tlen=263, kl=0.00356, act_lr=4.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.16it/s, pg=-0.0841, ret=0.000846, glen=109, tlen=269, kl=0.00309, act_lr=4.2e-7, ent=1.79]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=-0.0841, ret=0.000846, glen=109, tlen=269, kl=0.00309, act_lr=4.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=0.14, ret=-0.00068, glen=132, tlen=292, kl=0.00285, act_lr=4.2e-7, ent=2]      Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=0.14, ret=-0.00068, glen=132, tlen=292, kl=0.00285, act_lr=4.2e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.147, ret=0.00159, glen=89.7, tlen=250, kl=0.00411, act_lr=4.2e-7, ent=1.6]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.147, ret=0.00159, glen=89.7, tlen=250, kl=0.00411, act_lr=4.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.17it/s, pg=-0.135, ret=-0.000724, glen=104, tlen=265, kl=0.00334, act_lr=4.2e-7, ent=1.7]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.135, ret=-0.000724, glen=104, tlen=265, kl=0.00334, act_lr=4.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:21<00:28,  1.17it/s, pg=-0.048, ret=-0.000358, glen=109, tlen=269, kl=0.00333, act_lr=4.2e-7, ent=1.8]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.048, ret=-0.000358, glen=109, tlen=269, kl=0.00333, act_lr=4.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.0486, ret=0.00172, glen=100, tlen=261, kl=0.00374, act_lr=4.2e-7, ent=1.83]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0486, ret=0.00172, glen=100, tlen=261, kl=0.00374, act_lr=4.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.058, ret=0.000165, glen=109, tlen=269, kl=0.00364, act_lr=4.2e-7, ent=1.75] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.058, ret=0.000165, glen=109, tlen=269, kl=0.00364, act_lr=4.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.0982, ret=0.00102, glen=94.5, tlen=255, kl=0.00402, act_lr=4.2e-7, ent=1.74]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.0982, ret=0.00102, glen=94.5, tlen=255, kl=0.00402, act_lr=4.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0129, ret=-8.41e-5, glen=107, tlen=267, kl=0.00349, act_lr=4.2e-7, ent=1.91]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.0129, ret=-8.41e-5, glen=107, tlen=267, kl=0.00349, act_lr=4.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.0753, ret=0.000163, glen=99.3, tlen=260, kl=0.00371, act_lr=4.2e-7, ent=1.58]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.05it/s, pg=-0.0753, ret=0.000163, glen=99.3, tlen=260, kl=0.00371, act_lr=4.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.05it/s, pg=0.11, ret=-0.000868, glen=106, tlen=267, kl=0.00333, act_lr=4.2e-7, ent=1.63]   Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.09it/s, pg=0.11, ret=-0.000868, glen=106, tlen=267, kl=0.00333, act_lr=4.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.09it/s, pg=-0.0792, ret=0.000748, glen=115, tlen=275, kl=0.00325, act_lr=4.2e-7, ent=1.92]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.11it/s, pg=-0.0792, ret=0.000748, glen=115, tlen=275, kl=0.00325, act_lr=4.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.11it/s, pg=-0.274, ret=0.0014, glen=103, tlen=264, kl=0.00368, act_lr=4.2e-7, ent=1.63]   Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.274, ret=0.0014, glen=103, tlen=264, kl=0.00368, act_lr=4.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:29<00:21,  1.13it/s, pg=0.206, ret=0.000163, glen=336, tlen=497, kl=0.00298, act_lr=4.2e-7, ent=2.68]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.11it/s, pg=0.206, ret=0.000163, glen=336, tlen=497, kl=0.00298, act_lr=4.2e-7, ent=2.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.11it/s, pg=-0.119, ret=0.00131, glen=109, tlen=269, kl=0.00364, act_lr=4.2e-7, ent=1.74]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.12it/s, pg=-0.119, ret=0.00131, glen=109, tlen=269, kl=0.00364, act_lr=4.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.12it/s, pg=-0.0483, ret=-0.000487, glen=108, tlen=268, kl=0.00369, act_lr=4.2e-7, ent=1.8]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.14it/s, pg=-0.0483, ret=-0.000487, glen=108, tlen=268, kl=0.00369, act_lr=4.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.14it/s, pg=0.0147, ret=-0.0006, glen=96.2, tlen=256, kl=0.00385, act_lr=4.2e-7, ent=1.64] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=0.0147, ret=-0.0006, glen=96.2, tlen=256, kl=0.00385, act_lr=4.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=0.121, ret=-0.000373, glen=125, tlen=285, kl=0.00357, act_lr=4.2e-7, ent=2.01]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.15it/s, pg=0.121, ret=-0.000373, glen=125, tlen=285, kl=0.00357, act_lr=4.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.15it/s, pg=-0.0448, ret=0.00027, glen=103, tlen=264, kl=0.00346, act_lr=4.2e-7, ent=1.85]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=-0.0448, ret=0.00027, glen=103, tlen=264, kl=0.00346, act_lr=4.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.16it/s, pg=0.149, ret=-0.00134, glen=117, tlen=277, kl=0.00423, act_lr=4.2e-7, ent=1.83] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=0.149, ret=-0.00134, glen=117, tlen=277, kl=0.00423, act_lr=4.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:35<00:14,  1.16it/s, pg=0.0995, ret=-0.00945, glen=105, tlen=266, kl=0.00367, act_lr=4.2e-7, ent=1.81]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.0995, ret=-0.00945, glen=105, tlen=266, kl=0.00367, act_lr=4.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.0137, ret=-0.00084, glen=97.5, tlen=258, kl=0.00398, act_lr=4.2e-7, ent=1.73]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.0137, ret=-0.00084, glen=97.5, tlen=258, kl=0.00398, act_lr=4.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=-0.0695, ret=0.000632, glen=124, tlen=284, kl=0.00304, act_lr=4.2e-7, ent=1.78]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.15it/s, pg=-0.0695, ret=0.000632, glen=124, tlen=284, kl=0.00304, act_lr=4.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.15it/s, pg=0.181, ret=-0.000413, glen=106, tlen=266, kl=0.00348, act_lr=4.2e-7, ent=1.84] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.16it/s, pg=0.181, ret=-0.000413, glen=106, tlen=266, kl=0.00348, act_lr=4.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.16it/s, pg=0.0184, ret=-0.000554, glen=117, tlen=278, kl=0.00324, act_lr=4.2e-7, ent=1.75]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.16it/s, pg=0.0184, ret=-0.000554, glen=117, tlen=278, kl=0.00324, act_lr=4.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.16it/s, pg=0.146, ret=-0.000913, glen=105, tlen=265, kl=0.0036, act_lr=4.2e-7, ent=1.84]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.146, ret=-0.000913, glen=105, tlen=265, kl=0.0036, act_lr=4.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=0.157, ret=-0.000562, glen=142, tlen=303, kl=0.00262, act_lr=4.2e-7, ent=2.35]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.16it/s, pg=0.157, ret=-0.000562, glen=142, tlen=303, kl=0.00262, act_lr=4.2e-7, ent=2.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:41<00:08,  1.16it/s, pg=0.153, ret=-0.00112, glen=122, tlen=282, kl=0.00349, act_lr=4.2e-7, ent=1.91] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.16it/s, pg=0.153, ret=-0.00112, glen=122, tlen=282, kl=0.00349, act_lr=4.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.16it/s, pg=-0.235, ret=-7.34e-5, glen=107, tlen=268, kl=0.00336, act_lr=4.2e-7, ent=1.86]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.16it/s, pg=-0.235, ret=-7.34e-5, glen=107, tlen=268, kl=0.00336, act_lr=4.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.16it/s, pg=-0.00952, ret=0.000766, glen=111, tlen=272, kl=0.00338, act_lr=4.2e-7, ent=1.64]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:06,  1.17it/s, pg=-0.00952, ret=0.000766, glen=111, tlen=272, kl=0.00338, act_lr=4.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:06,  1.17it/s, pg=0.0956, ret=-0.00182, glen=107, tlen=268, kl=0.0034, act_lr=4.2e-7, ent=1.71]   Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=0.0956, ret=-0.00182, glen=107, tlen=268, kl=0.0034, act_lr=4.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.143, ret=-3.29e-6, glen=96.8, tlen=257, kl=0.00385, act_lr=4.2e-7, ent=1.71]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.143, ret=-3.29e-6, glen=96.8, tlen=257, kl=0.00385, act_lr=4.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=-0.249, ret=0.00142, glen=96.3, tlen=256, kl=0.004, act_lr=4.2e-7, ent=1.75]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.249, ret=0.00142, glen=96.3, tlen=256, kl=0.004, act_lr=4.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.17it/s, pg=-0.138, ret=-0.00128, glen=126, tlen=287, kl=0.00346, act_lr=4.2e-7, ent=1.95]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.138, ret=-0.00128, glen=126, tlen=287, kl=0.00346, act_lr=4.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:47<00:02,  1.17it/s, pg=-0.255, ret=0.00172, glen=97, tlen=257, kl=0.0036, act_lr=4.2e-7, ent=1.73]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.255, ret=0.00172, glen=97, tlen=257, kl=0.0036, act_lr=4.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.113, ret=0.000426, glen=103, tlen=264, kl=0.00348, act_lr=4.2e-7, ent=1.94]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.113, ret=0.000426, glen=103, tlen=264, kl=0.00348, act_lr=4.2e-7, ent=1.94]
2025-07-24 17:26:42.790 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.148, ret=0.000427, glen=104, tlen=265, kl=0.00337, act_lr=4.4e-7, ent=1.74]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=-0.148, ret=0.000427, glen=104, tlen=265, kl=0.00337, act_lr=4.4e-7, ent=1.74]
2025-07-24 17:26:43.660 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-24 17:26:46.220 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 17:26:46.547 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.77s
2025-07-24 17:26:46.554 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.016002646514347622, 'actor_lr': 4.203571352588499e-07, 'clip_ratio': 0.0, 'entropy': 1.8294285876410348, 'kl': 0.0034644603729248047, 'response_length': 112.85247598375592, 'total_length': 273.30153465270996, 'teacher_total_length': 285.5988671439035, 'return': -0.0001994470588085408, 'policy_update_steps': 1.0}

Episode [2/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [34:17<15:47, 236.85s/it][A2025-07-24 17:26:46.602 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:28:35.440 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:28:35.625 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:28:35.626 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 109.02s
2025-07-24 17:28:37.625 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0088,avg_pass_at_n: 1.0000,avg_num_tokens: 108.1300,std_num_tokens: 109.8295,avg_correct_num_tokens: 102.7215,std_correct_num_tokens: 81.2375,avg_incorrect_num_tokens: 123.4352,std_incorrect_num_tokens: 164.9389
2025-07-24 17:28:38.077 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.45s
2025-07-24 17:28:41.279 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.20s
2025-07-24 17:29:09.745 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:29:09.745 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.46s
2025-07-24 17:29:11.149 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.98s
2025-07-24 17:29:11.150 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.00034934204479474825, avg_kl: 0.003507639795140835, avg_response_length: 109.486484921032, avg_orm_score: 0.0, avg_custom_rewards: -0.00034934204479474825
2025-07-24 17:29:11.182 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter22_replay_buffer.jsonl
2025-07-24 17:29:13.028 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0217, ret=0.000279, glen=113, tlen=273, kl=0.00361, act_lr=4.4e-7, ent=2.05]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.0217, ret=0.000279, glen=113, tlen=273, kl=0.00361, act_lr=4.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.172, ret=-0.00109, glen=105, tlen=265, kl=0.00372, act_lr=4.4e-7, ent=1.81] Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.08it/s, pg=0.172, ret=-0.00109, glen=105, tlen=265, kl=0.00372, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.08it/s, pg=-0.0902, ret=-0.000116, glen=97.6, tlen=258, kl=0.00394, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.0902, ret=-0.000116, glen=97.6, tlen=258, kl=0.00394, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=-0.13, ret=9.54e-6, glen=97.3, tlen=258, kl=0.00376, act_lr=4.4e-7, ent=1.81]    Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:45,  1.14it/s, pg=-0.13, ret=9.54e-6, glen=97.3, tlen=258, kl=0.00376, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:45,  1.14it/s, pg=0.0125, ret=-0.0003, glen=127, tlen=287, kl=0.00311, act_lr=4.4e-7, ent=1.77]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.12it/s, pg=0.0125, ret=-0.0003, glen=127, tlen=287, kl=0.00311, act_lr=4.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.12it/s, pg=-0.00323, ret=0.00074, glen=97.8, tlen=258, kl=0.00357, act_lr=4.4e-7, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=-0.00323, ret=0.00074, glen=97.8, tlen=258, kl=0.00357, act_lr=4.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=0.0212, ret=-0.000329, glen=121, tlen=282, kl=0.00305, act_lr=4.4e-7, ent=1.91] Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.14it/s, pg=0.0212, ret=-0.000329, glen=121, tlen=282, kl=0.00305, act_lr=4.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.14it/s, pg=-0.00256, ret=0.000101, glen=112, tlen=272, kl=0.00327, act_lr=4.4e-7, ent=1.87]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=-0.00256, ret=0.000101, glen=112, tlen=272, kl=0.00327, act_lr=4.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.198, ret=-0.000625, glen=128, tlen=289, kl=0.00297, act_lr=4.4e-7, ent=1.86]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.16it/s, pg=0.198, ret=-0.000625, glen=128, tlen=289, kl=0.00297, act_lr=4.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=0.0173, ret=-0.000232, glen=99.5, tlen=260, kl=0.00363, act_lr=4.4e-7, ent=1.69]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=0.0173, ret=-0.000232, glen=99.5, tlen=260, kl=0.00363, act_lr=4.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=-0.244, ret=0.00103, glen=103, tlen=263, kl=0.00368, act_lr=4.4e-7, ent=1.71]   Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=-0.244, ret=0.00103, glen=103, tlen=263, kl=0.00368, act_lr=4.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0488, ret=-0.000153, glen=105, tlen=265, kl=0.0036, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0488, ret=-0.000153, glen=105, tlen=265, kl=0.0036, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.174, ret=-0.000312, glen=124, tlen=285, kl=0.0035, act_lr=4.4e-7, ent=1.59]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.174, ret=-0.000312, glen=124, tlen=285, kl=0.0035, act_lr=4.4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=0.109, ret=-0.00123, glen=114, tlen=274, kl=0.00348, act_lr=4.4e-7, ent=1.73]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=0.109, ret=-0.00123, glen=114, tlen=274, kl=0.00348, act_lr=4.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.0362, ret=-0.000437, glen=96.5, tlen=257, kl=0.00423, act_lr=4.4e-7, ent=1.66]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.0362, ret=-0.000437, glen=96.5, tlen=257, kl=0.00423, act_lr=4.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.244, ret=0.00143, glen=99.9, tlen=260, kl=0.00367, act_lr=4.4e-7, ent=1.77]   Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.15it/s, pg=-0.244, ret=0.00143, glen=99.9, tlen=260, kl=0.00367, act_lr=4.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.15it/s, pg=0.172, ret=-0.00275, glen=113, tlen=273, kl=0.00378, act_lr=4.4e-7, ent=1.81] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:34,  1.14it/s, pg=0.172, ret=-0.00275, glen=113, tlen=273, kl=0.00378, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:34,  1.14it/s, pg=-0.123, ret=0.000585, glen=101, tlen=262, kl=0.0036, act_lr=4.4e-7, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:33,  1.13it/s, pg=-0.123, ret=0.000585, glen=101, tlen=262, kl=0.0036, act_lr=4.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:33,  1.13it/s, pg=-0.124, ret=0.00197, glen=91.5, tlen=252, kl=0.00383, act_lr=4.4e-7, ent=1.65]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:32,  1.14it/s, pg=-0.124, ret=0.00197, glen=91.5, tlen=252, kl=0.00383, act_lr=4.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:32,  1.14it/s, pg=0.0342, ret=-8.01e-5, glen=115, tlen=275, kl=0.00335, act_lr=4.4e-7, ent=1.7] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.15it/s, pg=0.0342, ret=-8.01e-5, glen=115, tlen=275, kl=0.00335, act_lr=4.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.15it/s, pg=-0.0559, ret=0.000273, glen=99.6, tlen=260, kl=0.00387, act_lr=4.4e-7, ent=1.61]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=-0.0559, ret=0.000273, glen=99.6, tlen=260, kl=0.00387, act_lr=4.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=0.247, ret=0.000108, glen=169, tlen=329, kl=0.00283, act_lr=4.4e-7, ent=2.36]   Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.14it/s, pg=0.247, ret=0.000108, glen=169, tlen=329, kl=0.00283, act_lr=4.4e-7, ent=2.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.14it/s, pg=0.0611, ret=-0.00112, glen=104, tlen=265, kl=0.00344, act_lr=4.4e-7, ent=1.71]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.15it/s, pg=0.0611, ret=-0.00112, glen=104, tlen=265, kl=0.00344, act_lr=4.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.15it/s, pg=-0.181, ret=0.00108, glen=98.6, tlen=259, kl=0.00376, act_lr=4.4e-7, ent=1.78]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.15it/s, pg=-0.181, ret=0.00108, glen=98.6, tlen=259, kl=0.00376, act_lr=4.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.15it/s, pg=0.115, ret=-0.00105, glen=117, tlen=277, kl=0.00353, act_lr=4.4e-7, ent=1.81] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.16it/s, pg=0.115, ret=-0.00105, glen=117, tlen=277, kl=0.00353, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.16it/s, pg=-0.25, ret=0.00159, glen=114, tlen=274, kl=0.00349, act_lr=4.4e-7, ent=1.69] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.16it/s, pg=-0.25, ret=0.00159, glen=114, tlen=274, kl=0.00349, act_lr=4.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.16it/s, pg=0.0903, ret=-0.000844, glen=101, tlen=262, kl=0.00375, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.0903, ret=-0.000844, glen=101, tlen=262, kl=0.00375, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=0.142, ret=-0.00127, glen=104, tlen=265, kl=0.00346, act_lr=4.4e-7, ent=1.77]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=0.142, ret=-0.00127, glen=104, tlen=265, kl=0.00346, act_lr=4.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.0257, ret=-0.000252, glen=104, tlen=264, kl=0.00346, act_lr=4.4e-7, ent=1.8]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.05it/s, pg=-0.0257, ret=-0.000252, glen=104, tlen=264, kl=0.00346, act_lr=4.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.05it/s, pg=-0.23, ret=0.0012, glen=103, tlen=264, kl=0.00388, act_lr=4.4e-7, ent=1.86]    Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:24,  1.08it/s, pg=-0.23, ret=0.0012, glen=103, tlen=264, kl=0.00388, act_lr=4.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:24,  1.08it/s, pg=-0.0419, ret=-1.92e-5, glen=95.7, tlen=256, kl=0.00383, act_lr=4.4e-7, ent=1.64]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.11it/s, pg=-0.0419, ret=-1.92e-5, glen=95.7, tlen=256, kl=0.00383, act_lr=4.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.11it/s, pg=0.0255, ret=-0.000213, glen=110, tlen=271, kl=0.00385, act_lr=4.4e-7, ent=1.74] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=0.0255, ret=-0.000213, glen=110, tlen=271, kl=0.00385, act_lr=4.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.108, ret=0.00111, glen=112, tlen=272, kl=0.00361, act_lr=4.4e-7, ent=1.85]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.14it/s, pg=-0.108, ret=0.00111, glen=112, tlen=272, kl=0.00361, act_lr=4.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.14it/s, pg=-0.0392, ret=0.000574, glen=113, tlen=273, kl=0.00333, act_lr=4.4e-7, ent=1.93]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=-0.0392, ret=0.000574, glen=113, tlen=273, kl=0.00333, act_lr=4.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=0.0849, ret=0.00132, glen=139, tlen=300, kl=0.00291, act_lr=4.4e-7, ent=2.07]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=0.0849, ret=0.00132, glen=139, tlen=300, kl=0.00291, act_lr=4.4e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.0172, ret=0.000214, glen=99.9, tlen=261, kl=0.0037, act_lr=4.4e-7, ent=1.61]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.0172, ret=0.000214, glen=99.9, tlen=261, kl=0.0037, act_lr=4.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.109, ret=0.000698, glen=102, tlen=262, kl=0.00338, act_lr=4.4e-7, ent=1.73]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.109, ret=0.000698, glen=102, tlen=262, kl=0.00338, act_lr=4.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.0231, ret=-0.00131, glen=113, tlen=274, kl=0.00327, act_lr=4.4e-7, ent=1.64]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.0231, ret=-0.00131, glen=113, tlen=274, kl=0.00327, act_lr=4.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.17it/s, pg=-0.159, ret=0.000482, glen=100, tlen=260, kl=0.00357, act_lr=4.4e-7, ent=1.81] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.159, ret=0.000482, glen=100, tlen=260, kl=0.00357, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0623, ret=-0.000137, glen=115, tlen=275, kl=0.00324, act_lr=4.4e-7, ent=1.78]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0623, ret=-0.000137, glen=115, tlen=275, kl=0.00324, act_lr=4.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.0371, ret=-0.000156, glen=96.9, tlen=257, kl=0.00367, act_lr=4.4e-7, ent=1.7] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.0371, ret=-0.000156, glen=96.9, tlen=257, kl=0.00367, act_lr=4.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=-0.139, ret=0.001, glen=114, tlen=275, kl=0.00353, act_lr=4.4e-7, ent=1.81]    Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.18it/s, pg=-0.139, ret=0.001, glen=114, tlen=275, kl=0.00353, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.18it/s, pg=0.151, ret=0.000173, glen=112, tlen=272, kl=0.00338, act_lr=4.4e-7, ent=1.67]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=0.151, ret=0.000173, glen=112, tlen=272, kl=0.00338, act_lr=4.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.114, ret=0.000292, glen=109, tlen=270, kl=0.00346, act_lr=4.4e-7, ent=1.84]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.114, ret=0.000292, glen=109, tlen=270, kl=0.00346, act_lr=4.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0162, ret=-0.000816, glen=108, tlen=268, kl=0.00327, act_lr=4.4e-7, ent=1.8]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0162, ret=-0.000816, glen=108, tlen=268, kl=0.00327, act_lr=4.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=-0.0787, ret=0.000574, glen=101, tlen=262, kl=0.00356, act_lr=4.4e-7, ent=1.7] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.0787, ret=0.000574, glen=101, tlen=262, kl=0.00356, act_lr=4.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.101, ret=0.000497, glen=112, tlen=273, kl=0.00299, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.101, ret=0.000497, glen=112, tlen=273, kl=0.00299, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.171, ret=0.000299, glen=109, tlen=270, kl=0.00375, act_lr=4.4e-7, ent=1.72]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=-0.171, ret=0.000299, glen=109, tlen=270, kl=0.00375, act_lr=4.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.148, ret=0.000682, glen=99.1, tlen=260, kl=0.00367, act_lr=4.4e-7, ent=1.85]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.148, ret=0.000682, glen=99.1, tlen=260, kl=0.00367, act_lr=4.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.125, ret=0.000946, glen=107, tlen=268, kl=0.00343, act_lr=4.4e-7, ent=1.88] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.125, ret=0.000946, glen=107, tlen=268, kl=0.00343, act_lr=4.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0846, ret=-0.00117, glen=105, tlen=265, kl=0.00333, act_lr=4.4e-7, ent=1.83]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0846, ret=-0.00117, glen=105, tlen=265, kl=0.00333, act_lr=4.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=0.0985, ret=0.00146, glen=124, tlen=284, kl=0.00335, act_lr=4.4e-7, ent=2.1]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.0985, ret=0.00146, glen=124, tlen=284, kl=0.00335, act_lr=4.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.17it/s, pg=0.239, ret=-0.00137, glen=113, tlen=273, kl=0.00339, act_lr=4.4e-7, ent=1.92]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.239, ret=-0.00137, glen=113, tlen=273, kl=0.00339, act_lr=4.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.134, ret=-0.00156, glen=106, tlen=266, kl=0.00375, act_lr=4.4e-7, ent=1.69]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=0.134, ret=-0.00156, glen=106, tlen=266, kl=0.00375, act_lr=4.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0823, ret=-0.000484, glen=119, tlen=279, kl=0.00304, act_lr=4.4e-7, ent=1.73]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.0823, ret=-0.000484, glen=119, tlen=279, kl=0.00304, act_lr=4.4e-7, ent=1.73]
2025-07-24 17:30:01.899 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.69s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.00121, ret=-0.00158, glen=114, tlen=274, kl=0.00359, act_lr=4.6e-7, ent=1.78]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=0.00121, ret=-0.00158, glen=114, tlen=274, kl=0.00359, act_lr=4.6e-7, ent=1.78]
2025-07-24 17:30:02.757 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:30:05.325 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 17:30:05.656 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.56s
2025-07-24 17:30:05.713 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.012113503047398158, 'actor_lr': 4.4035715061698154e-07, 'clip_ratio': 0.0, 'entropy': 1.7877566942146845, 'kl': 0.003511462892804827, 'response_length': 109.3523450578962, 'total_length': 269.69858169555664, 'teacher_total_length': 282.0559790475028, 'return': -5.088860364464511e-06, 'policy_update_steps': 1.0}

Episode [2/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [37:37<11:15, 225.21s/it][A2025-07-24 17:30:05.748 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:31:29.817 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:31:30.001 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:31:30.002 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 84.25s
2025-07-24 17:31:32.065 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0095,avg_pass_at_n: 1.0000,avg_num_tokens: 104.3252,std_num_tokens: 102.5810,avg_correct_num_tokens: 100.6695,std_correct_num_tokens: 82.0475,avg_incorrect_num_tokens: 115.3279,std_incorrect_num_tokens: 147.5532
2025-07-24 17:31:32.372 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.37s
2025-07-24 17:31:35.547 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.17s
2025-07-24 17:32:03.446 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 220
2025-07-24 17:32:03.446 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.90s
2025-07-24 17:32:04.804 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 17:32:04.804 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.120751443192024e-05, avg_kl: 0.004078639637340199, avg_response_length: 106.26776275634765, avg_orm_score: 0.0, avg_custom_rewards: -1.120751443192024e-05
2025-07-24 17:32:04.839 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter23_replay_buffer.jsonl
2025-07-24 17:32:06.652 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.81s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:01<?, ?it/s, pg=0.225, ret=-0.00283, glen=115, tlen=275, kl=0.00427, act_lr=4.6e-7, ent=1.98]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:55,  1.03s/it, pg=0.225, ret=-0.00283, glen=115, tlen=275, kl=0.00427, act_lr=4.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:55,  1.03s/it, pg=-0.0505, ret=0.000131, glen=101, tlen=262, kl=0.00392, act_lr=4.6e-7, ent=1.58]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:49,  1.08it/s, pg=-0.0505, ret=0.000131, glen=101, tlen=262, kl=0.00392, act_lr=4.6e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:49,  1.08it/s, pg=0.2, ret=-0.000804, glen=137, tlen=298, kl=0.00404, act_lr=4.6e-7, ent=2.19]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:48,  1.08it/s, pg=0.2, ret=-0.000804, glen=137, tlen=298, kl=0.00404, act_lr=4.6e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:48,  1.08it/s, pg=0.0809, ret=-0.00143, glen=105, tlen=266, kl=0.00468, act_lr=4.6e-7, ent=1.71]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:46,  1.09it/s, pg=0.0809, ret=-0.00143, glen=105, tlen=266, kl=0.00468, act_lr=4.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:46,  1.09it/s, pg=-0.0951, ret=0.000814, glen=104, tlen=265, kl=0.00388, act_lr=4.6e-7, ent=1.67]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:44,  1.11it/s, pg=-0.0951, ret=0.000814, glen=104, tlen=265, kl=0.00388, act_lr=4.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:44,  1.11it/s, pg=-0.0663, ret=0.000369, glen=104, tlen=264, kl=0.00392, act_lr=4.6e-7, ent=1.79]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.13it/s, pg=-0.0663, ret=0.000369, glen=104, tlen=264, kl=0.00392, act_lr=4.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.13it/s, pg=0.0241, ret=-0.000163, glen=101, tlen=261, kl=0.00456, act_lr=4.6e-7, ent=1.63]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.15it/s, pg=0.0241, ret=-0.000163, glen=101, tlen=261, kl=0.00456, act_lr=4.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.15it/s, pg=-0.0546, ret=-0.000283, glen=99.8, tlen=261, kl=0.0043, act_lr=4.6e-7, ent=1.74]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=-0.0546, ret=-0.000283, glen=99.8, tlen=261, kl=0.0043, act_lr=4.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:08<00:41,  1.14it/s, pg=-0.0974, ret=0.000886, glen=95.2, tlen=256, kl=0.00419, act_lr=4.6e-7, ent=1.67]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.15it/s, pg=-0.0974, ret=0.000886, glen=95.2, tlen=256, kl=0.00419, act_lr=4.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.15it/s, pg=-0.0179, ret=-0.000529, glen=102, tlen=262, kl=0.00452, act_lr=4.6e-7, ent=1.74]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:38,  1.16it/s, pg=-0.0179, ret=-0.000529, glen=102, tlen=262, kl=0.00452, act_lr=4.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:38,  1.16it/s, pg=-0.141, ret=1.88e-5, glen=93.3, tlen=254, kl=0.00417, act_lr=4.6e-7, ent=1.59]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:37,  1.17it/s, pg=-0.141, ret=1.88e-5, glen=93.3, tlen=254, kl=0.00417, act_lr=4.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:37,  1.17it/s, pg=0.0419, ret=5.46e-5, glen=115, tlen=276, kl=0.00406, act_lr=4.6e-7, ent=1.73] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:36,  1.17it/s, pg=0.0419, ret=5.46e-5, glen=115, tlen=276, kl=0.00406, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:36,  1.17it/s, pg=-0.00787, ret=0.000158, glen=103, tlen=264, kl=0.004, act_lr=4.6e-7, ent=1.64]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:35,  1.17it/s, pg=-0.00787, ret=0.000158, glen=103, tlen=264, kl=0.004, act_lr=4.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:35,  1.17it/s, pg=0.0684, ret=-0.000685, glen=100, tlen=260, kl=0.0043, act_lr=4.6e-7, ent=1.63]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.17it/s, pg=0.0684, ret=-0.000685, glen=100, tlen=260, kl=0.0043, act_lr=4.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.17it/s, pg=-0.0417, ret=8.08e-5, glen=116, tlen=276, kl=0.00382, act_lr=4.6e-7, ent=1.99]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.0417, ret=8.08e-5, glen=116, tlen=276, kl=0.00382, act_lr=4.6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.0133, ret=0.000233, glen=110, tlen=270, kl=0.00417, act_lr=4.6e-7, ent=1.84]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=-0.0133, ret=0.000233, glen=110, tlen=270, kl=0.00417, act_lr=4.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.0276, ret=6.17e-5, glen=100, tlen=261, kl=0.00401, act_lr=4.6e-7, ent=1.7]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=-0.0276, ret=6.17e-5, glen=100, tlen=261, kl=0.00401, act_lr=4.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=0.329, ret=-0.00254, glen=113, tlen=273, kl=0.00349, act_lr=4.6e-7, ent=1.89]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=0.329, ret=-0.00254, glen=113, tlen=273, kl=0.00349, act_lr=4.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=-0.174, ret=0.00161, glen=104, tlen=265, kl=0.00378, act_lr=4.6e-7, ent=1.8] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.174, ret=0.00161, glen=104, tlen=265, kl=0.00378, act_lr=4.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=0.0262, ret=-0.000501, glen=98.5, tlen=259, kl=0.00461, act_lr=4.6e-7, ent=1.62]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.18it/s, pg=0.0262, ret=-0.000501, glen=98.5, tlen=259, kl=0.00461, act_lr=4.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.18it/s, pg=-0.0705, ret=0.000607, glen=103, tlen=264, kl=0.00374, act_lr=4.6e-7, ent=1.8]  Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:28,  1.18it/s, pg=-0.0705, ret=0.000607, glen=103, tlen=264, kl=0.00374, act_lr=4.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:28,  1.18it/s, pg=-0.127, ret=0.00203, glen=109, tlen=269, kl=0.00396, act_lr=4.6e-7, ent=1.64] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.18it/s, pg=-0.127, ret=0.00203, glen=109, tlen=269, kl=0.00396, act_lr=4.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.18it/s, pg=-0.132, ret=0.00146, glen=99.6, tlen=261, kl=0.00397, act_lr=4.6e-7, ent=1.73]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:19<00:27,  1.17it/s, pg=-0.132, ret=0.00146, glen=99.6, tlen=261, kl=0.00397, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.232, ret=0.00227, glen=103, tlen=263, kl=0.00422, act_lr=4.6e-7, ent=1.69] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.15it/s, pg=-0.232, ret=0.00227, glen=103, tlen=263, kl=0.00422, act_lr=4.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.15it/s, pg=-0.0967, ret=0.00194, glen=106, tlen=267, kl=0.00379, act_lr=4.6e-7, ent=1.78]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:26,  1.14it/s, pg=-0.0967, ret=0.00194, glen=106, tlen=267, kl=0.00379, act_lr=4.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:26,  1.14it/s, pg=0.00256, ret=0.000154, glen=116, tlen=276, kl=0.0035, act_lr=4.6e-7, ent=1.63]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:25,  1.14it/s, pg=0.00256, ret=0.000154, glen=116, tlen=276, kl=0.0035, act_lr=4.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:25,  1.14it/s, pg=-0.000175, ret=-0.00101, glen=112, tlen=273, kl=0.00429, act_lr=4.6e-7, ent=1.68]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:24,  1.15it/s, pg=-0.000175, ret=-0.00101, glen=112, tlen=273, kl=0.00429, act_lr=4.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:24,  1.15it/s, pg=-0.219, ret=0.00127, glen=110, tlen=271, kl=0.00422, act_lr=4.6e-7, ent=2.05]    Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.16it/s, pg=-0.219, ret=0.00127, glen=110, tlen=271, kl=0.00422, act_lr=4.6e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.16it/s, pg=0.196, ret=-0.00121, glen=106, tlen=266, kl=0.00411, act_lr=4.6e-7, ent=1.72]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.06it/s, pg=0.196, ret=-0.00121, glen=106, tlen=266, kl=0.00411, act_lr=4.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.06it/s, pg=0.104, ret=-0.00138, glen=120, tlen=281, kl=0.00369, act_lr=4.6e-7, ent=1.75]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.09it/s, pg=0.104, ret=-0.00138, glen=120, tlen=281, kl=0.00369, act_lr=4.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.09it/s, pg=0.053, ret=-0.000991, glen=99, tlen=260, kl=0.00419, act_lr=4.6e-7, ent=1.65]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.11it/s, pg=0.053, ret=-0.000991, glen=99, tlen=260, kl=0.00419, act_lr=4.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:28<00:21,  1.11it/s, pg=0.24, ret=-0.000603, glen=129, tlen=290, kl=0.00325, act_lr=4.6e-7, ent=1.97]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.13it/s, pg=0.24, ret=-0.000603, glen=129, tlen=290, kl=0.00325, act_lr=4.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.13it/s, pg=0.122, ret=-0.00159, glen=95.8, tlen=256, kl=0.004, act_lr=4.6e-7, ent=1.6]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=0.122, ret=-0.00159, glen=95.8, tlen=256, kl=0.004, act_lr=4.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=-0.02, ret=0.00028, glen=101, tlen=262, kl=0.00423, act_lr=4.6e-7, ent=1.59]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.15it/s, pg=-0.02, ret=0.00028, glen=101, tlen=262, kl=0.00423, act_lr=4.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.15it/s, pg=-0.085, ret=-0.00018, glen=96.9, tlen=257, kl=0.00462, act_lr=4.6e-7, ent=1.77]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.085, ret=-0.00018, glen=96.9, tlen=257, kl=0.00462, act_lr=4.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=-0.116, ret=0.00094, glen=102, tlen=263, kl=0.00393, act_lr=4.6e-7, ent=1.68]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.16it/s, pg=-0.116, ret=0.00094, glen=102, tlen=263, kl=0.00393, act_lr=4.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.16it/s, pg=0.0306, ret=-0.000831, glen=99.6, tlen=260, kl=0.00442, act_lr=4.6e-7, ent=1.6]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=0.0306, ret=-0.000831, glen=99.6, tlen=260, kl=0.00442, act_lr=4.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.17it/s, pg=0.0179, ret=-0.00019, glen=106, tlen=267, kl=0.00401, act_lr=4.6e-7, ent=1.76] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.0179, ret=-0.00019, glen=106, tlen=267, kl=0.00401, act_lr=4.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.00244, ret=-0.000931, glen=121, tlen=282, kl=0.00376, act_lr=4.6e-7, ent=2.12]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.17it/s, pg=-0.00244, ret=-0.000931, glen=121, tlen=282, kl=0.00376, act_lr=4.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0439, ret=0.000678, glen=99.6, tlen=260, kl=0.00386, act_lr=4.6e-7, ent=1.66] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.0439, ret=0.000678, glen=99.6, tlen=260, kl=0.00386, act_lr=4.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=-0.0206, ret=0.000488, glen=93.8, tlen=254, kl=0.00441, act_lr=4.6e-7, ent=1.65]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.18it/s, pg=-0.0206, ret=0.000488, glen=93.8, tlen=254, kl=0.00441, act_lr=4.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.18it/s, pg=-0.0817, ret=0.000973, glen=103, tlen=264, kl=0.00379, act_lr=4.6e-7, ent=1.67] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.18it/s, pg=-0.0817, ret=0.000973, glen=103, tlen=264, kl=0.00379, act_lr=4.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.18it/s, pg=0.119, ret=-0.000858, glen=99.3, tlen=260, kl=0.00431, act_lr=4.6e-7, ent=1.66]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.18it/s, pg=0.119, ret=-0.000858, glen=99.3, tlen=260, kl=0.00431, act_lr=4.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.18it/s, pg=0.0112, ret=-0.000356, glen=111, tlen=272, kl=0.00378, act_lr=4.6e-7, ent=1.76]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.18it/s, pg=0.0112, ret=-0.000356, glen=111, tlen=272, kl=0.00378, act_lr=4.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.18it/s, pg=0.217, ret=0.000713, glen=111, tlen=271, kl=0.0043, act_lr=4.6e-7, ent=1.66]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.18it/s, pg=0.217, ret=0.000713, glen=111, tlen=271, kl=0.0043, act_lr=4.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.18it/s, pg=0.0415, ret=-0.000916, glen=97.8, tlen=259, kl=0.00444, act_lr=4.6e-7, ent=1.73]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.18it/s, pg=0.0415, ret=-0.000916, glen=97.8, tlen=259, kl=0.00444, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.18it/s, pg=-0.0912, ret=-0.00028, glen=91.6, tlen=252, kl=0.00465, act_lr=4.6e-7, ent=1.7] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.18it/s, pg=-0.0912, ret=-0.00028, glen=91.6, tlen=252, kl=0.00465, act_lr=4.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.18it/s, pg=0.033, ret=0.0303, glen=157, tlen=317, kl=0.00319, act_lr=4.6e-7, ent=1.94]    Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.18it/s, pg=0.033, ret=0.0303, glen=157, tlen=317, kl=0.00319, act_lr=4.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.18it/s, pg=0.128, ret=-0.000269, glen=122, tlen=282, kl=0.00344, act_lr=4.6e-7, ent=2.36]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=0.128, ret=-0.000269, glen=122, tlen=282, kl=0.00344, act_lr=4.6e-7, ent=2.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=-0.162, ret=0.00068, glen=112, tlen=273, kl=0.00371, act_lr=4.6e-7, ent=1.74] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=-0.162, ret=0.00068, glen=112, tlen=273, kl=0.00371, act_lr=4.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=-0.174, ret=0.00087, glen=94.6, tlen=255, kl=0.00513, act_lr=4.6e-7, ent=1.68]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.174, ret=0.00087, glen=94.6, tlen=255, kl=0.00513, act_lr=4.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=-0.229, ret=0.00161, glen=100, tlen=260, kl=0.00419, act_lr=4.6e-7, ent=1.71] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.229, ret=0.00161, glen=100, tlen=260, kl=0.00419, act_lr=4.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.041, ret=-0.000881, glen=96.2, tlen=257, kl=0.00414, act_lr=4.6e-7, ent=1.64]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.18it/s, pg=-0.041, ret=-0.000881, glen=96.2, tlen=257, kl=0.00414, act_lr=4.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.18it/s, pg=-0.0928, ret=0.000446, glen=95.4, tlen=256, kl=0.00459, act_lr=4.6e-7, ent=1.62]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.18it/s, pg=-0.0928, ret=0.000446, glen=95.4, tlen=256, kl=0.00459, act_lr=4.6e-7, ent=1.62]
2025-07-24 17:32:54.557 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.72s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.18it/s, pg=0.0344, ret=-0.000538, glen=107, tlen=268, kl=0.00383, act_lr=4.8e-7, ent=1.85] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=0.0344, ret=-0.000538, glen=107, tlen=268, kl=0.00383, act_lr=4.8e-7, ent=1.85]
2025-07-24 17:32:55.516 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.90s
2025-07-24 17:32:58.033 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 17:32:58.353 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 51.64s
2025-07-24 17:32:58.372 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.008672956986860795, 'actor_lr': 4.603636315633511e-07, 'clip_ratio': 0.0, 'entropy': 1.7522447802803733, 'kl': 0.004078639637340199, 'response_length': 106.26776303378018, 'total_length': 266.82010775479404, 'teacher_total_length': 277.9160283868963, 'return': 0.0005340490478292023, 'policy_update_steps': 1.0}

Episode [2/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [40:29<06:58, 209.13s/it][A2025-07-24 17:32:58.422 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:35:06.843 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:35:07.030 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 17:35:07.030 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 128.61s
2025-07-24 17:35:08.971 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0072,avg_pass_at_n: 1.0000,avg_num_tokens: 106.7797,std_num_tokens: 128.9573,avg_correct_num_tokens: 100.6224,std_correct_num_tokens: 79.9240,avg_incorrect_num_tokens: 125.0961,std_incorrect_num_tokens: 215.9855
2025-07-24 17:35:09.428 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.40s
2025-07-24 17:35:12.650 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.22s
2025-07-24 17:35:41.244 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 222
2025-07-24 17:35:41.244 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.59s
2025-07-24 17:35:42.654 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.96s
2025-07-24 17:35:42.654 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0012577009142947023, avg_kl: 0.004244864523947776, avg_response_length: 109.23888149776974, avg_orm_score: 0.0, avg_custom_rewards: -0.0012577009142947023
2025-07-24 17:35:42.693 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter24_replay_buffer.jsonl
2025-07-24 17:35:44.541 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0471, ret=0.000771, glen=127, tlen=286, kl=0.00338, act_lr=4.8e-7, ent=2.06]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:57,  1.04s/it, pg=0.0471, ret=0.000771, glen=127, tlen=286, kl=0.00338, act_lr=4.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:57,  1.04s/it, pg=-0.151, ret=0.000727, glen=97, tlen=257, kl=0.00477, act_lr=4.8e-7, ent=1.71] Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.07it/s, pg=-0.151, ret=0.000727, glen=97, tlen=257, kl=0.00477, act_lr=4.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.07it/s, pg=-0.0686, ret=-0.000335, glen=117, tlen=277, kl=0.00422, act_lr=4.8e-7, ent=1.97]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.11it/s, pg=-0.0686, ret=-0.000335, glen=117, tlen=277, kl=0.00422, act_lr=4.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.11it/s, pg=-0.0857, ret=0.000866, glen=117, tlen=277, kl=0.00416, act_lr=4.8e-7, ent=1.78] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=-0.0857, ret=0.000866, glen=117, tlen=277, kl=0.00416, act_lr=4.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=-0.0511, ret=0.000684, glen=103, tlen=263, kl=0.00449, act_lr=4.8e-7, ent=1.83]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=-0.0511, ret=0.000684, glen=103, tlen=263, kl=0.00449, act_lr=4.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=-0.109, ret=0.000379, glen=118, tlen=278, kl=0.00375, act_lr=4.8e-7, ent=1.88] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=-0.109, ret=0.000379, glen=118, tlen=278, kl=0.00375, act_lr=4.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=0.131, ret=-0.00102, glen=110, tlen=270, kl=0.00436, act_lr=4.8e-7, ent=1.91] Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=0.131, ret=-0.00102, glen=110, tlen=270, kl=0.00436, act_lr=4.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.0462, ret=-0.00161, glen=98.4, tlen=258, kl=0.00457, act_lr=4.8e-7, ent=1.97]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0462, ret=-0.00161, glen=98.4, tlen=258, kl=0.00457, act_lr=4.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.0219, ret=0.000579, glen=108, tlen=268, kl=0.0038, act_lr=4.8e-7, ent=1.76]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:41,  1.14it/s, pg=0.0219, ret=0.000579, glen=108, tlen=268, kl=0.0038, act_lr=4.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.14it/s, pg=-0.0149, ret=-0.00118, glen=101, tlen=260, kl=0.00428, act_lr=4.8e-7, ent=1.77]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:40,  1.15it/s, pg=-0.0149, ret=-0.00118, glen=101, tlen=260, kl=0.00428, act_lr=4.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.15it/s, pg=-0.0453, ret=0.000173, glen=107, tlen=267, kl=0.00408, act_lr=4.8e-7, ent=1.78]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.15it/s, pg=-0.0453, ret=0.000173, glen=107, tlen=267, kl=0.00408, act_lr=4.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.15it/s, pg=-0.0919, ret=-0.000102, glen=97.2, tlen=257, kl=0.00448, act_lr=4.8e-7, ent=1.74]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.16it/s, pg=-0.0919, ret=-0.000102, glen=97.2, tlen=257, kl=0.00448, act_lr=4.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.16it/s, pg=0.199, ret=0.000239, glen=166, tlen=326, kl=0.00366, act_lr=4.8e-7, ent=1.9]     Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.14it/s, pg=0.199, ret=0.000239, glen=166, tlen=326, kl=0.00366, act_lr=4.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.14it/s, pg=-0.00305, ret=-0.000259, glen=115, tlen=275, kl=0.00433, act_lr=4.8e-7, ent=1.81]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.15it/s, pg=-0.00305, ret=-0.000259, glen=115, tlen=275, kl=0.00433, act_lr=4.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.15it/s, pg=0.0543, ret=-0.000935, glen=107, tlen=267, kl=0.00456, act_lr=4.8e-7, ent=1.65]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=0.0543, ret=-0.000935, glen=107, tlen=267, kl=0.00456, act_lr=4.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.16it/s, pg=0.104, ret=-1.67e-6, glen=101, tlen=261, kl=0.00445, act_lr=4.8e-7, ent=1.69]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=0.104, ret=-1.67e-6, glen=101, tlen=261, kl=0.00445, act_lr=4.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=0.0185, ret=0.00116, glen=119, tlen=279, kl=0.00347, act_lr=4.8e-7, ent=1.92]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.16it/s, pg=0.0185, ret=0.00116, glen=119, tlen=279, kl=0.00347, act_lr=4.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.0126, ret=-0.000257, glen=110, tlen=270, kl=0.00471, act_lr=4.8e-7, ent=1.93]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.0126, ret=-0.000257, glen=110, tlen=270, kl=0.00471, act_lr=4.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0159, ret=-0.000521, glen=102, tlen=261, kl=0.00447, act_lr=4.8e-7, ent=1.71]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0159, ret=-0.000521, glen=102, tlen=261, kl=0.00447, act_lr=4.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.203, ret=0.00223, glen=97, tlen=257, kl=0.00471, act_lr=4.8e-7, ent=1.71]    Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.15it/s, pg=-0.203, ret=0.00223, glen=97, tlen=257, kl=0.00471, act_lr=4.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.15it/s, pg=0.106, ret=-0.00123, glen=106, tlen=266, kl=0.00418, act_lr=4.8e-7, ent=1.63]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.14it/s, pg=0.106, ret=-0.00123, glen=106, tlen=266, kl=0.00418, act_lr=4.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.14it/s, pg=0.0437, ret=-0.000583, glen=108, tlen=268, kl=0.0041, act_lr=4.8e-7, ent=1.89]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:30,  1.11it/s, pg=0.0437, ret=-0.000583, glen=108, tlen=268, kl=0.0041, act_lr=4.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:30,  1.11it/s, pg=-0.0474, ret=0.000207, glen=105, tlen=265, kl=0.00376, act_lr=4.8e-7, ent=1.74]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:29,  1.13it/s, pg=-0.0474, ret=0.000207, glen=105, tlen=265, kl=0.00376, act_lr=4.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:21<00:29,  1.13it/s, pg=0.0695, ret=-0.00134, glen=101, tlen=261, kl=0.0048, act_lr=4.8e-7, ent=1.77]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:28,  1.14it/s, pg=0.0695, ret=-0.00134, glen=101, tlen=261, kl=0.0048, act_lr=4.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:28,  1.14it/s, pg=-0.168, ret=0.00035, glen=93.4, tlen=253, kl=0.0042, act_lr=4.8e-7, ent=1.69]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.15it/s, pg=-0.168, ret=0.00035, glen=93.4, tlen=253, kl=0.0042, act_lr=4.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.15it/s, pg=-0.103, ret=0.00201, glen=117, tlen=277, kl=0.00387, act_lr=4.8e-7, ent=1.73]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.16it/s, pg=-0.103, ret=0.00201, glen=117, tlen=277, kl=0.00387, act_lr=4.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.16it/s, pg=-0.141, ret=-0.000126, glen=116, tlen=276, kl=0.00379, act_lr=4.8e-7, ent=1.82]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.16it/s, pg=-0.141, ret=-0.000126, glen=116, tlen=276, kl=0.00379, act_lr=4.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.16it/s, pg=0.166, ret=-3.08e-5, glen=116, tlen=277, kl=0.00391, act_lr=4.8e-7, ent=1.89]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:24,  1.17it/s, pg=0.166, ret=-3.08e-5, glen=116, tlen=277, kl=0.00391, act_lr=4.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:24,  1.17it/s, pg=0.251, ret=-0.00312, glen=112, tlen=272, kl=0.0038, act_lr=4.8e-7, ent=1.9]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.06it/s, pg=0.251, ret=-0.00312, glen=112, tlen=272, kl=0.0038, act_lr=4.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.06it/s, pg=-0.15, ret=0.000622, glen=90.4, tlen=250, kl=0.00489, act_lr=4.8e-7, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.09it/s, pg=-0.15, ret=0.000622, glen=90.4, tlen=250, kl=0.00489, act_lr=4.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.09it/s, pg=-0.136, ret=0.00101, glen=115, tlen=275, kl=0.00405, act_lr=4.8e-7, ent=1.89] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.136, ret=0.00101, glen=115, tlen=275, kl=0.00405, act_lr=4.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.12it/s, pg=-0.126, ret=0.000288, glen=94.3, tlen=254, kl=0.00472, act_lr=4.8e-7, ent=1.56]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.126, ret=0.000288, glen=94.3, tlen=254, kl=0.00472, act_lr=4.8e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=0.115, ret=-0.000243, glen=116, tlen=276, kl=0.00374, act_lr=4.8e-7, ent=1.95] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=0.115, ret=-0.000243, glen=116, tlen=276, kl=0.00374, act_lr=4.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=-0.0356, ret=-0.000423, glen=107, tlen=266, kl=0.00444, act_lr=4.8e-7, ent=1.59]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=-0.0356, ret=-0.000423, glen=107, tlen=266, kl=0.00444, act_lr=4.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=-0.173, ret=0.000244, glen=129, tlen=289, kl=0.00365, act_lr=4.8e-7, ent=2.02]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.173, ret=0.000244, glen=129, tlen=289, kl=0.00365, act_lr=4.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=-0.114, ret=0.000163, glen=96, tlen=256, kl=0.00473, act_lr=4.8e-7, ent=1.83] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=-0.114, ret=0.000163, glen=96, tlen=256, kl=0.00473, act_lr=4.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=0.00903, ret=-0.00072, glen=97.8, tlen=258, kl=0.00539, act_lr=4.8e-7, ent=1.7]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=0.00903, ret=-0.00072, glen=97.8, tlen=258, kl=0.00539, act_lr=4.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.17it/s, pg=0.0149, ret=0.000483, glen=101, tlen=261, kl=0.00416, act_lr=4.8e-7, ent=1.81] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0149, ret=0.000483, glen=101, tlen=261, kl=0.00416, act_lr=4.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.17it/s, pg=-0.126, ret=-0.000238, glen=104, tlen=263, kl=0.00437, act_lr=4.8e-7, ent=1.75]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=-0.126, ret=-0.000238, glen=104, tlen=263, kl=0.00437, act_lr=4.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=0.0411, ret=-0.0107, glen=116, tlen=276, kl=0.00371, act_lr=4.8e-7, ent=1.65]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.16it/s, pg=0.0411, ret=-0.0107, glen=116, tlen=276, kl=0.00371, act_lr=4.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.16it/s, pg=0.244, ret=0.000249, glen=182, tlen=343, kl=0.0036, act_lr=4.8e-7, ent=2.09] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:13,  1.14it/s, pg=0.244, ret=0.000249, glen=182, tlen=343, kl=0.0036, act_lr=4.8e-7, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:13,  1.14it/s, pg=0.0347, ret=-0.000773, glen=94.9, tlen=255, kl=0.00506, act_lr=4.8e-7, ent=1.8]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.15it/s, pg=0.0347, ret=-0.000773, glen=94.9, tlen=255, kl=0.00506, act_lr=4.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.15it/s, pg=0.062, ret=2.95e-6, glen=102, tlen=263, kl=0.00419, act_lr=4.8e-7, ent=1.43]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.16it/s, pg=0.062, ret=2.95e-6, glen=102, tlen=263, kl=0.00419, act_lr=4.8e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.16it/s, pg=0.0315, ret=0.00035, glen=111, tlen=271, kl=0.00469, act_lr=4.8e-7, ent=1.92]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.16it/s, pg=0.0315, ret=0.00035, glen=111, tlen=271, kl=0.00469, act_lr=4.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.16it/s, pg=0.0892, ret=-0.000276, glen=113, tlen=273, kl=0.00409, act_lr=4.8e-7, ent=2.14]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.16it/s, pg=0.0892, ret=-0.000276, glen=113, tlen=273, kl=0.00409, act_lr=4.8e-7, ent=2.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.16it/s, pg=-0.0348, ret=-0.000605, glen=108, tlen=268, kl=0.00448, act_lr=4.8e-7, ent=1.66]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.0348, ret=-0.000605, glen=108, tlen=268, kl=0.00448, act_lr=4.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:41<00:08,  1.17it/s, pg=-0.0334, ret=-0.000182, glen=110, tlen=269, kl=0.004, act_lr=4.8e-7, ent=1.84]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.0334, ret=-0.000182, glen=110, tlen=269, kl=0.004, act_lr=4.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.0323, ret=-0.000426, glen=101, tlen=261, kl=0.00396, act_lr=4.8e-7, ent=1.7]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.0323, ret=-0.000426, glen=101, tlen=261, kl=0.00396, act_lr=4.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.111, ret=-8.98e-6, glen=114, tlen=274, kl=0.00365, act_lr=4.8e-7, ent=2.13]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.111, ret=-8.98e-6, glen=114, tlen=274, kl=0.00365, act_lr=4.8e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.0927, ret=0.000579, glen=101, tlen=262, kl=0.00441, act_lr=4.8e-7, ent=1.78]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.0927, ret=0.000579, glen=101, tlen=262, kl=0.00441, act_lr=4.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0973, ret=-0.00115, glen=110, tlen=270, kl=0.0046, act_lr=4.8e-7, ent=1.84]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.15it/s, pg=0.0973, ret=-0.00115, glen=110, tlen=270, kl=0.0046, act_lr=4.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.15it/s, pg=0.0909, ret=-0.00207, glen=102, tlen=262, kl=0.00459, act_lr=4.8e-7, ent=1.79]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.16it/s, pg=0.0909, ret=-0.00207, glen=102, tlen=262, kl=0.00459, act_lr=4.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.16it/s, pg=-0.205, ret=0.00122, glen=102, tlen=262, kl=0.00451, act_lr=4.8e-7, ent=1.63] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.16it/s, pg=-0.205, ret=0.00122, glen=102, tlen=262, kl=0.00451, act_lr=4.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:47<00:02,  1.16it/s, pg=0.177, ret=-0.00136, glen=108, tlen=268, kl=0.00458, act_lr=4.8e-7, ent=1.98]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.177, ret=-0.00136, glen=108, tlen=268, kl=0.00458, act_lr=4.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.0331, ret=0.000741, glen=100, tlen=260, kl=0.00436, act_lr=4.8e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=-0.0331, ret=0.000741, glen=100, tlen=260, kl=0.00436, act_lr=4.8e-7, ent=1.82]
2025-07-24 17:36:33.598 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.87s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.24, ret=0.00146, glen=96.4, tlen=256, kl=0.00444, act_lr=5e-7, ent=1.77]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=-0.24, ret=0.00146, glen=96.4, tlen=256, kl=0.00444, act_lr=5e-7, ent=1.77]
2025-07-24 17:36:34.267 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 17:36:36.643 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.37s
2025-07-24 17:36:36.991 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.38s
2025-07-24 17:36:36.999 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.012817246573311942, 'actor_lr': 4.803571260124175e-07, 'clip_ratio': 0.0, 'entropy': 1.8087147793599538, 'kl': 0.004252467836652484, 'response_length': 109.13803359440395, 'total_length': 269.0647964477539, 'teacher_total_length': 279.5110871451242, 'return': -0.00025089215016903054, 'policy_update_steps': 1.0}

Episode [2/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [44:08<03:32, 212.02s/it][A2025-07-24 17:36:37.004 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<00:58,  2.92it/s, est. speed input: 513.18 toks/s, output: 26.24 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/172 [00:02<00:01, 61.52it/s, est. speed input: 6446.80 toks/s, output: 1563.69 toks/s]Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/172 [00:02<00:01, 63.58it/s, est. speed input: 6730.29 toks/s, output: 1670.94 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:04<00:00, 22.17it/s, est. speed input: 6840.23 toks/s, output: 2720.85 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:05<00:01, 13.62it/s, est. speed input: 5352.53 toks/s, output: 2564.65 toks/s][32m [repeated 106x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:07<00:00,  4.14it/s, est. speed input: 3925.96 toks/s, output: 2158.18 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:07<00:00, 21.61it/s, est. speed input: 3925.96 toks/s, output: 2158.18 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 169/172 [00:09<00:01,  2.72it/s, est. speed input: 3192.40 toks/s, output: 1895.02 toks/s][32m [repeated 24x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:15<00:00,  1.44s/it, est. speed input: 1969.76 toks/s, output: 1284.60 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:15<00:00, 10.87it/s, est. speed input: 1969.76 toks/s, output: 1284.60 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:09<00:00,  2.44it/s, est. speed input: 3096.41 toks/s, output: 1856.59 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:16<00:00,  1.30s/it, est. speed input: 1886.24 toks/s, output: 983.24 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:16<00:00, 10.39it/s, est. speed input: 1886.24 toks/s, output: 983.24 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:21<00:02,  2.51s/it, est. speed input: 1433.54 toks/s, output: 920.87 toks/s] 
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:58<00:00,  9.87s/it, est. speed input: 527.41 toks/s, output: 408.91 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:58<00:00,  2.91it/s, est. speed input: 527.41 toks/s, output: 408.91 toks/s]
2025-07-24 17:37:37.017 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 508.0087,strategyqa_test/accuracy: 0.3945,eval_accuracy: 0.3945
2025-07-24 17:37:37.260 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:39:21.973 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:39:22.156 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:39:22.157 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 104.90s
2025-07-24 17:39:23.257 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0142,avg_reflection_pattern_score: 0.0100,avg_pass_at_n: 1.0000,avg_num_tokens: 105.5219,std_num_tokens: 137.3893,avg_correct_num_tokens: 99.5269,std_correct_num_tokens: 78.2446,avg_incorrect_num_tokens: 126.1658,std_incorrect_num_tokens: 249.4858
2025-07-24 17:39:23.562 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.41s
2025-07-24 17:39:25.182 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.62s
2025-07-24 17:39:40.260 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 116
2025-07-24 17:39:40.260 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 15.08s
2025-07-24 17:39:41.235 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.52s
2025-07-24 17:39:41.235 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008650166113009869, avg_kl: 0.00495147705078125, avg_response_length: 109.1684290129563, avg_orm_score: 0.0, avg_custom_rewards: -0.0008650166113009869
2025-07-24 17:39:41.261 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter25_replay_buffer.jsonl
2025-07-24 17:39:42.212 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.95s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s, pg=-0.0547, ret=-0.000201, glen=97.1, tlen=258, kl=0.00538, act_lr=5e-7, ent=1.76]Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:00<00:27,  1.00it/s, pg=-0.0547, ret=-0.000201, glen=97.1, tlen=258, kl=0.00538, act_lr=5e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:01<00:27,  1.00it/s, pg=0.0237, ret=-0.000254, glen=99.8, tlen=260, kl=0.00529, act_lr=5e-7, ent=1.7]  Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:01<00:24,  1.09it/s, pg=0.0237, ret=-0.000254, glen=99.8, tlen=260, kl=0.00529, act_lr=5e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:02<00:24,  1.09it/s, pg=0.164, ret=-6.92e-5, glen=112, tlen=272, kl=0.00491, act_lr=5e-7, ent=2.01]  Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:02<00:23,  1.11it/s, pg=0.164, ret=-6.92e-5, glen=112, tlen=272, kl=0.00491, act_lr=5e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:03<00:23,  1.11it/s, pg=-0.12, ret=0.000639, glen=108, tlen=268, kl=0.00482, act_lr=5e-7, ent=1.78]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:03<00:22,  1.13it/s, pg=-0.12, ret=0.000639, glen=108, tlen=268, kl=0.00482, act_lr=5e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:04<00:22,  1.13it/s, pg=-0.209, ret=0.00164, glen=103, tlen=264, kl=0.00497, act_lr=5e-7, ent=1.84]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:04<00:20,  1.15it/s, pg=-0.209, ret=0.00164, glen=103, tlen=264, kl=0.00497, act_lr=5e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:05<00:20,  1.15it/s, pg=0.0768, ret=-0.002, glen=111, tlen=271, kl=0.00436, act_lr=5e-7, ent=1.78] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:05<00:19,  1.16it/s, pg=0.0768, ret=-0.002, glen=111, tlen=271, kl=0.00436, act_lr=5e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:06<00:19,  1.16it/s, pg=0.121, ret=-0.000504, glen=101, tlen=261, kl=0.00519, act_lr=5e-7, ent=1.69]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:06<00:18,  1.16it/s, pg=0.121, ret=-0.000504, glen=101, tlen=261, kl=0.00519, act_lr=5e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:07<00:18,  1.16it/s, pg=-0.178, ret=0.00204, glen=102, tlen=262, kl=0.00491, act_lr=5e-7, ent=1.77] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.16it/s, pg=-0.178, ret=0.00204, glen=102, tlen=262, kl=0.00491, act_lr=5e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.16it/s, pg=0.00513, ret=-0.000255, glen=111, tlen=271, kl=0.00468, act_lr=5e-7, ent=1.77]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:07<00:17,  1.17it/s, pg=0.00513, ret=-0.000255, glen=111, tlen=271, kl=0.00468, act_lr=5e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:08<00:17,  1.17it/s, pg=-0.00595, ret=-0.00042, glen=102, tlen=262, kl=0.00494, act_lr=5e-7, ent=1.7] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:08<00:16,  1.17it/s, pg=-0.00595, ret=-0.00042, glen=102, tlen=262, kl=0.00494, act_lr=5e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:09<00:16,  1.17it/s, pg=-0.0571, ret=0.000254, glen=99.9, tlen=260, kl=0.00551, act_lr=5e-7, ent=1.77]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:09<00:15,  1.16it/s, pg=-0.0571, ret=0.000254, glen=99.9, tlen=260, kl=0.00551, act_lr=5e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:10<00:15,  1.16it/s, pg=0.0725, ret=-0.000732, glen=105, tlen=265, kl=0.0052, act_lr=5e-7, ent=1.67]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:10<00:14,  1.16it/s, pg=0.0725, ret=-0.000732, glen=105, tlen=265, kl=0.0052, act_lr=5e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:11<00:14,  1.16it/s, pg=0.517, ret=-0.00155, glen=245, tlen=405, kl=0.00349, act_lr=5e-7, ent=3.26] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:11<00:14,  1.14it/s, pg=0.517, ret=-0.00155, glen=245, tlen=405, kl=0.00349, act_lr=5e-7, ent=3.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:12<00:14,  1.14it/s, pg=-0.0725, ret=-0.00149, glen=104, tlen=264, kl=0.00486, act_lr=5e-7, ent=1.58]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:12<00:13,  1.15it/s, pg=-0.0725, ret=-0.00149, glen=104, tlen=264, kl=0.00486, act_lr=5e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:13<00:13,  1.15it/s, pg=-0.0229, ret=-0.000463, glen=94.3, tlen=255, kl=0.00511, act_lr=5e-7, ent=1.75]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.14it/s, pg=-0.0229, ret=-0.000463, glen=94.3, tlen=255, kl=0.00511, act_lr=5e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.14it/s, pg=-0.204, ret=0.000712, glen=113, tlen=274, kl=0.00454, act_lr=5e-7, ent=1.77]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:13<00:11,  1.15it/s, pg=-0.204, ret=0.000712, glen=113, tlen=274, kl=0.00454, act_lr=5e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:14<00:11,  1.15it/s, pg=-0.0366, ret=0.00022, glen=95.5, tlen=256, kl=0.0052, act_lr=5e-7, ent=1.67]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:14<00:10,  1.15it/s, pg=-0.0366, ret=0.00022, glen=95.5, tlen=256, kl=0.0052, act_lr=5e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:15<00:10,  1.15it/s, pg=-0.0178, ret=8.5e-5, glen=104, tlen=264, kl=0.00487, act_lr=5e-7, ent=1.74] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:15<00:09,  1.16it/s, pg=-0.0178, ret=8.5e-5, glen=104, tlen=264, kl=0.00487, act_lr=5e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:16<00:09,  1.16it/s, pg=0.0173, ret=-0.000597, glen=116, tlen=277, kl=0.00462, act_lr=5e-7, ent=1.74]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:16<00:08,  1.16it/s, pg=0.0173, ret=-0.000597, glen=116, tlen=277, kl=0.00462, act_lr=5e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:17<00:08,  1.16it/s, pg=-0.0421, ret=0.000408, glen=103, tlen=263, kl=0.00468, act_lr=5e-7, ent=1.76]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:17<00:07,  1.17it/s, pg=-0.0421, ret=0.000408, glen=103, tlen=263, kl=0.00468, act_lr=5e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:18<00:07,  1.17it/s, pg=-0.031, ret=-0.000795, glen=100, tlen=260, kl=0.00516, act_lr=5e-7, ent=1.69]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:18<00:06,  1.17it/s, pg=-0.031, ret=-0.000795, glen=100, tlen=260, kl=0.00516, act_lr=5e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:19<00:06,  1.17it/s, pg=-0.168, ret=0.00139, glen=99.5, tlen=260, kl=0.0053, act_lr=5e-7, ent=1.87]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=-0.168, ret=0.00139, glen=99.5, tlen=260, kl=0.0053, act_lr=5e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=-0.0605, ret=0.000374, glen=103, tlen=264, kl=0.00478, act_lr=5e-7, ent=1.73]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:19<00:05,  1.17it/s, pg=-0.0605, ret=0.000374, glen=103, tlen=264, kl=0.00478, act_lr=5e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:20<00:05,  1.17it/s, pg=0.0142, ret=-0.000424, glen=100, tlen=260, kl=0.00534, act_lr=5e-7, ent=1.78]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:20<00:04,  1.16it/s, pg=0.0142, ret=-0.000424, glen=100, tlen=260, kl=0.00534, act_lr=5e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:21<00:04,  1.16it/s, pg=0.113, ret=-0.00123, glen=113, tlen=273, kl=0.00535, act_lr=5e-7, ent=1.79]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:21<00:03,  1.16it/s, pg=0.113, ret=-0.00123, glen=113, tlen=273, kl=0.00535, act_lr=5e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:22<00:03,  1.16it/s, pg=-0.0759, ret=0.000615, glen=98.5, tlen=258, kl=0.00562, act_lr=5e-7, ent=1.6]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:22<00:02,  1.16it/s, pg=-0.0759, ret=0.000615, glen=98.5, tlen=258, kl=0.00562, act_lr=5e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:23<00:02,  1.16it/s, pg=-0.158, ret=0.000513, glen=99.8, tlen=260, kl=0.00504, act_lr=5e-7, ent=1.83]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:23<00:01,  1.17it/s, pg=-0.158, ret=0.000513, glen=99.8, tlen=260, kl=0.00504, act_lr=5e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:24<00:01,  1.17it/s, pg=0.135, ret=-0.00129, glen=121, tlen=281, kl=0.00454, act_lr=5e-7, ent=2.05]  Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:24<00:00,  1.17it/s, pg=0.135, ret=-0.00129, glen=121, tlen=281, kl=0.00454, act_lr=5e-7, ent=2.05]
2025-07-24 17:40:07.838 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 25.47s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.17it/s, pg=-0.194, ret=0.00146, glen=105, tlen=266, kl=0.00495, act_lr=5.2e-7, ent=1.82]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.10it/s, pg=-0.194, ret=0.00146, glen=105, tlen=266, kl=0.00495, act_lr=5.2e-7, ent=1.82]
2025-07-24 17:40:08.557 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.65s
2025-07-24 17:40:10.888 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.33s
2025-07-24 17:40:11.223 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 28.97s
2025-07-24 17:40:11.227 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.015440710659684807, 'actor_lr': 5.006896544571269e-07, 'clip_ratio': 0.0, 'entropy': 1.815878045969996, 'kl': 0.00495147705078125, 'response_length': 109.16842861833244, 'total_length': 269.4528640220905, 'teacher_total_length': 281.56471515523975, 'return': -6.695152139933459e-05, 'policy_update_steps': 1.0}

Episode [2/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [47:42<00:00, 212.69s/it][A
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,707] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 2.51 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,707] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.88 GB, percent = 21.6%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True[36m(RefRayActorBase pid=1435817)[0m }

[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
2025-07-24 17:40:19.986 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:56:37,132] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 16:52:28,250] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:00:16,014] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:03:19,608] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:07:22,061] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:10:54,800] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:14:53,699] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:26:42,783] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:30:01,889] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:36:33,590] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:40:07,832] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:17,882] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:18,076] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1017, num_elems = 5.33B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,586] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,586] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,593] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,594] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,801] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,801] [INFO] [utils.py:782:see_memory_usage] MA 2.22 GB         Max_MA 7.18 GB         CA 3.24 GB         Max_CA 36 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,802] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.22 GB, percent = 21.7%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [3/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [2/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [47:51<00:00, 220.87s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,980] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,981] [INFO] [utils.py:782:see_memory_usage] MA 2.22 GB         Max_MA 2.22 GB         CA 3.24 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,981] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.22 GB, percent = 21.7%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7cff9a7e0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,984] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 17:40:20.209 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:42:01.770 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:42:01.939 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 17:42:01.940 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 101.73s
2025-07-24 17:42:03.924 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0072,avg_pass_at_n: 1.0000,avg_num_tokens: 107.2848,std_num_tokens: 116.1118,avg_correct_num_tokens: 101.5149,std_correct_num_tokens: 80.5218,avg_incorrect_num_tokens: 126.3270,std_incorrect_num_tokens: 190.0139
2025-07-24 17:42:04.389 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.45s
2025-07-24 17:42:07.567 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 17:42:36.517 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 222
2025-07-24 17:42:36.517 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 17:42:37.941 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 17:42:37.941 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0002607087967057318, avg_kl: 0.0, avg_response_length: 108.8662691545916, avg_orm_score: 0.0, avg_custom_rewards: -0.0002607087967057318
2025-07-24 17:42:37.984 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter26_replay_buffer.jsonl
2025-07-24 17:42:39.822 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.00513, ret=0.000584, glen=128, tlen=288, kl=0, act_lr=5.2e-7, ent=1.84]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<01:02,  1.13s/it, pg=0.00513, ret=0.000584, glen=128, tlen=288, kl=0, act_lr=5.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<01:02,  1.13s/it, pg=0.00562, ret=-0.000626, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=1.79]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:52,  1.03it/s, pg=0.00562, ret=-0.000626, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:52,  1.03it/s, pg=-0.154, ret=0.000564, glen=96, tlen=256, kl=0, act_lr=5.2e-7, ent=1.62]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.09it/s, pg=-0.154, ret=0.000564, glen=96, tlen=256, kl=0, act_lr=5.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.09it/s, pg=0.0144, ret=0.00105, glen=117, tlen=278, kl=0, act_lr=5.2e-7, ent=1.97]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=0.0144, ret=0.00105, glen=117, tlen=278, kl=0, act_lr=5.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=-0.137, ret=-0.000604, glen=101, tlen=262, kl=0, act_lr=5.2e-7, ent=1.82]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=-0.137, ret=-0.000604, glen=101, tlen=262, kl=0, act_lr=5.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=-0.00951, ret=-0.000743, glen=119, tlen=280, kl=0, act_lr=5.2e-7, ent=1.95]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.14it/s, pg=-0.00951, ret=-0.000743, glen=119, tlen=280, kl=0, act_lr=5.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.14it/s, pg=-0.313, ret=0.00172, glen=108, tlen=268, kl=0, act_lr=5.2e-7, ent=1.68]    Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.14it/s, pg=-0.313, ret=0.00172, glen=108, tlen=268, kl=0, act_lr=5.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.14it/s, pg=0.0138, ret=-0.000485, glen=98.5, tlen=259, kl=0, act_lr=5.2e-7, ent=1.62]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0138, ret=-0.000485, glen=98.5, tlen=259, kl=0, act_lr=5.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:41,  1.15it/s, pg=-0.00684, ret=-0.000347, glen=112, tlen=273, kl=0, act_lr=5.2e-7, ent=1.66]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=-0.00684, ret=-0.000347, glen=112, tlen=273, kl=0, act_lr=5.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=0.0104, ret=-9.57e-5, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.81]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:40,  1.13it/s, pg=0.0104, ret=-9.57e-5, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.13it/s, pg=-0.114, ret=0.000384, glen=98.1, tlen=258, kl=0, act_lr=5.2e-7, ent=1.66]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.13it/s, pg=-0.114, ret=0.000384, glen=98.1, tlen=258, kl=0, act_lr=5.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.13it/s, pg=0.173, ret=-0.00201, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.88]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.14it/s, pg=0.173, ret=-0.00201, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.14it/s, pg=-0.117, ret=-0.000259, glen=95.9, tlen=256, kl=0, act_lr=5.2e-7, ent=1.75]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.15it/s, pg=-0.117, ret=-0.000259, glen=95.9, tlen=256, kl=0, act_lr=5.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.15it/s, pg=0.0807, ret=-0.000967, glen=107, tlen=267, kl=0, act_lr=5.2e-7, ent=1.73] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.16it/s, pg=0.0807, ret=-0.000967, glen=107, tlen=267, kl=0, act_lr=5.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.16it/s, pg=-0.095, ret=-0.000186, glen=111, tlen=271, kl=0, act_lr=5.2e-7, ent=1.73]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.095, ret=-0.000186, glen=111, tlen=271, kl=0, act_lr=5.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.16it/s, pg=-0.106, ret=0.00134, glen=108, tlen=269, kl=0, act_lr=5.2e-7, ent=1.83]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.106, ret=0.00134, glen=108, tlen=269, kl=0, act_lr=5.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.215, ret=0.00144, glen=105, tlen=266, kl=0, act_lr=5.2e-7, ent=1.72]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.215, ret=0.00144, glen=105, tlen=266, kl=0, act_lr=5.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.193, ret=-0.000139, glen=116, tlen=276, kl=0, act_lr=5.2e-7, ent=1.84]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.193, ret=-0.000139, glen=116, tlen=276, kl=0, act_lr=5.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.107, ret=-0.000805, glen=116, tlen=277, kl=0, act_lr=5.2e-7, ent=1.5] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.107, ret=-0.000805, glen=116, tlen=277, kl=0, act_lr=5.2e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.0544, ret=-8.52e-5, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.79]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.15it/s, pg=0.0544, ret=-8.52e-5, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.15it/s, pg=-0.262, ret=0.000403, glen=103, tlen=264, kl=0, act_lr=5.2e-7, ent=1.67]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.15it/s, pg=-0.262, ret=0.000403, glen=103, tlen=264, kl=0, act_lr=5.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.15it/s, pg=0.368, ret=-0.00152, glen=142, tlen=302, kl=0, act_lr=5.2e-7, ent=2.18] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.15it/s, pg=0.368, ret=-0.00152, glen=142, tlen=302, kl=0, act_lr=5.2e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.15it/s, pg=-0.0413, ret=-0.00119, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.86]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.0413, ret=-0.00119, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:21<00:28,  1.16it/s, pg=0.114, ret=0.00169, glen=134, tlen=295, kl=0, act_lr=5.2e-7, ent=2.33]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.16it/s, pg=0.114, ret=0.00169, glen=134, tlen=295, kl=0, act_lr=5.2e-7, ent=2.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.16it/s, pg=0.0276, ret=-0.000413, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.8]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.16it/s, pg=0.0276, ret=-0.000413, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.16it/s, pg=-0.0585, ret=-0.00038, glen=103, tlen=263, kl=0, act_lr=5.2e-7, ent=1.72]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.0585, ret=-0.00038, glen=103, tlen=263, kl=0, act_lr=5.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.041, ret=-0.00141, glen=98.8, tlen=259, kl=0, act_lr=5.2e-7, ent=1.58]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.041, ret=-0.00141, glen=98.8, tlen=259, kl=0, act_lr=5.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=0.0408, ret=-0.000623, glen=96.4, tlen=257, kl=0, act_lr=5.2e-7, ent=1.64]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=0.0408, ret=-0.000623, glen=96.4, tlen=257, kl=0, act_lr=5.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=0.0704, ret=-0.000864, glen=99, tlen=259, kl=0, act_lr=5.2e-7, ent=1.91]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=0.0704, ret=-0.000864, glen=99, tlen=259, kl=0, act_lr=5.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.00394, ret=0.000503, glen=91.9, tlen=252, kl=0, act_lr=5.2e-7, ent=1.76]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.00394, ret=0.000503, glen=91.9, tlen=252, kl=0, act_lr=5.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=-0.0636, ret=0.000371, glen=101, tlen=261, kl=0, act_lr=5.2e-7, ent=1.77]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.0636, ret=0.000371, glen=101, tlen=261, kl=0, act_lr=5.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.12it/s, pg=-0.184, ret=0.000532, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=1.81] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=-0.184, ret=0.000532, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=0.00464, ret=-0.000317, glen=108, tlen=267, kl=0, act_lr=5.2e-7, ent=1.9]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=0.00464, ret=-0.000317, glen=108, tlen=267, kl=0, act_lr=5.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=-0.203, ret=0.00182, glen=99.3, tlen=259, kl=0, act_lr=5.2e-7, ent=1.78] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=-0.203, ret=0.00182, glen=99.3, tlen=259, kl=0, act_lr=5.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=0.00708, ret=0.000601, glen=103, tlen=264, kl=0, act_lr=5.2e-7, ent=1.99]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=0.00708, ret=0.000601, glen=103, tlen=264, kl=0, act_lr=5.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.133, ret=-0.00228, glen=125, tlen=286, kl=0, act_lr=5.2e-7, ent=1.64]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.133, ret=-0.00228, glen=125, tlen=286, kl=0, act_lr=5.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.0609, ret=0.00091, glen=113, tlen=273, kl=0, act_lr=5.2e-7, ent=1.78]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.0609, ret=0.00091, glen=113, tlen=273, kl=0, act_lr=5.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.17it/s, pg=0.246, ret=-0.00142, glen=108, tlen=269, kl=0, act_lr=5.2e-7, ent=1.75] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.246, ret=-0.00142, glen=108, tlen=269, kl=0, act_lr=5.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.17it/s, pg=-0.0615, ret=0.000629, glen=109, tlen=270, kl=0, act_lr=5.2e-7, ent=1.61]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0615, ret=0.000629, glen=109, tlen=270, kl=0, act_lr=5.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0776, ret=9.49e-5, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.9]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0776, ret=9.49e-5, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.24, ret=0.00186, glen=104, tlen=264, kl=0, act_lr=5.2e-7, ent=1.73] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.24, ret=0.00186, glen=104, tlen=264, kl=0, act_lr=5.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=-0.165, ret=-0.000219, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.8]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=-0.165, ret=-0.000219, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=0.149, ret=0.00123, glen=142, tlen=303, kl=0, act_lr=5.2e-7, ent=2.13]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.16it/s, pg=0.149, ret=0.00123, glen=142, tlen=303, kl=0, act_lr=5.2e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.16it/s, pg=0.208, ret=0.000177, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.84]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.16it/s, pg=0.208, ret=0.000177, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.16it/s, pg=-0.14, ret=0.00124, glen=119, tlen=279, kl=0, act_lr=5.2e-7, ent=1.97] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.16it/s, pg=-0.14, ret=0.00124, glen=119, tlen=279, kl=0, act_lr=5.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.16it/s, pg=0.142, ret=-0.000843, glen=104, tlen=264, kl=0, act_lr=5.2e-7, ent=1.69]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=0.142, ret=-0.000843, glen=104, tlen=264, kl=0, act_lr=5.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.147, ret=0.000871, glen=107, tlen=267, kl=0, act_lr=5.2e-7, ent=1.64]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.147, ret=0.000871, glen=107, tlen=267, kl=0, act_lr=5.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.0242, ret=0.000601, glen=109, tlen=269, kl=0, act_lr=5.2e-7, ent=1.8] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.0242, ret=0.000601, glen=109, tlen=269, kl=0, act_lr=5.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=0.0784, ret=0.000772, glen=116, tlen=276, kl=0, act_lr=5.2e-7, ent=1.95]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=0.0784, ret=0.000772, glen=116, tlen=276, kl=0, act_lr=5.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=0.192, ret=-0.00247, glen=108, tlen=268, kl=0, act_lr=5.2e-7, ent=1.52] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=0.192, ret=-0.00247, glen=108, tlen=268, kl=0, act_lr=5.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.0771, ret=0.000909, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=2.12]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0771, ret=0.000909, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=-0.0386, ret=-0.000214, glen=105, tlen=265, kl=0, act_lr=5.2e-7, ent=1.58]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.0386, ret=-0.000214, glen=105, tlen=265, kl=0, act_lr=5.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.17it/s, pg=0.0365, ret=-0.0014, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.65]   Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.0365, ret=-0.0014, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.0208, ret=-0.000235, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.92]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=0.0208, ret=-0.000235, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.13, ret=0.000539, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.66]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=-0.13, ret=0.000539, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.66]
2025-07-24 17:43:28.724 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.72s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.144, ret=0.000315, glen=108, tlen=268, kl=0, act_lr=5.4e-7, ent=1.76]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=-0.144, ret=0.000315, glen=108, tlen=268, kl=0, act_lr=5.4e-7, ent=1.76]
2025-07-24 17:43:29.524 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.73s
2025-07-24 17:43:32.070 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 17:43:32.398 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.52s
2025-07-24 17:43:32.406 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01580289431980678, 'actor_lr': 5.203571567286807e-07, 'clip_ratio': 0.0, 'entropy': 1.791975634438651, 'kl': 0.0, 'response_length': 108.88418538229806, 'total_length': 269.1925193241664, 'teacher_total_length': 281.2260295322963, 'return': -6.109829493133085e-08, 'policy_update_steps': 1.0}
Episode [3/20]:   8%|‚ñä         | 1/13 [03:12<38:29, 192.42s/it]2025-07-24 17:43:32.451 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:46:01.198 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:46:01.371 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 17:46:01.371 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 148.92s
2025-07-24 17:46:03.347 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 105.7161,std_num_tokens: 141.7641,avg_correct_num_tokens: 100.2446,std_correct_num_tokens: 89.1933,avg_incorrect_num_tokens: 123.8353,std_incorrect_num_tokens: 244.6988
2025-07-24 17:46:03.649 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.28s
2025-07-24 17:46:06.827 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.17s
2025-07-24 17:46:35.217 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 221
2025-07-24 17:46:35.217 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.39s
2025-07-24 17:46:36.651 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.00s
2025-07-24 17:46:36.652 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0011973463843536576, avg_kl: 0.0009316060338085053, avg_response_length: 111.36141760640554, avg_orm_score: 0.0, avg_custom_rewards: -0.0011973463843536576
2025-07-24 17:46:36.688 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter27_replay_buffer.jsonl
2025-07-24 17:46:38.493 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.81s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.145, ret=-0.00148, glen=104, tlen=265, kl=0.00096, act_lr=5.4e-7, ent=1.81]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.02s/it, pg=0.145, ret=-0.00148, glen=104, tlen=265, kl=0.00096, act_lr=5.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.02s/it, pg=0.161, ret=0.000582, glen=125, tlen=285, kl=0.000893, act_lr=5.4e-7, ent=2.14]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.08it/s, pg=0.161, ret=0.000582, glen=125, tlen=285, kl=0.000893, act_lr=5.4e-7, ent=2.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.08it/s, pg=-0.035, ret=7e-5, glen=88.7, tlen=249, kl=0.000972, act_lr=5.4e-7, ent=1.66]  Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.035, ret=7e-5, glen=88.7, tlen=249, kl=0.000972, act_lr=5.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=-0.0692, ret=-0.000347, glen=92.6, tlen=253, kl=0.00097, act_lr=5.4e-7, ent=1.7]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:45,  1.14it/s, pg=-0.0692, ret=-0.000347, glen=92.6, tlen=253, kl=0.00097, act_lr=5.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:45,  1.14it/s, pg=-0.0928, ret=0.000153, glen=111, tlen=271, kl=0.000952, act_lr=5.4e-7, ent=1.88]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.15it/s, pg=-0.0928, ret=0.000153, glen=111, tlen=271, kl=0.000952, act_lr=5.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.15it/s, pg=0.0926, ret=-3.36e-5, glen=112, tlen=272, kl=0.000924, act_lr=5.4e-7, ent=1.84] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=0.0926, ret=-3.36e-5, glen=112, tlen=272, kl=0.000924, act_lr=5.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=-0.00244, ret=0.000114, glen=110, tlen=270, kl=0.000949, act_lr=5.4e-7, ent=1.91]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.14it/s, pg=-0.00244, ret=0.000114, glen=110, tlen=270, kl=0.000949, act_lr=5.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.14it/s, pg=-0.165, ret=0.000724, glen=97.7, tlen=258, kl=0.000937, act_lr=5.4e-7, ent=1.71] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.14it/s, pg=-0.165, ret=0.000724, glen=97.7, tlen=258, kl=0.000937, act_lr=5.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.14it/s, pg=-0.0622, ret=0.000479, glen=96.8, tlen=257, kl=0.000931, act_lr=5.4e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:41,  1.14it/s, pg=-0.0622, ret=0.000479, glen=96.8, tlen=257, kl=0.000931, act_lr=5.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.14it/s, pg=-0.00806, ret=-0.00121, glen=108, tlen=268, kl=0.000952, act_lr=5.4e-7, ent=1.78]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.15it/s, pg=-0.00806, ret=-0.00121, glen=108, tlen=268, kl=0.000952, act_lr=5.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.15it/s, pg=-0.144, ret=7.49e-5, glen=87.4, tlen=248, kl=0.000947, act_lr=5.4e-7, ent=1.57]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=-0.144, ret=7.49e-5, glen=87.4, tlen=248, kl=0.000947, act_lr=5.4e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=0.0238, ret=1.24e-5, glen=98.5, tlen=259, kl=0.000916, act_lr=5.4e-7, ent=1.67]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.16it/s, pg=0.0238, ret=1.24e-5, glen=98.5, tlen=259, kl=0.000916, act_lr=5.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.16it/s, pg=-0.013, ret=-0.000522, glen=115, tlen=275, kl=0.00095, act_lr=5.4e-7, ent=1.72]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.16it/s, pg=-0.013, ret=-0.000522, glen=115, tlen=275, kl=0.00095, act_lr=5.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.16it/s, pg=0.0139, ret=0.000248, glen=104, tlen=265, kl=0.000922, act_lr=5.4e-7, ent=1.72]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.14it/s, pg=0.0139, ret=0.000248, glen=104, tlen=265, kl=0.000922, act_lr=5.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.14it/s, pg=0.127, ret=-0.000323, glen=129, tlen=289, kl=0.000919, act_lr=5.4e-7, ent=1.87]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.15it/s, pg=0.127, ret=-0.000323, glen=129, tlen=289, kl=0.000919, act_lr=5.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.15it/s, pg=0.0728, ret=-0.00123, glen=94.5, tlen=255, kl=0.000931, act_lr=5.4e-7, ent=1.62]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=0.0728, ret=-0.00123, glen=94.5, tlen=255, kl=0.000931, act_lr=5.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.245, ret=0.00196, glen=102, tlen=262, kl=0.00097, act_lr=5.4e-7, ent=1.67]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.16it/s, pg=-0.245, ret=0.00196, glen=102, tlen=262, kl=0.00097, act_lr=5.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.216, ret=-0.000152, glen=102, tlen=262, kl=0.000933, act_lr=5.4e-7, ent=1.72]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.16it/s, pg=-0.216, ret=-0.000152, glen=102, tlen=262, kl=0.000933, act_lr=5.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.16it/s, pg=0.0112, ret=0.000325, glen=97.3, tlen=258, kl=0.000997, act_lr=5.4e-7, ent=1.68]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0112, ret=0.000325, glen=97.3, tlen=258, kl=0.000997, act_lr=5.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.115, ret=0.000795, glen=107, tlen=268, kl=0.000969, act_lr=5.4e-7, ent=1.89]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.115, ret=0.000795, glen=107, tlen=268, kl=0.000969, act_lr=5.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=0.0566, ret=-0.000517, glen=121, tlen=282, kl=0.00095, act_lr=5.4e-7, ent=1.88]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=0.0566, ret=-0.000517, glen=121, tlen=282, kl=0.00095, act_lr=5.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=0.107, ret=-8.92e-5, glen=118, tlen=279, kl=0.000847, act_lr=5.4e-7, ent=1.53] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=0.107, ret=-8.92e-5, glen=118, tlen=279, kl=0.000847, act_lr=5.4e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.0721, ret=-1.36e-5, glen=101, tlen=261, kl=0.000916, act_lr=5.4e-7, ent=1.72]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.0721, ret=-1.36e-5, glen=101, tlen=261, kl=0.000916, act_lr=5.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.208, ret=0.00148, glen=109, tlen=269, kl=0.000947, act_lr=5.4e-7, ent=1.86]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.208, ret=0.00148, glen=109, tlen=269, kl=0.000947, act_lr=5.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.0753, ret=0.000549, glen=108, tlen=268, kl=0.000885, act_lr=5.4e-7, ent=2.29]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.0753, ret=0.000549, glen=108, tlen=268, kl=0.000885, act_lr=5.4e-7, ent=2.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.000153, ret=-0.000636, glen=103, tlen=263, kl=0.000915, act_lr=5.4e-7, ent=1.58]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.000153, ret=-0.000636, glen=103, tlen=263, kl=0.000915, act_lr=5.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.107, ret=-7.72e-5, glen=94.9, tlen=255, kl=0.000942, act_lr=5.4e-7, ent=1.61]   Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.107, ret=-7.72e-5, glen=94.9, tlen=255, kl=0.000942, act_lr=5.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0172, ret=-0.00118, glen=109, tlen=270, kl=0.00093, act_lr=5.4e-7, ent=1.82] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.0172, ret=-0.00118, glen=109, tlen=270, kl=0.00093, act_lr=5.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.342, ret=0.00193, glen=89.7, tlen=250, kl=0.000925, act_lr=5.4e-7, ent=1.68]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.342, ret=0.00193, glen=89.7, tlen=250, kl=0.000925, act_lr=5.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=0.215, ret=-0.00119, glen=113, tlen=274, kl=0.000915, act_lr=5.4e-7, ent=1.74] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.215, ret=-0.00119, glen=113, tlen=274, kl=0.000915, act_lr=5.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=-0.24, ret=0.00163, glen=130, tlen=291, kl=0.000856, act_lr=5.4e-7, ent=1.48] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.11it/s, pg=-0.24, ret=0.00163, glen=130, tlen=291, kl=0.000856, act_lr=5.4e-7, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.11it/s, pg=0.00793, ret=-0.000953, glen=103, tlen=263, kl=0.000966, act_lr=5.4e-7, ent=1.99]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.13it/s, pg=0.00793, ret=-0.000953, glen=103, tlen=263, kl=0.000966, act_lr=5.4e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.0734, ret=0.000426, glen=97.1, tlen=258, kl=0.000947, act_lr=5.4e-7, ent=1.64]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.14it/s, pg=-0.0734, ret=0.000426, glen=97.1, tlen=258, kl=0.000947, act_lr=5.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.14it/s, pg=-0.014, ret=0.00177, glen=115, tlen=275, kl=0.000952, act_lr=5.4e-7, ent=1.87]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=-0.014, ret=0.00177, glen=115, tlen=275, kl=0.000952, act_lr=5.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=0.192, ret=-0.00216, glen=102, tlen=262, kl=0.000932, act_lr=5.4e-7, ent=1.83]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=0.192, ret=-0.00216, glen=102, tlen=262, kl=0.000932, act_lr=5.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.0291, ret=0.000527, glen=116, tlen=277, kl=0.000896, act_lr=5.4e-7, ent=2]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=0.0291, ret=0.000527, glen=116, tlen=277, kl=0.000896, act_lr=5.4e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=-0.105, ret=-0.00046, glen=123, tlen=284, kl=0.000932, act_lr=5.4e-7, ent=2]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.105, ret=-0.00046, glen=123, tlen=284, kl=0.000932, act_lr=5.4e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.253, ret=0.00156, glen=113, tlen=274, kl=0.000929, act_lr=5.4e-7, ent=1.91]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=-0.253, ret=0.00156, glen=113, tlen=274, kl=0.000929, act_lr=5.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0251, ret=-0.000978, glen=94.8, tlen=255, kl=0.000934, act_lr=5.4e-7, ent=1.78]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.16it/s, pg=0.0251, ret=-0.000978, glen=94.8, tlen=255, kl=0.000934, act_lr=5.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=-0.15, ret=0.000926, glen=104, tlen=264, kl=0.000972, act_lr=5.4e-7, ent=1.65]   Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.15, ret=0.000926, glen=104, tlen=264, kl=0.000972, act_lr=5.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.151, ret=0.00097, glen=98.6, tlen=259, kl=0.000977, act_lr=5.4e-7, ent=1.69]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.151, ret=0.00097, glen=98.6, tlen=259, kl=0.000977, act_lr=5.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=-0.146, ret=0.00218, glen=114, tlen=274, kl=0.000907, act_lr=5.4e-7, ent=1.9]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=-0.146, ret=0.00218, glen=114, tlen=274, kl=0.000907, act_lr=5.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.101, ret=-2.66e-6, glen=102, tlen=262, kl=0.00091, act_lr=5.4e-7, ent=1.72]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.101, ret=-2.66e-6, glen=102, tlen=262, kl=0.00091, act_lr=5.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0921, ret=-0.00154, glen=97.7, tlen=258, kl=0.00094, act_lr=5.4e-7, ent=1.74]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0921, ret=-0.00154, glen=97.7, tlen=258, kl=0.00094, act_lr=5.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=0.0702, ret=-0.00124, glen=108, tlen=268, kl=0.00097, act_lr=5.4e-7, ent=1.76] 
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.0702, ret=-0.00124, glen=108, tlen=268, kl=0.00097, act_lr=5.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0938, ret=0.000685, glen=110, tlen=271, kl=0.000947, act_lr=5.4e-7, ent=1.88]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.0938, ret=0.000685, glen=110, tlen=271, kl=0.000947, act_lr=5.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.00732, ret=0.000503, glen=123, tlen=284, kl=0.00094, act_lr=5.4e-7, ent=2.02]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.00732, ret=0.000503, glen=123, tlen=284, kl=0.00094, act_lr=5.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.496, ret=-0.0124, glen=388, tlen=549, kl=0.000811, act_lr=5.4e-7, ent=2.82]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:07,  1.14it/s, pg=0.496, ret=-0.0124, glen=388, tlen=549, kl=0.000811, act_lr=5.4e-7, ent=2.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:07,  1.14it/s, pg=-0.0883, ret=-0.000456, glen=92, tlen=252, kl=0.000937, act_lr=5.4e-7, ent=1.65]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:06,  1.15it/s, pg=-0.0883, ret=-0.000456, glen=92, tlen=252, kl=0.000937, act_lr=5.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:06,  1.15it/s, pg=0.0468, ret=-0.000804, glen=105, tlen=266, kl=0.00093, act_lr=5.4e-7, ent=1.9]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.16it/s, pg=0.0468, ret=-0.000804, glen=105, tlen=266, kl=0.00093, act_lr=5.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.16it/s, pg=-0.0744, ret=0.000218, glen=90.6, tlen=251, kl=0.000952, act_lr=5.4e-7, ent=1.61]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.16it/s, pg=-0.0744, ret=0.000218, glen=90.6, tlen=251, kl=0.000952, act_lr=5.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.16it/s, pg=0.0439, ret=-0.000301, glen=101, tlen=261, kl=0.000916, act_lr=5.4e-7, ent=1.78] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.16it/s, pg=0.0439, ret=-0.000301, glen=101, tlen=261, kl=0.000916, act_lr=5.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.16it/s, pg=0.216, ret=-0.00144, glen=110, tlen=270, kl=0.000956, act_lr=5.4e-7, ent=1.96]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=0.216, ret=-0.00144, glen=110, tlen=270, kl=0.000956, act_lr=5.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0383, ret=-0.00045, glen=101, tlen=262, kl=0.000931, act_lr=5.4e-7, ent=1.88]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0383, ret=-0.00045, glen=101, tlen=262, kl=0.000931, act_lr=5.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.159, ret=-0.000172, glen=128, tlen=289, kl=0.000858, act_lr=5.4e-7, ent=2.5]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.159, ret=-0.000172, glen=128, tlen=289, kl=0.000858, act_lr=5.4e-7, ent=2.5]
2025-07-24 17:47:27.398 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.72s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.0588, ret=-0.000476, glen=107, tlen=267, kl=0.000929, act_lr=5.6e-7, ent=1.8]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=-0.0588, ret=-0.000476, glen=107, tlen=267, kl=0.000929, act_lr=5.6e-7, ent=1.8]
2025-07-24 17:47:28.212 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 17:47:30.816 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 17:47:31.146 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.59s
2025-07-24 17:47:31.153 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.014297178813389369, 'actor_lr': 5.40357116765985e-07, 'clip_ratio': 0.0, 'entropy': 1.8172655871936254, 'kl': 0.000931952680860247, 'response_length': 111.09012235913958, 'total_length': 271.5563586098807, 'teacher_total_length': 284.7479057312012, 'return': -0.00021363721511339203, 'policy_update_steps': 1.0}
Episode [3/20]:  15%|‚ñà‚ñå        | 2/13 [07:11<40:16, 219.67s/it]2025-07-24 17:47:31.197 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:49:58.414 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:49:58.589 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 17:49:58.589 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 147.39s
2025-07-24 17:50:00.668 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0150,avg_reflection_pattern_score: 0.0085,avg_pass_at_n: 1.0000,avg_num_tokens: 111.1156,std_num_tokens: 189.6987,avg_correct_num_tokens: 100.9595,std_correct_num_tokens: 83.2064,avg_incorrect_num_tokens: 147.9114,std_incorrect_num_tokens: 373.5658
2025-07-24 17:50:01.108 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.52s
2025-07-24 17:50:04.087 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.98s
2025-07-24 17:50:33.062 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 225
2025-07-24 17:50:33.062 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.97s
2025-07-24 17:50:34.647 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.17s
2025-07-24 17:50:34.648 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.00047820416589577995, avg_kl: 0.0009674411349826389, avg_response_length: 119.5765818617079, avg_orm_score: 0.0, avg_custom_rewards: -0.00047820416589577995
2025-07-24 17:50:34.695 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter28_replay_buffer.jsonl
2025-07-24 17:50:36.581 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.89s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.135, ret=0.00166, glen=102, tlen=263, kl=0.00099, act_lr=5.6e-7, ent=1.82]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.135, ret=0.00166, glen=102, tlen=263, kl=0.00099, act_lr=5.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0201, ret=8.35e-5, glen=123, tlen=284, kl=0.000953, act_lr=5.6e-7, ent=2.01]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=-0.0201, ret=8.35e-5, glen=123, tlen=284, kl=0.000953, act_lr=5.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=-0.31, ret=0.00181, glen=97.8, tlen=258, kl=0.00104, act_lr=5.6e-7, ent=1.79]  Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=-0.31, ret=0.00181, glen=97.8, tlen=258, kl=0.00104, act_lr=5.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=0.171, ret=-0.00193, glen=98.9, tlen=259, kl=0.00101, act_lr=5.6e-7, ent=1.81]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.13it/s, pg=0.171, ret=-0.00193, glen=98.9, tlen=259, kl=0.00101, act_lr=5.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.13it/s, pg=-0.0615, ret=0.000645, glen=130, tlen=291, kl=0.000864, act_lr=5.6e-7, ent=2.4]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.0615, ret=0.000645, glen=130, tlen=291, kl=0.000864, act_lr=5.6e-7, ent=2.4]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.214, ret=0.000188, glen=146, tlen=307, kl=0.000877, act_lr=5.6e-7, ent=2.54] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.214, ret=0.000188, glen=146, tlen=307, kl=0.000877, act_lr=5.6e-7, ent=2.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.206, ret=0.00125, glen=109, tlen=270, kl=0.000962, act_lr=5.6e-7, ent=2.04]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.13it/s, pg=-0.206, ret=0.00125, glen=109, tlen=270, kl=0.000962, act_lr=5.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.13it/s, pg=-0.128, ret=0.000868, glen=107, tlen=268, kl=0.00102, act_lr=5.6e-7, ent=1.66]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=-0.128, ret=0.000868, glen=107, tlen=268, kl=0.00102, act_lr=5.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=6.1e-5, ret=-1.04e-5, glen=114, tlen=274, kl=0.000984, act_lr=5.6e-7, ent=1.93]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=6.1e-5, ret=-1.04e-5, glen=114, tlen=274, kl=0.000984, act_lr=5.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.303, ret=-0.00191, glen=140, tlen=300, kl=0.000853, act_lr=5.6e-7, ent=1.87] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=0.303, ret=-0.00191, glen=140, tlen=300, kl=0.000853, act_lr=5.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=-0.107, ret=0.00101, glen=103, tlen=263, kl=0.000972, act_lr=5.6e-7, ent=1.75]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=-0.107, ret=0.00101, glen=103, tlen=263, kl=0.000972, act_lr=5.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=0.0222, ret=-0.000606, glen=112, tlen=273, kl=0.00098, act_lr=5.6e-7, ent=1.97]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=0.0222, ret=-0.000606, glen=112, tlen=273, kl=0.00098, act_lr=5.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.0439, ret=-0.000265, glen=104, tlen=265, kl=0.001, act_lr=5.6e-7, ent=1.8]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=-0.0439, ret=-0.000265, glen=104, tlen=265, kl=0.001, act_lr=5.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=-0.0675, ret=-0.000215, glen=100, tlen=260, kl=0.00102, act_lr=5.6e-7, ent=1.65]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=-0.0675, ret=-0.000215, glen=100, tlen=260, kl=0.00102, act_lr=5.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=0.134, ret=-0.00189, glen=127, tlen=288, kl=0.000982, act_lr=5.6e-7, ent=1.75]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.134, ret=-0.00189, glen=127, tlen=288, kl=0.000982, act_lr=5.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.172, ret=0.000483, glen=101, tlen=261, kl=0.000955, act_lr=5.6e-7, ent=1.87]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:34,  1.17it/s, pg=-0.172, ret=0.000483, glen=101, tlen=261, kl=0.000955, act_lr=5.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:34,  1.17it/s, pg=0.0171, ret=-0.000105, glen=114, tlen=274, kl=0.000957, act_lr=5.6e-7, ent=1.8]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.0171, ret=-0.000105, glen=114, tlen=274, kl=0.000957, act_lr=5.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.0937, ret=2.28e-5, glen=94.6, tlen=255, kl=0.00106, act_lr=5.6e-7, ent=1.82]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.0937, ret=2.28e-5, glen=94.6, tlen=255, kl=0.00106, act_lr=5.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.135, ret=-0.000146, glen=118, tlen=278, kl=0.000947, act_lr=5.6e-7, ent=1.76]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.135, ret=-0.000146, glen=118, tlen=278, kl=0.000947, act_lr=5.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.204, ret=0.0011, glen=103, tlen=263, kl=0.000976, act_lr=5.6e-7, ent=1.63]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.204, ret=0.0011, glen=103, tlen=263, kl=0.000976, act_lr=5.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.124, ret=0.000872, glen=107, tlen=267, kl=0.00102, act_lr=5.6e-7, ent=1.81]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.124, ret=0.000872, glen=107, tlen=267, kl=0.00102, act_lr=5.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.293, ret=-0.00155, glen=164, tlen=324, kl=0.00089, act_lr=5.6e-7, ent=2.6]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=0.293, ret=-0.00155, glen=164, tlen=324, kl=0.00089, act_lr=5.6e-7, ent=2.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=0.102, ret=-0.000655, glen=123, tlen=283, kl=0.000989, act_lr=5.6e-7, ent=2.04]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.16it/s, pg=0.102, ret=-0.000655, glen=123, tlen=283, kl=0.000989, act_lr=5.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.16it/s, pg=-0.2, ret=0.00102, glen=108, tlen=268, kl=0.000978, act_lr=5.6e-7, ent=1.97]   Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.16it/s, pg=-0.2, ret=0.00102, glen=108, tlen=268, kl=0.000978, act_lr=5.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.16it/s, pg=-0.202, ret=0.000529, glen=99.6, tlen=260, kl=0.000921, act_lr=5.6e-7, ent=1.61]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.202, ret=0.000529, glen=99.6, tlen=260, kl=0.000921, act_lr=5.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.242, ret=0.000986, glen=99.9, tlen=260, kl=0.00096, act_lr=5.6e-7, ent=1.79] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.242, ret=0.000986, glen=99.9, tlen=260, kl=0.00096, act_lr=5.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.243, ret=0.000776, glen=90.8, tlen=251, kl=0.000977, act_lr=5.6e-7, ent=1.68]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:26,  1.14it/s, pg=-0.243, ret=0.000776, glen=90.8, tlen=251, kl=0.000977, act_lr=5.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:26,  1.14it/s, pg=0.212, ret=-0.000864, glen=128, tlen=289, kl=0.000973, act_lr=5.6e-7, ent=2.13] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:25,  1.14it/s, pg=0.212, ret=-0.000864, glen=128, tlen=289, kl=0.000973, act_lr=5.6e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:25,  1.14it/s, pg=-0.0607, ret=-0.000439, glen=103, tlen=263, kl=0.000988, act_lr=5.6e-7, ent=1.77]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.05it/s, pg=-0.0607, ret=-0.000439, glen=103, tlen=263, kl=0.000988, act_lr=5.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.05it/s, pg=0.199, ret=0.00226, glen=183, tlen=344, kl=0.000894, act_lr=5.6e-7, ent=2.15]    Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.07it/s, pg=0.199, ret=0.00226, glen=183, tlen=344, kl=0.000894, act_lr=5.6e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.07it/s, pg=-0.0339, ret=-0.00111, glen=102, tlen=262, kl=0.000971, act_lr=5.6e-7, ent=1.86]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.0339, ret=-0.00111, glen=102, tlen=262, kl=0.000971, act_lr=5.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.09it/s, pg=0.173, ret=-0.00126, glen=137, tlen=297, kl=0.00102, act_lr=5.6e-7, ent=2.09]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.173, ret=-0.00126, glen=137, tlen=297, kl=0.00102, act_lr=5.6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=-0.207, ret=0.000248, glen=109, tlen=269, kl=0.000937, act_lr=5.6e-7, ent=1.75]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.12it/s, pg=-0.207, ret=0.000248, glen=109, tlen=269, kl=0.000937, act_lr=5.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.12it/s, pg=-0.318, ret=0.000211, glen=111, tlen=271, kl=0.000927, act_lr=5.6e-7, ent=1.75]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.11it/s, pg=-0.318, ret=0.000211, glen=111, tlen=271, kl=0.000927, act_lr=5.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.11it/s, pg=-0.121, ret=-0.000487, glen=93.1, tlen=253, kl=0.001, act_lr=5.6e-7, ent=1.76] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.13it/s, pg=-0.121, ret=-0.000487, glen=93.1, tlen=253, kl=0.001, act_lr=5.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.13it/s, pg=-0.215, ret=0.000159, glen=107, tlen=268, kl=0.000983, act_lr=5.6e-7, ent=1.57]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.14it/s, pg=-0.215, ret=0.000159, glen=107, tlen=268, kl=0.000983, act_lr=5.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.14it/s, pg=-0.0378, ret=0.000233, glen=85.1, tlen=245, kl=0.000992, act_lr=5.6e-7, ent=1.65]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.15it/s, pg=-0.0378, ret=0.000233, glen=85.1, tlen=245, kl=0.000992, act_lr=5.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.15it/s, pg=0.0979, ret=-0.000462, glen=132, tlen=293, kl=0.000953, act_lr=5.6e-7, ent=1.84] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.15it/s, pg=0.0979, ret=-0.000462, glen=132, tlen=293, kl=0.000953, act_lr=5.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.15it/s, pg=-0.267, ret=0.000977, glen=96.6, tlen=257, kl=0.00104, act_lr=5.6e-7, ent=1.51] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.16it/s, pg=-0.267, ret=0.000977, glen=96.6, tlen=257, kl=0.00104, act_lr=5.6e-7, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.16it/s, pg=-0.318, ret=0.00176, glen=99.7, tlen=260, kl=0.000931, act_lr=5.6e-7, ent=1.63]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.16it/s, pg=-0.318, ret=0.00176, glen=99.7, tlen=260, kl=0.000931, act_lr=5.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.16it/s, pg=0.111, ret=-0.00142, glen=164, tlen=324, kl=0.00102, act_lr=5.6e-7, ent=2.05]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.16it/s, pg=0.111, ret=-0.00142, glen=164, tlen=324, kl=0.00102, act_lr=5.6e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.16it/s, pg=-0.154, ret=-0.000474, glen=93.9, tlen=254, kl=0.00101, act_lr=5.6e-7, ent=1.73]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.16it/s, pg=-0.154, ret=-0.000474, glen=93.9, tlen=254, kl=0.00101, act_lr=5.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.16it/s, pg=-0.158, ret=-0.0003, glen=136, tlen=296, kl=0.000996, act_lr=5.6e-7, ent=2.23]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.16it/s, pg=-0.158, ret=-0.0003, glen=136, tlen=296, kl=0.000996, act_lr=5.6e-7, ent=2.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=0.018, ret=-0.00153, glen=103, tlen=264, kl=0.00102, act_lr=5.6e-7, ent=1.62] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.018, ret=-0.00153, glen=103, tlen=264, kl=0.00102, act_lr=5.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.196, ret=-0.000257, glen=104, tlen=264, kl=0.00104, act_lr=5.6e-7, ent=1.92]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.196, ret=-0.000257, glen=104, tlen=264, kl=0.00104, act_lr=5.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.17it/s, pg=0.0211, ret=-0.000222, glen=102, tlen=262, kl=0.000981, act_lr=5.6e-7, ent=1.76]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0211, ret=-0.000222, glen=102, tlen=262, kl=0.000981, act_lr=5.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0729, ret=9.92e-5, glen=151, tlen=311, kl=0.000906, act_lr=5.6e-7, ent=2.54]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.0729, ret=9.92e-5, glen=151, tlen=311, kl=0.000906, act_lr=5.6e-7, ent=2.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.0603, ret=-0.000458, glen=140, tlen=300, kl=0.000983, act_lr=5.6e-7, ent=2.02]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.16it/s, pg=0.0603, ret=-0.000458, glen=140, tlen=300, kl=0.000983, act_lr=5.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.16it/s, pg=0.471, ret=0.000378, glen=354, tlen=515, kl=0.000752, act_lr=5.6e-7, ent=1.43]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:07,  1.13it/s, pg=0.471, ret=0.000378, glen=354, tlen=515, kl=0.000752, act_lr=5.6e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:07,  1.13it/s, pg=-0.0323, ret=-6.33e-5, glen=112, tlen=272, kl=0.000923, act_lr=5.6e-7, ent=1.86]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.14it/s, pg=-0.0323, ret=-6.33e-5, glen=112, tlen=272, kl=0.000923, act_lr=5.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.14it/s, pg=-0.0679, ret=-0.000275, glen=95, tlen=255, kl=0.00101, act_lr=5.6e-7, ent=1.72] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.15it/s, pg=-0.0679, ret=-0.000275, glen=95, tlen=255, kl=0.00101, act_lr=5.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.15it/s, pg=-0.191, ret=-0.000695, glen=106, tlen=266, kl=0.00101, act_lr=5.6e-7, ent=1.8] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=-0.191, ret=-0.000695, glen=106, tlen=266, kl=0.00101, act_lr=5.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.0793, ret=-0.000668, glen=110, tlen=270, kl=0.00101, act_lr=5.6e-7, ent=1.81]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.16it/s, pg=-0.0793, ret=-0.000668, glen=110, tlen=270, kl=0.00101, act_lr=5.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.16it/s, pg=-0.151, ret=0.000211, glen=94.1, tlen=254, kl=0.000965, act_lr=5.6e-7, ent=1.7] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.151, ret=0.000211, glen=94.1, tlen=254, kl=0.000965, act_lr=5.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=0.353, ret=-0.00228, glen=204, tlen=364, kl=0.000875, act_lr=5.6e-7, ent=2.61] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.15it/s, pg=0.353, ret=-0.00228, glen=204, tlen=364, kl=0.000875, act_lr=5.6e-7, ent=2.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.15it/s, pg=-0.141, ret=0.00173, glen=101, tlen=261, kl=0.000959, act_lr=5.6e-7, ent=1.68]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.16it/s, pg=-0.141, ret=0.00173, glen=101, tlen=261, kl=0.000959, act_lr=5.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.16it/s, pg=-0.093, ret=7.93e-5, glen=100, tlen=261, kl=0.001, act_lr=5.8e-7, ent=1.6]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.093, ret=7.93e-5, glen=100, tlen=261, kl=0.001, act_lr=5.8e-7, ent=1.6]
2025-07-24 17:51:26.517 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.75s
2025-07-24 17:51:27.338 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 17:51:29.866 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-24 17:51:30.194 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.55s
2025-07-24 17:51:30.202 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.04368526057193154, 'actor_lr': 5.603508669625163e-07, 'clip_ratio': 0.0, 'entropy': 1.8722109564563685, 'kl': 0.000968565020644874, 'response_length': 119.21872470253392, 'total_length': 279.51132844623766, 'teacher_total_length': 293.70204242907073, 'return': -1.5770505661637064e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:10<38:05, 228.52s/it]2025-07-24 17:51:30.250 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:54:13.865 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:54:14.042 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:54:14.042 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 163.79s
2025-07-24 17:54:16.018 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0109,avg_pass_at_n: 1.0000,avg_num_tokens: 105.7294,std_num_tokens: 142.0107,avg_correct_num_tokens: 103.3659,std_correct_num_tokens: 108.0079,avg_incorrect_num_tokens: 119.9426,std_incorrect_num_tokens: 266.5631
2025-07-24 17:54:16.463 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.42s
2025-07-24 17:54:19.441 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.98s
2025-07-24 17:54:47.779 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 222
2025-07-24 17:54:47.780 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.34s
2025-07-24 17:54:49.220 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.01s
2025-07-24 17:54:49.220 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0003289105845216729, avg_kl: 0.025493140693183418, avg_response_length: 110.81133586436779, avg_orm_score: 0.0, avg_custom_rewards: 0.0003289105845216729
2025-07-24 17:54:49.251 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter29_replay_buffer.jsonl
2025-07-24 17:54:51.077 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.83s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.176, ret=0.000733, glen=96, tlen=257, kl=0.0287, act_lr=5.8e-7, ent=1.69]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.176, ret=0.000733, glen=96, tlen=257, kl=0.0287, act_lr=5.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.0118, ret=-0.000373, glen=104, tlen=266, kl=0.0144, act_lr=5.8e-7, ent=1.88]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=0.0118, ret=-0.000373, glen=104, tlen=266, kl=0.0144, act_lr=5.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=-0.0643, ret=8.92e-8, glen=108, tlen=269, kl=0.0184, act_lr=5.8e-7, ent=1.7]  Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.0643, ret=8.92e-8, glen=108, tlen=269, kl=0.0184, act_lr=5.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=0.276, ret=-0.000675, glen=126, tlen=288, kl=0.00921, act_lr=5.8e-7, ent=2.17]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=0.276, ret=-0.000675, glen=126, tlen=288, kl=0.00921, act_lr=5.8e-7, ent=2.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=0.104, ret=-0.000267, glen=115, tlen=276, kl=0.0114, act_lr=5.8e-7, ent=1.89] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.104, ret=-0.000267, glen=115, tlen=276, kl=0.0114, act_lr=5.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=0.117, ret=-0.00181, glen=101, tlen=263, kl=0.22, act_lr=5.8e-7, ent=1.61]   Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=0.117, ret=-0.00181, glen=101, tlen=263, kl=0.22, act_lr=5.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=0.154, ret=-0.00138, glen=105, tlen=267, kl=0.0129, act_lr=5.8e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=0.154, ret=-0.00138, glen=105, tlen=267, kl=0.0129, act_lr=5.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=-0.0416, ret=0.000399, glen=102, tlen=263, kl=0.0125, act_lr=5.8e-7, ent=1.67]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0416, ret=0.000399, glen=102, tlen=263, kl=0.0125, act_lr=5.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.076, ret=-3.62e-5, glen=106, tlen=267, kl=0.0112, act_lr=5.8e-7, ent=1.59]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.16it/s, pg=0.076, ret=-3.62e-5, glen=106, tlen=267, kl=0.0112, act_lr=5.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=-0.0226, ret=-0.00019, glen=110, tlen=272, kl=0.0107, act_lr=5.8e-7, ent=1.8]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=-0.0226, ret=-0.00019, glen=110, tlen=272, kl=0.0107, act_lr=5.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=-0.114, ret=0.00069, glen=103, tlen=264, kl=0.0117, act_lr=5.8e-7, ent=1.78] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=-0.114, ret=0.00069, glen=103, tlen=264, kl=0.0117, act_lr=5.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0422, ret=-0.000103, glen=103, tlen=265, kl=0.0112, act_lr=5.8e-7, ent=1.76]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0422, ret=-0.000103, glen=103, tlen=265, kl=0.0112, act_lr=5.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=-0.191, ret=0.00085, glen=100, tlen=262, kl=0.0118, act_lr=5.8e-7, ent=1.69]   Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=-0.191, ret=0.00085, glen=100, tlen=262, kl=0.0118, act_lr=5.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.133, ret=0.00116, glen=102, tlen=263, kl=0.0115, act_lr=5.8e-7, ent=1.74]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.133, ret=0.00116, glen=102, tlen=263, kl=0.0115, act_lr=5.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.0936, ret=0.00021, glen=98.8, tlen=260, kl=0.0281, act_lr=5.8e-7, ent=1.76]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.0936, ret=0.00021, glen=98.8, tlen=260, kl=0.0281, act_lr=5.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=0.021, ret=0.000583, glen=115, tlen=276, kl=0.0105, act_lr=5.8e-7, ent=2.03]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=0.021, ret=0.000583, glen=115, tlen=276, kl=0.0105, act_lr=5.8e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.00372, ret=-0.00103, glen=98.8, tlen=260, kl=0.0157, act_lr=5.8e-7, ent=1.7]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.00372, ret=-0.00103, glen=98.8, tlen=260, kl=0.0157, act_lr=5.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.029, ret=-0.00113, glen=98.6, tlen=261, kl=0.0127, act_lr=5.8e-7, ent=1.71]  Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.029, ret=-0.00113, glen=98.6, tlen=261, kl=0.0127, act_lr=5.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.221, ret=-0.000946, glen=107, tlen=268, kl=0.016, act_lr=5.8e-7, ent=1.86] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:32,  1.15it/s, pg=0.221, ret=-0.000946, glen=107, tlen=268, kl=0.016, act_lr=5.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:32,  1.15it/s, pg=-0.142, ret=0.000315, glen=101, tlen=262, kl=0.0107, act_lr=5.8e-7, ent=1.77]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.15it/s, pg=-0.142, ret=0.000315, glen=101, tlen=262, kl=0.0107, act_lr=5.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.15it/s, pg=-0.191, ret=0.00183, glen=94.7, tlen=256, kl=0.0111, act_lr=5.8e-7, ent=1.73]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=-0.191, ret=0.00183, glen=94.7, tlen=256, kl=0.0111, act_lr=5.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=-0.116, ret=0.000335, glen=102, tlen=264, kl=0.0105, act_lr=5.8e-7, ent=1.78]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.116, ret=0.000335, glen=102, tlen=264, kl=0.0105, act_lr=5.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.0824, ret=0.00099, glen=94.5, tlen=256, kl=0.0118, act_lr=5.8e-7, ent=1.71]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.0824, ret=0.00099, glen=94.5, tlen=256, kl=0.0118, act_lr=5.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=0.268, ret=-0.00164, glen=113, tlen=274, kl=0.0101, act_lr=5.8e-7, ent=1.82]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=0.268, ret=-0.00164, glen=113, tlen=274, kl=0.0101, act_lr=5.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.0137, ret=-0.000604, glen=95, tlen=257, kl=0.0116, act_lr=5.8e-7, ent=1.74]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.0137, ret=-0.000604, glen=95, tlen=257, kl=0.0116, act_lr=5.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.0861, ret=0.000309, glen=104, tlen=266, kl=0.0169, act_lr=5.8e-7, ent=1.68]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.0861, ret=0.000309, glen=104, tlen=266, kl=0.0169, act_lr=5.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.139, ret=0.000933, glen=136, tlen=298, kl=0.00906, act_lr=5.8e-7, ent=2.29]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.139, ret=0.000933, glen=136, tlen=298, kl=0.00906, act_lr=5.8e-7, ent=2.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0476, ret=-0.000404, glen=96.7, tlen=258, kl=0.0122, act_lr=5.8e-7, ent=1.68]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.0476, ret=-0.000404, glen=96.7, tlen=258, kl=0.0122, act_lr=5.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.0538, ret=0.000304, glen=103, tlen=265, kl=0.0128, act_lr=5.8e-7, ent=1.7]   Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.0538, ret=0.000304, glen=103, tlen=265, kl=0.0128, act_lr=5.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.0793, ret=-0.000631, glen=106, tlen=267, kl=0.0117, act_lr=5.8e-7, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.0793, ret=-0.000631, glen=106, tlen=267, kl=0.0117, act_lr=5.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=-0.0496, ret=0.00187, glen=179, tlen=340, kl=0.00957, act_lr=5.8e-7, ent=2.65] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.10it/s, pg=-0.0496, ret=0.00187, glen=179, tlen=340, kl=0.00957, act_lr=5.8e-7, ent=2.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.10it/s, pg=0.0112, ret=-0.000656, glen=105, tlen=267, kl=0.016, act_lr=5.8e-7, ent=1.76] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.12it/s, pg=0.0112, ret=-0.000656, glen=105, tlen=267, kl=0.016, act_lr=5.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=-0.069, ret=0.000809, glen=92.9, tlen=254, kl=0.0141, act_lr=5.8e-7, ent=1.72]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.12it/s, pg=-0.069, ret=0.000809, glen=92.9, tlen=254, kl=0.0141, act_lr=5.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.12it/s, pg=-0.0978, ret=0.000297, glen=108, tlen=270, kl=0.0121, act_lr=5.8e-7, ent=1.77]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.13it/s, pg=-0.0978, ret=0.000297, glen=108, tlen=270, kl=0.0121, act_lr=5.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.13it/s, pg=0.138, ret=-0.00177, glen=107, tlen=268, kl=0.0104, act_lr=5.8e-7, ent=1.78]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.15it/s, pg=0.138, ret=-0.00177, glen=107, tlen=268, kl=0.0104, act_lr=5.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.15it/s, pg=-0.0264, ret=-1.04e-5, glen=111, tlen=272, kl=0.0105, act_lr=5.8e-7, ent=1.98]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=-0.0264, ret=-1.04e-5, glen=111, tlen=272, kl=0.0105, act_lr=5.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=0.162, ret=-0.000418, glen=120, tlen=282, kl=0.507, act_lr=5.8e-7, ent=1.96]  Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=0.162, ret=-0.000418, glen=120, tlen=282, kl=0.507, act_lr=5.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.0352, ret=0.000767, glen=103, tlen=264, kl=0.0118, act_lr=5.8e-7, ent=1.82]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=-0.0352, ret=0.000767, glen=103, tlen=264, kl=0.0118, act_lr=5.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0122, ret=-1.36e-5, glen=104, tlen=265, kl=0.0108, act_lr=5.8e-7, ent=1.79] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0122, ret=-1.36e-5, glen=104, tlen=265, kl=0.0108, act_lr=5.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.087, ret=0.000869, glen=123, tlen=285, kl=0.00967, act_lr=5.8e-7, ent=1.83]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.087, ret=0.000869, glen=123, tlen=285, kl=0.00967, act_lr=5.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.0444, ret=-9.35e-5, glen=115, tlen=276, kl=0.00887, act_lr=5.8e-7, ent=1.91]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.0444, ret=-9.35e-5, glen=115, tlen=276, kl=0.00887, act_lr=5.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.133, ret=-0.00172, glen=107, tlen=268, kl=0.0118, act_lr=5.8e-7, ent=1.93]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.133, ret=-0.00172, glen=107, tlen=268, kl=0.0118, act_lr=5.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.0724, ret=-0.000309, glen=108, tlen=270, kl=0.0144, act_lr=5.8e-7, ent=1.73]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.0724, ret=-0.000309, glen=108, tlen=270, kl=0.0144, act_lr=5.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.0898, ret=0.000264, glen=100, tlen=261, kl=0.0119, act_lr=5.8e-7, ent=1.66] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0898, ret=0.000264, glen=100, tlen=261, kl=0.0119, act_lr=5.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=0.111, ret=-0.000377, glen=107, tlen=268, kl=0.0117, act_lr=5.8e-7, ent=1.64] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.111, ret=-0.000377, glen=107, tlen=268, kl=0.0117, act_lr=5.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.113, ret=0.000351, glen=105, tlen=267, kl=0.0109, act_lr=5.8e-7, ent=1.85]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.113, ret=0.000351, glen=105, tlen=267, kl=0.0109, act_lr=5.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=0.0143, ret=-0.000409, glen=93.6, tlen=255, kl=0.02, act_lr=5.8e-7, ent=1.66]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=0.0143, ret=-0.000409, glen=93.6, tlen=255, kl=0.02, act_lr=5.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.124, ret=0.00104, glen=98.6, tlen=260, kl=0.0114, act_lr=5.8e-7, ent=1.72]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=-0.124, ret=0.00104, glen=98.6, tlen=260, kl=0.0114, act_lr=5.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.0312, ret=7.28e-5, glen=100, tlen=262, kl=0.0146, act_lr=5.8e-7, ent=1.77]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.0312, ret=7.28e-5, glen=100, tlen=262, kl=0.0146, act_lr=5.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.128, ret=-0.00106, glen=111, tlen=272, kl=0.0106, act_lr=5.8e-7, ent=1.91]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.128, ret=-0.00106, glen=111, tlen=272, kl=0.0106, act_lr=5.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.0726, ret=-0.00022, glen=111, tlen=272, kl=0.0146, act_lr=5.8e-7, ent=1.86]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0726, ret=-0.00022, glen=111, tlen=272, kl=0.0146, act_lr=5.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.117, ret=0.000322, glen=105, tlen=266, kl=0.0132, act_lr=5.8e-7, ent=1.86] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=-0.117, ret=0.000322, glen=105, tlen=266, kl=0.0132, act_lr=5.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.142, ret=0.00107, glen=99.7, tlen=260, kl=0.0128, act_lr=5.8e-7, ent=1.76]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.142, ret=0.00107, glen=99.7, tlen=260, kl=0.0128, act_lr=5.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0856, ret=0.000222, glen=94.9, tlen=257, kl=0.0134, act_lr=5.8e-7, ent=1.76]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0856, ret=0.000222, glen=94.9, tlen=257, kl=0.0134, act_lr=5.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.042, ret=0.00102, glen=101, tlen=262, kl=0.0103, act_lr=5.8e-7, ent=1.66]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=-0.042, ret=0.00102, glen=101, tlen=262, kl=0.0103, act_lr=5.8e-7, ent=1.66]
2025-07-24 17:55:39.858 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.59s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.386, ret=0.00204, glen=343, tlen=505, kl=0.01, act_lr=6e-7, ent=2.82]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=0.386, ret=0.00204, glen=343, tlen=505, kl=0.01, act_lr=6e-7, ent=2.82]
2025-07-24 17:55:40.678 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 17:55:43.271 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 17:55:43.608 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.47s
2025-07-24 17:55:43.616 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.016053293432508196, 'actor_lr': 5.803571484973093e-07, 'clip_ratio': 0.0, 'entropy': 1.8178153463772364, 'kl': 0.025361435737327805, 'response_length': 110.68770013536725, 'total_length': 272.04997117178783, 'teacher_total_length': 284.4767412458147, 'return': 4.211468844264995e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [15:23<35:45, 238.35s/it]2025-07-24 17:55:43.623 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:36,  1.77it/s, est. speed input: 318.22 toks/s, output: 33.59 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/172 [00:02<00:02, 40.71it/s, est. speed input: 4641.70 toks/s, output: 1179.11 toks/s]Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 66/172 [00:02<00:01, 55.22it/s, est. speed input: 5213.94 toks/s, output: 1410.88 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:04<00:00, 27.61it/s, est. speed input: 6765.84 toks/s, output: 3107.24 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:04<00:00, 31.57it/s, est. speed input: 6260.12 toks/s, output: 2887.96 toks/s][32m [repeated 101x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:09<00:00, 18.17it/s, est. speed input: 3297.55 toks/s, output: 1831.83 toks/s][32m [repeated 22x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00,  1.55it/s, est. speed input: 3156.71 toks/s, output: 1704.63 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00, 17.41it/s, est. speed input: 3156.71 toks/s, output: 1704.63 toks/s]
2025-07-24 17:55:56.258 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 453.9214,strategyqa_test/accuracy: 0.4862,eval_accuracy: 0.4862
2025-07-24 17:55:56.535 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:58:30.069 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:58:30.249 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:58:30.249 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 153.71s
2025-07-24 17:58:32.230 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0138,avg_reflection_pattern_score: 0.0115,avg_pass_at_n: 1.0000,avg_num_tokens: 107.6633,std_num_tokens: 185.9540,avg_correct_num_tokens: 101.1277,std_correct_num_tokens: 80.1968,avg_incorrect_num_tokens: 154.1376,std_incorrect_num_tokens: 481.9421
2025-07-24 17:58:32.842 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.59s
2025-07-24 17:58:35.837 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.99s
2025-07-24 17:59:04.290 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 224
2025-07-24 17:59:04.291 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.45s
2025-07-24 17:59:05.649 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 17:59:05.650 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -3.73673248239876e-05, avg_kl: 0.01773759296962193, avg_response_length: 118.98230872835431, avg_orm_score: 0.0, avg_custom_rewards: -3.73673248239876e-05
2025-07-24 17:59:05.684 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter30_replay_buffer.jsonl
2025-07-24 17:59:07.527 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00,  1.90it/s, est. speed input: 2665.62 toks/s, output: 1462.11 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00, 14.71it/s, est. speed input: 2665.62 toks/s, output: 1462.11 toks/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.157, ret=-0.000247, glen=123, tlen=284, kl=0.0186, act_lr=6e-7, ent=1.54]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.02s/it, pg=0.157, ret=-0.000247, glen=123, tlen=284, kl=0.0186, act_lr=6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.02s/it, pg=-0.0166, ret=-0.00282, glen=104, tlen=265, kl=0.0139, act_lr=6e-7, ent=1.74]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.08it/s, pg=-0.0166, ret=-0.00282, glen=104, tlen=265, kl=0.0139, act_lr=6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.08it/s, pg=-0.0605, ret=0.000742, glen=113, tlen=273, kl=0.0134, act_lr=6e-7, ent=2.1] Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.0605, ret=0.000742, glen=113, tlen=273, kl=0.0134, act_lr=6e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=-0.159, ret=0.000358, glen=100, tlen=261, kl=0.0161, act_lr=6e-7, ent=1.81]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.11it/s, pg=-0.159, ret=0.000358, glen=100, tlen=261, kl=0.0161, act_lr=6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.11it/s, pg=-0.0728, ret=-0.000277, glen=102, tlen=262, kl=0.0154, act_lr=6e-7, ent=1.82]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=-0.0728, ret=-0.000277, glen=102, tlen=262, kl=0.0154, act_lr=6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=0.0757, ret=-0.00136, glen=111, tlen=271, kl=0.015, act_lr=6e-7, ent=2.07]   Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=0.0757, ret=-0.00136, glen=111, tlen=271, kl=0.015, act_lr=6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=-0.191, ret=0.000406, glen=101, tlen=261, kl=0.0148, act_lr=6e-7, ent=1.74]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.14it/s, pg=-0.191, ret=0.000406, glen=101, tlen=261, kl=0.0148, act_lr=6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.14it/s, pg=0.418, ret=-0.00183, glen=117, tlen=277, kl=0.012, act_lr=6e-7, ent=2.06]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.12it/s, pg=0.418, ret=-0.00183, glen=117, tlen=277, kl=0.012, act_lr=6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:42,  1.12it/s, pg=-0.246, ret=0.000765, glen=112, tlen=273, kl=0.0137, act_lr=6e-7, ent=1.73]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.13it/s, pg=-0.246, ret=0.000765, glen=112, tlen=273, kl=0.0137, act_lr=6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.13it/s, pg=-0.0539, ret=6.05e-5, glen=95.7, tlen=256, kl=0.0137, act_lr=6e-7, ent=1.69]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:41,  1.12it/s, pg=-0.0539, ret=6.05e-5, glen=95.7, tlen=256, kl=0.0137, act_lr=6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:41,  1.12it/s, pg=0.325, ret=-0.000261, glen=123, tlen=284, kl=0.013, act_lr=6e-7, ent=2.39]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.13it/s, pg=0.325, ret=-0.000261, glen=123, tlen=284, kl=0.013, act_lr=6e-7, ent=2.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.13it/s, pg=-0.147, ret=-5.17e-5, glen=96.4, tlen=257, kl=0.0146, act_lr=6e-7, ent=1.8]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.14it/s, pg=-0.147, ret=-5.17e-5, glen=96.4, tlen=257, kl=0.0146, act_lr=6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.14it/s, pg=-0.00439, ret=-0.000532, glen=94.3, tlen=255, kl=0.0283, act_lr=6e-7, ent=1.74]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.15it/s, pg=-0.00439, ret=-0.000532, glen=94.3, tlen=255, kl=0.0283, act_lr=6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.15it/s, pg=-0.117, ret=0.000418, glen=100, tlen=262, kl=0.0144, act_lr=6e-7, ent=1.77]    Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.16it/s, pg=-0.117, ret=0.000418, glen=100, tlen=262, kl=0.0144, act_lr=6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.16it/s, pg=-0.0293, ret=0.00021, glen=118, tlen=279, kl=0.0152, act_lr=6e-7, ent=1.87]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.0293, ret=0.00021, glen=118, tlen=279, kl=0.0152, act_lr=6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.16it/s, pg=-0.104, ret=-0.000156, glen=98.8, tlen=260, kl=0.0135, act_lr=6e-7, ent=1.75]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:35,  1.13it/s, pg=-0.104, ret=-0.000156, glen=98.8, tlen=260, kl=0.0135, act_lr=6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:15<00:35,  1.13it/s, pg=-0.224, ret=0.00112, glen=91.3, tlen=252, kl=0.0181, act_lr=6e-7, ent=1.61]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:34,  1.14it/s, pg=-0.224, ret=0.00112, glen=91.3, tlen=252, kl=0.0181, act_lr=6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:34,  1.14it/s, pg=0.354, ret=-0.000494, glen=144, tlen=305, kl=0.0161, act_lr=6e-7, ent=1.88]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:33,  1.14it/s, pg=0.354, ret=-0.000494, glen=144, tlen=305, kl=0.0161, act_lr=6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:33,  1.14it/s, pg=0.355, ret=0.00072, glen=339, tlen=500, kl=0.0118, act_lr=6e-7, ent=2.55]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:33,  1.12it/s, pg=0.355, ret=0.00072, glen=339, tlen=500, kl=0.0118, act_lr=6e-7, ent=2.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:33,  1.12it/s, pg=0.0978, ret=-0.00101, glen=107, tlen=268, kl=0.0139, act_lr=6e-7, ent=1.86]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.13it/s, pg=0.0978, ret=-0.00101, glen=107, tlen=268, kl=0.0139, act_lr=6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.13it/s, pg=-0.199, ret=0.000551, glen=98.4, tlen=259, kl=0.0135, act_lr=6e-7, ent=1.94]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.15it/s, pg=-0.199, ret=0.000551, glen=98.4, tlen=259, kl=0.0135, act_lr=6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.15it/s, pg=-0.293, ret=0.00158, glen=123, tlen=284, kl=0.0114, act_lr=6e-7, ent=2.04]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.15it/s, pg=-0.293, ret=0.00158, glen=123, tlen=284, kl=0.0114, act_lr=6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.15it/s, pg=-0.213, ret=0.000374, glen=108, tlen=268, kl=0.0202, act_lr=6e-7, ent=1.67]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.213, ret=0.000374, glen=108, tlen=268, kl=0.0202, act_lr=6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:21<00:28,  1.16it/s, pg=0.0171, ret=-0.000148, glen=116, tlen=277, kl=0.0122, act_lr=6e-7, ent=1.86]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.16it/s, pg=0.0171, ret=-0.000148, glen=116, tlen=277, kl=0.0122, act_lr=6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.16it/s, pg=-0.272, ret=0.000939, glen=99.1, tlen=260, kl=0.0187, act_lr=6e-7, ent=1.76]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.272, ret=0.000939, glen=99.1, tlen=260, kl=0.0187, act_lr=6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.25, ret=0.000629, glen=93.9, tlen=255, kl=0.018, act_lr=6e-7, ent=1.73]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.25, ret=0.000629, glen=93.9, tlen=255, kl=0.018, act_lr=6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.0231, ret=-0.000997, glen=102, tlen=262, kl=0.0326, act_lr=6e-7, ent=1.88]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.0231, ret=-0.000997, glen=102, tlen=262, kl=0.0326, act_lr=6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.247, ret=0.000272, glen=108, tlen=269, kl=0.0141, act_lr=6e-7, ent=1.95] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.247, ret=0.000272, glen=108, tlen=269, kl=0.0141, act_lr=6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.116, ret=-0.000201, glen=102, tlen=263, kl=0.0188, act_lr=6e-7, ent=1.7]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.116, ret=-0.000201, glen=102, tlen=263, kl=0.0188, act_lr=6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.279, ret=0.000841, glen=102, tlen=263, kl=0.0152, act_lr=6e-7, ent=1.75]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.279, ret=0.000841, glen=102, tlen=263, kl=0.0152, act_lr=6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=-0.141, ret=0.00136, glen=107, tlen=267, kl=0.0148, act_lr=6e-7, ent=2.05] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.10it/s, pg=-0.141, ret=0.00136, glen=107, tlen=267, kl=0.0148, act_lr=6e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.10it/s, pg=-0.115, ret=-0.000107, glen=102, tlen=263, kl=0.0164, act_lr=6e-7, ent=1.84]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=-0.115, ret=-0.000107, glen=102, tlen=263, kl=0.0164, act_lr=6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:29<00:21,  1.12it/s, pg=-0.254, ret=0.00131, glen=104, tlen=265, kl=0.102, act_lr=6e-7, ent=1.77]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.14it/s, pg=-0.254, ret=0.00131, glen=104, tlen=265, kl=0.102, act_lr=6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:30<00:20,  1.14it/s, pg=0.347, ret=0.00094, glen=358, tlen=519, kl=0.00999, act_lr=6e-7, ent=2.8]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.11it/s, pg=0.347, ret=0.00094, glen=358, tlen=519, kl=0.00999, act_lr=6e-7, ent=2.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.11it/s, pg=-0.155, ret=0.000396, glen=104, tlen=265, kl=0.0152, act_lr=6e-7, ent=1.75]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.13it/s, pg=-0.155, ret=0.000396, glen=104, tlen=265, kl=0.0152, act_lr=6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.13it/s, pg=0.49, ret=1.51e-5, glen=342, tlen=502, kl=0.0118, act_lr=6e-7, ent=3.14]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:18,  1.11it/s, pg=0.49, ret=1.51e-5, glen=342, tlen=502, kl=0.0118, act_lr=6e-7, ent=3.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:18,  1.11it/s, pg=-0.104, ret=-0.00109, glen=119, tlen=280, kl=0.0168, act_lr=6e-7, ent=1.86]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.13it/s, pg=-0.104, ret=-0.00109, glen=119, tlen=280, kl=0.0168, act_lr=6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.13it/s, pg=-0.181, ret=-0.000205, glen=105, tlen=265, kl=0.0161, act_lr=6e-7, ent=1.73]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.14it/s, pg=-0.181, ret=-0.000205, glen=105, tlen=265, kl=0.0161, act_lr=6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.14it/s, pg=-0.038, ret=0.000333, glen=97.5, tlen=258, kl=0.0177, act_lr=6e-7, ent=1.75]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.15it/s, pg=-0.038, ret=0.000333, glen=97.5, tlen=258, kl=0.0177, act_lr=6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:35<00:14,  1.15it/s, pg=0.00465, ret=-0.000927, glen=101, tlen=262, kl=0.0157, act_lr=6e-7, ent=1.7]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.16it/s, pg=0.00465, ret=-0.000927, glen=101, tlen=262, kl=0.0157, act_lr=6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:36<00:13,  1.16it/s, pg=-0.294, ret=0.00101, glen=112, tlen=273, kl=0.0149, act_lr=6e-7, ent=1.91]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=-0.294, ret=0.00101, glen=112, tlen=273, kl=0.0149, act_lr=6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=-0.0337, ret=5.71e-5, glen=107, tlen=267, kl=0.0145, act_lr=6e-7, ent=1.62]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.17it/s, pg=-0.0337, ret=5.71e-5, glen=107, tlen=267, kl=0.0145, act_lr=6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.17it/s, pg=-0.277, ret=0.00152, glen=99.7, tlen=260, kl=0.0157, act_lr=6e-7, ent=2]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.277, ret=0.00152, glen=99.7, tlen=260, kl=0.0157, act_lr=6e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0793, ret=-0.000269, glen=111, tlen=271, kl=0.016, act_lr=6e-7, ent=1.91]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0793, ret=-0.000269, glen=111, tlen=271, kl=0.016, act_lr=6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.111, ret=-0.000765, glen=113, tlen=274, kl=0.0144, act_lr=6e-7, ent=1.93]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.111, ret=-0.000765, glen=113, tlen=274, kl=0.0144, act_lr=6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=0.0641, ret=-0.000531, glen=104, tlen=265, kl=0.0148, act_lr=6e-7, ent=1.97]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=0.0641, ret=-0.000531, glen=104, tlen=265, kl=0.0148, act_lr=6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:41<00:08,  1.17it/s, pg=-0.161, ret=-0.000151, glen=106, tlen=266, kl=0.0146, act_lr=6e-7, ent=1.88]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.161, ret=-0.000151, glen=106, tlen=266, kl=0.0146, act_lr=6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:42<00:07,  1.17it/s, pg=0.00513, ret=-0.000724, glen=105, tlen=266, kl=0.0131, act_lr=6e-7, ent=1.85]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=0.00513, ret=-0.000724, glen=105, tlen=266, kl=0.0131, act_lr=6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.0425, ret=-0.000227, glen=97.7, tlen=259, kl=0.0148, act_lr=6e-7, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.0425, ret=-0.000227, glen=97.7, tlen=259, kl=0.0148, act_lr=6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.0486, ret=-2.4e-5, glen=111, tlen=272, kl=0.0244, act_lr=6e-7, ent=1.9]    Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.0486, ret=-2.4e-5, glen=111, tlen=272, kl=0.0244, act_lr=6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0508, ret=-0.00127, glen=108, tlen=269, kl=0.0394, act_lr=6e-7, ent=1.74]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0508, ret=-0.00127, glen=108, tlen=269, kl=0.0394, act_lr=6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=-0.105, ret=0.000143, glen=100, tlen=261, kl=0.014, act_lr=6e-7, ent=1.87] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.105, ret=0.000143, glen=100, tlen=261, kl=0.014, act_lr=6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.17it/s, pg=0.251, ret=0.000275, glen=113, tlen=274, kl=0.0136, act_lr=6e-7, ent=2.27]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.251, ret=0.000275, glen=113, tlen=274, kl=0.0136, act_lr=6e-7, ent=2.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:47<00:02,  1.17it/s, pg=0.00273, ret=-0.000384, glen=99.1, tlen=260, kl=0.0165, act_lr=6e-7, ent=1.75]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.00273, ret=-0.000384, glen=99.1, tlen=260, kl=0.0165, act_lr=6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:48<00:01,  1.17it/s, pg=-0.0644, ret=0.000136, glen=101, tlen=261, kl=0.0152, act_lr=6e-7, ent=1.89]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.0644, ret=0.000136, glen=101, tlen=261, kl=0.0152, act_lr=6e-7, ent=1.89]
2025-07-24 17:59:56.753 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.00s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.0353, ret=-0.000435, glen=91.7, tlen=252, kl=0.0199, act_lr=6.2e-7, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.12it/s, pg=-0.0353, ret=-0.000435, glen=91.7, tlen=252, kl=0.0199, act_lr=6.2e-7, ent=1.69]
2025-07-24 17:59:57.613 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 18:00:00.217 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 18:00:00.557 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.93s
2025-07-24 18:00:00.587 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.041760308401925225, 'actor_lr': 6.003571633479104e-07, 'clip_ratio': 0.0, 'entropy': 1.893116376229695, 'kl': 0.01773759296962193, 'response_length': 118.98230838775635, 'total_length': 279.5668452126639, 'teacher_total_length': 292.31802095685686, 'return': -3.4362559680240725e-07, 'policy_update_steps': 1.0}
Episode [3/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [19:40<32:40, 245.06s/it]2025-07-24 18:00:00.634 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:02:36.303 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:02:36.489 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 18:02:36.489 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 155.86s
2025-07-24 18:02:38.593 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0141,avg_reflection_pattern_score: 0.0088,avg_pass_at_n: 1.0000,avg_num_tokens: 105.1335,std_num_tokens: 146.3215,avg_correct_num_tokens: 101.9555,std_correct_num_tokens: 95.5187,avg_incorrect_num_tokens: 128.0421,std_incorrect_num_tokens: 330.7207
2025-07-24 18:02:39.051 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.56s
2025-07-24 18:02:41.962 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.91s
2025-07-24 18:03:09.973 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 220
2025-07-24 18:03:09.975 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.01s
2025-07-24 18:03:11.661 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.23s
2025-07-24 18:03:11.662 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0002805455254433169, avg_kl: 0.02517540671608665, avg_response_length: 110.29566435380416, avg_orm_score: 0.0, avg_custom_rewards: 0.0002805455254433169
2025-07-24 18:03:11.699 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter31_replay_buffer.jsonl
2025-07-24 18:03:13.502 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.80s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s, pg=0.15, ret=0.000106, glen=108, tlen=268, kl=0.0198, act_lr=6.2e-7, ent=2.19]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:00<00:53,  1.01it/s, pg=0.15, ret=0.000106, glen=108, tlen=268, kl=0.0198, act_lr=6.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:53,  1.01it/s, pg=-0.0374, ret=-4.84e-5, glen=89.7, tlen=250, kl=0.0175, act_lr=6.2e-7, ent=1.61]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.09it/s, pg=-0.0374, ret=-4.84e-5, glen=89.7, tlen=250, kl=0.0175, act_lr=6.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.09it/s, pg=0.0651, ret=-0.000592, glen=91.9, tlen=252, kl=0.0177, act_lr=6.2e-7, ent=1.7] Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:46,  1.12it/s, pg=0.0651, ret=-0.000592, glen=91.9, tlen=252, kl=0.0177, act_lr=6.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:46,  1.12it/s, pg=0.138, ret=-0.000192, glen=103, tlen=264, kl=0.0129, act_lr=6.2e-7, ent=1.96] Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.11it/s, pg=0.138, ret=-0.000192, glen=103, tlen=264, kl=0.0129, act_lr=6.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.11it/s, pg=-0.226, ret=0.00132, glen=97.8, tlen=258, kl=0.0161, act_lr=6.2e-7, ent=1.78]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:44,  1.13it/s, pg=-0.226, ret=0.00132, glen=97.8, tlen=258, kl=0.0161, act_lr=6.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:44,  1.13it/s, pg=0.122, ret=-0.000138, glen=107, tlen=268, kl=0.0159, act_lr=6.2e-7, ent=1.95]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.12it/s, pg=0.122, ret=-0.000138, glen=107, tlen=268, kl=0.0159, act_lr=6.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.12it/s, pg=0.383, ret=0.00109, glen=158, tlen=319, kl=0.0168, act_lr=6.2e-7, ent=1.62]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:42,  1.13it/s, pg=0.383, ret=0.00109, glen=158, tlen=319, kl=0.0168, act_lr=6.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:42,  1.13it/s, pg=-0.261, ret=0.001, glen=104, tlen=265, kl=0.0143, act_lr=6.2e-7, ent=1.81] Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=-0.261, ret=0.001, glen=104, tlen=265, kl=0.0143, act_lr=6.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=-0.0165, ret=-0.000712, glen=105, tlen=266, kl=0.0192, act_lr=6.2e-7, ent=2.02]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:40,  1.15it/s, pg=-0.0165, ret=-0.000712, glen=105, tlen=266, kl=0.0192, act_lr=6.2e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:40,  1.15it/s, pg=0.0471, ret=-0.000846, glen=113, tlen=273, kl=0.017, act_lr=6.2e-7, ent=2.17]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:38,  1.15it/s, pg=0.0471, ret=-0.000846, glen=113, tlen=273, kl=0.017, act_lr=6.2e-7, ent=2.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:38,  1.15it/s, pg=0.133, ret=-0.0008, glen=104, tlen=264, kl=0.0195, act_lr=6.2e-7, ent=1.92]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.14it/s, pg=0.133, ret=-0.0008, glen=104, tlen=264, kl=0.0195, act_lr=6.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.14it/s, pg=-0.149, ret=0.000317, glen=109, tlen=270, kl=0.0141, act_lr=6.2e-7, ent=2.05]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.15it/s, pg=-0.149, ret=0.000317, glen=109, tlen=270, kl=0.0141, act_lr=6.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.15it/s, pg=-0.158, ret=0.000676, glen=103, tlen=264, kl=0.0202, act_lr=6.2e-7, ent=2.25]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.15it/s, pg=-0.158, ret=0.000676, glen=103, tlen=264, kl=0.0202, act_lr=6.2e-7, ent=2.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.15it/s, pg=-0.0524, ret=4.6e-5, glen=96.6, tlen=257, kl=0.0209, act_lr=6.2e-7, ent=1.69]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.16it/s, pg=-0.0524, ret=4.6e-5, glen=96.6, tlen=257, kl=0.0209, act_lr=6.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.16it/s, pg=0.0424, ret=0.0002, glen=123, tlen=284, kl=0.014, act_lr=6.2e-7, ent=2.13]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.16it/s, pg=0.0424, ret=0.0002, glen=123, tlen=284, kl=0.014, act_lr=6.2e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:14<00:34,  1.16it/s, pg=-0.0865, ret=0.000488, glen=101, tlen=262, kl=0.127, act_lr=6.2e-7, ent=1.65]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.16it/s, pg=-0.0865, ret=0.000488, glen=101, tlen=262, kl=0.127, act_lr=6.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.16it/s, pg=-0.0869, ret=-1.04e-5, glen=110, tlen=271, kl=0.0177, act_lr=6.2e-7, ent=1.96]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.16it/s, pg=-0.0869, ret=-1.04e-5, glen=110, tlen=271, kl=0.0177, act_lr=6.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.16it/s, pg=-0.142, ret=0.000287, glen=103, tlen=263, kl=0.0159, act_lr=6.2e-7, ent=1.89] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.142, ret=0.000287, glen=103, tlen=263, kl=0.0159, act_lr=6.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=0.00464, ret=-0.000642, glen=113, tlen=274, kl=0.015, act_lr=6.2e-7, ent=1.99]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=0.00464, ret=-0.000642, glen=113, tlen=274, kl=0.015, act_lr=6.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=0.013, ret=-0.0008, glen=107, tlen=268, kl=0.0162, act_lr=6.2e-7, ent=1.8]    Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:30,  1.16it/s, pg=0.013, ret=-0.0008, glen=107, tlen=268, kl=0.0162, act_lr=6.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:30,  1.16it/s, pg=0.0284, ret=-0.000732, glen=99.3, tlen=260, kl=0.348, act_lr=6.2e-7, ent=1.84]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.17it/s, pg=0.0284, ret=-0.000732, glen=99.3, tlen=260, kl=0.348, act_lr=6.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.17it/s, pg=0.189, ret=0.00132, glen=147, tlen=308, kl=0.0147, act_lr=6.2e-7, ent=2.22]   Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.16it/s, pg=0.189, ret=0.00132, glen=147, tlen=308, kl=0.0147, act_lr=6.2e-7, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:20<00:28,  1.16it/s, pg=-0.128, ret=0.000279, glen=98.1, tlen=258, kl=0.0179, act_lr=6.2e-7, ent=1.95]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.128, ret=0.000279, glen=98.1, tlen=258, kl=0.0179, act_lr=6.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.0981, ret=-5.85e-5, glen=98, tlen=258, kl=0.017, act_lr=6.2e-7, ent=1.83]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.0981, ret=-5.85e-5, glen=98, tlen=258, kl=0.017, act_lr=6.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=-0.0824, ret=0.000243, glen=101, tlen=261, kl=0.0255, act_lr=6.2e-7, ent=1.76]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=-0.0824, ret=0.000243, glen=101, tlen=261, kl=0.0255, act_lr=6.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=0.151, ret=-0.00112, glen=108, tlen=269, kl=0.0168, act_lr=6.2e-7, ent=1.84]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=0.151, ret=-0.00112, glen=108, tlen=269, kl=0.0168, act_lr=6.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.143, ret=0.000711, glen=117, tlen=278, kl=0.0267, act_lr=6.2e-7, ent=1.93]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:23,  1.17it/s, pg=-0.143, ret=0.000711, glen=117, tlen=278, kl=0.0267, act_lr=6.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:23,  1.17it/s, pg=-0.149, ret=0.00036, glen=93.1, tlen=254, kl=0.0215, act_lr=6.2e-7, ent=1.69]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.17it/s, pg=-0.149, ret=0.00036, glen=93.1, tlen=254, kl=0.0215, act_lr=6.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.17it/s, pg=-0.173, ret=0.000652, glen=101, tlen=261, kl=0.0171, act_lr=6.2e-7, ent=1.79]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.07it/s, pg=-0.173, ret=0.000652, glen=101, tlen=261, kl=0.0171, act_lr=6.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.07it/s, pg=-0.0902, ret=0.000656, glen=104, tlen=264, kl=0.0177, act_lr=6.2e-7, ent=1.86]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.10it/s, pg=-0.0902, ret=0.000656, glen=104, tlen=264, kl=0.0177, act_lr=6.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.10it/s, pg=-0.169, ret=6.06e-5, glen=102, tlen=263, kl=0.0174, act_lr=6.2e-7, ent=1.86]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.169, ret=6.06e-5, glen=102, tlen=263, kl=0.0174, act_lr=6.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=0.0205, ret=-0.00117, glen=110, tlen=271, kl=0.0147, act_lr=6.2e-7, ent=1.85]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:20,  1.13it/s, pg=0.0205, ret=-0.00117, glen=110, tlen=271, kl=0.0147, act_lr=6.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.13it/s, pg=0.00684, ret=0.00043, glen=98, tlen=258, kl=0.0176, act_lr=6.2e-7, ent=1.66] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=0.00684, ret=0.00043, glen=98, tlen=258, kl=0.0176, act_lr=6.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=-0.0197, ret=0.000523, glen=103, tlen=264, kl=0.0145, act_lr=6.2e-7, ent=1.76]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.15it/s, pg=-0.0197, ret=0.000523, glen=103, tlen=264, kl=0.0145, act_lr=6.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.15it/s, pg=-0.181, ret=0.000621, glen=95.2, tlen=256, kl=0.0177, act_lr=6.2e-7, ent=1.65]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.181, ret=0.000621, glen=95.2, tlen=256, kl=0.0177, act_lr=6.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=0.0493, ret=-0.000367, glen=106, tlen=267, kl=0.0148, act_lr=6.2e-7, ent=1.8] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.16it/s, pg=0.0493, ret=-0.000367, glen=106, tlen=267, kl=0.0148, act_lr=6.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.16it/s, pg=0.0761, ret=-0.000775, glen=101, tlen=262, kl=0.0138, act_lr=6.2e-7, ent=1.83]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.16it/s, pg=0.0761, ret=-0.000775, glen=101, tlen=262, kl=0.0138, act_lr=6.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.16it/s, pg=-0.148, ret=0.000361, glen=101, tlen=262, kl=0.0177, act_lr=6.2e-7, ent=1.72] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.148, ret=0.000361, glen=101, tlen=262, kl=0.0177, act_lr=6.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.00769, ret=-0.000265, glen=93.7, tlen=254, kl=0.0167, act_lr=6.2e-7, ent=1.72]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.17it/s, pg=-0.00769, ret=-0.000265, glen=93.7, tlen=254, kl=0.0167, act_lr=6.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0821, ret=-0.00026, glen=104, tlen=264, kl=0.0208, act_lr=6.2e-7, ent=1.75]   Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.0821, ret=-0.00026, glen=104, tlen=264, kl=0.0208, act_lr=6.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=-0.0303, ret=-0.000524, glen=107, tlen=267, kl=0.0222, act_lr=6.2e-7, ent=1.83]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=-0.0303, ret=-0.000524, glen=107, tlen=267, kl=0.0222, act_lr=6.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=-0.0594, ret=-0.000458, glen=102, tlen=262, kl=0.0152, act_lr=6.2e-7, ent=1.68]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=-0.0594, ret=-0.000458, glen=102, tlen=262, kl=0.0152, act_lr=6.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=-0.201, ret=0.000398, glen=94, tlen=255, kl=0.0169, act_lr=6.2e-7, ent=1.74]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.17it/s, pg=-0.201, ret=0.000398, glen=94, tlen=255, kl=0.0169, act_lr=6.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.17it/s, pg=-0.209, ret=8.13e-5, glen=117, tlen=278, kl=0.0166, act_lr=6.2e-7, ent=2.05]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=-0.209, ret=8.13e-5, glen=117, tlen=278, kl=0.0166, act_lr=6.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.17it/s, pg=-0.012, ret=-0.000363, glen=101, tlen=262, kl=0.0172, act_lr=6.2e-7, ent=1.81]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=-0.012, ret=-0.000363, glen=101, tlen=262, kl=0.0172, act_lr=6.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.12, ret=-0.00119, glen=107, tlen=267, kl=0.0164, act_lr=6.2e-7, ent=1.96]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.17it/s, pg=0.12, ret=-0.00119, glen=107, tlen=267, kl=0.0164, act_lr=6.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=0.691, ret=-0.00179, glen=342, tlen=503, kl=0.0148, act_lr=6.2e-7, ent=2.31]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:07,  1.14it/s, pg=0.691, ret=-0.00179, glen=342, tlen=503, kl=0.0148, act_lr=6.2e-7, ent=2.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:07,  1.14it/s, pg=-0.214, ret=0.00106, glen=101, tlen=261, kl=0.0151, act_lr=6.2e-7, ent=1.93]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:06,  1.15it/s, pg=-0.214, ret=0.00106, glen=101, tlen=261, kl=0.0151, act_lr=6.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:06,  1.15it/s, pg=0.00256, ret=0.00033, glen=121, tlen=281, kl=0.0137, act_lr=6.2e-7, ent=1.83]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.15it/s, pg=0.00256, ret=0.00033, glen=121, tlen=281, kl=0.0137, act_lr=6.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.15it/s, pg=0.0293, ret=-0.00072, glen=96.3, tlen=257, kl=0.0169, act_lr=6.2e-7, ent=1.68]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.16it/s, pg=0.0293, ret=-0.00072, glen=96.3, tlen=257, kl=0.0169, act_lr=6.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.16it/s, pg=-0.117, ret=0.000395, glen=102, tlen=263, kl=0.0154, act_lr=6.2e-7, ent=1.71] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.16it/s, pg=-0.117, ret=0.000395, glen=102, tlen=263, kl=0.0154, act_lr=6.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.16it/s, pg=0.00891, ret=-0.000226, glen=98.4, tlen=259, kl=0.0197, act_lr=6.2e-7, ent=1.7]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.00891, ret=-0.000226, glen=98.4, tlen=259, kl=0.0197, act_lr=6.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.0493, ret=-0.0005, glen=110, tlen=270, kl=0.0153, act_lr=6.2e-7, ent=1.69]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.17it/s, pg=0.0493, ret=-0.0005, glen=110, tlen=270, kl=0.0153, act_lr=6.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=-0.0574, ret=0.000463, glen=110, tlen=271, kl=0.0175, act_lr=6.2e-7, ent=1.81]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.17it/s, pg=-0.0574, ret=0.000463, glen=110, tlen=271, kl=0.0175, act_lr=6.2e-7, ent=1.81]
2025-07-24 18:04:01.524 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.17it/s, pg=-0.151, ret=0.00132, glen=129, tlen=290, kl=0.0152, act_lr=6.4e-7, ent=2.14]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=-0.151, ret=0.00132, glen=129, tlen=290, kl=0.0152, act_lr=6.4e-7, ent=2.14]
2025-07-24 18:04:02.397 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 18:04:04.956 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 18:04:05.304 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 51.74s
2025-07-24 18:04:05.311 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.02208071621981534, 'actor_lr': 6.203636168968594e-07, 'clip_ratio': 0.0, 'entropy': 1.860045407035134, 'kl': 0.02517540671608665, 'response_length': 110.29566372958097, 'total_length': 270.9041867342862, 'teacher_total_length': 282.11173373135654, 'return': 8.991045483112843e-06, 'policy_update_steps': 1.0}
Episode [3/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [23:45<28:34, 244.95s/it]2025-07-24 18:04:05.354 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:06:40.381 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:06:40.564 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:06:40.564 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 155.21s
2025-07-24 18:06:42.840 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0140,avg_reflection_pattern_score: 0.0103,avg_pass_at_n: 1.0000,avg_num_tokens: 106.9336,std_num_tokens: 174.4918,avg_correct_num_tokens: 101.4755,std_correct_num_tokens: 90.7150,avg_incorrect_num_tokens: 144.5931,std_incorrect_num_tokens: 426.7583
2025-07-24 18:06:43.291 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.73s
2025-07-24 18:06:46.281 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.99s
2025-07-24 18:07:14.852 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 222
2025-07-24 18:07:14.853 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.57s
2025-07-24 18:07:16.320 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.00s
2025-07-24 18:07:16.320 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0019841962105905018, avg_kl: 0.023475715705940314, avg_response_length: 114.75991690695822, avg_orm_score: 0.0, avg_custom_rewards: -0.0019841962105905018
2025-07-24 18:07:16.357 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter32_replay_buffer.jsonl
2025-07-24 18:07:18.197 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.19, ret=-0.000571, glen=109, tlen=270, kl=0.0161, act_lr=6.4e-7, ent=1.96]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.02s/it, pg=0.19, ret=-0.000571, glen=109, tlen=270, kl=0.0161, act_lr=6.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.02s/it, pg=-0.18, ret=0.000479, glen=93.9, tlen=255, kl=0.0239, act_lr=6.4e-7, ent=1.73]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=-0.18, ret=0.000479, glen=93.9, tlen=255, kl=0.0239, act_lr=6.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=0.0164, ret=0.000133, glen=111, tlen=272, kl=0.0208, act_lr=6.4e-7, ent=1.9] Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.09it/s, pg=0.0164, ret=0.000133, glen=111, tlen=272, kl=0.0208, act_lr=6.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.09it/s, pg=0.0793, ret=0.000556, glen=117, tlen=278, kl=0.0222, act_lr=6.4e-7, ent=1.78]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=0.0793, ret=0.000556, glen=117, tlen=278, kl=0.0222, act_lr=6.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=-0.0237, ret=-0.00103, glen=118, tlen=279, kl=0.0129, act_lr=6.4e-7, ent=1.95]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=-0.0237, ret=-0.00103, glen=118, tlen=279, kl=0.0129, act_lr=6.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=-0.0355, ret=-0.00085, glen=102, tlen=263, kl=0.017, act_lr=6.4e-7, ent=1.71] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=-0.0355, ret=-0.00085, glen=102, tlen=263, kl=0.017, act_lr=6.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=-0.136, ret=-3.82e-5, glen=106, tlen=267, kl=0.0165, act_lr=6.4e-7, ent=1.74]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.14it/s, pg=-0.136, ret=-3.82e-5, glen=106, tlen=267, kl=0.0165, act_lr=6.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.14it/s, pg=-0.111, ret=-0.0004, glen=100, tlen=261, kl=0.0247, act_lr=6.4e-7, ent=1.73] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=-0.111, ret=-0.0004, glen=100, tlen=261, kl=0.0247, act_lr=6.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=-0.143, ret=1.05e-5, glen=96.4, tlen=257, kl=0.0203, act_lr=6.4e-7, ent=1.9]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.15it/s, pg=-0.143, ret=1.05e-5, glen=96.4, tlen=257, kl=0.0203, act_lr=6.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=-0.176, ret=0.000441, glen=94.1, tlen=255, kl=0.0231, act_lr=6.4e-7, ent=1.78]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=-0.176, ret=0.000441, glen=94.1, tlen=255, kl=0.0231, act_lr=6.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=-0.243, ret=0.000474, glen=94.6, tlen=255, kl=0.0161, act_lr=6.4e-7, ent=1.77]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=-0.243, ret=0.000474, glen=94.6, tlen=255, kl=0.0161, act_lr=6.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=0.0597, ret=0.000264, glen=116, tlen=277, kl=0.017, act_lr=6.4e-7, ent=1.93]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.15it/s, pg=0.0597, ret=0.000264, glen=116, tlen=277, kl=0.017, act_lr=6.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.15it/s, pg=-0.195, ret=0.000356, glen=101, tlen=262, kl=0.0216, act_lr=6.4e-7, ent=1.68]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.15it/s, pg=-0.195, ret=0.000356, glen=101, tlen=262, kl=0.0216, act_lr=6.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.15it/s, pg=-0.234, ret=0.000673, glen=103, tlen=264, kl=0.0211, act_lr=6.4e-7, ent=1.78]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.16it/s, pg=-0.234, ret=0.000673, glen=103, tlen=264, kl=0.0211, act_lr=6.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.16it/s, pg=-0.106, ret=0.000351, glen=92.2, tlen=253, kl=0.0227, act_lr=6.4e-7, ent=1.74]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.106, ret=0.000351, glen=92.2, tlen=253, kl=0.0227, act_lr=6.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.16it/s, pg=0.0988, ret=-5.32e-5, glen=120, tlen=281, kl=0.016, act_lr=6.4e-7, ent=2.27]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=0.0988, ret=-5.32e-5, glen=120, tlen=281, kl=0.016, act_lr=6.4e-7, ent=2.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.143, ret=-0.000122, glen=89.2, tlen=250, kl=0.0245, act_lr=6.4e-7, ent=1.73]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:34,  1.15it/s, pg=-0.143, ret=-0.000122, glen=89.2, tlen=250, kl=0.0245, act_lr=6.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:34,  1.15it/s, pg=0.223, ret=-0.00188, glen=117, tlen=277, kl=0.028, act_lr=6.4e-7, ent=2.06]    Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.15it/s, pg=0.223, ret=-0.00188, glen=117, tlen=277, kl=0.028, act_lr=6.4e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.15it/s, pg=-0.166, ret=0.000166, glen=119, tlen=280, kl=0.0161, act_lr=6.4e-7, ent=1.86]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.16it/s, pg=-0.166, ret=0.000166, glen=119, tlen=280, kl=0.0161, act_lr=6.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.16it/s, pg=-0.0793, ret=0.00134, glen=115, tlen=275, kl=0.0151, act_lr=6.4e-7, ent=2.03]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=-0.0793, ret=0.00134, glen=115, tlen=275, kl=0.0151, act_lr=6.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.218, ret=0.000552, glen=103, tlen=264, kl=0.0165, act_lr=6.4e-7, ent=1.81]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.218, ret=0.000552, glen=103, tlen=264, kl=0.0165, act_lr=6.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.0823, ret=-0.000932, glen=118, tlen=279, kl=0.268, act_lr=6.4e-7, ent=1.87]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.0823, ret=-0.000932, glen=118, tlen=279, kl=0.268, act_lr=6.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.17it/s, pg=-0.207, ret=0.00132, glen=91.5, tlen=252, kl=0.0169, act_lr=6.4e-7, ent=1.6]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.207, ret=0.00132, glen=91.5, tlen=252, kl=0.0169, act_lr=6.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.00244, ret=-0.000176, glen=103, tlen=264, kl=0.0232, act_lr=6.4e-7, ent=1.8]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.15it/s, pg=-0.00244, ret=-0.000176, glen=103, tlen=264, kl=0.0232, act_lr=6.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.15it/s, pg=-0.0768, ret=-0.000829, glen=104, tlen=265, kl=0.0193, act_lr=6.4e-7, ent=1.85]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.16it/s, pg=-0.0768, ret=-0.000829, glen=104, tlen=265, kl=0.0193, act_lr=6.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.16it/s, pg=0.0919, ret=-0.000812, glen=104, tlen=265, kl=0.0158, act_lr=6.4e-7, ent=1.77] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.16it/s, pg=0.0919, ret=-0.000812, glen=104, tlen=265, kl=0.0158, act_lr=6.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.16it/s, pg=-0.151, ret=-0.000351, glen=102, tlen=263, kl=0.0225, act_lr=6.4e-7, ent=1.69]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.16it/s, pg=-0.151, ret=-0.000351, glen=102, tlen=263, kl=0.0225, act_lr=6.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.16it/s, pg=0.0535, ret=-5.81e-5, glen=128, tlen=290, kl=0.0155, act_lr=6.4e-7, ent=2.04] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:24,  1.16it/s, pg=0.0535, ret=-5.81e-5, glen=128, tlen=290, kl=0.0155, act_lr=6.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:24,  1.16it/s, pg=0.375, ret=-0.0045, glen=246, tlen=406, kl=0.0117, act_lr=6.4e-7, ent=2.8]   Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.04it/s, pg=0.375, ret=-0.0045, glen=246, tlen=406, kl=0.0117, act_lr=6.4e-7, ent=2.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.04it/s, pg=0.301, ret=-0.00146, glen=129, tlen=290, kl=0.013, act_lr=6.4e-7, ent=2.43]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:24,  1.07it/s, pg=0.301, ret=-0.00146, glen=129, tlen=290, kl=0.013, act_lr=6.4e-7, ent=2.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:24,  1.07it/s, pg=0.077, ret=-0.00115, glen=99, tlen=260, kl=0.0232, act_lr=6.4e-7, ent=1.81]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.10it/s, pg=0.077, ret=-0.00115, glen=99, tlen=260, kl=0.0232, act_lr=6.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.10it/s, pg=-0.126, ret=8.74e-5, glen=103, tlen=264, kl=0.0181, act_lr=6.4e-7, ent=1.88]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=-0.126, ret=8.74e-5, glen=103, tlen=264, kl=0.0181, act_lr=6.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=-0.0652, ret=0.000696, glen=100, tlen=261, kl=0.0232, act_lr=6.4e-7, ent=1.7]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.13it/s, pg=-0.0652, ret=0.000696, glen=100, tlen=261, kl=0.0232, act_lr=6.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.13it/s, pg=-0.0815, ret=0.000262, glen=101, tlen=262, kl=0.0155, act_lr=6.4e-7, ent=1.72]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.12it/s, pg=-0.0815, ret=0.000262, glen=101, tlen=262, kl=0.0155, act_lr=6.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.12it/s, pg=-0.0544, ret=0.000453, glen=121, tlen=282, kl=0.0207, act_lr=6.4e-7, ent=2.02]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.13it/s, pg=-0.0544, ret=0.000453, glen=121, tlen=282, kl=0.0207, act_lr=6.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.13it/s, pg=0.226, ret=0.000157, glen=131, tlen=292, kl=0.0369, act_lr=6.4e-7, ent=2.27]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.14it/s, pg=0.226, ret=0.000157, glen=131, tlen=292, kl=0.0369, act_lr=6.4e-7, ent=2.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.14it/s, pg=0.38, ret=-0.00615, glen=342, tlen=502, kl=0.0187, act_lr=6.4e-7, ent=1.67] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:17,  1.12it/s, pg=0.38, ret=-0.00615, glen=342, tlen=502, kl=0.0187, act_lr=6.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:17,  1.12it/s, pg=-0.0448, ret=-0.000645, glen=108, tlen=268, kl=0.0146, act_lr=6.4e-7, ent=1.91]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.13it/s, pg=-0.0448, ret=-0.000645, glen=108, tlen=268, kl=0.0146, act_lr=6.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.13it/s, pg=0.0665, ret=-0.000431, glen=92.3, tlen=253, kl=0.0225, act_lr=6.4e-7, ent=1.83]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.14it/s, pg=0.0665, ret=-0.000431, glen=92.3, tlen=253, kl=0.0225, act_lr=6.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:35<00:14,  1.14it/s, pg=-0.0729, ret=-5.02e-5, glen=99.3, tlen=260, kl=0.0173, act_lr=6.4e-7, ent=1.88]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.15it/s, pg=-0.0729, ret=-5.02e-5, glen=99.3, tlen=260, kl=0.0173, act_lr=6.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.15it/s, pg=-0.172, ret=0.000422, glen=95.5, tlen=256, kl=0.0219, act_lr=6.4e-7, ent=1.87] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.16it/s, pg=-0.172, ret=0.000422, glen=95.5, tlen=256, kl=0.0219, act_lr=6.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=0.0269, ret=-0.000814, glen=126, tlen=287, kl=0.0194, act_lr=6.4e-7, ent=1.92]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.16it/s, pg=0.0269, ret=-0.000814, glen=126, tlen=287, kl=0.0194, act_lr=6.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.16it/s, pg=-0.169, ret=0.000599, glen=97.9, tlen=259, kl=0.0171, act_lr=6.4e-7, ent=1.94]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.16it/s, pg=-0.169, ret=0.000599, glen=97.9, tlen=259, kl=0.0171, act_lr=6.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.16it/s, pg=-0.196, ret=0.000823, glen=100, tlen=261, kl=0.0189, act_lr=6.4e-7, ent=1.83] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.16it/s, pg=-0.196, ret=0.000823, glen=100, tlen=261, kl=0.0189, act_lr=6.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.16it/s, pg=-0.169, ret=0.000171, glen=103, tlen=264, kl=0.0178, act_lr=6.4e-7, ent=1.73]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.169, ret=0.000171, glen=103, tlen=264, kl=0.0178, act_lr=6.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=-0.0952, ret=-0.00128, glen=148, tlen=309, kl=0.0123, act_lr=6.4e-7, ent=2.32]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.16it/s, pg=-0.0952, ret=-0.00128, glen=148, tlen=309, kl=0.0123, act_lr=6.4e-7, ent=2.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:41<00:08,  1.16it/s, pg=-0.182, ret=0.000419, glen=113, tlen=273, kl=0.018, act_lr=6.4e-7, ent=1.87]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.16it/s, pg=-0.182, ret=0.000419, glen=113, tlen=273, kl=0.018, act_lr=6.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.16it/s, pg=-0.0745, ret=-0.000347, glen=96.9, tlen=257, kl=0.0156, act_lr=6.4e-7, ent=1.8]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=-0.0745, ret=-0.000347, glen=96.9, tlen=257, kl=0.0156, act_lr=6.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.187, ret=0.000468, glen=105, tlen=265, kl=0.0157, act_lr=6.4e-7, ent=1.88]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:06,  1.17it/s, pg=-0.187, ret=0.000468, glen=105, tlen=265, kl=0.0157, act_lr=6.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:06,  1.17it/s, pg=-0.127, ret=0.000666, glen=91.5, tlen=252, kl=0.0182, act_lr=6.4e-7, ent=1.8]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.127, ret=0.000666, glen=91.5, tlen=252, kl=0.0182, act_lr=6.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.121, ret=-0.000439, glen=88.7, tlen=249, kl=0.0281, act_lr=6.4e-7, ent=1.79]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.121, ret=-0.000439, glen=88.7, tlen=249, kl=0.0281, act_lr=6.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=0.341, ret=0.000361, glen=180, tlen=341, kl=0.0152, act_lr=6.4e-7, ent=1.79]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.341, ret=0.000361, glen=180, tlen=341, kl=0.0152, act_lr=6.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.17it/s, pg=-0.123, ret=0.000639, glen=103, tlen=264, kl=0.0147, act_lr=6.4e-7, ent=1.94]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.123, ret=0.000639, glen=103, tlen=264, kl=0.0147, act_lr=6.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:47<00:02,  1.17it/s, pg=-0.125, ret=0.00103, glen=105, tlen=266, kl=0.0206, act_lr=6.4e-7, ent=1.96] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.125, ret=0.00103, glen=105, tlen=266, kl=0.0206, act_lr=6.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0126, ret=-0.000873, glen=97.3, tlen=258, kl=0.0159, act_lr=6.4e-7, ent=1.74]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.0126, ret=-0.000873, glen=97.3, tlen=258, kl=0.0159, act_lr=6.4e-7, ent=1.74]
2025-07-24 18:08:07.362 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.98s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.192, ret=-0.000505, glen=134, tlen=295, kl=0.0138, act_lr=6.6e-7, ent=2.43]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.12it/s, pg=0.192, ret=-0.000505, glen=134, tlen=295, kl=0.0138, act_lr=6.6e-7, ent=2.43]
2025-07-24 18:08:08.204 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 18:08:10.793 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 18:08:11.127 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.87s
2025-07-24 18:08:11.148 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.037187116486685615, 'actor_lr': 6.403571392508769e-07, 'clip_ratio': 0.0, 'entropy': 1.8933601783854621, 'kl': 0.023435047694614956, 'response_length': 114.71611336299351, 'total_length': 275.47575759887695, 'teacher_total_length': 287.7519220624651, 'return': -0.0002212322521830044, 'policy_update_steps': 1.0}
Episode [3/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [27:51<24:31, 245.24s/it]2025-07-24 18:08:11.197 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:09:56.711 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:09:56.893 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:09:56.893 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 105.70s
2025-07-24 18:09:58.890 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0140,avg_reflection_pattern_score: 0.0131,avg_pass_at_n: 1.0000,avg_num_tokens: 103.0260,std_num_tokens: 119.4948,avg_correct_num_tokens: 100.4555,std_correct_num_tokens: 99.9034,avg_incorrect_num_tokens: 122.8332,std_incorrect_num_tokens: 216.6958
2025-07-24 18:09:59.346 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.45s
2025-07-24 18:10:02.516 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.17s
2025-07-24 18:10:30.525 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 219
2025-07-24 18:10:30.526 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.01s
2025-07-24 18:10:31.902 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.94s
2025-07-24 18:10:31.902 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0005102014595953127, avg_kl: 0.022585934155607876, avg_response_length: 105.07573341125767, avg_orm_score: 0.0, avg_custom_rewards: -0.0005102014595953127
2025-07-24 18:10:31.936 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter33_replay_buffer.jsonl
2025-07-24 18:10:33.743 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.81s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:01<?, ?it/s, pg=-0.00677, ret=-0.000345, glen=97.7, tlen=258, kl=0.0192, act_lr=6.6e-7, ent=1.7]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:54,  1.01s/it, pg=-0.00677, ret=-0.000345, glen=97.7, tlen=258, kl=0.0192, act_lr=6.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:54,  1.01s/it, pg=-0.0603, ret=0.000372, glen=97.2, tlen=258, kl=0.0164, act_lr=6.6e-7, ent=1.7]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:49,  1.08it/s, pg=-0.0603, ret=0.000372, glen=97.2, tlen=258, kl=0.0164, act_lr=6.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:49,  1.08it/s, pg=0.311, ret=-0.000145, glen=147, tlen=308, kl=0.0159, act_lr=6.6e-7, ent=1.54] Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:47,  1.09it/s, pg=0.311, ret=-0.000145, glen=147, tlen=308, kl=0.0159, act_lr=6.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:47,  1.09it/s, pg=-0.0764, ret=-0.00108, glen=103, tlen=264, kl=0.0235, act_lr=6.6e-7, ent=1.93]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.12it/s, pg=-0.0764, ret=-0.00108, glen=103, tlen=264, kl=0.0235, act_lr=6.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.12it/s, pg=-0.0546, ret=-1.52e-5, glen=88.1, tlen=249, kl=0.0254, act_lr=6.6e-7, ent=1.78]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:43,  1.14it/s, pg=-0.0546, ret=-1.52e-5, glen=88.1, tlen=249, kl=0.0254, act_lr=6.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:43,  1.14it/s, pg=-0.109, ret=0.000479, glen=96.6, tlen=257, kl=0.0184, act_lr=6.6e-7, ent=1.71] Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.13it/s, pg=-0.109, ret=0.000479, glen=96.6, tlen=257, kl=0.0184, act_lr=6.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.13it/s, pg=0.111, ret=-0.000928, glen=111, tlen=272, kl=0.0175, act_lr=6.6e-7, ent=1.69] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.14it/s, pg=0.111, ret=-0.000928, glen=111, tlen=272, kl=0.0175, act_lr=6.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.14it/s, pg=0.156, ret=-0.000856, glen=100, tlen=261, kl=0.0163, act_lr=6.6e-7, ent=1.79]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.15it/s, pg=0.156, ret=-0.000856, glen=100, tlen=261, kl=0.0163, act_lr=6.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.15it/s, pg=-0.104, ret=6.52e-5, glen=90.5, tlen=251, kl=0.052, act_lr=6.6e-7, ent=1.81] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:39,  1.16it/s, pg=-0.104, ret=6.52e-5, glen=90.5, tlen=251, kl=0.052, act_lr=6.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.16it/s, pg=-0.183, ret=0.00104, glen=108, tlen=269, kl=0.0157, act_lr=6.6e-7, ent=1.86]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:38,  1.16it/s, pg=-0.183, ret=0.00104, glen=108, tlen=269, kl=0.0157, act_lr=6.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:38,  1.16it/s, pg=0.175, ret=-4.37e-6, glen=107, tlen=267, kl=0.0215, act_lr=6.6e-7, ent=2.13]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:37,  1.16it/s, pg=0.175, ret=-4.37e-6, glen=107, tlen=267, kl=0.0215, act_lr=6.6e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:37,  1.16it/s, pg=0.226, ret=3.71e-5, glen=116, tlen=277, kl=0.015, act_lr=6.6e-7, ent=1.96]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:36,  1.16it/s, pg=0.226, ret=3.71e-5, glen=116, tlen=277, kl=0.015, act_lr=6.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:36,  1.16it/s, pg=-0.157, ret=0.000202, glen=112, tlen=272, kl=0.0152, act_lr=6.6e-7, ent=1.85]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:35,  1.17it/s, pg=-0.157, ret=0.000202, glen=112, tlen=272, kl=0.0152, act_lr=6.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:35,  1.17it/s, pg=0.137, ret=-0.000299, glen=113, tlen=274, kl=0.018, act_lr=6.6e-7, ent=2.06] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.17it/s, pg=0.137, ret=-0.000299, glen=113, tlen=274, kl=0.018, act_lr=6.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.17it/s, pg=-0.219, ret=0.00133, glen=95.2, tlen=255, kl=0.0226, act_lr=6.6e-7, ent=1.77]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.219, ret=0.00133, glen=95.2, tlen=255, kl=0.0226, act_lr=6.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=0.0295, ret=0.000698, glen=102, tlen=262, kl=0.0185, act_lr=6.6e-7, ent=1.7] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=0.0295, ret=0.000698, glen=102, tlen=262, kl=0.0185, act_lr=6.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.221, ret=0.0011, glen=97.8, tlen=259, kl=0.0175, act_lr=6.6e-7, ent=1.91]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=-0.221, ret=0.0011, glen=97.8, tlen=259, kl=0.0175, act_lr=6.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=-0.0367, ret=2.3e-5, glen=101, tlen=262, kl=0.0254, act_lr=6.6e-7, ent=1.95]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.0367, ret=2.3e-5, glen=101, tlen=262, kl=0.0254, act_lr=6.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=-0.142, ret=0.000628, glen=113, tlen=274, kl=0.0237, act_lr=6.6e-7, ent=1.82]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.142, ret=0.000628, glen=113, tlen=274, kl=0.0237, act_lr=6.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=-0.0605, ret=-0.000105, glen=102, tlen=263, kl=0.0235, act_lr=6.6e-7, ent=1.89]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=-0.0605, ret=-0.000105, glen=102, tlen=263, kl=0.0235, act_lr=6.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=0.0945, ret=-0.00169, glen=88.8, tlen=249, kl=0.0244, act_lr=6.6e-7, ent=1.78] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.17it/s, pg=0.0945, ret=-0.00169, glen=88.8, tlen=249, kl=0.0244, act_lr=6.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.17it/s, pg=-0.113, ret=0.000768, glen=104, tlen=264, kl=0.0187, act_lr=6.6e-7, ent=1.91] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.113, ret=0.000768, glen=104, tlen=264, kl=0.0187, act_lr=6.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.0547, ret=0.000876, glen=99.7, tlen=260, kl=0.0292, act_lr=6.6e-7, ent=1.77]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:19<00:27,  1.17it/s, pg=-0.0547, ret=0.000876, glen=99.7, tlen=260, kl=0.0292, act_lr=6.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.114, ret=0.000608, glen=99.8, tlen=261, kl=0.0219, act_lr=6.6e-7, ent=1.87] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.114, ret=0.000608, glen=99.8, tlen=261, kl=0.0219, act_lr=6.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=-0.01, ret=4.25e-5, glen=100, tlen=261, kl=0.0236, act_lr=6.6e-7, ent=1.76]   Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=-0.01, ret=4.25e-5, glen=100, tlen=261, kl=0.0236, act_lr=6.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.102, ret=0.000244, glen=90.3, tlen=251, kl=0.0255, act_lr=6.6e-7, ent=1.62]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.102, ret=0.000244, glen=90.3, tlen=251, kl=0.0255, act_lr=6.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.0815, ret=0.000197, glen=99.3, tlen=260, kl=0.0193, act_lr=6.6e-7, ent=1.71]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:23,  1.17it/s, pg=-0.0815, ret=0.000197, glen=99.3, tlen=260, kl=0.0193, act_lr=6.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:23,  1.17it/s, pg=0.144, ret=-0.000228, glen=101, tlen=262, kl=0.0217, act_lr=6.6e-7, ent=1.8]   Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.17it/s, pg=0.144, ret=-0.000228, glen=101, tlen=262, kl=0.0217, act_lr=6.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.17it/s, pg=0.0331, ret=-2.1e-5, glen=118, tlen=279, kl=0.0176, act_lr=6.6e-7, ent=1.96]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.07it/s, pg=0.0331, ret=-2.1e-5, glen=118, tlen=279, kl=0.0176, act_lr=6.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.07it/s, pg=-0.175, ret=0.000995, glen=89.2, tlen=250, kl=0.0236, act_lr=6.6e-7, ent=1.73]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.10it/s, pg=-0.175, ret=0.000995, glen=89.2, tlen=250, kl=0.0236, act_lr=6.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.10it/s, pg=-0.0582, ret=1.02e-5, glen=109, tlen=270, kl=0.0416, act_lr=6.6e-7, ent=1.85] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.0582, ret=1.02e-5, glen=109, tlen=270, kl=0.0416, act_lr=6.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.202, ret=0.00112, glen=101, tlen=262, kl=0.0239, act_lr=6.6e-7, ent=1.81] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:20,  1.13it/s, pg=-0.202, ret=0.00112, glen=101, tlen=262, kl=0.0239, act_lr=6.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.13it/s, pg=-0.112, ret=0.000462, glen=101, tlen=262, kl=0.0161, act_lr=6.6e-7, ent=1.66]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.14it/s, pg=-0.112, ret=0.000462, glen=101, tlen=262, kl=0.0161, act_lr=6.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.14it/s, pg=-0.0578, ret=6.44e-5, glen=121, tlen=282, kl=0.0441, act_lr=6.6e-7, ent=2.11]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.15it/s, pg=-0.0578, ret=6.44e-5, glen=121, tlen=282, kl=0.0441, act_lr=6.6e-7, ent=2.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.15it/s, pg=-0.0687, ret=-0.000414, glen=105, tlen=266, kl=0.0274, act_lr=6.6e-7, ent=1.76]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.0687, ret=-0.000414, glen=105, tlen=266, kl=0.0274, act_lr=6.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=0.107, ret=-0.000842, glen=100, tlen=261, kl=0.0209, act_lr=6.6e-7, ent=1.64]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.16it/s, pg=0.107, ret=-0.000842, glen=100, tlen=261, kl=0.0209, act_lr=6.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.16it/s, pg=0.107, ret=-0.00049, glen=102, tlen=263, kl=0.0182, act_lr=6.6e-7, ent=1.79] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=0.107, ret=-0.00049, glen=102, tlen=263, kl=0.0182, act_lr=6.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=0.115, ret=-0.0014, glen=97.3, tlen=258, kl=0.0186, act_lr=6.6e-7, ent=1.71]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:32<00:14,  1.17it/s, pg=0.115, ret=-0.0014, glen=97.3, tlen=258, kl=0.0186, act_lr=6.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.0474, ret=0.000468, glen=112, tlen=272, kl=0.0193, act_lr=6.6e-7, ent=2.12]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.17it/s, pg=0.0474, ret=0.000468, glen=112, tlen=272, kl=0.0193, act_lr=6.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0952, ret=-0.000236, glen=112, tlen=273, kl=0.0161, act_lr=6.6e-7, ent=1.85]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.0952, ret=-0.000236, glen=112, tlen=273, kl=0.0161, act_lr=6.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=0.0293, ret=-0.000497, glen=90.8, tlen=251, kl=0.0304, act_lr=6.6e-7, ent=1.69]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=0.0293, ret=-0.000497, glen=90.8, tlen=251, kl=0.0304, act_lr=6.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=-0.112, ret=-0.000182, glen=100, tlen=261, kl=0.0171, act_lr=6.6e-7, ent=1.97] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=-0.112, ret=-0.000182, glen=100, tlen=261, kl=0.0171, act_lr=6.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=-0.0193, ret=0.000203, glen=115, tlen=276, kl=0.018, act_lr=6.6e-7, ent=2.31] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.17it/s, pg=-0.0193, ret=0.000203, glen=115, tlen=276, kl=0.018, act_lr=6.6e-7, ent=2.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.17it/s, pg=0.00336, ret=6.1e-5, glen=98.7, tlen=260, kl=0.026, act_lr=6.6e-7, ent=1.87] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=0.00336, ret=6.1e-5, glen=98.7, tlen=260, kl=0.026, act_lr=6.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=-0.107, ret=-0.000118, glen=96.4, tlen=258, kl=0.017, act_lr=6.6e-7, ent=1.74]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:38<00:08,  1.17it/s, pg=-0.107, ret=-0.000118, glen=96.4, tlen=258, kl=0.017, act_lr=6.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.124, ret=0.000138, glen=121, tlen=282, kl=0.0223, act_lr=6.6e-7, ent=2.2]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.17it/s, pg=0.124, ret=0.000138, glen=121, tlen=282, kl=0.0223, act_lr=6.6e-7, ent=2.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=0.0291, ret=-0.000756, glen=100, tlen=261, kl=0.0217, act_lr=6.6e-7, ent=1.73]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=0.0291, ret=-0.000756, glen=100, tlen=261, kl=0.0217, act_lr=6.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=-0.0243, ret=-0.000628, glen=98, tlen=259, kl=0.0212, act_lr=6.6e-7, ent=1.77]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=-0.0243, ret=-0.000628, glen=98, tlen=259, kl=0.0212, act_lr=6.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=-0.0886, ret=0.000359, glen=101, tlen=262, kl=0.0168, act_lr=6.6e-7, ent=1.82]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=-0.0886, ret=0.000359, glen=101, tlen=262, kl=0.0168, act_lr=6.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=-0.0182, ret=7.32e-6, glen=98.4, tlen=259, kl=0.0204, act_lr=6.6e-7, ent=1.74]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=-0.0182, ret=7.32e-6, glen=98.4, tlen=259, kl=0.0204, act_lr=6.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=0.0966, ret=-0.00273, glen=170, tlen=330, kl=0.0151, act_lr=6.6e-7, ent=2.45] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.16it/s, pg=0.0966, ret=-0.00273, glen=170, tlen=330, kl=0.0151, act_lr=6.6e-7, ent=2.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.16it/s, pg=0.00862, ret=-0.000406, glen=95.5, tlen=256, kl=0.0597, act_lr=6.6e-7, ent=1.66]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:44<00:02,  1.16it/s, pg=0.00862, ret=-0.000406, glen=95.5, tlen=256, kl=0.0597, act_lr=6.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.16it/s, pg=0.155, ret=0.000364, glen=130, tlen=291, kl=0.0175, act_lr=6.6e-7, ent=2.29]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.16it/s, pg=0.155, ret=0.000364, glen=130, tlen=291, kl=0.0175, act_lr=6.6e-7, ent=2.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.16it/s, pg=-0.00711, ret=-0.000166, glen=107, tlen=267, kl=0.0257, act_lr=6.6e-7, ent=1.87]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.16it/s, pg=-0.00711, ret=-0.000166, glen=107, tlen=267, kl=0.0257, act_lr=6.6e-7, ent=1.87]
2025-07-24 18:11:21.657 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.68s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.16it/s, pg=0.0897, ret=-0.000631, glen=106, tlen=267, kl=0.0205, act_lr=6.8e-7, ent=1.67]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=0.0897, ret=-0.000631, glen=106, tlen=267, kl=0.0205, act_lr=6.8e-7, ent=1.67]
2025-07-24 18:11:22.336 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 18:11:24.599 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.26s
2025-07-24 18:11:24.928 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 51.07s
2025-07-24 18:11:24.936 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.013164554400877519, 'actor_lr': 6.603636486281837e-07, 'clip_ratio': 0.0, 'entropy': 1.8464384078979492, 'kl': 0.022589804909446023, 'response_length': 105.03203222101385, 'total_length': 265.8030997536399, 'teacher_total_length': 277.8591480601918, 'return': -4.10461378454453e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [31:04<19:04, 228.86s/it]2025-07-24 18:11:24.981 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:13:55.046 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:13:55.238 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 18:13:55.239 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 150.26s
2025-07-24 18:13:57.441 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0142,avg_reflection_pattern_score: 0.0109,avg_pass_at_n: 1.0000,avg_num_tokens: 106.3116,std_num_tokens: 182.5449,avg_correct_num_tokens: 100.0345,std_correct_num_tokens: 88.9346,avg_incorrect_num_tokens: 151.0993,std_incorrect_num_tokens: 460.8261
2025-07-24 18:13:57.884 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.64s
2025-07-24 18:14:00.848 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.96s
2025-07-24 18:14:29.281 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 222
2025-07-24 18:14:29.282 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.43s
2025-07-24 18:14:30.670 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.98s
2025-07-24 18:14:30.671 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0011115364636229519, avg_kl: 0.021955747862119932, avg_response_length: 113.83006977390599, avg_orm_score: 0.0, avg_custom_rewards: -0.0011115364636229519
2025-07-24 18:14:30.723 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter34_replay_buffer.jsonl
2025-07-24 18:14:32.571 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.319, ret=-0.00314, glen=344, tlen=504, kl=0.0198, act_lr=6.8e-7, ent=2.85]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<01:00,  1.10s/it, pg=0.319, ret=-0.00314, glen=344, tlen=504, kl=0.0198, act_lr=6.8e-7, ent=2.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<01:00,  1.10s/it, pg=-0.071, ret=-0.000463, glen=96.5, tlen=257, kl=0.0217, act_lr=6.8e-7, ent=1.7]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:51,  1.04it/s, pg=-0.071, ret=-0.000463, glen=96.5, tlen=257, kl=0.0217, act_lr=6.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:51,  1.04it/s, pg=-0.112, ret=0.000365, glen=97.4, tlen=257, kl=0.0225, act_lr=6.8e-7, ent=1.64]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:49,  1.07it/s, pg=-0.112, ret=0.000365, glen=97.4, tlen=257, kl=0.0225, act_lr=6.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:49,  1.07it/s, pg=0.0453, ret=0.000206, glen=103, tlen=263, kl=0.0172, act_lr=6.8e-7, ent=1.76] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:48,  1.07it/s, pg=0.0453, ret=0.000206, glen=103, tlen=263, kl=0.0172, act_lr=6.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:48,  1.07it/s, pg=0.331, ret=6.09e-5, glen=204, tlen=365, kl=0.0156, act_lr=6.8e-7, ent=2.78]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:47,  1.08it/s, pg=0.331, ret=6.09e-5, glen=204, tlen=365, kl=0.0156, act_lr=6.8e-7, ent=2.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:47,  1.08it/s, pg=0.0629, ret=-0.00176, glen=109, tlen=269, kl=0.0254, act_lr=6.8e-7, ent=1.94]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:45,  1.11it/s, pg=0.0629, ret=-0.00176, glen=109, tlen=269, kl=0.0254, act_lr=6.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:45,  1.11it/s, pg=-0.0679, ret=0.001, glen=103, tlen=263, kl=0.0189, act_lr=6.8e-7, ent=1.99]  Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.13it/s, pg=-0.0679, ret=0.001, glen=103, tlen=263, kl=0.0189, act_lr=6.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.13it/s, pg=-0.062, ret=-0.000221, glen=99.3, tlen=260, kl=0.018, act_lr=6.8e-7, ent=1.9]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:43,  1.11it/s, pg=-0.062, ret=-0.000221, glen=99.3, tlen=260, kl=0.018, act_lr=6.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:43,  1.11it/s, pg=-0.289, ret=0.000841, glen=110, tlen=270, kl=0.0203, act_lr=6.8e-7, ent=1.84]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.13it/s, pg=-0.289, ret=0.000841, glen=110, tlen=270, kl=0.0203, act_lr=6.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:09<00:41,  1.13it/s, pg=0.00188, ret=-0.00051, glen=109, tlen=269, kl=0.0198, act_lr=6.8e-7, ent=1.8]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.14it/s, pg=0.00188, ret=-0.00051, glen=109, tlen=269, kl=0.0198, act_lr=6.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.14it/s, pg=0.157, ret=-0.00102, glen=107, tlen=267, kl=0.026, act_lr=6.8e-7, ent=1.89]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.15it/s, pg=0.157, ret=-0.00102, glen=107, tlen=267, kl=0.026, act_lr=6.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.15it/s, pg=-0.127, ret=-0.000618, glen=100, tlen=260, kl=0.0176, act_lr=6.8e-7, ent=1.67]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.15it/s, pg=-0.127, ret=-0.000618, glen=100, tlen=260, kl=0.0176, act_lr=6.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.15it/s, pg=0.0188, ret=-0.000747, glen=109, tlen=269, kl=0.0212, act_lr=6.8e-7, ent=1.88]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.14it/s, pg=0.0188, ret=-0.000747, glen=109, tlen=269, kl=0.0212, act_lr=6.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.14it/s, pg=-0.0673, ret=-0.000497, glen=108, tlen=268, kl=0.026, act_lr=6.8e-7, ent=1.94]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.14it/s, pg=-0.0673, ret=-0.000497, glen=108, tlen=268, kl=0.026, act_lr=6.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.14it/s, pg=-0.111, ret=-0.000144, glen=102, tlen=263, kl=0.0196, act_lr=6.8e-7, ent=1.78]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.15it/s, pg=-0.111, ret=-0.000144, glen=102, tlen=263, kl=0.0196, act_lr=6.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.15it/s, pg=0.366, ret=-0.00123, glen=207, tlen=368, kl=0.016, act_lr=6.8e-7, ent=2.64]   Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:35,  1.14it/s, pg=0.366, ret=-0.00123, glen=207, tlen=368, kl=0.016, act_lr=6.8e-7, ent=2.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:15<00:35,  1.14it/s, pg=-0.0839, ret=-0.000226, glen=105, tlen=265, kl=0.0286, act_lr=6.8e-7, ent=1.71]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.15it/s, pg=-0.0839, ret=-0.000226, glen=105, tlen=265, kl=0.0286, act_lr=6.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:16<00:33,  1.15it/s, pg=-0.0276, ret=0.00051, glen=109, tlen=269, kl=0.0208, act_lr=6.8e-7, ent=1.92]  Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.15it/s, pg=-0.0276, ret=0.00051, glen=109, tlen=269, kl=0.0208, act_lr=6.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.15it/s, pg=0.203, ret=-0.000513, glen=146, tlen=306, kl=0.0257, act_lr=6.8e-7, ent=2.58]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:32,  1.15it/s, pg=0.203, ret=-0.000513, glen=146, tlen=306, kl=0.0257, act_lr=6.8e-7, ent=2.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:32,  1.15it/s, pg=-0.0332, ret=-0.00054, glen=98.9, tlen=259, kl=0.0239, act_lr=6.8e-7, ent=1.78]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.16it/s, pg=-0.0332, ret=-0.00054, glen=98.9, tlen=259, kl=0.0239, act_lr=6.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.16it/s, pg=-0.139, ret=-0.000829, glen=100, tlen=260, kl=0.0253, act_lr=6.8e-7, ent=1.95] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=-0.139, ret=-0.000829, glen=100, tlen=260, kl=0.0253, act_lr=6.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=-0.19, ret=0.000417, glen=91.7, tlen=251, kl=0.0202, act_lr=6.8e-7, ent=1.72] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.19, ret=0.000417, glen=91.7, tlen=251, kl=0.0202, act_lr=6.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.17it/s, pg=-0.242, ret=0.000748, glen=117, tlen=277, kl=0.0197, act_lr=6.8e-7, ent=2.36]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.242, ret=0.000748, glen=117, tlen=277, kl=0.0197, act_lr=6.8e-7, ent=2.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:21<00:28,  1.16it/s, pg=-0.285, ret=0.00161, glen=96.8, tlen=257, kl=0.0227, act_lr=6.8e-7, ent=1.82]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.285, ret=0.00161, glen=96.8, tlen=257, kl=0.0227, act_lr=6.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:22<00:27,  1.17it/s, pg=-0.0608, ret=-0.000501, glen=96.6, tlen=257, kl=0.0209, act_lr=6.8e-7, ent=1.71]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.0608, ret=-0.000501, glen=96.6, tlen=257, kl=0.0209, act_lr=6.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.219, ret=0.000825, glen=102, tlen=262, kl=0.0196, act_lr=6.8e-7, ent=1.76]   Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.219, ret=0.000825, glen=102, tlen=262, kl=0.0196, act_lr=6.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.199, ret=-5.71e-7, glen=95, tlen=255, kl=0.0219, act_lr=6.8e-7, ent=1.83] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.199, ret=-5.71e-7, glen=95, tlen=255, kl=0.0219, act_lr=6.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0522, ret=-0.000171, glen=96.8, tlen=257, kl=0.0192, act_lr=6.8e-7, ent=1.79]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:24,  1.16it/s, pg=-0.0522, ret=-0.000171, glen=96.8, tlen=257, kl=0.0192, act_lr=6.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:24,  1.16it/s, pg=-0.21, ret=0.000383, glen=113, tlen=273, kl=0.019, act_lr=6.8e-7, ent=2.05]     Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.06it/s, pg=-0.21, ret=0.000383, glen=113, tlen=273, kl=0.019, act_lr=6.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.06it/s, pg=-0.148, ret=-0.000182, glen=100, tlen=260, kl=0.019, act_lr=6.8e-7, ent=1.87]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.09it/s, pg=-0.148, ret=-0.000182, glen=100, tlen=260, kl=0.019, act_lr=6.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.09it/s, pg=-0.188, ret=-1.04e-5, glen=98, tlen=258, kl=0.0276, act_lr=6.8e-7, ent=1.78] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.188, ret=-1.04e-5, glen=98, tlen=258, kl=0.0276, act_lr=6.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:22,  1.12it/s, pg=-0.159, ret=0.000632, glen=102, tlen=263, kl=0.0228, act_lr=6.8e-7, ent=1.71]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.159, ret=0.000632, glen=102, tlen=263, kl=0.0228, act_lr=6.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:29<00:21,  1.13it/s, pg=0.106, ret=-0.00063, glen=112, tlen=272, kl=0.034, act_lr=6.8e-7, ent=1.95]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.14it/s, pg=0.106, ret=-0.00063, glen=112, tlen=272, kl=0.034, act_lr=6.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.14it/s, pg=-0.213, ret=0.000673, glen=102, tlen=262, kl=0.0186, act_lr=6.8e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=-0.213, ret=0.000673, glen=102, tlen=262, kl=0.0186, act_lr=6.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=-0.1, ret=-0.000259, glen=115, tlen=275, kl=0.0199, act_lr=6.8e-7, ent=1.86] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.1, ret=-0.000259, glen=115, tlen=275, kl=0.0199, act_lr=6.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.331, ret=-0.00196, glen=146, tlen=306, kl=0.0162, act_lr=6.8e-7, ent=2.44]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=0.331, ret=-0.00196, glen=146, tlen=306, kl=0.0162, act_lr=6.8e-7, ent=2.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=0.00928, ret=-0.00145, glen=101, tlen=261, kl=0.0208, act_lr=6.8e-7, ent=1.85]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=0.00928, ret=-0.00145, glen=101, tlen=261, kl=0.0208, act_lr=6.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=0.172, ret=-0.00126, glen=123, tlen=283, kl=0.0207, act_lr=6.8e-7, ent=1.9]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.172, ret=-0.00126, glen=123, tlen=283, kl=0.0207, act_lr=6.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.16it/s, pg=0.178, ret=0.000246, glen=131, tlen=292, kl=0.0168, act_lr=6.8e-7, ent=2.05]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=0.178, ret=0.000246, glen=131, tlen=292, kl=0.0168, act_lr=6.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:35<00:14,  1.16it/s, pg=0.146, ret=0.000499, glen=119, tlen=279, kl=0.0224, act_lr=6.8e-7, ent=2.09]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.16it/s, pg=0.146, ret=0.000499, glen=119, tlen=279, kl=0.0224, act_lr=6.8e-7, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:36<00:13,  1.16it/s, pg=0.394, ret=-0.000647, glen=122, tlen=282, kl=0.0203, act_lr=6.8e-7, ent=2.06]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=0.394, ret=-0.000647, glen=122, tlen=282, kl=0.0203, act_lr=6.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=-0.111, ret=0.000759, glen=101, tlen=261, kl=0.0186, act_lr=6.8e-7, ent=1.8] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.16it/s, pg=-0.111, ret=0.000759, glen=101, tlen=261, kl=0.0186, act_lr=6.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.16it/s, pg=-0.129, ret=0.000112, glen=102, tlen=263, kl=0.0182, act_lr=6.8e-7, ent=1.87]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.129, ret=0.000112, glen=102, tlen=263, kl=0.0182, act_lr=6.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.133, ret=0.000123, glen=100, tlen=261, kl=0.0169, act_lr=6.8e-7, ent=1.71]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.133, ret=0.000123, glen=100, tlen=261, kl=0.0169, act_lr=6.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.105, ret=-0.000434, glen=105, tlen=265, kl=0.0199, act_lr=6.8e-7, ent=1.79]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.105, ret=-0.000434, glen=105, tlen=265, kl=0.0199, act_lr=6.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=-0.182, ret=-0.000526, glen=92.3, tlen=252, kl=0.0565, act_lr=6.8e-7, ent=1.69]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.182, ret=-0.000526, glen=92.3, tlen=252, kl=0.0565, act_lr=6.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:41<00:08,  1.17it/s, pg=-0.117, ret=2.53e-5, glen=110, tlen=270, kl=0.0172, act_lr=6.8e-7, ent=1.9]    Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.117, ret=2.53e-5, glen=110, tlen=270, kl=0.0172, act_lr=6.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.14, ret=-0.000759, glen=109, tlen=269, kl=0.0273, act_lr=6.8e-7, ent=1.79]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=-0.14, ret=-0.000759, glen=109, tlen=269, kl=0.0273, act_lr=6.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.294, ret=0.00108, glen=86.6, tlen=246, kl=0.0201, act_lr=6.8e-7, ent=1.58]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.294, ret=0.00108, glen=86.6, tlen=246, kl=0.0201, act_lr=6.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=-0.12, ret=-3.55e-5, glen=95.1, tlen=255, kl=0.0253, act_lr=6.8e-7, ent=1.72]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.18it/s, pg=-0.12, ret=-3.55e-5, glen=95.1, tlen=255, kl=0.0253, act_lr=6.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.18it/s, pg=-0.0847, ret=0.000809, glen=102, tlen=262, kl=0.0175, act_lr=6.8e-7, ent=2]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.0847, ret=0.000809, glen=102, tlen=262, kl=0.0175, act_lr=6.8e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.18it/s, pg=-0.0846, ret=9.92e-5, glen=107, tlen=267, kl=0.0327, act_lr=6.8e-7, ent=1.93]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.18it/s, pg=-0.0846, ret=9.92e-5, glen=107, tlen=267, kl=0.0327, act_lr=6.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.18it/s, pg=0.0385, ret=0.000309, glen=111, tlen=271, kl=0.0229, act_lr=6.8e-7, ent=2.03]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.18it/s, pg=0.0385, ret=0.000309, glen=111, tlen=271, kl=0.0229, act_lr=6.8e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:47<00:02,  1.18it/s, pg=-0.219, ret=0.00041, glen=95.2, tlen=255, kl=0.0216, act_lr=6.8e-7, ent=1.59]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.18it/s, pg=-0.219, ret=0.00041, glen=95.2, tlen=255, kl=0.0216, act_lr=6.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.18it/s, pg=-0.232, ret=0.000853, glen=100, tlen=260, kl=0.0201, act_lr=6.8e-7, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=-0.232, ret=0.000853, glen=100, tlen=260, kl=0.0201, act_lr=6.8e-7, ent=1.69]
2025-07-24 18:15:21.651 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.89s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=-0.132, ret=0.000439, glen=100, tlen=260, kl=0.0215, act_lr=7e-7, ent=1.72]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=-0.132, ret=0.000439, glen=100, tlen=260, kl=0.0215, act_lr=7e-7, ent=1.72]
2025-07-24 18:15:22.344 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 18:15:24.498 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.15s
2025-07-24 18:15:24.833 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.19s
2025-07-24 18:15:24.843 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.04752473320279803, 'actor_lr': 6.8035716996714e-07, 'clip_ratio': 0.0, 'entropy': 1.9125815374510629, 'kl': 0.021931920732770647, 'response_length': 113.70866203308105, 'total_length': 273.7652827671596, 'teacher_total_length': 286.57485907418385, 'return': -0.00012950540427222483, 'policy_update_steps': 1.0}
Episode [3/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [35:04<15:29, 232.31s/it]2025-07-24 18:15:24.894 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:17:15.401 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:17:15.583 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:17:15.584 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 110.69s
2025-07-24 18:17:17.892 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0139,avg_reflection_pattern_score: 0.0098,avg_pass_at_n: 1.0000,avg_num_tokens: 103.2671,std_num_tokens: 122.8617,avg_correct_num_tokens: 100.9076,std_correct_num_tokens: 97.2127,avg_incorrect_num_tokens: 122.1251,std_incorrect_num_tokens: 244.5588
2025-07-24 18:17:18.200 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.62s
2025-07-24 18:17:21.182 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.98s
2025-07-24 18:17:49.093 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 219
2025-07-24 18:17:49.094 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.91s
2025-07-24 18:17:50.419 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.94s
2025-07-24 18:17:50.419 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00012231706514036003, avg_kl: 0.03166526202197489, avg_response_length: 105.61130704836214, avg_orm_score: 0.0, avg_custom_rewards: 0.00012231706514036003
2025-07-24 18:17:50.461 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter35_replay_buffer.jsonl
2025-07-24 18:17:52.275 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.82s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:01<?, ?it/s, pg=-0.0488, ret=-0.000529, glen=108, tlen=268, kl=0.018, act_lr=7e-7, ent=1.88]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:54,  1.02s/it, pg=-0.0488, ret=-0.000529, glen=108, tlen=268, kl=0.018, act_lr=7e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:54,  1.02s/it, pg=0.0859, ret=-0.0015, glen=93.1, tlen=253, kl=0.0201, act_lr=7e-7, ent=1.76] Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.08it/s, pg=0.0859, ret=-0.0015, glen=93.1, tlen=253, kl=0.0201, act_lr=7e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.08it/s, pg=-0.198, ret=0.000547, glen=107, tlen=266, kl=0.0183, act_lr=7e-7, ent=1.97]Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:46,  1.12it/s, pg=-0.198, ret=0.000547, glen=107, tlen=266, kl=0.0183, act_lr=7e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:46,  1.12it/s, pg=-0.198, ret=0.000672, glen=91.9, tlen=252, kl=0.0397, act_lr=7e-7, ent=1.66]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.12it/s, pg=-0.198, ret=0.000672, glen=91.9, tlen=252, kl=0.0397, act_lr=7e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.12it/s, pg=-0.0952, ret=0.000311, glen=108, tlen=268, kl=0.229, act_lr=7e-7, ent=1.73] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:44,  1.12it/s, pg=-0.0952, ret=0.000311, glen=108, tlen=268, kl=0.229, act_lr=7e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:44,  1.12it/s, pg=-0.0336, ret=-0.00053, glen=95.8, tlen=255, kl=0.0206, act_lr=7e-7, ent=1.65]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.14it/s, pg=-0.0336, ret=-0.00053, glen=95.8, tlen=255, kl=0.0206, act_lr=7e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.14it/s, pg=0.000427, ret=0.000284, glen=94.6, tlen=255, kl=0.0391, act_lr=7e-7, ent=1.62]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.15it/s, pg=0.000427, ret=0.000284, glen=94.6, tlen=255, kl=0.0391, act_lr=7e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.15it/s, pg=-0.0992, ret=0.000361, glen=99.3, tlen=259, kl=0.0214, act_lr=7e-7, ent=1.71] Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.0992, ret=0.000361, glen=99.3, tlen=259, kl=0.0214, act_lr=7e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.0123, ret=0.000533, glen=126, tlen=286, kl=0.0347, act_lr=7e-7, ent=2.03] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:39,  1.15it/s, pg=-0.0123, ret=0.000533, glen=126, tlen=286, kl=0.0347, act_lr=7e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.15it/s, pg=-0.124, ret=-0.000238, glen=114, tlen=273, kl=0.0338, act_lr=7e-7, ent=2.02]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:38,  1.16it/s, pg=-0.124, ret=-0.000238, glen=114, tlen=273, kl=0.0338, act_lr=7e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:38,  1.16it/s, pg=0.0492, ret=-0.00096, glen=89.9, tlen=250, kl=0.0193, act_lr=7e-7, ent=1.68]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:37,  1.16it/s, pg=0.0492, ret=-0.00096, glen=89.9, tlen=250, kl=0.0193, act_lr=7e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:37,  1.16it/s, pg=-0.036, ret=-0.000101, glen=98.1, tlen=258, kl=0.0221, act_lr=7e-7, ent=1.69]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:36,  1.17it/s, pg=-0.036, ret=-0.000101, glen=98.1, tlen=258, kl=0.0221, act_lr=7e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:36,  1.17it/s, pg=-0.158, ret=0.00103, glen=110, tlen=270, kl=0.0237, act_lr=7e-7, ent=1.74]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:35,  1.17it/s, pg=-0.158, ret=0.00103, glen=110, tlen=270, kl=0.0237, act_lr=7e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:35,  1.17it/s, pg=0.309, ret=0.000261, glen=137, tlen=297, kl=0.0182, act_lr=7e-7, ent=2.18]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.17it/s, pg=0.309, ret=0.000261, glen=137, tlen=297, kl=0.0182, act_lr=7e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.17it/s, pg=-0.0712, ret=-8.14e-5, glen=114, tlen=274, kl=0.0206, act_lr=7e-7, ent=1.81]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.0712, ret=-8.14e-5, glen=114, tlen=274, kl=0.0206, act_lr=7e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.0803, ret=9.76e-5, glen=96.5, tlen=257, kl=0.019, act_lr=7e-7, ent=1.65] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=-0.0803, ret=9.76e-5, glen=96.5, tlen=257, kl=0.019, act_lr=7e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.0641, ret=-0.000381, glen=93.4, tlen=253, kl=0.0215, act_lr=7e-7, ent=1.66]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.18it/s, pg=-0.0641, ret=-0.000381, glen=93.4, tlen=253, kl=0.0215, act_lr=7e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.18it/s, pg=0.43, ret=-0.003, glen=172, tlen=332, kl=0.0203, act_lr=7e-7, ent=2.43]       Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.16it/s, pg=0.43, ret=-0.003, glen=172, tlen=332, kl=0.0203, act_lr=7e-7, ent=2.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.16it/s, pg=-0.213, ret=0.000955, glen=103, tlen=262, kl=0.021, act_lr=7e-7, ent=1.7]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.213, ret=0.000955, glen=103, tlen=262, kl=0.021, act_lr=7e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=-0.131, ret=0.000497, glen=101, tlen=261, kl=0.0268, act_lr=7e-7, ent=1.76]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=-0.131, ret=0.000497, glen=101, tlen=261, kl=0.0268, act_lr=7e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=0.0638, ret=1.09e-5, glen=98.8, tlen=259, kl=0.0277, act_lr=7e-7, ent=1.76]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.17it/s, pg=0.0638, ret=1.09e-5, glen=98.8, tlen=259, kl=0.0277, act_lr=7e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.17it/s, pg=-0.168, ret=0.000728, glen=104, tlen=264, kl=0.0268, act_lr=7e-7, ent=1.85]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.168, ret=0.000728, glen=104, tlen=264, kl=0.0268, act_lr=7e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=0.048, ret=-0.00172, glen=106, tlen=266, kl=0.0214, act_lr=7e-7, ent=1.95] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:19<00:27,  1.17it/s, pg=0.048, ret=-0.00172, glen=106, tlen=266, kl=0.0214, act_lr=7e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.216, ret=0.000943, glen=120, tlen=280, kl=0.0233, act_lr=7e-7, ent=2.04]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.216, ret=0.000943, glen=120, tlen=280, kl=0.0233, act_lr=7e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=0.0629, ret=-0.000472, glen=99.8, tlen=260, kl=0.0294, act_lr=7e-7, ent=1.69]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=0.0629, ret=-0.000472, glen=99.8, tlen=260, kl=0.0294, act_lr=7e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.0373, ret=-6.12e-5, glen=96.3, tlen=256, kl=0.0259, act_lr=7e-7, ent=1.81]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.0373, ret=-6.12e-5, glen=96.3, tlen=256, kl=0.0259, act_lr=7e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.105, ret=0.000349, glen=106, tlen=266, kl=0.021, act_lr=7e-7, ent=1.92]   Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:23,  1.17it/s, pg=-0.105, ret=0.000349, glen=106, tlen=266, kl=0.021, act_lr=7e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:23,  1.17it/s, pg=0.0736, ret=-0.000344, glen=105, tlen=264, kl=0.0197, act_lr=7e-7, ent=1.89]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.17it/s, pg=0.0736, ret=-0.000344, glen=105, tlen=264, kl=0.0197, act_lr=7e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.17it/s, pg=-0.0973, ret=0.00559, glen=109, tlen=270, kl=0.0192, act_lr=7e-7, ent=1.72] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.05it/s, pg=-0.0973, ret=0.00559, glen=109, tlen=270, kl=0.0192, act_lr=7e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.05it/s, pg=0.059, ret=-0.000399, glen=98.2, tlen=258, kl=0.0206, act_lr=7e-7, ent=1.65]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:23,  1.09it/s, pg=0.059, ret=-0.000399, glen=98.2, tlen=258, kl=0.0206, act_lr=7e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:23,  1.09it/s, pg=0.0792, ret=-7.7e-5, glen=103, tlen=263, kl=0.0186, act_lr=7e-7, ent=1.68]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.11it/s, pg=0.0792, ret=-7.7e-5, glen=103, tlen=263, kl=0.0186, act_lr=7e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.11it/s, pg=0.00739, ret=-0.000549, glen=92.3, tlen=252, kl=0.196, act_lr=7e-7, ent=1.68]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:20,  1.13it/s, pg=0.00739, ret=-0.000549, glen=92.3, tlen=252, kl=0.196, act_lr=7e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.13it/s, pg=-0.068, ret=0.000237, glen=99.5, tlen=259, kl=0.0209, act_lr=7e-7, ent=1.77] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.14it/s, pg=-0.068, ret=0.000237, glen=99.5, tlen=259, kl=0.0209, act_lr=7e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.14it/s, pg=-0.0823, ret=0.000949, glen=97.5, tlen=257, kl=0.0317, act_lr=7e-7, ent=1.68]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.15it/s, pg=-0.0823, ret=0.000949, glen=97.5, tlen=257, kl=0.0317, act_lr=7e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.15it/s, pg=-0.105, ret=0.000145, glen=105, tlen=265, kl=0.0506, act_lr=7e-7, ent=1.69]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.105, ret=0.000145, glen=105, tlen=265, kl=0.0506, act_lr=7e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=0.343, ret=-0.00158, glen=105, tlen=265, kl=0.0228, act_lr=7e-7, ent=1.82] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=0.343, ret=-0.00158, glen=105, tlen=265, kl=0.0228, act_lr=7e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=-0.238, ret=0.00137, glen=97.2, tlen=257, kl=0.028, act_lr=7e-7, ent=1.9] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.238, ret=0.00137, glen=97.2, tlen=257, kl=0.028, act_lr=7e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.0598, ret=0.000197, glen=93.1, tlen=253, kl=0.0247, act_lr=7e-7, ent=1.68]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:32<00:14,  1.17it/s, pg=-0.0598, ret=0.000197, glen=93.1, tlen=253, kl=0.0247, act_lr=7e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.136, ret=0.00109, glen=98.5, tlen=258, kl=0.0254, act_lr=7e-7, ent=1.62]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.17it/s, pg=-0.136, ret=0.00109, glen=98.5, tlen=258, kl=0.0254, act_lr=7e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0439, ret=0.000967, glen=115, tlen=275, kl=0.0267, act_lr=7e-7, ent=1.82]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.0439, ret=0.000967, glen=115, tlen=275, kl=0.0267, act_lr=7e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=-0.0707, ret=-0.000165, glen=101, tlen=261, kl=0.0227, act_lr=7e-7, ent=1.76]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=-0.0707, ret=-0.000165, glen=101, tlen=261, kl=0.0227, act_lr=7e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=0.00293, ret=-0.000152, glen=96.4, tlen=256, kl=0.0273, act_lr=7e-7, ent=1.66]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=0.00293, ret=-0.000152, glen=96.4, tlen=256, kl=0.0273, act_lr=7e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=0.1, ret=-0.000638, glen=93.7, tlen=254, kl=0.0328, act_lr=7e-7, ent=1.64]    Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.18it/s, pg=0.1, ret=-0.000638, glen=93.7, tlen=254, kl=0.0328, act_lr=7e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.18it/s, pg=-0.0463, ret=-0.000434, glen=101, tlen=260, kl=0.0327, act_lr=7e-7, ent=1.69]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.18it/s, pg=-0.0463, ret=-0.000434, glen=101, tlen=260, kl=0.0327, act_lr=7e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.18it/s, pg=-0.0314, ret=-0.00023, glen=137, tlen=297, kl=0.0222, act_lr=7e-7, ent=2.07] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:38<00:08,  1.17it/s, pg=-0.0314, ret=-0.00023, glen=137, tlen=297, kl=0.0222, act_lr=7e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=-0.0151, ret=0.000152, glen=102, tlen=261, kl=0.0302, act_lr=7e-7, ent=1.74]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.17it/s, pg=-0.0151, ret=0.000152, glen=102, tlen=261, kl=0.0302, act_lr=7e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.105, ret=0.000171, glen=99.5, tlen=260, kl=0.021, act_lr=7e-7, ent=1.55] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=-0.105, ret=0.000171, glen=99.5, tlen=260, kl=0.021, act_lr=7e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=0.404, ret=-0.000471, glen=143, tlen=302, kl=0.0171, act_lr=7e-7, ent=2.47]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:06,  1.17it/s, pg=0.404, ret=-0.000471, glen=143, tlen=302, kl=0.0171, act_lr=7e-7, ent=2.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:06,  1.17it/s, pg=0.0211, ret=0.000478, glen=106, tlen=266, kl=0.017, act_lr=7e-7, ent=1.59] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=0.0211, ret=0.000478, glen=106, tlen=266, kl=0.017, act_lr=7e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=0.183, ret=-3.17e-5, glen=115, tlen=275, kl=0.0178, act_lr=7e-7, ent=2.02]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=0.183, ret=-3.17e-5, glen=115, tlen=275, kl=0.0178, act_lr=7e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=-0.005, ret=0.000331, glen=111, tlen=270, kl=0.0202, act_lr=7e-7, ent=2]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.005, ret=0.000331, glen=111, tlen=270, kl=0.0202, act_lr=7e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.0538, ret=0.000143, glen=96.1, tlen=256, kl=0.0398, act_lr=7e-7, ent=1.76]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:44<00:02,  1.17it/s, pg=-0.0538, ret=0.000143, glen=96.1, tlen=256, kl=0.0398, act_lr=7e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.00732, ret=4.05e-5, glen=107, tlen=267, kl=0.0282, act_lr=7e-7, ent=1.79]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.18it/s, pg=0.00732, ret=4.05e-5, glen=107, tlen=267, kl=0.0282, act_lr=7e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.18it/s, pg=0.145, ret=-0.00119, glen=103, tlen=263, kl=0.0197, act_lr=7e-7, ent=1.67] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.18it/s, pg=0.145, ret=-0.00119, glen=103, tlen=263, kl=0.0197, act_lr=7e-7, ent=1.67]
2025-07-24 18:18:40.039 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.59s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.18it/s, pg=-0.111, ret=0.000317, glen=96.8, tlen=257, kl=0.0215, act_lr=7.2e-7, ent=1.67]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.14it/s, pg=-0.111, ret=0.000317, glen=96.8, tlen=257, kl=0.0215, act_lr=7.2e-7, ent=1.67]
2025-07-24 18:18:40.729 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 18:18:42.781 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.05s
2025-07-24 18:18:43.121 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 50.78s
2025-07-24 18:18:43.128 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.016082468899813566, 'actor_lr': 7.003636235160891e-07, 'clip_ratio': 0.0, 'entropy': 1.8003989718177102, 'kl': 0.03159845525568182, 'response_length': 105.5801796653054, 'total_length': 265.46073580655184, 'teacher_total_length': 277.54592118696735, 'return': 7.155945535156537e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [38:23<11:05, 221.81s/it]2025-07-24 18:18:43.173 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:20:19.623 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:20:19.800 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:20:19.800 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 96.63s
2025-07-24 18:20:21.740 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0135,avg_reflection_pattern_score: 0.0098,avg_pass_at_n: 1.0000,avg_num_tokens: 104.3143,std_num_tokens: 107.7796,avg_correct_num_tokens: 102.0886,std_correct_num_tokens: 88.2565,avg_incorrect_num_tokens: 121.4242,std_incorrect_num_tokens: 201.7576
2025-07-24 18:20:22.174 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.37s
2025-07-24 18:20:25.337 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.16s
2025-07-24 18:20:53.274 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 220
2025-07-24 18:20:53.274 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.94s
2025-07-24 18:20:54.629 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.94s
2025-07-24 18:20:54.630 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.0162705836013298e-05, avg_kl: 0.027875033291903408, avg_response_length: 105.70610899491744, avg_orm_score: 0.0, avg_custom_rewards: 1.0162705836013298e-05
2025-07-24 18:20:54.662 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter36_replay_buffer.jsonl
2025-07-24 18:20:56.477 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.82s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:01<?, ?it/s, pg=0.328, ret=-0.000408, glen=133, tlen=293, kl=0.0208, act_lr=7.2e-7, ent=2.21]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:56,  1.05s/it, pg=0.328, ret=-0.000408, glen=133, tlen=293, kl=0.0208, act_lr=7.2e-7, ent=2.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:56,  1.05s/it, pg=0.0234, ret=1.82e-5, glen=99.1, tlen=259, kl=0.0258, act_lr=7.2e-7, ent=1.67]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:49,  1.07it/s, pg=0.0234, ret=1.82e-5, glen=99.1, tlen=259, kl=0.0258, act_lr=7.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:49,  1.07it/s, pg=-0.177, ret=0.000757, glen=102, tlen=262, kl=0.0278, act_lr=7.2e-7, ent=1.58]Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:47,  1.09it/s, pg=-0.177, ret=0.000757, glen=102, tlen=262, kl=0.0278, act_lr=7.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:47,  1.09it/s, pg=-0.0479, ret=0.000488, glen=104, tlen=264, kl=0.0223, act_lr=7.2e-7, ent=1.72]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.12it/s, pg=-0.0479, ret=0.000488, glen=104, tlen=264, kl=0.0223, act_lr=7.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.12it/s, pg=-0.0497, ret=0.000201, glen=101, tlen=261, kl=0.026, act_lr=7.2e-7, ent=1.76] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:44,  1.12it/s, pg=-0.0497, ret=0.000201, glen=101, tlen=261, kl=0.026, act_lr=7.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:44,  1.12it/s, pg=-0.152, ret=0.000687, glen=87.1, tlen=247, kl=0.0294, act_lr=7.2e-7, ent=1.54]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.14it/s, pg=-0.152, ret=0.000687, glen=87.1, tlen=247, kl=0.0294, act_lr=7.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.14it/s, pg=0.263, ret=-0.00125, glen=103, tlen=263, kl=0.0247, act_lr=7.2e-7, ent=1.68]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:43,  1.11it/s, pg=0.263, ret=-0.00125, glen=103, tlen=263, kl=0.0247, act_lr=7.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:43,  1.11it/s, pg=0.249, ret=-0.00193, glen=108, tlen=268, kl=0.0393, act_lr=7.2e-7, ent=1.87]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:42,  1.11it/s, pg=0.249, ret=-0.00193, glen=108, tlen=268, kl=0.0393, act_lr=7.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:08<00:42,  1.11it/s, pg=0.063, ret=-0.000866, glen=91.8, tlen=251, kl=0.0375, act_lr=7.2e-7, ent=1.77]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:40,  1.13it/s, pg=0.063, ret=-0.000866, glen=91.8, tlen=251, kl=0.0375, act_lr=7.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:40,  1.13it/s, pg=-0.0154, ret=-0.000607, glen=109, tlen=269, kl=0.0363, act_lr=7.2e-7, ent=1.75]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.14it/s, pg=-0.0154, ret=-0.000607, glen=109, tlen=269, kl=0.0363, act_lr=7.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.14it/s, pg=-0.0845, ret=0.000294, glen=97.8, tlen=258, kl=0.0333, act_lr=7.2e-7, ent=1.73]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.15it/s, pg=-0.0845, ret=0.000294, glen=97.8, tlen=258, kl=0.0333, act_lr=7.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.15it/s, pg=-0.0923, ret=0.000504, glen=93, tlen=253, kl=0.0302, act_lr=7.2e-7, ent=1.7]   Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.16it/s, pg=-0.0923, ret=0.000504, glen=93, tlen=253, kl=0.0302, act_lr=7.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.16it/s, pg=-0.171, ret=0.000807, glen=96.2, tlen=256, kl=0.0299, act_lr=7.2e-7, ent=1.66]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.16it/s, pg=-0.171, ret=0.000807, glen=96.2, tlen=256, kl=0.0299, act_lr=7.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.16it/s, pg=0.0101, ret=-0.000415, glen=110, tlen=270, kl=0.0217, act_lr=7.2e-7, ent=1.77]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.16it/s, pg=0.0101, ret=-0.000415, glen=110, tlen=270, kl=0.0217, act_lr=7.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.16it/s, pg=-0.16, ret=0.00144, glen=105, tlen=265, kl=0.027, act_lr=7.2e-7, ent=1.77]    Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.16, ret=0.00144, glen=105, tlen=265, kl=0.027, act_lr=7.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:14<00:34,  1.17it/s, pg=-0.0712, ret=-0.000596, glen=93.7, tlen=253, kl=0.0361, act_lr=7.2e-7, ent=1.71]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.0712, ret=-0.000596, glen=93.7, tlen=253, kl=0.0361, act_lr=7.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.182, ret=0.00141, glen=104, tlen=264, kl=0.0234, act_lr=7.2e-7, ent=1.8]     Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=-0.182, ret=0.00141, glen=104, tlen=264, kl=0.0234, act_lr=7.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=-0.0112, ret=-0.000193, glen=99.2, tlen=259, kl=0.0211, act_lr=7.2e-7, ent=1.56]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.0112, ret=-0.000193, glen=99.2, tlen=259, kl=0.0211, act_lr=7.2e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=-0.157, ret=0.000744, glen=101, tlen=261, kl=0.0384, act_lr=7.2e-7, ent=1.64]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.157, ret=0.000744, glen=101, tlen=261, kl=0.0384, act_lr=7.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=-0.164, ret=0.000406, glen=106, tlen=266, kl=0.0299, act_lr=7.2e-7, ent=1.73]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:30,  1.14it/s, pg=-0.164, ret=0.000406, glen=106, tlen=266, kl=0.0299, act_lr=7.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:30,  1.14it/s, pg=0.0406, ret=-0.00115, glen=106, tlen=266, kl=0.0249, act_lr=7.2e-7, ent=1.64]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.15it/s, pg=0.0406, ret=-0.00115, glen=106, tlen=266, kl=0.0249, act_lr=7.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.15it/s, pg=-0.152, ret=0.000712, glen=108, tlen=268, kl=0.0247, act_lr=7.2e-7, ent=1.67]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.16it/s, pg=-0.152, ret=0.000712, glen=108, tlen=268, kl=0.0247, act_lr=7.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:20<00:28,  1.16it/s, pg=-0.226, ret=0.00124, glen=102, tlen=262, kl=0.0216, act_lr=7.2e-7, ent=1.72] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.226, ret=0.00124, glen=102, tlen=262, kl=0.0216, act_lr=7.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=0.194, ret=-0.000704, glen=119, tlen=280, kl=0.0263, act_lr=7.2e-7, ent=2.19]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.16it/s, pg=0.194, ret=-0.000704, glen=119, tlen=280, kl=0.0263, act_lr=7.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.16it/s, pg=0.126, ret=-0.00116, glen=97.8, tlen=257, kl=0.0292, act_lr=7.2e-7, ent=1.65]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=0.126, ret=-0.00116, glen=97.8, tlen=257, kl=0.0292, act_lr=7.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.0671, ret=0.000513, glen=100, tlen=260, kl=0.026, act_lr=7.2e-7, ent=1.68]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.0671, ret=0.000513, glen=100, tlen=260, kl=0.026, act_lr=7.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=0.0129, ret=0.00238, glen=109, tlen=268, kl=0.0362, act_lr=7.2e-7, ent=1.81] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:23,  1.17it/s, pg=0.0129, ret=0.00238, glen=109, tlen=268, kl=0.0362, act_lr=7.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:23,  1.17it/s, pg=0.138, ret=0.000342, glen=122, tlen=281, kl=0.0194, act_lr=7.2e-7, ent=2.09]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.17it/s, pg=0.138, ret=0.000342, glen=122, tlen=281, kl=0.0194, act_lr=7.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.17it/s, pg=-0.0911, ret=8.06e-5, glen=99.8, tlen=259, kl=0.0251, act_lr=7.2e-7, ent=1.79]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.07it/s, pg=-0.0911, ret=8.06e-5, glen=99.8, tlen=259, kl=0.0251, act_lr=7.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.07it/s, pg=-0.19, ret=0.00147, glen=94.9, tlen=255, kl=0.0241, act_lr=7.2e-7, ent=1.73]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.10it/s, pg=-0.19, ret=0.00147, glen=94.9, tlen=255, kl=0.0241, act_lr=7.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.10it/s, pg=-0.0269, ret=-0.000703, glen=116, tlen=275, kl=0.0342, act_lr=7.2e-7, ent=1.9]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.0269, ret=-0.000703, glen=116, tlen=275, kl=0.0342, act_lr=7.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:28<00:21,  1.12it/s, pg=-0.0641, ret=0.000305, glen=103, tlen=263, kl=0.0247, act_lr=7.2e-7, ent=1.65]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.14it/s, pg=-0.0641, ret=0.000305, glen=103, tlen=263, kl=0.0247, act_lr=7.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.14it/s, pg=-0.0496, ret=-0.000268, glen=102, tlen=261, kl=0.0292, act_lr=7.2e-7, ent=1.74]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=-0.0496, ret=-0.000268, glen=102, tlen=261, kl=0.0292, act_lr=7.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=-0.115, ret=-1.41e-5, glen=98.9, tlen=258, kl=0.0227, act_lr=7.2e-7, ent=1.61] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.16it/s, pg=-0.115, ret=-1.41e-5, glen=98.9, tlen=258, kl=0.0227, act_lr=7.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.16it/s, pg=-0.103, ret=3.52e-5, glen=98.5, tlen=259, kl=0.0389, act_lr=7.2e-7, ent=1.79] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.103, ret=3.52e-5, glen=98.5, tlen=259, kl=0.0389, act_lr=7.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=-0.0361, ret=-0.000604, glen=101, tlen=261, kl=0.0346, act_lr=7.2e-7, ent=1.65]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=-0.0361, ret=-0.000604, glen=101, tlen=261, kl=0.0346, act_lr=7.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=0.487, ret=-0.00186, glen=141, tlen=300, kl=0.028, act_lr=7.2e-7, ent=1.83]    Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.16it/s, pg=0.487, ret=-0.00186, glen=141, tlen=300, kl=0.028, act_lr=7.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.16it/s, pg=-0.0427, ret=0.000114, glen=117, tlen=277, kl=0.0229, act_lr=7.2e-7, ent=1.97]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.16it/s, pg=-0.0427, ret=0.000114, glen=117, tlen=277, kl=0.0229, act_lr=7.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:34<00:14,  1.16it/s, pg=-0.0759, ret=0.000861, glen=94.4, tlen=254, kl=0.0218, act_lr=7.2e-7, ent=1.64]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0759, ret=0.000861, glen=94.4, tlen=254, kl=0.0218, act_lr=7.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.103, ret=0.000178, glen=114, tlen=274, kl=0.0234, act_lr=7.2e-7, ent=1.8]   Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.103, ret=0.000178, glen=114, tlen=274, kl=0.0234, act_lr=7.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=0.143, ret=0.000103, glen=135, tlen=295, kl=0.0292, act_lr=7.2e-7, ent=2.04]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=0.143, ret=0.000103, glen=135, tlen=295, kl=0.0292, act_lr=7.2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=-0.112, ret=0.000592, glen=105, tlen=265, kl=0.0202, act_lr=7.2e-7, ent=1.76]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=-0.112, ret=0.000592, glen=105, tlen=265, kl=0.0202, act_lr=7.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=0.136, ret=-0.00106, glen=110, tlen=270, kl=0.031, act_lr=7.2e-7, ent=1.73]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.17it/s, pg=0.136, ret=-0.00106, glen=110, tlen=270, kl=0.031, act_lr=7.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.17it/s, pg=0.125, ret=-0.000582, glen=114, tlen=273, kl=0.0274, act_lr=7.2e-7, ent=1.8]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=0.125, ret=-0.000582, glen=114, tlen=273, kl=0.0274, act_lr=7.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.17it/s, pg=0.0463, ret=-0.000472, glen=100, tlen=260, kl=0.0286, act_lr=7.2e-7, ent=1.74]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.0463, ret=-0.000472, glen=100, tlen=260, kl=0.0286, act_lr=7.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:40<00:08,  1.17it/s, pg=-0.104, ret=0.000517, glen=106, tlen=266, kl=0.0224, act_lr=7.2e-7, ent=1.69] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.104, ret=0.000517, glen=106, tlen=266, kl=0.0224, act_lr=7.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=0.0749, ret=-0.000179, glen=96.9, tlen=257, kl=0.0286, act_lr=7.2e-7, ent=1.63]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=0.0749, ret=-0.000179, glen=96.9, tlen=257, kl=0.0286, act_lr=7.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=0.00507, ret=-0.000369, glen=99.2, tlen=259, kl=0.0411, act_lr=7.2e-7, ent=1.76]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=0.00507, ret=-0.000369, glen=99.2, tlen=259, kl=0.0411, act_lr=7.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=-0.0225, ret=0.000129, glen=95.8, tlen=256, kl=0.0238, act_lr=7.2e-7, ent=1.69] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=-0.0225, ret=0.000129, glen=95.8, tlen=256, kl=0.0238, act_lr=7.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=-0.067, ret=0.000132, glen=95.4, tlen=255, kl=0.0233, act_lr=7.2e-7, ent=1.71] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=-0.067, ret=0.000132, glen=95.4, tlen=255, kl=0.0233, act_lr=7.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=0.0464, ret=-0.000615, glen=102, tlen=262, kl=0.0394, act_lr=7.2e-7, ent=1.67]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=0.0464, ret=-0.000615, glen=102, tlen=262, kl=0.0394, act_lr=7.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=-0.0465, ret=-0.000276, glen=103, tlen=263, kl=0.0275, act_lr=7.2e-7, ent=1.65]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.0465, ret=-0.000276, glen=103, tlen=263, kl=0.0275, act_lr=7.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.0931, ret=-0.0007, glen=112, tlen=272, kl=0.0286, act_lr=7.2e-7, ent=1.94]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.17it/s, pg=0.0931, ret=-0.0007, glen=112, tlen=272, kl=0.0286, act_lr=7.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=-0.00833, ret=0.000104, glen=136, tlen=296, kl=0.0201, act_lr=7.2e-7, ent=2.13]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.16it/s, pg=-0.00833, ret=0.000104, glen=136, tlen=296, kl=0.0201, act_lr=7.2e-7, ent=2.13]
2025-07-24 18:21:44.493 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.83s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.16it/s, pg=-0.163, ret=0.000917, glen=115, tlen=275, kl=0.0231, act_lr=7.4e-7, ent=1.8]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=-0.163, ret=0.000917, glen=115, tlen=275, kl=0.0231, act_lr=7.4e-7, ent=1.8]
2025-07-24 18:21:45.162 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 18:21:47.555 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.39s
2025-07-24 18:21:47.893 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 51.35s
2025-07-24 18:21:47.901 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.014435182918201794, 'actor_lr': 7.203636393817512e-07, 'clip_ratio': 0.0, 'entropy': 1.7622247956015846, 'kl': 0.027875033291903408, 'response_length': 105.70610989657315, 'total_length': 265.5621914950284, 'teacher_total_length': 277.95507867986504, 'return': 2.7430069299457086e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [41:27<07:00, 210.47s/it]2025-07-24 18:21:47.949 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:23:50.630 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:23:50.814 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:23:50.815 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 122.87s
2025-07-24 18:23:53.003 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0138,avg_reflection_pattern_score: 0.0109,avg_pass_at_n: 1.0000,avg_num_tokens: 103.4288,std_num_tokens: 139.0389,avg_correct_num_tokens: 99.5823,std_correct_num_tokens: 85.9677,avg_incorrect_num_tokens: 137.3198,std_incorrect_num_tokens: 351.0922
2025-07-24 18:23:53.413 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.60s
2025-07-24 18:23:56.304 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.89s
2025-07-24 18:24:24.319 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 219
2025-07-24 18:24:24.319 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.01s
2025-07-24 18:24:25.679 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.96s
2025-07-24 18:24:25.680 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00012706118844742952, avg_kl: 0.0357553142390839, avg_response_length: 106.08437166257536, avg_orm_score: 0.0, avg_custom_rewards: 0.00012706118844742952
2025-07-24 18:24:25.714 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter37_replay_buffer.jsonl
2025-07-24 18:24:27.527 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.82s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:01<?, ?it/s, pg=-0.0421, ret=0.000664, glen=106, tlen=266, kl=0.0289, act_lr=7.4e-7, ent=1.89]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:54,  1.01s/it, pg=-0.0421, ret=0.000664, glen=106, tlen=266, kl=0.0289, act_lr=7.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:54,  1.01s/it, pg=0.114, ret=-0.000636, glen=100, tlen=260, kl=0.0309, act_lr=7.4e-7, ent=1.94] Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.09it/s, pg=0.114, ret=-0.000636, glen=100, tlen=260, kl=0.0309, act_lr=7.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.09it/s, pg=0.464, ret=-0.00116, glen=126, tlen=286, kl=0.0525, act_lr=7.4e-7, ent=2.22] Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:46,  1.11it/s, pg=0.464, ret=-0.00116, glen=126, tlen=286, kl=0.0525, act_lr=7.4e-7, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:46,  1.11it/s, pg=0.0765, ret=-0.00123, glen=96.4, tlen=257, kl=0.0333, act_lr=7.4e-7, ent=1.76]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:44,  1.14it/s, pg=0.0765, ret=-0.00123, glen=96.4, tlen=257, kl=0.0333, act_lr=7.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:44,  1.14it/s, pg=-0.189, ret=0.000537, glen=106, tlen=266, kl=0.0256, act_lr=7.4e-7, ent=2.04] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:43,  1.15it/s, pg=-0.189, ret=0.000537, glen=106, tlen=266, kl=0.0256, act_lr=7.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:43,  1.15it/s, pg=-0.0948, ret=-4.81e-5, glen=90.3, tlen=250, kl=0.0288, act_lr=7.4e-7, ent=1.64]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.14it/s, pg=-0.0948, ret=-4.81e-5, glen=90.3, tlen=250, kl=0.0288, act_lr=7.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.14it/s, pg=-0.0796, ret=-0.000118, glen=106, tlen=266, kl=0.0324, act_lr=7.4e-7, ent=2.04]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.15it/s, pg=-0.0796, ret=-0.000118, glen=106, tlen=266, kl=0.0324, act_lr=7.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.15it/s, pg=-0.141, ret=-0.000146, glen=131, tlen=291, kl=0.0481, act_lr=7.4e-7, ent=2.35] Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=-0.141, ret=-0.000146, glen=131, tlen=291, kl=0.0481, act_lr=7.4e-7, ent=2.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=0.0583, ret=-0.00131, glen=102, tlen=262, kl=0.0401, act_lr=7.4e-7, ent=1.73] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:40,  1.13it/s, pg=0.0583, ret=-0.00131, glen=102, tlen=262, kl=0.0401, act_lr=7.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:40,  1.13it/s, pg=0.0604, ret=-0.0007, glen=108, tlen=268, kl=0.0223, act_lr=7.4e-7, ent=1.87] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.14it/s, pg=0.0604, ret=-0.0007, glen=108, tlen=268, kl=0.0223, act_lr=7.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.14it/s, pg=-0.0572, ret=1.87e-5, glen=105, tlen=265, kl=0.0218, act_lr=7.4e-7, ent=1.71]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.15it/s, pg=-0.0572, ret=1.87e-5, glen=105, tlen=265, kl=0.0218, act_lr=7.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.15it/s, pg=-0.109, ret=0.000269, glen=100, tlen=260, kl=0.0624, act_lr=7.4e-7, ent=1.72]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.16it/s, pg=-0.109, ret=0.000269, glen=100, tlen=260, kl=0.0624, act_lr=7.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.16it/s, pg=-0.11, ret=8.73e-5, glen=109, tlen=269, kl=0.032, act_lr=7.4e-7, ent=1.71]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.16it/s, pg=-0.11, ret=8.73e-5, glen=109, tlen=269, kl=0.032, act_lr=7.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.16it/s, pg=0.0738, ret=0.000216, glen=125, tlen=285, kl=0.0253, act_lr=7.4e-7, ent=1.95]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.16it/s, pg=0.0738, ret=0.000216, glen=125, tlen=285, kl=0.0253, act_lr=7.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.16it/s, pg=-0.0198, ret=-0.000336, glen=96.9, tlen=257, kl=0.0263, act_lr=7.4e-7, ent=1.76]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.16it/s, pg=-0.0198, ret=-0.000336, glen=96.9, tlen=257, kl=0.0263, act_lr=7.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.16it/s, pg=-0.0816, ret=3.84e-5, glen=94, tlen=254, kl=0.0293, act_lr=7.4e-7, ent=1.66]    Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=-0.0816, ret=3.84e-5, glen=94, tlen=254, kl=0.0293, act_lr=7.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.209, ret=0.000341, glen=105, tlen=265, kl=0.0263, act_lr=7.4e-7, ent=1.86]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=-0.209, ret=0.000341, glen=105, tlen=265, kl=0.0263, act_lr=7.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=-0.166, ret=0.000604, glen=95.2, tlen=255, kl=0.0291, act_lr=7.4e-7, ent=1.56]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.166, ret=0.000604, glen=95.2, tlen=255, kl=0.0291, act_lr=7.4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=-0.0388, ret=9.15e-5, glen=96.7, tlen=257, kl=0.0298, act_lr=7.4e-7, ent=1.66]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.0388, ret=9.15e-5, glen=96.7, tlen=257, kl=0.0298, act_lr=7.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=-0.124, ret=0.000365, glen=97, tlen=257, kl=0.0303, act_lr=7.4e-7, ent=1.74]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=-0.124, ret=0.000365, glen=97, tlen=257, kl=0.0303, act_lr=7.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=0.19, ret=0.000312, glen=118, tlen=278, kl=0.0264, act_lr=7.4e-7, ent=2.28] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.17it/s, pg=0.19, ret=0.000312, glen=118, tlen=278, kl=0.0264, act_lr=7.4e-7, ent=2.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.17it/s, pg=-0.219, ret=0.000835, glen=99.8, tlen=260, kl=0.15, act_lr=7.4e-7, ent=1.61]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.219, ret=0.000835, glen=99.8, tlen=260, kl=0.15, act_lr=7.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.131, ret=-8.07e-5, glen=97.8, tlen=258, kl=0.0258, act_lr=7.4e-7, ent=1.56]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:19<00:27,  1.16it/s, pg=-0.131, ret=-8.07e-5, glen=97.8, tlen=258, kl=0.0258, act_lr=7.4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.178, ret=0.000503, glen=94.9, tlen=255, kl=0.0459, act_lr=7.4e-7, ent=1.62]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.16it/s, pg=-0.178, ret=0.000503, glen=94.9, tlen=255, kl=0.0459, act_lr=7.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.16it/s, pg=-0.148, ret=0.000597, glen=104, tlen=264, kl=0.0313, act_lr=7.4e-7, ent=1.72] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:26,  1.15it/s, pg=-0.148, ret=0.000597, glen=104, tlen=264, kl=0.0313, act_lr=7.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:26,  1.15it/s, pg=0.423, ret=-0.000645, glen=128, tlen=288, kl=0.0214, act_lr=7.4e-7, ent=1.54]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:25,  1.15it/s, pg=0.423, ret=-0.000645, glen=128, tlen=288, kl=0.0214, act_lr=7.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:25,  1.15it/s, pg=0.0434, ret=-0.00117, glen=90, tlen=250, kl=0.0385, act_lr=7.4e-7, ent=1.6]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:24,  1.16it/s, pg=0.0434, ret=-0.00117, glen=90, tlen=250, kl=0.0385, act_lr=7.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:24,  1.16it/s, pg=-0.077, ret=-0.000308, glen=108, tlen=268, kl=0.0358, act_lr=7.4e-7, ent=1.79]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.16it/s, pg=-0.077, ret=-0.000308, glen=108, tlen=268, kl=0.0358, act_lr=7.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.16it/s, pg=-0.106, ret=-0.000112, glen=106, tlen=265, kl=0.0222, act_lr=7.4e-7, ent=1.93]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.06it/s, pg=-0.106, ret=-0.000112, glen=106, tlen=265, kl=0.0222, act_lr=7.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.06it/s, pg=-0.0509, ret=-0.000629, glen=94.1, tlen=254, kl=0.0261, act_lr=7.4e-7, ent=1.74]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.09it/s, pg=-0.0509, ret=-0.000629, glen=94.1, tlen=254, kl=0.0261, act_lr=7.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.09it/s, pg=-0.136, ret=0.000324, glen=98.5, tlen=259, kl=0.0758, act_lr=7.4e-7, ent=1.61]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.136, ret=0.000324, glen=98.5, tlen=259, kl=0.0758, act_lr=7.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.214, ret=0.000793, glen=92.8, tlen=253, kl=0.0368, act_lr=7.4e-7, ent=1.59]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:20,  1.13it/s, pg=-0.214, ret=0.000793, glen=92.8, tlen=253, kl=0.0368, act_lr=7.4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.13it/s, pg=-0.00409, ret=-0.000111, glen=106, tlen=266, kl=0.0339, act_lr=7.4e-7, ent=1.75]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=-0.00409, ret=-0.000111, glen=106, tlen=266, kl=0.0339, act_lr=7.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=0.132, ret=0.000389, glen=117, tlen=277, kl=0.0256, act_lr=7.4e-7, ent=2.1]     Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.15it/s, pg=0.132, ret=0.000389, glen=117, tlen=277, kl=0.0256, act_lr=7.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.15it/s, pg=0.0293, ret=-0.000416, glen=97, tlen=257, kl=0.0441, act_lr=7.4e-7, ent=1.71]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=0.0293, ret=-0.000416, glen=97, tlen=257, kl=0.0441, act_lr=7.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=-0.0987, ret=0.000151, glen=103, tlen=263, kl=0.0322, act_lr=7.4e-7, ent=1.7]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.16it/s, pg=-0.0987, ret=0.000151, glen=103, tlen=263, kl=0.0322, act_lr=7.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.16it/s, pg=-0.0692, ret=0.00032, glen=93.2, tlen=253, kl=0.0284, act_lr=7.4e-7, ent=1.75]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.0692, ret=0.00032, glen=93.2, tlen=253, kl=0.0284, act_lr=7.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.17it/s, pg=0.197, ret=0.000182, glen=117, tlen=277, kl=0.0197, act_lr=7.4e-7, ent=1.5]   Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.197, ret=0.000182, glen=117, tlen=277, kl=0.0197, act_lr=7.4e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.222, ret=0.00123, glen=167, tlen=327, kl=0.0214, act_lr=7.4e-7, ent=2.07]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.16it/s, pg=0.222, ret=0.00123, glen=167, tlen=327, kl=0.0214, act_lr=7.4e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.16it/s, pg=-0.0816, ret=0.000801, glen=87.5, tlen=248, kl=0.0362, act_lr=7.4e-7, ent=1.54]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.16it/s, pg=-0.0816, ret=0.000801, glen=87.5, tlen=248, kl=0.0362, act_lr=7.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.16it/s, pg=-0.0514, ret=0.000341, glen=116, tlen=276, kl=0.0357, act_lr=7.4e-7, ent=2.01] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:12,  1.17it/s, pg=-0.0514, ret=0.000341, glen=116, tlen=276, kl=0.0357, act_lr=7.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:12,  1.17it/s, pg=-0.00723, ret=-0.000122, glen=101, tlen=261, kl=0.0353, act_lr=7.4e-7, ent=1.77]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=-0.00723, ret=-0.000122, glen=101, tlen=261, kl=0.0353, act_lr=7.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=0.122, ret=-0.00109, glen=102, tlen=262, kl=0.0714, act_lr=7.4e-7, ent=1.75]    Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.17it/s, pg=0.122, ret=-0.00109, glen=102, tlen=262, kl=0.0714, act_lr=7.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.17it/s, pg=-0.152, ret=-0.000122, glen=108, tlen=269, kl=0.054, act_lr=7.4e-7, ent=1.87]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=-0.152, ret=-0.000122, glen=108, tlen=269, kl=0.054, act_lr=7.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.17it/s, pg=-0.207, ret=0.00049, glen=99.8, tlen=260, kl=0.0275, act_lr=7.4e-7, ent=1.73]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=-0.207, ret=0.00049, glen=99.8, tlen=260, kl=0.0275, act_lr=7.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.462, ret=-0.000605, glen=187, tlen=347, kl=0.0288, act_lr=7.4e-7, ent=2.49]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.16it/s, pg=0.462, ret=-0.000605, glen=187, tlen=347, kl=0.0288, act_lr=7.4e-7, ent=2.49]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.16it/s, pg=-0.108, ret=-8.26e-5, glen=106, tlen=266, kl=0.0279, act_lr=7.4e-7, ent=1.86]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.16it/s, pg=-0.108, ret=-8.26e-5, glen=106, tlen=266, kl=0.0279, act_lr=7.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.16it/s, pg=-0.17, ret=0.000797, glen=101, tlen=261, kl=0.0347, act_lr=7.4e-7, ent=1.7]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=-0.17, ret=0.000797, glen=101, tlen=261, kl=0.0347, act_lr=7.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=0.021, ret=-0.000833, glen=90.5, tlen=251, kl=0.0394, act_lr=7.4e-7, ent=1.62]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=0.021, ret=-0.000833, glen=90.5, tlen=251, kl=0.0394, act_lr=7.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=-0.017, ret=-8.12e-5, glen=95.2, tlen=256, kl=0.0347, act_lr=7.4e-7, ent=1.62]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=-0.017, ret=-8.12e-5, glen=95.2, tlen=256, kl=0.0347, act_lr=7.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=-0.235, ret=0.000915, glen=104, tlen=264, kl=0.0262, act_lr=7.4e-7, ent=1.69] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.235, ret=0.000915, glen=104, tlen=264, kl=0.0262, act_lr=7.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=-0.142, ret=0.000397, glen=108, tlen=268, kl=0.0301, act_lr=7.4e-7, ent=1.73]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.142, ret=0.000397, glen=108, tlen=268, kl=0.0301, act_lr=7.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.155, ret=0.000464, glen=99.1, tlen=260, kl=0.0247, act_lr=7.4e-7, ent=1.7]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.17it/s, pg=-0.155, ret=0.000464, glen=99.1, tlen=260, kl=0.0247, act_lr=7.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=0.212, ret=-0.00125, glen=111, tlen=272, kl=0.0287, act_lr=7.4e-7, ent=1.66] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.17it/s, pg=0.212, ret=-0.00125, glen=111, tlen=272, kl=0.0287, act_lr=7.4e-7, ent=1.66]
2025-07-24 18:25:15.477 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.73s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.17it/s, pg=-0.176, ret=0.000333, glen=95.8, tlen=256, kl=0.0303, act_lr=7.6e-7, ent=1.72]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=-0.176, ret=0.000333, glen=95.8, tlen=256, kl=0.0303, act_lr=7.6e-7, ent=1.72]
2025-07-24 18:25:16.341 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 18:25:18.940 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 18:25:19.290 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 51.67s
2025-07-24 18:25:19.297 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.027202571522105822, 'actor_lr': 7.403636542138966e-07, 'clip_ratio': 0.0, 'entropy': 1.7900362274863504, 'kl': 0.03566991629248316, 'response_length': 106.13297826593572, 'total_length': 266.2973491321911, 'teacher_total_length': 278.2939630681818, 'return': 1.3755942663093182e-06, 'policy_update_steps': 1.0}
Episode [3/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [44:59<03:30, 210.75s/it]2025-07-24 18:25:19.303 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:02,  2.73it/s, est. speed input: 487.97 toks/s, output: 27.26 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:04<00:00, 29.99it/s, est. speed input: 7012.96 toks/s, output: 3046.55 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:04<00:00, 23.14it/s, est. speed input: 6270.58 toks/s, output: 2885.22 toks/s][32m [repeated 104x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:08<00:00,  2.82it/s, est. speed input: 3466.24 toks/s, output: 1938.98 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:08<00:00, 19.12it/s, est. speed input: 3466.24 toks/s, output: 1938.98 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:10<00:00,  2.11it/s, est. speed input: 3078.53 toks/s, output: 1760.08 toks/s][32m [repeated 21x across cluster][0m
2025-07-24 18:25:31.411 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 454.6550,strategyqa_test/accuracy: 0.5240,eval_accuracy: 0.5240
2025-07-24 18:25:31.680 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:26:47.183 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:26:47.362 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:26:47.362 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 75.68s
2025-07-24 18:26:48.510 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0139,avg_reflection_pattern_score: 0.0121,avg_pass_at_n: 1.0000,avg_num_tokens: 95.0970,std_num_tokens: 94.7902,avg_correct_num_tokens: 92.9268,std_correct_num_tokens: 65.1435,avg_incorrect_num_tokens: 114.8750,std_incorrect_num_tokens: 227.5115
2025-07-24 18:26:48.814 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.45s
2025-07-24 18:26:50.162 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.35s
2025-07-24 18:27:05.150 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 111
2025-07-24 18:27:05.150 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 14.99s
2025-07-24 18:27:06.227 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.63s
2025-07-24 18:27:06.228 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00048322579194162344, avg_kl: 0.07313496357685811, avg_response_length: 96.76882260984128, avg_orm_score: 0.0, avg_custom_rewards: 0.00048322579194162344
2025-07-24 18:27:06.253 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter38_replay_buffer.jsonl
2025-07-24 18:27:07.144 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.89s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/28 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00,  1.69it/s, est. speed input: 2776.18 toks/s, output: 1639.73 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00, 15.28it/s, est. speed input: 2776.18 toks/s, output: 1639.73 toks/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/28 [00:00<?, ?it/s, pg=-0.0823, ret=0.000193, glen=98.5, tlen=258, kl=0.0255, act_lr=7.6e-7, ent=1.69]Actor Train epoch [1/1]:   4%|‚ñé         | 1/28 [00:00<00:26,  1.02it/s, pg=-0.0823, ret=0.000193, glen=98.5, tlen=258, kl=0.0255, act_lr=7.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 1/28 [00:01<00:26,  1.02it/s, pg=-0.0761, ret=3.66e-5, glen=101, tlen=261, kl=1.16, act_lr=7.6e-7, ent=1.76]    Actor Train epoch [1/1]:   7%|‚ñã         | 2/28 [00:01<00:23,  1.10it/s, pg=-0.0761, ret=3.66e-5, glen=101, tlen=261, kl=1.16, act_lr=7.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/28 [00:02<00:23,  1.10it/s, pg=-0.135, ret=-1.84e-5, glen=105, tlen=264, kl=0.0255, act_lr=7.6e-7, ent=1.89]Actor Train epoch [1/1]:  11%|‚ñà         | 3/28 [00:02<00:22,  1.13it/s, pg=-0.135, ret=-1.84e-5, glen=105, tlen=264, kl=0.0255, act_lr=7.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 3/28 [00:03<00:22,  1.13it/s, pg=-0.0259, ret=-0.000372, glen=88.9, tlen=248, kl=0.0321, act_lr=7.6e-7, ent=1.57]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/28 [00:03<00:21,  1.13it/s, pg=-0.0259, ret=-0.000372, glen=88.9, tlen=248, kl=0.0321, act_lr=7.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/28 [00:04<00:21,  1.13it/s, pg=-0.0417, ret=4.57e-5, glen=98.4, tlen=258, kl=0.0267, act_lr=7.6e-7, ent=1.63]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 5/28 [00:04<00:20,  1.15it/s, pg=-0.0417, ret=4.57e-5, glen=98.4, tlen=258, kl=0.0267, act_lr=7.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 5/28 [00:05<00:20,  1.15it/s, pg=-0.0451, ret=-0.000421, glen=94.7, tlen=255, kl=0.044, act_lr=7.6e-7, ent=1.79]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 6/28 [00:05<00:19,  1.16it/s, pg=-0.0451, ret=-0.000421, glen=94.7, tlen=255, kl=0.044, act_lr=7.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 6/28 [00:06<00:19,  1.16it/s, pg=0.0176, ret=-2.68e-5, glen=89.7, tlen=250, kl=0.0265, act_lr=7.6e-7, ent=1.65] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 7/28 [00:06<00:18,  1.13it/s, pg=0.0176, ret=-2.68e-5, glen=89.7, tlen=250, kl=0.0265, act_lr=7.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 7/28 [00:07<00:18,  1.13it/s, pg=-0.0605, ret=0.000484, glen=96.8, tlen=257, kl=0.033, act_lr=7.6e-7, ent=1.73]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 8/28 [00:07<00:17,  1.14it/s, pg=-0.0605, ret=0.000484, glen=96.8, tlen=257, kl=0.033, act_lr=7.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 8/28 [00:07<00:17,  1.14it/s, pg=0.0699, ret=-0.000401, glen=88.6, tlen=248, kl=0.0259, act_lr=7.6e-7, ent=1.57]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 9/28 [00:07<00:16,  1.15it/s, pg=0.0699, ret=-0.000401, glen=88.6, tlen=248, kl=0.0259, act_lr=7.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 9/28 [00:08<00:16,  1.15it/s, pg=0.00571, ret=-0.000207, glen=96.2, tlen=256, kl=0.0346, act_lr=7.6e-7, ent=1.61]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 10/28 [00:08<00:15,  1.16it/s, pg=0.00571, ret=-0.000207, glen=96.2, tlen=256, kl=0.0346, act_lr=7.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 10/28 [00:09<00:15,  1.16it/s, pg=-0.115, ret=0.000506, glen=94.4, tlen=254, kl=0.0384, act_lr=7.6e-7, ent=1.72]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 11/28 [00:09<00:14,  1.16it/s, pg=-0.115, ret=0.000506, glen=94.4, tlen=254, kl=0.0384, act_lr=7.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 11/28 [00:10<00:14,  1.16it/s, pg=0.0011, ret=-0.000984, glen=96.1, tlen=255, kl=0.0354, act_lr=7.6e-7, ent=1.69]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 12/28 [00:10<00:13,  1.17it/s, pg=0.0011, ret=-0.000984, glen=96.1, tlen=255, kl=0.0354, act_lr=7.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 12/28 [00:11<00:13,  1.17it/s, pg=0.182, ret=-0.00104, glen=100, tlen=260, kl=0.0494, act_lr=7.6e-7, ent=1.76]   Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 13/28 [00:11<00:12,  1.17it/s, pg=0.182, ret=-0.00104, glen=100, tlen=260, kl=0.0494, act_lr=7.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 13/28 [00:12<00:12,  1.17it/s, pg=-0.0996, ret=0.000526, glen=94.4, tlen=254, kl=0.0275, act_lr=7.6e-7, ent=1.64]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 14/28 [00:12<00:12,  1.14it/s, pg=-0.0996, ret=0.000526, glen=94.4, tlen=254, kl=0.0275, act_lr=7.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 14/28 [00:13<00:12,  1.14it/s, pg=-0.171, ret=0.000548, glen=97.7, tlen=258, kl=0.0231, act_lr=7.6e-7, ent=1.63] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 15/28 [00:13<00:11,  1.15it/s, pg=-0.171, ret=0.000548, glen=97.7, tlen=258, kl=0.0231, act_lr=7.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 15/28 [00:13<00:11,  1.15it/s, pg=-0.0979, ret=-0.00023, glen=90.9, tlen=250, kl=0.0283, act_lr=7.6e-7, ent=1.54]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 16/28 [00:13<00:10,  1.16it/s, pg=-0.0979, ret=-0.00023, glen=90.9, tlen=250, kl=0.0283, act_lr=7.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 16/28 [00:14<00:10,  1.16it/s, pg=-0.219, ret=0.000964, glen=92.4, tlen=252, kl=0.0337, act_lr=7.6e-7, ent=1.6]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 17/28 [00:14<00:09,  1.16it/s, pg=-0.219, ret=0.000964, glen=92.4, tlen=252, kl=0.0337, act_lr=7.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 17/28 [00:15<00:09,  1.16it/s, pg=-0.0651, ret=0.000264, glen=93.4, tlen=253, kl=0.0555, act_lr=7.6e-7, ent=1.64]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18/28 [00:15<00:08,  1.17it/s, pg=-0.0651, ret=0.000264, glen=93.4, tlen=253, kl=0.0555, act_lr=7.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18/28 [00:16<00:08,  1.17it/s, pg=0.0115, ret=0.00312, glen=91.1, tlen=251, kl=0.0237, act_lr=7.6e-7, ent=1.89]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19/28 [00:16<00:07,  1.17it/s, pg=0.0115, ret=0.00312, glen=91.1, tlen=251, kl=0.0237, act_lr=7.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19/28 [00:17<00:07,  1.17it/s, pg=0.421, ret=-0.00207, glen=100, tlen=260, kl=0.0291, act_lr=7.6e-7, ent=1.81] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 20/28 [00:17<00:06,  1.16it/s, pg=0.421, ret=-0.00207, glen=100, tlen=260, kl=0.0291, act_lr=7.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 20/28 [00:18<00:06,  1.16it/s, pg=-0.153, ret=0.000543, glen=91.3, tlen=251, kl=0.0314, act_lr=7.6e-7, ent=1.68]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 21/28 [00:18<00:06,  1.16it/s, pg=-0.153, ret=0.000543, glen=91.3, tlen=251, kl=0.0314, act_lr=7.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 21/28 [00:19<00:06,  1.16it/s, pg=-0.00526, ret=-0.000288, glen=85.5, tlen=245, kl=0.0354, act_lr=7.6e-7, ent=1.56]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 22/28 [00:19<00:05,  1.16it/s, pg=-0.00526, ret=-0.000288, glen=85.5, tlen=245, kl=0.0354, act_lr=7.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 22/28 [00:19<00:05,  1.16it/s, pg=0.314, ret=6.34e-6, glen=161, tlen=321, kl=0.0225, act_lr=7.6e-7, ent=2.12]      Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 23/28 [00:19<00:04,  1.16it/s, pg=0.314, ret=6.34e-6, glen=161, tlen=321, kl=0.0225, act_lr=7.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 23/28 [00:20<00:04,  1.16it/s, pg=0.136, ret=0.000121, glen=102, tlen=261, kl=0.0285, act_lr=7.6e-7, ent=1.95]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 24/28 [00:20<00:03,  1.16it/s, pg=0.136, ret=0.000121, glen=102, tlen=261, kl=0.0285, act_lr=7.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 24/28 [00:21<00:03,  1.16it/s, pg=0.00516, ret=-0.00024, glen=89.3, tlen=249, kl=0.033, act_lr=7.6e-7, ent=1.65]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 25/28 [00:21<00:02,  1.16it/s, pg=0.00516, ret=-0.00024, glen=89.3, tlen=249, kl=0.033, act_lr=7.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 25/28 [00:22<00:02,  1.16it/s, pg=-0.025, ret=2.07e-5, glen=85.8, tlen=245, kl=0.0342, act_lr=7.6e-7, ent=1.67] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/28 [00:22<00:01,  1.17it/s, pg=-0.025, ret=2.07e-5, glen=85.8, tlen=245, kl=0.0342, act_lr=7.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/28 [00:23<00:01,  1.17it/s, pg=-0.166, ret=0.000792, glen=92.7, tlen=252, kl=0.0392, act_lr=7.6e-7, ent=1.51]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 27/28 [00:23<00:00,  1.17it/s, pg=-0.166, ret=0.000792, glen=92.7, tlen=252, kl=0.0392, act_lr=7.6e-7, ent=1.51]
2025-07-24 18:27:31.629 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 24.33s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 27/28 [00:24<00:00,  1.17it/s, pg=-0.111, ret=0.000497, glen=95.8, tlen=256, kl=0.038, act_lr=7.8e-7, ent=1.55] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 27/28 [00:24<00:00,  1.11it/s, pg=-0.111, ret=0.000497, glen=95.8, tlen=256, kl=0.038, act_lr=7.8e-7, ent=1.55]
2025-07-24 18:27:32.292 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 18:27:34.527 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.23s
2025-07-24 18:27:34.855 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 27.67s
2025-07-24 18:27:34.859 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.018938796860831126, 'actor_lr': 7.607142638984701e-07, 'clip_ratio': 0.0, 'entropy': 1.6960617814745222, 'kl': 0.07269777570452009, 'response_length': 96.82276780264718, 'total_length': 256.5611348833357, 'teacher_total_length': 268.3715700422014, 'return': 8.48982378686612e-05, 'policy_update_steps': 1.0}
Episode [3/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [47:14<00:00, 187.98s/it]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,981] [INFO] [utils.py:782:see_memory_usage] MA 2.22 GB         Max_MA 2.22 GB         CA 3.24 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,981] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.22 GB, percent = 21.7%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7cff9a7e0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,982] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,983] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True2025-07-24 18:27:43.722 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:43:28,717] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 17:40:19,586] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:47:27,391] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 17:59:56,742] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:04:01,516] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:08:07,354] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:11:21,650] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:15:21,644] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:18:40,032] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:21:44,486] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:25:15,470] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:27:31,622] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:41,559] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:41,745] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1356, num_elems = 7.11B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,258] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,258] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,265] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,266] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,520] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,521] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 7.91 GB         CA 3.97 GB         Max_CA 45 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,521] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.49 GB, percent = 21.8%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [4/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [3/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [47:23<00:00, 218.75s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,716] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,717] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 2.95 GB         CA 3.97 GB         Max_CA 4 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,717] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.48 GB, percent = 21.7%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7cc3ddc40>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,720] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 18:27:44.055 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:30:10.466 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:30:10.656 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 18:30:10.657 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 146.60s
2025-07-24 18:30:12.958 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0139,avg_reflection_pattern_score: 0.0115,avg_pass_at_n: 1.0000,avg_num_tokens: 103.4795,std_num_tokens: 148.8309,avg_correct_num_tokens: 99.8100,std_correct_num_tokens: 87.1656,avg_incorrect_num_tokens: 132.5556,std_incorrect_num_tokens: 369.4739
2025-07-24 18:30:13.409 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.75s
2025-07-24 18:30:16.426 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.02s
2025-07-24 18:30:44.888 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 219
2025-07-24 18:30:44.889 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.46s
2025-07-24 18:30:46.353 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.98s
2025-07-24 18:30:46.354 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0005056015158669606, avg_kl: 0.0, avg_response_length: 108.94455199916614, avg_orm_score: 0.0, avg_custom_rewards: -0.0005056015158669606
2025-07-24 18:30:46.388 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter39_replay_buffer.jsonl
2025-07-24 18:30:48.203 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.82s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:01<?, ?it/s, pg=-0.0677, ret=0.000157, glen=99.6, tlen=260, kl=0, act_lr=7.8e-7, ent=1.68]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<01:00,  1.11s/it, pg=-0.0677, ret=0.000157, glen=99.6, tlen=260, kl=0, act_lr=7.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<01:00,  1.11s/it, pg=0.423, ret=-0.0021, glen=122, tlen=282, kl=0, act_lr=7.8e-7, ent=2.4]     Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:51,  1.03it/s, pg=0.423, ret=-0.0021, glen=122, tlen=282, kl=0, act_lr=7.8e-7, ent=2.4]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:51,  1.03it/s, pg=-0.143, ret=0.000611, glen=99.4, tlen=259, kl=0, act_lr=7.8e-7, ent=1.71]Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:47,  1.09it/s, pg=-0.143, ret=0.000611, glen=99.4, tlen=259, kl=0, act_lr=7.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:47,  1.09it/s, pg=0.0132, ret=-0.000115, glen=106, tlen=267, kl=0, act_lr=7.8e-7, ent=1.9] Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.12it/s, pg=0.0132, ret=-0.000115, glen=106, tlen=267, kl=0, act_lr=7.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.12it/s, pg=-0.00195, ret=0.000272, glen=98.8, tlen=259, kl=0, act_lr=7.8e-7, ent=1.63]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:43,  1.14it/s, pg=-0.00195, ret=0.000272, glen=98.8, tlen=259, kl=0, act_lr=7.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:43,  1.14it/s, pg=-0.177, ret=0.000699, glen=98.1, tlen=258, kl=0, act_lr=7.8e-7, ent=1.6]   Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:42,  1.15it/s, pg=-0.177, ret=0.000699, glen=98.1, tlen=258, kl=0, act_lr=7.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:42,  1.15it/s, pg=0.041, ret=-0.000467, glen=98.8, tlen=259, kl=0, act_lr=7.8e-7, ent=1.62]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.16it/s, pg=0.041, ret=-0.000467, glen=98.8, tlen=259, kl=0, act_lr=7.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.16it/s, pg=-0.0388, ret=-0.00123, glen=122, tlen=283, kl=0, act_lr=7.8e-7, ent=1.38]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.0388, ret=-0.00123, glen=122, tlen=283, kl=0, act_lr=7.8e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.155, ret=0.000517, glen=101, tlen=261, kl=0, act_lr=7.8e-7, ent=1.69] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:39,  1.16it/s, pg=-0.155, ret=0.000517, glen=101, tlen=261, kl=0, act_lr=7.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.16it/s, pg=-0.161, ret=0.00032, glen=99.9, tlen=260, kl=0, act_lr=7.8e-7, ent=1.59]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.15it/s, pg=-0.161, ret=0.00032, glen=99.9, tlen=260, kl=0, act_lr=7.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.15it/s, pg=0.11, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=7.8e-7, ent=1.82]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.16it/s, pg=0.11, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=7.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.16it/s, pg=-0.085, ret=-0.000133, glen=93.9, tlen=255, kl=0, act_lr=7.8e-7, ent=1.64]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.13it/s, pg=-0.085, ret=-0.000133, glen=93.9, tlen=255, kl=0, act_lr=7.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.13it/s, pg=-0.0958, ret=0.000338, glen=95.3, tlen=256, kl=0, act_lr=7.8e-7, ent=1.51]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.15it/s, pg=-0.0958, ret=0.000338, glen=95.3, tlen=256, kl=0, act_lr=7.8e-7, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.15it/s, pg=-0.105, ret=0.00017, glen=104, tlen=264, kl=0, act_lr=7.8e-7, ent=1.78]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.16it/s, pg=-0.105, ret=0.00017, glen=104, tlen=264, kl=0, act_lr=7.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.16it/s, pg=-0.0701, ret=0.000202, glen=101, tlen=261, kl=0, act_lr=7.8e-7, ent=1.73]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.16it/s, pg=-0.0701, ret=0.000202, glen=101, tlen=261, kl=0, act_lr=7.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:14<00:34,  1.16it/s, pg=-0.0186, ret=0.000961, glen=101, tlen=261, kl=0, act_lr=7.8e-7, ent=1.65]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.0186, ret=0.000961, glen=101, tlen=261, kl=0, act_lr=7.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.00366, ret=-0.000157, glen=104, tlen=265, kl=0, act_lr=7.8e-7, ent=1.71]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=-0.00366, ret=-0.000157, glen=104, tlen=265, kl=0, act_lr=7.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=0.0756, ret=-0.000494, glen=99.4, tlen=260, kl=0, act_lr=7.8e-7, ent=1.58] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=0.0756, ret=-0.000494, glen=99.4, tlen=260, kl=0, act_lr=7.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=0.0332, ret=-0.000147, glen=96.7, tlen=257, kl=0, act_lr=7.8e-7, ent=1.71]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=0.0332, ret=-0.000147, glen=96.7, tlen=257, kl=0, act_lr=7.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=-0.0831, ret=-0.000436, glen=91.4, tlen=252, kl=0, act_lr=7.8e-7, ent=1.6]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=-0.0831, ret=-0.000436, glen=91.4, tlen=252, kl=0, act_lr=7.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=-0.0304, ret=5.11e-5, glen=97.2, tlen=258, kl=0, act_lr=7.8e-7, ent=1.74] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:28,  1.17it/s, pg=-0.0304, ret=5.11e-5, glen=97.2, tlen=258, kl=0, act_lr=7.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:28,  1.17it/s, pg=0.356, ret=0.000197, glen=143, tlen=304, kl=0, act_lr=7.8e-7, ent=2.23]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.16it/s, pg=0.356, ret=0.000197, glen=143, tlen=304, kl=0, act_lr=7.8e-7, ent=2.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:20<00:28,  1.16it/s, pg=0.0369, ret=-0.00115, glen=95.9, tlen=256, kl=0, act_lr=7.8e-7, ent=1.57]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=0.0369, ret=-0.00115, glen=95.9, tlen=256, kl=0, act_lr=7.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.141, ret=0.000396, glen=96.1, tlen=256, kl=0, act_lr=7.8e-7, ent=1.67]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.141, ret=0.000396, glen=96.1, tlen=256, kl=0, act_lr=7.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=-0.188, ret=0.000799, glen=94.6, tlen=254, kl=0, act_lr=7.8e-7, ent=1.54]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=-0.188, ret=0.000799, glen=94.6, tlen=254, kl=0, act_lr=7.8e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=0.0646, ret=-0.000452, glen=99.6, tlen=260, kl=0, act_lr=7.8e-7, ent=1.63]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=0.0646, ret=-0.000452, glen=99.6, tlen=260, kl=0, act_lr=7.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.074, ret=-0.00012, glen=95.6, tlen=256, kl=0, act_lr=7.8e-7, ent=1.53] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:23,  1.18it/s, pg=-0.074, ret=-0.00012, glen=95.6, tlen=256, kl=0, act_lr=7.8e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:23,  1.18it/s, pg=-0.112, ret=9.78e-5, glen=97.2, tlen=258, kl=0, act_lr=7.8e-7, ent=1.65] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:23,  1.17it/s, pg=-0.112, ret=9.78e-5, glen=97.2, tlen=258, kl=0, act_lr=7.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:23,  1.17it/s, pg=-0.239, ret=0.00118, glen=102, tlen=263, kl=0, act_lr=7.8e-7, ent=1.88] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:24,  1.07it/s, pg=-0.239, ret=0.00118, glen=102, tlen=263, kl=0, act_lr=7.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:24,  1.07it/s, pg=-0.212, ret=0.00127, glen=96.6, tlen=257, kl=0, act_lr=7.8e-7, ent=1.54]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.10it/s, pg=-0.212, ret=0.00127, glen=96.6, tlen=257, kl=0, act_lr=7.8e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.10it/s, pg=-0.0551, ret=-0.00173, glen=93, tlen=253, kl=0, act_lr=7.8e-7, ent=1.7] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=-0.0551, ret=-0.00173, glen=93, tlen=253, kl=0, act_lr=7.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.12it/s, pg=0.117, ret=-0.00116, glen=106, tlen=266, kl=0, act_lr=7.8e-7, ent=1.77]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:20,  1.14it/s, pg=0.117, ret=-0.00116, glen=106, tlen=266, kl=0, act_lr=7.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.14it/s, pg=-0.119, ret=0.000216, glen=97.1, tlen=257, kl=0, act_lr=7.8e-7, ent=1.59]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=-0.119, ret=0.000216, glen=97.1, tlen=257, kl=0, act_lr=7.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=-0.16, ret=0.000518, glen=106, tlen=266, kl=0, act_lr=7.8e-7, ent=1.79]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.16it/s, pg=-0.16, ret=0.000518, glen=106, tlen=266, kl=0, act_lr=7.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.16it/s, pg=-0.0453, ret=0.000388, glen=115, tlen=275, kl=0, act_lr=7.8e-7, ent=2.16]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.0453, ret=0.000388, glen=115, tlen=275, kl=0, act_lr=7.8e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=-0.102, ret=-0.00041, glen=104, tlen=264, kl=0, act_lr=7.8e-7, ent=1.67] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=-0.102, ret=-0.00041, glen=104, tlen=264, kl=0, act_lr=7.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=-0.115, ret=0.000518, glen=116, tlen=277, kl=0, act_lr=7.8e-7, ent=1.83]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.115, ret=0.000518, glen=116, tlen=277, kl=0, act_lr=7.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.17it/s, pg=-0.0913, ret=0.000303, glen=92.3, tlen=253, kl=0, act_lr=7.8e-7, ent=1.57]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.0913, ret=0.000303, glen=92.3, tlen=253, kl=0, act_lr=7.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.127, ret=0.000264, glen=102, tlen=263, kl=0, act_lr=7.8e-7, ent=1.63]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.17it/s, pg=-0.127, ret=0.000264, glen=102, tlen=263, kl=0, act_lr=7.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0662, ret=-0.00041, glen=107, tlen=267, kl=0, act_lr=7.8e-7, ent=1.76]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.0662, ret=-0.00041, glen=107, tlen=267, kl=0, act_lr=7.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=-0.0622, ret=0.000143, glen=111, tlen=271, kl=0, act_lr=7.8e-7, ent=1.92]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=-0.0622, ret=0.000143, glen=111, tlen=271, kl=0, act_lr=7.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=-0.173, ret=0.000446, glen=93.9, tlen=255, kl=0, act_lr=7.8e-7, ent=1.63]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=-0.173, ret=0.000446, glen=93.9, tlen=255, kl=0, act_lr=7.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=0.538, ret=-0.00153, glen=145, tlen=305, kl=0, act_lr=7.8e-7, ent=2]     Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.16it/s, pg=0.538, ret=-0.00153, glen=145, tlen=305, kl=0, act_lr=7.8e-7, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.16it/s, pg=-0.193, ret=0.000854, glen=87.1, tlen=247, kl=0, act_lr=7.8e-7, ent=1.65]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.15it/s, pg=-0.193, ret=0.000854, glen=87.1, tlen=247, kl=0, act_lr=7.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.15it/s, pg=-0.129, ret=0.000568, glen=107, tlen=267, kl=0, act_lr=7.8e-7, ent=1.77] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.16it/s, pg=-0.129, ret=0.000568, glen=107, tlen=267, kl=0, act_lr=7.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.16it/s, pg=-0.148, ret=4.38e-6, glen=94.7, tlen=255, kl=0, act_lr=7.8e-7, ent=1.71]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.16it/s, pg=-0.148, ret=4.38e-6, glen=94.7, tlen=255, kl=0, act_lr=7.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.16it/s, pg=-0.13, ret=0.000473, glen=111, tlen=272, kl=0, act_lr=7.8e-7, ent=1.85] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.16it/s, pg=-0.13, ret=0.000473, glen=111, tlen=272, kl=0, act_lr=7.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.16it/s, pg=-0.0273, ret=-0.000446, glen=95.6, tlen=256, kl=0, act_lr=7.8e-7, ent=1.65]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:06,  1.17it/s, pg=-0.0273, ret=-0.000446, glen=95.6, tlen=256, kl=0, act_lr=7.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:06,  1.17it/s, pg=-0.0809, ret=-0.000284, glen=86.8, tlen=247, kl=0, act_lr=7.8e-7, ent=1.59]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=-0.0809, ret=-0.000284, glen=86.8, tlen=247, kl=0, act_lr=7.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=0.346, ret=0.000205, glen=139, tlen=300, kl=0, act_lr=7.8e-7, ent=1.82]    Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.16it/s, pg=0.346, ret=0.000205, glen=139, tlen=300, kl=0, act_lr=7.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.16it/s, pg=-0.163, ret=0.000281, glen=105, tlen=266, kl=0, act_lr=7.8e-7, ent=1.74]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.16it/s, pg=-0.163, ret=0.000281, glen=105, tlen=266, kl=0, act_lr=7.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.16it/s, pg=-0.0582, ret=-0.000231, glen=107, tlen=268, kl=0, act_lr=7.8e-7, ent=1.84]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.0582, ret=-0.000231, glen=107, tlen=268, kl=0, act_lr=7.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:46<00:02,  1.17it/s, pg=0.458, ret=0.000385, glen=394, tlen=555, kl=0, act_lr=7.8e-7, ent=1.83]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.13it/s, pg=0.458, ret=0.000385, glen=394, tlen=555, kl=0, act_lr=7.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.13it/s, pg=0.0234, ret=-0.00103, glen=106, tlen=267, kl=0, act_lr=7.8e-7, ent=1.67]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.15it/s, pg=0.0234, ret=-0.00103, glen=106, tlen=267, kl=0, act_lr=7.8e-7, ent=1.67]
2025-07-24 18:31:36.234 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 47.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.15it/s, pg=0.112, ret=-0.00103, glen=108, tlen=268, kl=0, act_lr=8e-7, ent=1.62]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=0.112, ret=-0.00103, glen=108, tlen=268, kl=0, act_lr=8e-7, ent=1.62]
2025-07-24 18:31:37.059 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 18:31:39.655 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 18:31:40.003 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 51.74s
2025-07-24 18:31:40.010 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.02724491466175426, 'actor_lr': 7.803636301353188e-07, 'clip_ratio': 0.0, 'entropy': 1.7210500066930599, 'kl': 0.0, 'response_length': 108.89970952814275, 'total_length': 269.1976318359375, 'teacher_total_length': 282.40222722833806, 'return': -4.739125814177731e-05, 'policy_update_steps': 1.0}

Episode [4/20]:   8%|‚ñä         | 1/13 [03:56<47:15, 236.29s/it][A2025-07-24 18:31:40.020 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   1%|          | 1/172 [00:00<01:28,  1.94it/s, est. speed input: 343.11 toks/s, output: 32.95 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/172 [00:03<00:00, 52.24it/s, est. speed input: 8072.13 toks/s, output: 3273.87 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:04<00:00, 27.77it/s, est. speed input: 6575.78 toks/s, output: 3021.02 toks/s][32m [repeated 94x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:06<00:00,  7.03it/s, est. speed input: 4976.62 toks/s, output: 2693.81 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:06<00:00, 27.45it/s, est. speed input: 4976.62 toks/s, output: 2693.81 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:09<00:00, 18.08it/s, est. speed input: 3274.84 toks/s, output: 1595.12 toks/s][32m [repeated 15x across cluster][0m
2025-07-24 18:31:50.826 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 424.5328,strategyqa_test/accuracy: 0.5255,eval_accuracy: 0.5255
2025-07-24 18:31:51.105 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:34:13.014 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:34:13.203 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 18:34:13.204 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 142.10s
2025-07-24 18:34:15.134 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0136,avg_reflection_pattern_score: 0.0105,avg_pass_at_n: 1.0000,avg_num_tokens: 98.4158,std_num_tokens: 121.7519,avg_correct_num_tokens: 96.0879,std_correct_num_tokens: 78.8803,avg_incorrect_num_tokens: 117.0671,std_incorrect_num_tokens: 288.7002
2025-07-24 18:34:15.588 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 18:34:18.871 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.28s
2025-07-24 18:34:46.351 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 214
2025-07-24 18:34:46.352 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.48s
2025-07-24 18:34:47.725 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.96s
2025-07-24 18:34:47.726 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.001014440770725563, avg_kl: 0.001335839244806878, avg_response_length: 102.93667673841816, avg_orm_score: 0.0, avg_custom_rewards: -0.001014440770725563
2025-07-24 18:34:47.781 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter40_replay_buffer.jsonl
2025-07-24 18:34:49.514 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.73s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:09<00:00, 17.31it/s, est. speed input: 3146.30 toks/s, output: 1718.41 toks/s][32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:01<?, ?it/s, pg=-0.0652, ret=0.000725, glen=106, tlen=265, kl=0.00127, act_lr=8e-7, ent=1.69]Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:01<00:53,  1.02s/it, pg=-0.0652, ret=0.000725, glen=106, tlen=265, kl=0.00127, act_lr=8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:01<00:53,  1.02s/it, pg=-0.0813, ret=2.75e-5, glen=103, tlen=263, kl=0.00128, act_lr=8e-7, ent=1.77] Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:01<00:48,  1.08it/s, pg=-0.0813, ret=2.75e-5, glen=103, tlen=263, kl=0.00128, act_lr=8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:02<00:48,  1.08it/s, pg=-0.0157, ret=0.000595, glen=92.7, tlen=252, kl=0.00134, act_lr=8e-7, ent=1.61]Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:02<00:47,  1.08it/s, pg=-0.0157, ret=0.000595, glen=92.7, tlen=252, kl=0.00134, act_lr=8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:03<00:47,  1.08it/s, pg=-0.00868, ret=-0.000748, glen=102, tlen=262, kl=0.00134, act_lr=8e-7, ent=1.75]Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:03<00:45,  1.11it/s, pg=-0.00868, ret=-0.000748, glen=102, tlen=262, kl=0.00134, act_lr=8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:04<00:45,  1.11it/s, pg=-0.0449, ret=3.31e-5, glen=93.3, tlen=253, kl=0.00127, act_lr=8e-7, ent=1.63]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:04<00:43,  1.13it/s, pg=-0.0449, ret=3.31e-5, glen=93.3, tlen=253, kl=0.00127, act_lr=8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:05<00:43,  1.13it/s, pg=0.168, ret=-0.000642, glen=94.4, tlen=254, kl=0.00135, act_lr=8e-7, ent=1.73]Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:05<00:42,  1.14it/s, pg=0.168, ret=-0.000642, glen=94.4, tlen=254, kl=0.00135, act_lr=8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:06<00:42,  1.14it/s, pg=0.0276, ret=-0.000792, glen=92.1, tlen=251, kl=0.00142, act_lr=8e-7, ent=1.75]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:06<00:40,  1.15it/s, pg=0.0276, ret=-0.000792, glen=92.1, tlen=251, kl=0.00142, act_lr=8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:07<00:40,  1.15it/s, pg=0.0174, ret=-0.000653, glen=93, tlen=253, kl=0.00137, act_lr=8e-7, ent=1.64]  Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:07<00:40,  1.14it/s, pg=0.0174, ret=-0.000653, glen=93, tlen=253, kl=0.00137, act_lr=8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:08<00:40,  1.14it/s, pg=0.741, ret=-0.00566, glen=347, tlen=507, kl=0.00113, act_lr=8e-7, ent=2.59] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:08<00:40,  1.10it/s, pg=0.741, ret=-0.00566, glen=347, tlen=507, kl=0.00113, act_lr=8e-7, ent=2.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:08<00:40,  1.10it/s, pg=0.0659, ret=-0.000517, glen=102, tlen=261, kl=0.00132, act_lr=8e-7, ent=1.77]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:08<00:39,  1.12it/s, pg=0.0659, ret=-0.000517, glen=102, tlen=261, kl=0.00132, act_lr=8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:09<00:39,  1.12it/s, pg=0.0526, ret=-0.000935, glen=94.3, tlen=254, kl=0.00142, act_lr=8e-7, ent=1.74]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:09<00:37,  1.14it/s, pg=0.0526, ret=-0.000935, glen=94.3, tlen=254, kl=0.00142, act_lr=8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:10<00:37,  1.14it/s, pg=-0.102, ret=0.000906, glen=96.7, tlen=257, kl=0.00139, act_lr=8e-7, ent=1.58] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:10<00:36,  1.15it/s, pg=-0.102, ret=0.000906, glen=96.7, tlen=257, kl=0.00139, act_lr=8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:11<00:36,  1.15it/s, pg=0.0652, ret=-0.000927, glen=88.4, tlen=248, kl=0.0014, act_lr=8e-7, ent=1.61]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:11<00:35,  1.16it/s, pg=0.0652, ret=-0.000927, glen=88.4, tlen=248, kl=0.0014, act_lr=8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:12<00:35,  1.16it/s, pg=0.0661, ret=-0.000692, glen=102, tlen=262, kl=0.00133, act_lr=8e-7, ent=1.75]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:12<00:34,  1.15it/s, pg=0.0661, ret=-0.000692, glen=102, tlen=262, kl=0.00133, act_lr=8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:13<00:34,  1.15it/s, pg=-0.173, ret=0.00102, glen=100, tlen=260, kl=0.00134, act_lr=8e-7, ent=1.71]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:13<00:33,  1.15it/s, pg=-0.173, ret=0.00102, glen=100, tlen=260, kl=0.00134, act_lr=8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:14<00:33,  1.15it/s, pg=-0.038, ret=0.0004, glen=96.7, tlen=256, kl=0.00133, act_lr=8e-7, ent=1.71]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:14<00:32,  1.16it/s, pg=-0.038, ret=0.0004, glen=96.7, tlen=256, kl=0.00133, act_lr=8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:14<00:32,  1.16it/s, pg=-0.107, ret=-3.62e-7, glen=86.6, tlen=246, kl=0.00128, act_lr=8e-7, ent=1.57]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:14<00:31,  1.17it/s, pg=-0.107, ret=-3.62e-7, glen=86.6, tlen=246, kl=0.00128, act_lr=8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:15<00:31,  1.17it/s, pg=-0.173, ret=0.000955, glen=98.2, tlen=258, kl=0.0014, act_lr=8e-7, ent=1.76] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:15<00:30,  1.17it/s, pg=-0.173, ret=0.000955, glen=98.2, tlen=258, kl=0.0014, act_lr=8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:16<00:30,  1.17it/s, pg=-0.142, ret=0.000484, glen=84.9, tlen=245, kl=0.00134, act_lr=8e-7, ent=1.65]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:16<00:29,  1.17it/s, pg=-0.142, ret=0.000484, glen=84.9, tlen=245, kl=0.00134, act_lr=8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:17<00:29,  1.17it/s, pg=-0.0894, ret=-6.74e-5, glen=106, tlen=266, kl=0.0013, act_lr=8e-7, ent=1.97] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:17<00:29,  1.17it/s, pg=-0.0894, ret=-6.74e-5, glen=106, tlen=266, kl=0.0013, act_lr=8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:18<00:29,  1.17it/s, pg=0.14, ret=-0.0024, glen=105, tlen=265, kl=0.00135, act_lr=8e-7, ent=1.6]    Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:18<00:28,  1.17it/s, pg=0.14, ret=-0.0024, glen=105, tlen=265, kl=0.00135, act_lr=8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:19<00:28,  1.17it/s, pg=-0.101, ret=0.000177, glen=101, tlen=261, kl=0.00137, act_lr=8e-7, ent=1.71]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:19<00:27,  1.17it/s, pg=-0.101, ret=0.000177, glen=101, tlen=261, kl=0.00137, act_lr=8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:20<00:27,  1.17it/s, pg=-0.0818, ret=-0.000385, glen=93.8, tlen=254, kl=0.00133, act_lr=8e-7, ent=1.61]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:20<00:26,  1.17it/s, pg=-0.0818, ret=-0.000385, glen=93.8, tlen=254, kl=0.00133, act_lr=8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:20<00:26,  1.17it/s, pg=-0.0254, ret=0.000668, glen=103, tlen=263, kl=0.00132, act_lr=8e-7, ent=1.69]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:20<00:25,  1.17it/s, pg=-0.0254, ret=0.000668, glen=103, tlen=263, kl=0.00132, act_lr=8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:21<00:25,  1.17it/s, pg=-0.027, ret=0.000124, glen=92.8, tlen=253, kl=0.00132, act_lr=8e-7, ent=1.63]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:21<00:24,  1.17it/s, pg=-0.027, ret=0.000124, glen=92.8, tlen=253, kl=0.00132, act_lr=8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:22<00:24,  1.17it/s, pg=0.0242, ret=-0.000446, glen=91.7, tlen=251, kl=0.00134, act_lr=8e-7, ent=1.68]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:22<00:23,  1.18it/s, pg=0.0242, ret=-0.000446, glen=91.7, tlen=251, kl=0.00134, act_lr=8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:23<00:23,  1.18it/s, pg=0.00562, ret=3.67e-5, glen=98.9, tlen=258, kl=0.00134, act_lr=8e-7, ent=1.68] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:23<00:22,  1.18it/s, pg=0.00562, ret=3.67e-5, glen=98.9, tlen=258, kl=0.00134, act_lr=8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:24<00:22,  1.18it/s, pg=-0.153, ret=0.000885, glen=99.9, tlen=260, kl=0.0014, act_lr=8e-7, ent=1.72] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:24<00:22,  1.18it/s, pg=-0.153, ret=0.000885, glen=99.9, tlen=260, kl=0.0014, act_lr=8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:25<00:22,  1.18it/s, pg=-0.155, ret=0.000604, glen=102, tlen=262, kl=0.0013, act_lr=8e-7, ent=1.72] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:25<00:23,  1.07it/s, pg=-0.155, ret=0.000604, glen=102, tlen=262, kl=0.0013, act_lr=8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:26<00:23,  1.07it/s, pg=0.158, ret=-0.000466, glen=95.6, tlen=255, kl=0.00132, act_lr=8e-7, ent=1.77]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:26<00:21,  1.10it/s, pg=0.158, ret=-0.000466, glen=95.6, tlen=255, kl=0.00132, act_lr=8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:27<00:21,  1.10it/s, pg=-0.033, ret=-0.000337, glen=104, tlen=264, kl=0.00132, act_lr=8e-7, ent=1.85]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:27<00:20,  1.12it/s, pg=-0.033, ret=-0.000337, glen=104, tlen=264, kl=0.00132, act_lr=8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:27<00:20,  1.12it/s, pg=0.0165, ret=-0.000794, glen=90.2, tlen=251, kl=0.0014, act_lr=8e-7, ent=1.63]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:27<00:19,  1.14it/s, pg=0.0165, ret=-0.000794, glen=90.2, tlen=251, kl=0.0014, act_lr=8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:28<00:19,  1.14it/s, pg=-0.0517, ret=0.000231, glen=99.9, tlen=259, kl=0.00131, act_lr=8e-7, ent=1.6]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:28<00:18,  1.15it/s, pg=-0.0517, ret=0.000231, glen=99.9, tlen=259, kl=0.00131, act_lr=8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:29<00:18,  1.15it/s, pg=-0.0235, ret=-5.15e-5, glen=91, tlen=251, kl=0.00142, act_lr=8e-7, ent=1.54] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:29<00:17,  1.16it/s, pg=-0.0235, ret=-5.15e-5, glen=91, tlen=251, kl=0.00142, act_lr=8e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:30<00:17,  1.16it/s, pg=-0.223, ret=0.00133, glen=101, tlen=261, kl=0.00134, act_lr=8e-7, ent=1.64] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:30<00:16,  1.16it/s, pg=-0.223, ret=0.00133, glen=101, tlen=261, kl=0.00134, act_lr=8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:31<00:16,  1.16it/s, pg=-0.181, ret=-2.38e-7, glen=119, tlen=278, kl=0.00122, act_lr=8e-7, ent=2.23]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:31<00:15,  1.16it/s, pg=-0.181, ret=-2.38e-7, glen=119, tlen=278, kl=0.00122, act_lr=8e-7, ent=2.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:32<00:15,  1.16it/s, pg=-0.0491, ret=9.87e-5, glen=96.9, tlen=257, kl=0.0014, act_lr=8e-7, ent=1.72]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:32<00:14,  1.17it/s, pg=-0.0491, ret=9.87e-5, glen=96.9, tlen=257, kl=0.0014, act_lr=8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:33<00:14,  1.17it/s, pg=0.431, ret=-0.000701, glen=124, tlen=284, kl=0.00122, act_lr=8e-7, ent=2.04]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:33<00:13,  1.17it/s, pg=0.431, ret=-0.000701, glen=124, tlen=284, kl=0.00122, act_lr=8e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:33<00:13,  1.17it/s, pg=-0.0511, ret=-0.00024, glen=95.1, tlen=255, kl=0.00141, act_lr=8e-7, ent=1.57]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:33<00:12,  1.17it/s, pg=-0.0511, ret=-0.00024, glen=95.1, tlen=255, kl=0.00141, act_lr=8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:34<00:12,  1.17it/s, pg=-0.0639, ret=-5.69e-5, glen=97.8, tlen=258, kl=0.00127, act_lr=8e-7, ent=1.67]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:34<00:11,  1.17it/s, pg=-0.0639, ret=-5.69e-5, glen=97.8, tlen=258, kl=0.00127, act_lr=8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:35<00:11,  1.17it/s, pg=-0.0847, ret=0.000405, glen=103, tlen=263, kl=0.00138, act_lr=8e-7, ent=1.74] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:35<00:11,  1.17it/s, pg=-0.0847, ret=0.000405, glen=103, tlen=263, kl=0.00138, act_lr=8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:36<00:11,  1.17it/s, pg=0.354, ret=-0.000844, glen=109, tlen=269, kl=0.0013, act_lr=8e-7, ent=1.48]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:36<00:10,  1.17it/s, pg=0.354, ret=-0.000844, glen=109, tlen=269, kl=0.0013, act_lr=8e-7, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:37<00:10,  1.17it/s, pg=-0.068, ret=-0.000174, glen=93.4, tlen=253, kl=0.00143, act_lr=8e-7, ent=1.68]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:37<00:09,  1.17it/s, pg=-0.068, ret=-0.000174, glen=93.4, tlen=253, kl=0.00143, act_lr=8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:38<00:09,  1.17it/s, pg=-0.0011, ret=-0.000855, glen=95.9, tlen=256, kl=0.00135, act_lr=8e-7, ent=1.68]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:38<00:08,  1.17it/s, pg=-0.0011, ret=-0.000855, glen=95.9, tlen=256, kl=0.00135, act_lr=8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:39<00:08,  1.17it/s, pg=0.12, ret=-0.000206, glen=105, tlen=265, kl=0.00136, act_lr=8e-7, ent=1.74]    Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:39<00:07,  1.17it/s, pg=0.12, ret=-0.000206, glen=105, tlen=265, kl=0.00136, act_lr=8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:39<00:07,  1.17it/s, pg=-0.132, ret=0.000386, glen=88.9, tlen=249, kl=0.00145, act_lr=8e-7, ent=1.55]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:39<00:06,  1.17it/s, pg=-0.132, ret=0.000386, glen=88.9, tlen=249, kl=0.00145, act_lr=8e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:40<00:06,  1.17it/s, pg=-0.157, ret=0.000288, glen=96.3, tlen=256, kl=0.00132, act_lr=8e-7, ent=1.6] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:40<00:05,  1.17it/s, pg=-0.157, ret=0.000288, glen=96.3, tlen=256, kl=0.00132, act_lr=8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:41<00:05,  1.17it/s, pg=-0.0513, ret=0.000492, glen=115, tlen=274, kl=0.00128, act_lr=8e-7, ent=1.82]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:41<00:05,  1.17it/s, pg=-0.0513, ret=0.000492, glen=115, tlen=274, kl=0.00128, act_lr=8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:42<00:05,  1.17it/s, pg=-0.162, ret=0.000382, glen=93.9, tlen=254, kl=0.00134, act_lr=8e-7, ent=1.67]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:42<00:04,  1.17it/s, pg=-0.162, ret=0.000382, glen=93.9, tlen=254, kl=0.00134, act_lr=8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:43<00:04,  1.17it/s, pg=-0.0842, ret=0.000128, glen=92.2, tlen=252, kl=0.00126, act_lr=8e-7, ent=1.78]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:43<00:03,  1.17it/s, pg=-0.0842, ret=0.000128, glen=92.2, tlen=252, kl=0.00126, act_lr=8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:44<00:03,  1.17it/s, pg=-0.017, ret=-0.000436, glen=99.4, tlen=259, kl=0.00136, act_lr=8e-7, ent=1.64]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:44<00:02,  1.17it/s, pg=-0.017, ret=-0.000436, glen=99.4, tlen=259, kl=0.00136, act_lr=8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:45<00:02,  1.17it/s, pg=-0.0305, ret=0.000345, glen=89.7, tlen=249, kl=0.00143, act_lr=8e-7, ent=1.58]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:45<00:01,  1.18it/s, pg=-0.0305, ret=0.000345, glen=89.7, tlen=249, kl=0.00143, act_lr=8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:45<00:01,  1.18it/s, pg=-0.255, ret=0.00142, glen=91.2, tlen=251, kl=0.00128, act_lr=8e-7, ent=1.59]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:45<00:00,  1.17it/s, pg=-0.255, ret=0.00142, glen=91.2, tlen=251, kl=0.00128, act_lr=8e-7, ent=1.59]
2025-07-24 18:35:36.541 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 46.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:46<00:00,  1.17it/s, pg=-0.0747, ret=-0.000455, glen=104, tlen=264, kl=0.00127, act_lr=8.2e-7, ent=1.95]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:46<00:00,  1.13it/s, pg=-0.0747, ret=-0.000455, glen=104, tlen=264, kl=0.00127, act_lr=8.2e-7, ent=1.95]
2025-07-24 18:35:37.371 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 18:35:39.963 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 18:35:40.304 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 50.73s
2025-07-24 18:35:40.311 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01710704520896629, 'actor_lr': 8.003703800130568e-07, 'clip_ratio': 0.0, 'entropy': 1.717871712313758, 'kl': 0.0013358681290237993, 'response_length': 102.92946596498842, 'total_length': 262.7645331488715, 'teacher_total_length': 274.56021005135995, 'return': -0.0001357672158014288, 'policy_update_steps': 1.0}

Episode [4/20]:  15%|‚ñà‚ñå        | 2/13 [07:56<43:45, 238.65s/it][A2025-07-24 18:35:40.354 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:38:11.681 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:38:11.864 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:38:11.864 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 151.51s
2025-07-24 18:38:13.755 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0135,avg_reflection_pattern_score: 0.0085,avg_pass_at_n: 1.0000,avg_num_tokens: 93.1266,std_num_tokens: 113.2958,avg_correct_num_tokens: 90.9427,std_correct_num_tokens: 57.2014,avg_incorrect_num_tokens: 116.4636,std_incorrect_num_tokens: 338.2930
2025-07-24 18:38:14.211 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.35s
2025-07-24 18:38:17.122 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.91s
2025-07-24 18:38:44.167 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 209
2025-07-24 18:38:44.168 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.04s
2025-07-24 18:38:45.494 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.90s
2025-07-24 18:38:45.494 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008407249260230355, avg_kl: 0.005599911703447406, avg_response_length: 97.8304187099329, avg_orm_score: 0.0, avg_custom_rewards: -0.0008407249260230355
2025-07-24 18:38:45.529 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter41_replay_buffer.jsonl
2025-07-24 18:38:47.205 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.68s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/53 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/53 [00:01<?, ?it/s, pg=0.047, ret=-0.000811, glen=89, tlen=248, kl=0.00583, act_lr=8.2e-7, ent=1.36]Actor Train epoch [1/1]:   2%|‚ñè         | 1/53 [00:01<00:53,  1.02s/it, pg=0.047, ret=-0.000811, glen=89, tlen=248, kl=0.00583, act_lr=8.2e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/53 [00:01<00:53,  1.02s/it, pg=-0.0686, ret=-0.000473, glen=97.5, tlen=257, kl=0.00523, act_lr=8.2e-7, ent=1.47]Actor Train epoch [1/1]:   4%|‚ñç         | 2/53 [00:01<00:47,  1.08it/s, pg=-0.0686, ret=-0.000473, glen=97.5, tlen=257, kl=0.00523, act_lr=8.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/53 [00:02<00:47,  1.08it/s, pg=0.104, ret=3.5e-5, glen=89.1, tlen=248, kl=0.00548, act_lr=8.2e-7, ent=1.59]     Actor Train epoch [1/1]:   6%|‚ñå         | 3/53 [00:02<00:44,  1.12it/s, pg=0.104, ret=3.5e-5, glen=89.1, tlen=248, kl=0.00548, act_lr=8.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/53 [00:03<00:44,  1.12it/s, pg=-0.0179, ret=0.0006, glen=93.6, tlen=254, kl=0.00471, act_lr=8.2e-7, ent=1.36]Actor Train epoch [1/1]:   8%|‚ñä         | 4/53 [00:03<00:43,  1.12it/s, pg=-0.0179, ret=0.0006, glen=93.6, tlen=254, kl=0.00471, act_lr=8.2e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/53 [00:04<00:43,  1.12it/s, pg=0.0489, ret=-0.000398, glen=92.3, tlen=252, kl=0.006, act_lr=8.2e-7, ent=1.52]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/53 [00:04<00:43,  1.11it/s, pg=0.0489, ret=-0.000398, glen=92.3, tlen=252, kl=0.006, act_lr=8.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/53 [00:05<00:43,  1.11it/s, pg=-0.218, ret=0.000854, glen=86.7, tlen=246, kl=0.00616, act_lr=8.2e-7, ent=1.39]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 6/53 [00:05<00:42,  1.11it/s, pg=-0.218, ret=0.000854, glen=86.7, tlen=246, kl=0.00616, act_lr=8.2e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 6/53 [00:06<00:42,  1.11it/s, pg=-0.116, ret=0.000756, glen=87.3, tlen=247, kl=0.00642, act_lr=8.2e-7, ent=1.45]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/53 [00:06<00:40,  1.13it/s, pg=-0.116, ret=0.000756, glen=87.3, tlen=247, kl=0.00642, act_lr=8.2e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/53 [00:07<00:40,  1.13it/s, pg=-0.027, ret=0.000205, glen=90.1, tlen=250, kl=0.00648, act_lr=8.2e-7, ent=1.47]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/53 [00:07<00:40,  1.11it/s, pg=-0.027, ret=0.000205, glen=90.1, tlen=250, kl=0.00648, act_lr=8.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/53 [00:08<00:40,  1.11it/s, pg=0.0959, ret=-0.00013, glen=91.5, tlen=251, kl=0.00548, act_lr=8.2e-7, ent=1.44]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/53 [00:08<00:38,  1.13it/s, pg=0.0959, ret=-0.00013, glen=91.5, tlen=251, kl=0.00548, act_lr=8.2e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/53 [00:08<00:38,  1.13it/s, pg=-0.0422, ret=-8.74e-5, glen=91.5, tlen=251, kl=0.00561, act_lr=8.2e-7, ent=1.54]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/53 [00:08<00:37,  1.14it/s, pg=-0.0422, ret=-8.74e-5, glen=91.5, tlen=251, kl=0.00561, act_lr=8.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/53 [00:09<00:37,  1.14it/s, pg=-0.0515, ret=-0.000111, glen=99, tlen=259, kl=0.00539, act_lr=8.2e-7, ent=1.86] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/53 [00:09<00:36,  1.15it/s, pg=-0.0515, ret=-0.000111, glen=99, tlen=259, kl=0.00539, act_lr=8.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/53 [00:10<00:36,  1.15it/s, pg=0.03, ret=-0.000642, glen=91.9, tlen=251, kl=0.00555, act_lr=8.2e-7, ent=1.52] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/53 [00:10<00:35,  1.16it/s, pg=0.03, ret=-0.000642, glen=91.9, tlen=251, kl=0.00555, act_lr=8.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/53 [00:11<00:35,  1.16it/s, pg=-0.0812, ret=-0.000314, glen=95, tlen=255, kl=0.00613, act_lr=8.2e-7, ent=1.48]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 13/53 [00:11<00:34,  1.16it/s, pg=-0.0812, ret=-0.000314, glen=95, tlen=255, kl=0.00613, act_lr=8.2e-7, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 13/53 [00:12<00:34,  1.16it/s, pg=-0.0168, ret=-0.000219, glen=94.8, tlen=255, kl=0.00573, act_lr=8.2e-7, ent=1.51]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 14/53 [00:12<00:33,  1.17it/s, pg=-0.0168, ret=-0.000219, glen=94.8, tlen=255, kl=0.00573, act_lr=8.2e-7, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 14/53 [00:13<00:33,  1.17it/s, pg=0.0225, ret=-0.000462, glen=91.1, tlen=251, kl=0.00613, act_lr=8.2e-7, ent=1.45] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/53 [00:13<00:32,  1.15it/s, pg=0.0225, ret=-0.000462, glen=91.1, tlen=251, kl=0.00613, act_lr=8.2e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/53 [00:14<00:32,  1.15it/s, pg=0.0157, ret=-5.9e-5, glen=87.8, tlen=248, kl=0.00536, act_lr=8.2e-7, ent=1.41]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 16/53 [00:14<00:31,  1.16it/s, pg=0.0157, ret=-5.9e-5, glen=87.8, tlen=248, kl=0.00536, act_lr=8.2e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 16/53 [00:14<00:31,  1.16it/s, pg=0.0438, ret=-0.000816, glen=89.4, tlen=249, kl=0.0052, act_lr=8.2e-7, ent=1.46]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 17/53 [00:14<00:30,  1.16it/s, pg=0.0438, ret=-0.000816, glen=89.4, tlen=249, kl=0.0052, act_lr=8.2e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 17/53 [00:15<00:30,  1.16it/s, pg=0.0305, ret=-0.00112, glen=89, tlen=248, kl=0.00588, act_lr=8.2e-7, ent=1.38]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 18/53 [00:15<00:30,  1.17it/s, pg=0.0305, ret=-0.00112, glen=89, tlen=248, kl=0.00588, act_lr=8.2e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 18/53 [00:16<00:30,  1.17it/s, pg=0.418, ret=-0.000665, glen=152, tlen=312, kl=0.00417, act_lr=8.2e-7, ent=1.85]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 19/53 [00:16<00:29,  1.16it/s, pg=0.418, ret=-0.000665, glen=152, tlen=312, kl=0.00417, act_lr=8.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 19/53 [00:17<00:29,  1.16it/s, pg=-0.122, ret=2.36e-5, glen=95.2, tlen=255, kl=0.00536, act_lr=8.2e-7, ent=1.63]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/53 [00:17<00:28,  1.16it/s, pg=-0.122, ret=2.36e-5, glen=95.2, tlen=255, kl=0.00536, act_lr=8.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/53 [00:18<00:28,  1.16it/s, pg=-0.149, ret=0.00041, glen=92.4, tlen=252, kl=0.00507, act_lr=8.2e-7, ent=1.43]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 21/53 [00:18<00:27,  1.17it/s, pg=-0.149, ret=0.00041, glen=92.4, tlen=252, kl=0.00507, act_lr=8.2e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 21/53 [00:19<00:27,  1.17it/s, pg=-0.0139, ret=1.57e-5, glen=87.7, tlen=247, kl=0.0064, act_lr=8.2e-7, ent=1.43]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/53 [00:19<00:26,  1.17it/s, pg=-0.0139, ret=1.57e-5, glen=87.7, tlen=247, kl=0.0064, act_lr=8.2e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/53 [00:20<00:26,  1.17it/s, pg=0.00522, ret=-0.000143, glen=94, tlen=254, kl=0.00635, act_lr=8.2e-7, ent=1.47]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/53 [00:20<00:25,  1.17it/s, pg=0.00522, ret=-0.000143, glen=94, tlen=254, kl=0.00635, act_lr=8.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/53 [00:20<00:25,  1.17it/s, pg=-0.206, ret=0.00127, glen=100, tlen=260, kl=0.00487, act_lr=8.2e-7, ent=1.58]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/53 [00:20<00:24,  1.17it/s, pg=-0.206, ret=0.00127, glen=100, tlen=260, kl=0.00487, act_lr=8.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/53 [00:21<00:24,  1.17it/s, pg=-0.0974, ret=0.000298, glen=88.5, tlen=248, kl=0.00605, act_lr=8.2e-7, ent=1.45]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/53 [00:21<00:23,  1.17it/s, pg=-0.0974, ret=0.000298, glen=88.5, tlen=248, kl=0.00605, act_lr=8.2e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/53 [00:22<00:23,  1.17it/s, pg=0.109, ret=-0.00113, glen=87.5, tlen=247, kl=0.00601, act_lr=8.2e-7, ent=1.53]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26/53 [00:22<00:22,  1.18it/s, pg=0.109, ret=-0.00113, glen=87.5, tlen=247, kl=0.00601, act_lr=8.2e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26/53 [00:23<00:22,  1.18it/s, pg=-0.0759, ret=0.000133, glen=88, tlen=247, kl=0.00564, act_lr=8.2e-7, ent=1.38]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/53 [00:23<00:22,  1.18it/s, pg=-0.0759, ret=0.000133, glen=88, tlen=247, kl=0.00564, act_lr=8.2e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/53 [00:24<00:22,  1.18it/s, pg=0.0167, ret=-0.000559, glen=93.4, tlen=253, kl=0.00508, act_lr=8.2e-7, ent=1.45]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28/53 [00:24<00:21,  1.18it/s, pg=0.0167, ret=-0.000559, glen=93.4, tlen=253, kl=0.00508, act_lr=8.2e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28/53 [00:25<00:21,  1.18it/s, pg=-0.214, ret=0.000731, glen=87, tlen=246, kl=0.00587, act_lr=8.2e-7, ent=1.46]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 29/53 [00:25<00:23,  1.01it/s, pg=-0.214, ret=0.000731, glen=87, tlen=246, kl=0.00587, act_lr=8.2e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 29/53 [00:26<00:23,  1.01it/s, pg=0.0251, ret=-0.000462, glen=96.5, tlen=256, kl=0.0065, act_lr=8.2e-7, ent=1.6]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30/53 [00:26<00:21,  1.06it/s, pg=0.0251, ret=-0.000462, glen=96.5, tlen=256, kl=0.0065, act_lr=8.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30/53 [00:27<00:21,  1.06it/s, pg=-0.0669, ret=-0.000243, glen=83.2, tlen=243, kl=0.00594, act_lr=8.2e-7, ent=1.48]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 31/53 [00:27<00:20,  1.09it/s, pg=-0.0669, ret=-0.000243, glen=83.2, tlen=243, kl=0.00594, act_lr=8.2e-7, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 31/53 [00:28<00:20,  1.09it/s, pg=-0.0908, ret=0.000248, glen=96.2, tlen=256, kl=0.00605, act_lr=8.2e-7, ent=1.46] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 32/53 [00:28<00:18,  1.12it/s, pg=-0.0908, ret=0.000248, glen=96.2, tlen=256, kl=0.00605, act_lr=8.2e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 32/53 [00:29<00:18,  1.12it/s, pg=-0.124, ret=0.000477, glen=91.3, tlen=251, kl=0.00636, act_lr=8.2e-7, ent=1.44] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 33/53 [00:29<00:17,  1.13it/s, pg=-0.124, ret=0.000477, glen=91.3, tlen=251, kl=0.00636, act_lr=8.2e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 33/53 [00:29<00:17,  1.13it/s, pg=-0.156, ret=0.000561, glen=95.7, tlen=255, kl=0.00526, act_lr=8.2e-7, ent=1.48]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34/53 [00:29<00:16,  1.15it/s, pg=-0.156, ret=0.000561, glen=95.7, tlen=255, kl=0.00526, act_lr=8.2e-7, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34/53 [00:30<00:16,  1.15it/s, pg=-0.0378, ret=-0.000548, glen=93.9, tlen=253, kl=0.00538, act_lr=8.2e-7, ent=1.56]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 35/53 [00:30<00:15,  1.15it/s, pg=-0.0378, ret=-0.000548, glen=93.9, tlen=253, kl=0.00538, act_lr=8.2e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 35/53 [00:31<00:15,  1.15it/s, pg=-0.0718, ret=-4.15e-5, glen=90.1, tlen=250, kl=0.0064, act_lr=8.2e-7, ent=1.55]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 36/53 [00:31<00:14,  1.16it/s, pg=-0.0718, ret=-4.15e-5, glen=90.1, tlen=250, kl=0.0064, act_lr=8.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 36/53 [00:32<00:14,  1.16it/s, pg=-0.0509, ret=0.000283, glen=93.1, tlen=252, kl=0.00523, act_lr=8.2e-7, ent=1.64]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 37/53 [00:32<00:13,  1.17it/s, pg=-0.0509, ret=0.000283, glen=93.1, tlen=252, kl=0.00523, act_lr=8.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 37/53 [00:33<00:13,  1.17it/s, pg=0.00623, ret=-0.000251, glen=87.5, tlen=248, kl=0.00597, act_lr=8.2e-7, ent=1.41]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38/53 [00:33<00:12,  1.17it/s, pg=0.00623, ret=-0.000251, glen=87.5, tlen=248, kl=0.00597, act_lr=8.2e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38/53 [00:34<00:12,  1.17it/s, pg=-0.204, ret=0.00058, glen=91, tlen=250, kl=0.00662, act_lr=8.2e-7, ent=1.47]     Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 39/53 [00:34<00:11,  1.17it/s, pg=-0.204, ret=0.00058, glen=91, tlen=250, kl=0.00662, act_lr=8.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 39/53 [00:34<00:11,  1.17it/s, pg=-0.00519, ret=-0.000219, glen=94.8, tlen=254, kl=0.00628, act_lr=8.2e-7, ent=1.37]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 40/53 [00:34<00:11,  1.17it/s, pg=-0.00519, ret=-0.000219, glen=94.8, tlen=254, kl=0.00628, act_lr=8.2e-7, ent=1.37]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 40/53 [00:35<00:11,  1.17it/s, pg=0.019, ret=-0.000601, glen=102, tlen=262, kl=0.00521, act_lr=8.2e-7, ent=1.59]    Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 41/53 [00:35<00:10,  1.17it/s, pg=0.019, ret=-0.000601, glen=102, tlen=262, kl=0.00521, act_lr=8.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 41/53 [00:36<00:10,  1.17it/s, pg=-0.1, ret=0.000384, glen=89.6, tlen=249, kl=0.00481, act_lr=8.2e-7, ent=1.41] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 42/53 [00:36<00:09,  1.17it/s, pg=-0.1, ret=0.000384, glen=89.6, tlen=249, kl=0.00481, act_lr=8.2e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 42/53 [00:37<00:09,  1.17it/s, pg=-0.155, ret=0.000995, glen=99.5, tlen=259, kl=0.00485, act_lr=8.2e-7, ent=1.66]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 43/53 [00:37<00:08,  1.17it/s, pg=-0.155, ret=0.000995, glen=99.5, tlen=259, kl=0.00485, act_lr=8.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 43/53 [00:38<00:08,  1.17it/s, pg=-0.0752, ret=0.000372, glen=98.4, tlen=258, kl=0.0047, act_lr=8.2e-7, ent=1.73]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 44/53 [00:38<00:07,  1.18it/s, pg=-0.0752, ret=0.000372, glen=98.4, tlen=258, kl=0.0047, act_lr=8.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 44/53 [00:39<00:07,  1.18it/s, pg=0.00323, ret=-0.000336, glen=87, tlen=247, kl=0.00566, act_lr=8.2e-7, ent=1.43]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 45/53 [00:39<00:06,  1.18it/s, pg=0.00323, ret=-0.000336, glen=87, tlen=247, kl=0.00566, act_lr=8.2e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 45/53 [00:40<00:06,  1.18it/s, pg=-0.0181, ret=-0.000352, glen=98.1, tlen=258, kl=0.0062, act_lr=8.2e-7, ent=1.49]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 46/53 [00:40<00:05,  1.18it/s, pg=-0.0181, ret=-0.000352, glen=98.1, tlen=258, kl=0.0062, act_lr=8.2e-7, ent=1.49]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 46/53 [00:40<00:05,  1.18it/s, pg=0.757, ret=-0.00507, glen=340, tlen=499, kl=0.00382, act_lr=8.2e-7, ent=1.14]   Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 47/53 [00:41<00:05,  1.14it/s, pg=0.757, ret=-0.00507, glen=340, tlen=499, kl=0.00382, act_lr=8.2e-7, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 47/53 [00:41<00:05,  1.14it/s, pg=-0.198, ret=0.00108, glen=89.7, tlen=249, kl=0.006, act_lr=8.2e-7, ent=1.5]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 48/53 [00:41<00:04,  1.15it/s, pg=-0.198, ret=0.00108, glen=89.7, tlen=249, kl=0.006, act_lr=8.2e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 48/53 [00:42<00:04,  1.15it/s, pg=-0.036, ret=-9.47e-5, glen=82.4, tlen=242, kl=0.00562, act_lr=8.2e-7, ent=1.41]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 49/53 [00:42<00:03,  1.16it/s, pg=-0.036, ret=-9.47e-5, glen=82.4, tlen=242, kl=0.00562, act_lr=8.2e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 49/53 [00:43<00:03,  1.16it/s, pg=-0.0789, ret=0.000246, glen=90.7, tlen=251, kl=0.00544, act_lr=8.2e-7, ent=1.43]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 50/53 [00:43<00:02,  1.17it/s, pg=-0.0789, ret=0.000246, glen=90.7, tlen=251, kl=0.00544, act_lr=8.2e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 50/53 [00:44<00:02,  1.17it/s, pg=-0.0312, ret=0.000321, glen=102, tlen=262, kl=0.00433, act_lr=8.2e-7, ent=1.56] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 51/53 [00:44<00:01,  1.17it/s, pg=-0.0312, ret=0.000321, glen=102, tlen=262, kl=0.00433, act_lr=8.2e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 51/53 [00:45<00:01,  1.17it/s, pg=-0.0295, ret=-3.38e-5, glen=91.2, tlen=251, kl=0.00499, act_lr=8.2e-7, ent=1.43]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:45<00:00,  1.17it/s, pg=-0.0295, ret=-3.38e-5, glen=91.2, tlen=251, kl=0.00499, act_lr=8.2e-7, ent=1.43]
2025-07-24 18:39:33.849 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 46.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:46<00:00,  1.17it/s, pg=-0.143, ret=0.000313, glen=84.8, tlen=245, kl=0.00559, act_lr=8.4e-7, ent=1.39] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:46<00:00,  1.13it/s, pg=-0.143, ret=0.000313, glen=84.8, tlen=245, kl=0.00559, act_lr=8.4e-7, ent=1.39]
2025-07-24 18:39:34.527 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 18:39:36.829 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.30s
2025-07-24 18:39:37.160 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 49.64s
2025-07-24 18:39:37.167 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.026274771060583728, 'actor_lr': 8.203773829319409e-07, 'clip_ratio': 0.0, 'entropy': 1.4899661765908294, 'kl': 0.005600992238746499, 'response_length': 97.75277868306861, 'total_length': 257.3760099590949, 'teacher_total_length': 268.8832771733122, 'return': -9.817475012892348e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:53<39:38, 237.83s/it][A2025-07-24 18:39:37.214 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:40:40.692 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:40:40.880 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 18:40:40.880 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 63.67s
2025-07-24 18:40:42.774 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0130,avg_reflection_pattern_score: 0.0059,avg_pass_at_n: 1.0000,avg_num_tokens: 92.3911,std_num_tokens: 51.3537,avg_correct_num_tokens: 91.5415,std_correct_num_tokens: 49.4040,avg_incorrect_num_tokens: 102.3655,std_incorrect_num_tokens: 69.5384
2025-07-24 18:40:43.465 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.58s
2025-07-24 18:40:46.394 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.93s
2025-07-24 18:41:13.339 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 209
2025-07-24 18:41:13.340 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.95s
2025-07-24 18:41:14.701 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.92s
2025-07-24 18:41:14.702 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.3806674469030646e-05, avg_kl: 0.014628287137410287, avg_response_length: 92.66023579282624, avg_orm_score: 0.0, avg_custom_rewards: 1.3806674469030646e-05
2025-07-24 18:41:14.736 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter42_replay_buffer.jsonl
2025-07-24 18:41:16.414 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.68s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/53 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/53 [00:01<?, ?it/s, pg=0.0294, ret=-8.03e-5, glen=96.6, tlen=258, kl=0.0126, act_lr=8.4e-7, ent=1.41]Actor Train epoch [1/1]:   2%|‚ñè         | 1/53 [00:01<00:52,  1.02s/it, pg=0.0294, ret=-8.03e-5, glen=96.6, tlen=258, kl=0.0126, act_lr=8.4e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/53 [00:01<00:52,  1.02s/it, pg=0.0889, ret=-0.000783, glen=94.9, tlen=255, kl=0.0119, act_lr=8.4e-7, ent=1.44]Actor Train epoch [1/1]:   4%|‚ñç         | 2/53 [00:01<00:47,  1.08it/s, pg=0.0889, ret=-0.000783, glen=94.9, tlen=255, kl=0.0119, act_lr=8.4e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/53 [00:02<00:47,  1.08it/s, pg=0.224, ret=-0.000281, glen=92.1, tlen=252, kl=0.0138, act_lr=8.4e-7, ent=1.53] Actor Train epoch [1/1]:   6%|‚ñå         | 3/53 [00:02<00:44,  1.12it/s, pg=0.224, ret=-0.000281, glen=92.1, tlen=252, kl=0.0138, act_lr=8.4e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/53 [00:03<00:44,  1.12it/s, pg=-0.0279, ret=-0.000392, glen=88.8, tlen=249, kl=0.0141, act_lr=8.4e-7, ent=1.39]Actor Train epoch [1/1]:   8%|‚ñä         | 4/53 [00:03<00:43,  1.14it/s, pg=-0.0279, ret=-0.000392, glen=88.8, tlen=249, kl=0.0141, act_lr=8.4e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/53 [00:04<00:43,  1.14it/s, pg=0.00836, ret=-0.000155, glen=98, tlen=258, kl=0.0115, act_lr=8.4e-7, ent=1.43]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/53 [00:04<00:41,  1.15it/s, pg=0.00836, ret=-0.000155, glen=98, tlen=258, kl=0.0115, act_lr=8.4e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/53 [00:05<00:41,  1.15it/s, pg=-0.035, ret=0.000271, glen=94.2, tlen=255, kl=0.0166, act_lr=8.4e-7, ent=1.42]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 6/53 [00:05<00:41,  1.14it/s, pg=-0.035, ret=0.000271, glen=94.2, tlen=255, kl=0.0166, act_lr=8.4e-7, ent=1.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 6/53 [00:06<00:41,  1.14it/s, pg=0.0917, ret=-0.000436, glen=86.7, tlen=247, kl=0.0131, act_lr=8.4e-7, ent=1.37]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/53 [00:06<00:39,  1.15it/s, pg=0.0917, ret=-0.000436, glen=86.7, tlen=247, kl=0.0131, act_lr=8.4e-7, ent=1.37]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/53 [00:07<00:39,  1.15it/s, pg=0.0115, ret=-0.000386, glen=90.2, tlen=251, kl=0.0183, act_lr=8.4e-7, ent=1.3] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/53 [00:07<00:38,  1.16it/s, pg=0.0115, ret=-0.000386, glen=90.2, tlen=251, kl=0.0183, act_lr=8.4e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/53 [00:07<00:38,  1.16it/s, pg=0.124, ret=-0.00106, glen=95.6, tlen=257, kl=0.0144, act_lr=8.4e-7, ent=1.46] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/53 [00:07<00:37,  1.16it/s, pg=0.124, ret=-0.00106, glen=95.6, tlen=257, kl=0.0144, act_lr=8.4e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/53 [00:08<00:37,  1.16it/s, pg=-0.15, ret=0.000946, glen=95, tlen=256, kl=0.0348, act_lr=8.4e-7, ent=1.35]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/53 [00:08<00:36,  1.17it/s, pg=-0.15, ret=0.000946, glen=95, tlen=256, kl=0.0348, act_lr=8.4e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/53 [00:09<00:36,  1.17it/s, pg=-0.0212, ret=0.000478, glen=97.1, tlen=257, kl=0.0126, act_lr=8.4e-7, ent=1.43]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/53 [00:09<00:35,  1.17it/s, pg=-0.0212, ret=0.000478, glen=97.1, tlen=257, kl=0.0126, act_lr=8.4e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/53 [00:10<00:35,  1.17it/s, pg=-0.131, ret=0.000483, glen=90.1, tlen=251, kl=0.0107, act_lr=8.4e-7, ent=1.36] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/53 [00:10<00:35,  1.17it/s, pg=-0.131, ret=0.000483, glen=90.1, tlen=251, kl=0.0107, act_lr=8.4e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/53 [00:11<00:35,  1.17it/s, pg=-0.0132, ret=-9.32e-5, glen=85.3, tlen=246, kl=0.0161, act_lr=8.4e-7, ent=1.36]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 13/53 [00:11<00:35,  1.14it/s, pg=-0.0132, ret=-9.32e-5, glen=85.3, tlen=246, kl=0.0161, act_lr=8.4e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 13/53 [00:12<00:35,  1.14it/s, pg=0.0905, ret=-0.000628, glen=87, tlen=247, kl=0.0115, act_lr=8.4e-7, ent=1.27]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 14/53 [00:12<00:34,  1.15it/s, pg=0.0905, ret=-0.000628, glen=87, tlen=247, kl=0.0115, act_lr=8.4e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 14/53 [00:13<00:34,  1.15it/s, pg=0.0543, ret=-0.000137, glen=89.7, tlen=251, kl=0.0215, act_lr=8.4e-7, ent=1.28]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/53 [00:13<00:32,  1.16it/s, pg=0.0543, ret=-0.000137, glen=89.7, tlen=251, kl=0.0215, act_lr=8.4e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/53 [00:13<00:32,  1.16it/s, pg=0.159, ret=-0.00079, glen=90.9, tlen=251, kl=0.0169, act_lr=8.4e-7, ent=1.37]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 16/53 [00:13<00:31,  1.16it/s, pg=0.159, ret=-0.00079, glen=90.9, tlen=251, kl=0.0169, act_lr=8.4e-7, ent=1.37]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 16/53 [00:14<00:31,  1.16it/s, pg=-0.0837, ret=0.000395, glen=101, tlen=262, kl=0.0175, act_lr=8.4e-7, ent=1.41]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 17/53 [00:14<00:31,  1.15it/s, pg=-0.0837, ret=0.000395, glen=101, tlen=262, kl=0.0175, act_lr=8.4e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 17/53 [00:15<00:31,  1.15it/s, pg=0.0182, ret=-0.000653, glen=94.6, tlen=255, kl=0.0143, act_lr=8.4e-7, ent=1.4]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 18/53 [00:15<00:30,  1.15it/s, pg=0.0182, ret=-0.000653, glen=94.6, tlen=255, kl=0.0143, act_lr=8.4e-7, ent=1.4]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 18/53 [00:16<00:30,  1.15it/s, pg=0.0626, ret=-0.000214, glen=95.6, tlen=256, kl=0.0131, act_lr=8.4e-7, ent=1.32]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 19/53 [00:16<00:29,  1.16it/s, pg=0.0626, ret=-0.000214, glen=95.6, tlen=256, kl=0.0131, act_lr=8.4e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 19/53 [00:17<00:29,  1.16it/s, pg=-0.0488, ret=0.000391, glen=95.3, tlen=256, kl=0.0118, act_lr=8.4e-7, ent=1.37]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/53 [00:17<00:28,  1.16it/s, pg=-0.0488, ret=0.000391, glen=95.3, tlen=256, kl=0.0118, act_lr=8.4e-7, ent=1.37]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/53 [00:18<00:28,  1.16it/s, pg=-0.0471, ret=3.94e-6, glen=95.3, tlen=256, kl=0.0136, act_lr=8.4e-7, ent=1.38] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 21/53 [00:18<00:27,  1.17it/s, pg=-0.0471, ret=3.94e-6, glen=95.3, tlen=256, kl=0.0136, act_lr=8.4e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 21/53 [00:19<00:27,  1.17it/s, pg=0.0201, ret=0.000145, glen=89, tlen=249, kl=0.0138, act_lr=8.4e-7, ent=1.45]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/53 [00:19<00:26,  1.17it/s, pg=0.0201, ret=0.000145, glen=89, tlen=249, kl=0.0138, act_lr=8.4e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/53 [00:19<00:26,  1.17it/s, pg=0.117, ret=-8.63e-5, glen=96.2, tlen=257, kl=0.013, act_lr=8.4e-7, ent=1.5] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/53 [00:19<00:25,  1.17it/s, pg=0.117, ret=-8.63e-5, glen=96.2, tlen=257, kl=0.013, act_lr=8.4e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/53 [00:20<00:25,  1.17it/s, pg=-0.0726, ret=-5.4e-5, glen=94.1, tlen=255, kl=0.0139, act_lr=8.4e-7, ent=1.35]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/53 [00:20<00:24,  1.17it/s, pg=-0.0726, ret=-5.4e-5, glen=94.1, tlen=255, kl=0.0139, act_lr=8.4e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/53 [00:21<00:24,  1.17it/s, pg=-0.0888, ret=0.00011, glen=86.8, tlen=247, kl=0.0144, act_lr=8.4e-7, ent=1.36]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/53 [00:21<00:23,  1.17it/s, pg=-0.0888, ret=0.00011, glen=86.8, tlen=247, kl=0.0144, act_lr=8.4e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/53 [00:22<00:23,  1.17it/s, pg=-0.00195, ret=-6.33e-5, glen=89.4, tlen=250, kl=0.014, act_lr=8.4e-7, ent=1.33]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26/53 [00:22<00:23,  1.17it/s, pg=-0.00195, ret=-6.33e-5, glen=89.4, tlen=250, kl=0.014, act_lr=8.4e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26/53 [00:23<00:23,  1.17it/s, pg=-0.126, ret=0.000466, glen=88.4, tlen=249, kl=0.0151, act_lr=8.4e-7, ent=1.35] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/53 [00:23<00:22,  1.17it/s, pg=-0.126, ret=0.000466, glen=88.4, tlen=249, kl=0.0151, act_lr=8.4e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/53 [00:24<00:22,  1.17it/s, pg=-0.11, ret=0.000444, glen=90.1, tlen=251, kl=0.0114, act_lr=8.4e-7, ent=1.37] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28/53 [00:24<00:21,  1.17it/s, pg=-0.11, ret=0.000444, glen=90.1, tlen=251, kl=0.0114, act_lr=8.4e-7, ent=1.37]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28/53 [00:25<00:21,  1.17it/s, pg=-0.135, ret=0.000584, glen=93.5, tlen=254, kl=0.0135, act_lr=8.4e-7, ent=1.43]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 29/53 [00:25<00:22,  1.07it/s, pg=-0.135, ret=0.000584, glen=93.5, tlen=254, kl=0.0135, act_lr=8.4e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 29/53 [00:26<00:22,  1.07it/s, pg=-0.141, ret=0.000663, glen=94.1, tlen=255, kl=0.025, act_lr=8.4e-7, ent=1.31] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30/53 [00:26<00:20,  1.10it/s, pg=-0.141, ret=0.000663, glen=94.1, tlen=255, kl=0.025, act_lr=8.4e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30/53 [00:27<00:20,  1.10it/s, pg=0.0241, ret=0.000185, glen=94.6, tlen=255, kl=0.0155, act_lr=8.4e-7, ent=1.43]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 31/53 [00:27<00:19,  1.12it/s, pg=0.0241, ret=0.000185, glen=94.6, tlen=255, kl=0.0155, act_lr=8.4e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 31/53 [00:27<00:19,  1.12it/s, pg=0.106, ret=0.000147, glen=101, tlen=262, kl=0.0136, act_lr=8.4e-7, ent=1.43]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 32/53 [00:27<00:18,  1.14it/s, pg=0.106, ret=0.000147, glen=101, tlen=262, kl=0.0136, act_lr=8.4e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 32/53 [00:28<00:18,  1.14it/s, pg=-0.0331, ret=0.000171, glen=83, tlen=243, kl=0.0134, act_lr=8.4e-7, ent=1.32]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 33/53 [00:28<00:17,  1.14it/s, pg=-0.0331, ret=0.000171, glen=83, tlen=243, kl=0.0134, act_lr=8.4e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 33/53 [00:29<00:17,  1.14it/s, pg=-0.0115, ret=-6.83e-5, glen=93.7, tlen=254, kl=0.0147, act_lr=8.4e-7, ent=1.28]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34/53 [00:29<00:16,  1.15it/s, pg=-0.0115, ret=-6.83e-5, glen=93.7, tlen=254, kl=0.0147, act_lr=8.4e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34/53 [00:30<00:16,  1.15it/s, pg=-0.0817, ret=0.00105, glen=95.1, tlen=256, kl=0.0112, act_lr=8.4e-7, ent=1.39] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 35/53 [00:30<00:15,  1.16it/s, pg=-0.0817, ret=0.00105, glen=95.1, tlen=256, kl=0.0112, act_lr=8.4e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 35/53 [00:31<00:15,  1.16it/s, pg=0.13, ret=-0.00107, glen=91.8, tlen=252, kl=0.0138, act_lr=8.4e-7, ent=1.42]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 36/53 [00:31<00:14,  1.16it/s, pg=0.13, ret=-0.00107, glen=91.8, tlen=252, kl=0.0138, act_lr=8.4e-7, ent=1.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 36/53 [00:32<00:14,  1.16it/s, pg=0.0991, ret=-0.000487, glen=85.1, tlen=245, kl=0.0124, act_lr=8.4e-7, ent=1.34]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 37/53 [00:32<00:13,  1.17it/s, pg=0.0991, ret=-0.000487, glen=85.1, tlen=245, kl=0.0124, act_lr=8.4e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 37/53 [00:33<00:13,  1.17it/s, pg=0.0122, ret=2.64e-5, glen=97.6, tlen=258, kl=0.015, act_lr=8.4e-7, ent=1.41]   Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38/53 [00:33<00:12,  1.17it/s, pg=0.0122, ret=2.64e-5, glen=97.6, tlen=258, kl=0.015, act_lr=8.4e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38/53 [00:33<00:12,  1.17it/s, pg=-0.106, ret=0.000507, glen=91.6, tlen=252, kl=0.0147, act_lr=8.4e-7, ent=1.38]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 39/53 [00:33<00:11,  1.17it/s, pg=-0.106, ret=0.000507, glen=91.6, tlen=252, kl=0.0147, act_lr=8.4e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 39/53 [00:34<00:11,  1.17it/s, pg=0.128, ret=-0.000898, glen=87.8, tlen=248, kl=0.0133, act_lr=8.4e-7, ent=1.35]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 40/53 [00:34<00:11,  1.17it/s, pg=0.128, ret=-0.000898, glen=87.8, tlen=248, kl=0.0133, act_lr=8.4e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 40/53 [00:35<00:11,  1.17it/s, pg=0.018, ret=-9.8e-5, glen=90.4, tlen=251, kl=0.0136, act_lr=8.4e-7, ent=1.39]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 41/53 [00:35<00:10,  1.17it/s, pg=0.018, ret=-9.8e-5, glen=90.4, tlen=251, kl=0.0136, act_lr=8.4e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 41/53 [00:36<00:10,  1.17it/s, pg=0.0211, ret=0.000331, glen=91.4, tlen=252, kl=0.0155, act_lr=8.4e-7, ent=1.39]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 42/53 [00:36<00:09,  1.17it/s, pg=0.0211, ret=0.000331, glen=91.4, tlen=252, kl=0.0155, act_lr=8.4e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 42/53 [00:37<00:09,  1.17it/s, pg=-0.00488, ret=4.18e-5, glen=95.6, tlen=257, kl=0.0153, act_lr=8.4e-7, ent=1.39]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 43/53 [00:37<00:08,  1.17it/s, pg=-0.00488, ret=4.18e-5, glen=95.6, tlen=257, kl=0.0153, act_lr=8.4e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 43/53 [00:38<00:08,  1.17it/s, pg=0.0457, ret=-8.14e-5, glen=96.1, tlen=257, kl=0.0102, act_lr=8.4e-7, ent=1.44] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 44/53 [00:38<00:07,  1.17it/s, pg=0.0457, ret=-8.14e-5, glen=96.1, tlen=257, kl=0.0102, act_lr=8.4e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 44/53 [00:38<00:07,  1.17it/s, pg=-0.0398, ret=0.000423, glen=94.8, tlen=255, kl=0.0163, act_lr=8.4e-7, ent=1.41]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 45/53 [00:38<00:06,  1.17it/s, pg=-0.0398, ret=0.000423, glen=94.8, tlen=255, kl=0.0163, act_lr=8.4e-7, ent=1.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 45/53 [00:39<00:06,  1.17it/s, pg=-0.25, ret=0.00141, glen=88.9, tlen=249, kl=0.0157, act_lr=8.4e-7, ent=1.39]   Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 46/53 [00:39<00:05,  1.17it/s, pg=-0.25, ret=0.00141, glen=88.9, tlen=249, kl=0.0157, act_lr=8.4e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 46/53 [00:40<00:05,  1.17it/s, pg=-0.0161, ret=-0.000276, glen=108, tlen=269, kl=0.0168, act_lr=8.4e-7, ent=1.61]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 47/53 [00:40<00:05,  1.17it/s, pg=-0.0161, ret=-0.000276, glen=108, tlen=269, kl=0.0168, act_lr=8.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 47/53 [00:41<00:05,  1.17it/s, pg=-0.076, ret=0.000496, glen=87, tlen=248, kl=0.0131, act_lr=8.4e-7, ent=1.35]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 48/53 [00:41<00:04,  1.17it/s, pg=-0.076, ret=0.000496, glen=87, tlen=248, kl=0.0131, act_lr=8.4e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 48/53 [00:42<00:04,  1.17it/s, pg=-0.119, ret=0.00093, glen=86.3, tlen=246, kl=0.0141, act_lr=8.4e-7, ent=1.38]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 49/53 [00:42<00:03,  1.17it/s, pg=-0.119, ret=0.00093, glen=86.3, tlen=246, kl=0.0141, act_lr=8.4e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 49/53 [00:43<00:03,  1.17it/s, pg=-0.131, ret=0.000769, glen=91.1, tlen=252, kl=0.0115, act_lr=8.4e-7, ent=1.43]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 50/53 [00:43<00:02,  1.17it/s, pg=-0.131, ret=0.000769, glen=91.1, tlen=252, kl=0.0115, act_lr=8.4e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 50/53 [00:44<00:02,  1.17it/s, pg=-0.00317, ret=-0.000268, glen=93.7, tlen=254, kl=0.0166, act_lr=8.4e-7, ent=1.42]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 51/53 [00:44<00:01,  1.17it/s, pg=-0.00317, ret=-0.000268, glen=93.7, tlen=254, kl=0.0166, act_lr=8.4e-7, ent=1.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 51/53 [00:44<00:01,  1.17it/s, pg=0.0335, ret=0.000389, glen=97.2, tlen=258, kl=0.0117, act_lr=8.4e-7, ent=1.42]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:44<00:00,  1.17it/s, pg=0.0335, ret=0.000389, glen=97.2, tlen=258, kl=0.0117, act_lr=8.4e-7, ent=1.42]
2025-07-24 18:42:02.501 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 45.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:45<00:00,  1.17it/s, pg=0.207, ret=-0.00188, glen=95.8, tlen=257, kl=0.0127, act_lr=8.6e-7, ent=1.39] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:45<00:00,  1.13it/s, pg=0.207, ret=-0.00188, glen=95.8, tlen=257, kl=0.0127, act_lr=8.6e-7, ent=1.39]
2025-07-24 18:42:03.316 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 18:42:05.896 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 18:42:06.224 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 49.75s
2025-07-24 18:42:06.231 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0034207757913841392, 'actor_lr': 8.403773430267015e-07, 'clip_ratio': 0.0, 'entropy': 1.3902373246426851, 'kl': 0.014634330300087074, 'response_length': 92.71169093869767, 'total_length': 253.25340818009286, 'teacher_total_length': 264.96051601193983, 'return': 1.5754419058245026e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:22<30:25, 202.79s/it][A2025-07-24 18:42:06.279 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:43:08.794 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:43:08.975 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:43:08.976 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 62.70s
2025-07-24 18:43:10.828 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0129,avg_reflection_pattern_score: 0.0082,avg_pass_at_n: 1.0000,avg_num_tokens: 89.9683,std_num_tokens: 50.1039,avg_correct_num_tokens: 89.0763,std_correct_num_tokens: 48.1469,avg_incorrect_num_tokens: 100.8613,std_incorrect_num_tokens: 68.7770
2025-07-24 18:43:11.275 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.30s
2025-07-24 18:43:14.168 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.89s
2025-07-24 18:43:40.749 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 207
2025-07-24 18:43:40.749 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.58s
2025-07-24 18:43:42.134 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 18:43:42.135 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.140993908452599e-05, avg_kl: 0.024483058763586956, avg_response_length: 90.25201017960258, avg_orm_score: 0.0, avg_custom_rewards: 1.140993908452599e-05
2025-07-24 18:43:42.165 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter43_replay_buffer.jsonl
2025-07-24 18:43:43.818 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.66s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:01<?, ?it/s, pg=-0.186, ret=0.00111, glen=96.6, tlen=257, kl=0.0378, act_lr=8.6e-7, ent=1.34]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:52,  1.03s/it, pg=-0.186, ret=0.00111, glen=96.6, tlen=257, kl=0.0378, act_lr=8.6e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:52,  1.03s/it, pg=-0.13, ret=0.000564, glen=81.8, tlen=242, kl=0.0221, act_lr=8.6e-7, ent=1.23]Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:46,  1.08it/s, pg=-0.13, ret=0.000564, glen=81.8, tlen=242, kl=0.0221, act_lr=8.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:46,  1.08it/s, pg=-0.00659, ret=-3.85e-5, glen=91.2, tlen=252, kl=0.0226, act_lr=8.6e-7, ent=1.28]Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:45,  1.08it/s, pg=-0.00659, ret=-3.85e-5, glen=91.2, tlen=252, kl=0.0226, act_lr=8.6e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:45,  1.08it/s, pg=-0.121, ret=0.000461, glen=92.4, tlen=253, kl=0.024, act_lr=8.6e-7, ent=1.27]   Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:43,  1.11it/s, pg=-0.121, ret=0.000461, glen=92.4, tlen=253, kl=0.024, act_lr=8.6e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:43,  1.11it/s, pg=-0.118, ret=0.000186, glen=92.1, tlen=253, kl=0.0172, act_lr=8.6e-7, ent=1.34]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:42,  1.11it/s, pg=-0.118, ret=0.000186, glen=92.1, tlen=253, kl=0.0172, act_lr=8.6e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:42,  1.11it/s, pg=-0.214, ret=0.00103, glen=91.5, tlen=252, kl=0.0169, act_lr=8.6e-7, ent=1.32] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:41,  1.12it/s, pg=-0.214, ret=0.00103, glen=91.5, tlen=252, kl=0.0169, act_lr=8.6e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:41,  1.12it/s, pg=0.0129, ret=5.52e-5, glen=89.2, tlen=250, kl=0.0168, act_lr=8.6e-7, ent=1.29]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:39,  1.14it/s, pg=0.0129, ret=5.52e-5, glen=89.2, tlen=250, kl=0.0168, act_lr=8.6e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:07<00:39,  1.14it/s, pg=-0.025, ret=0.000104, glen=84.3, tlen=244, kl=0.0339, act_lr=8.6e-7, ent=1.23]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:38,  1.15it/s, pg=-0.025, ret=0.000104, glen=84.3, tlen=244, kl=0.0339, act_lr=8.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:08<00:38,  1.15it/s, pg=0.0359, ret=-0.000905, glen=91.6, tlen=252, kl=0.0172, act_lr=8.6e-7, ent=1.23]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.16it/s, pg=0.0359, ret=-0.000905, glen=91.6, tlen=252, kl=0.0172, act_lr=8.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.16it/s, pg=-0.0137, ret=-2.27e-5, glen=92.8, tlen=254, kl=0.026, act_lr=8.6e-7, ent=1.29] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:36,  1.16it/s, pg=-0.0137, ret=-2.27e-5, glen=92.8, tlen=254, kl=0.026, act_lr=8.6e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:36,  1.16it/s, pg=0.206, ret=-0.00123, glen=91.1, tlen=252, kl=0.0165, act_lr=8.6e-7, ent=1.23] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:35,  1.15it/s, pg=0.206, ret=-0.00123, glen=91.1, tlen=252, kl=0.0165, act_lr=8.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:35,  1.15it/s, pg=0.146, ret=-0.000773, glen=86.5, tlen=247, kl=0.0288, act_lr=8.6e-7, ent=1.31]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.16it/s, pg=0.146, ret=-0.000773, glen=86.5, tlen=247, kl=0.0288, act_lr=8.6e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.16it/s, pg=-0.132, ret=0.000646, glen=90.8, tlen=251, kl=0.017, act_lr=8.6e-7, ent=1.32] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.16it/s, pg=-0.132, ret=0.000646, glen=90.8, tlen=251, kl=0.017, act_lr=8.6e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.16it/s, pg=0.0385, ret=-0.000515, glen=84.3, tlen=245, kl=0.0515, act_lr=8.6e-7, ent=1.2]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.16it/s, pg=0.0385, ret=-0.000515, glen=84.3, tlen=245, kl=0.0515, act_lr=8.6e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:13<00:32,  1.16it/s, pg=0.00848, ret=4.44e-6, glen=91.9, tlen=252, kl=0.0287, act_lr=8.6e-7, ent=1.33]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.16it/s, pg=0.00848, ret=4.44e-6, glen=91.9, tlen=252, kl=0.0287, act_lr=8.6e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:14<00:31,  1.16it/s, pg=-0.102, ret=0.000252, glen=89.5, tlen=250, kl=0.0189, act_lr=8.6e-7, ent=1.34]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=-0.102, ret=0.000252, glen=89.5, tlen=250, kl=0.0189, act_lr=8.6e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=-0.0241, ret=4.15e-5, glen=88.7, tlen=249, kl=0.0185, act_lr=8.6e-7, ent=1.25]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:29,  1.17it/s, pg=-0.0241, ret=4.15e-5, glen=88.7, tlen=249, kl=0.0185, act_lr=8.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:29,  1.17it/s, pg=0.00732, ret=0.000191, glen=90.6, tlen=251, kl=0.0193, act_lr=8.6e-7, ent=1.32]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:29,  1.17it/s, pg=0.00732, ret=0.000191, glen=90.6, tlen=251, kl=0.0193, act_lr=8.6e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:29,  1.17it/s, pg=0.00684, ret=0.000371, glen=86.4, tlen=247, kl=0.0241, act_lr=8.6e-7, ent=1.25]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.17it/s, pg=0.00684, ret=0.000371, glen=86.4, tlen=247, kl=0.0241, act_lr=8.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.17it/s, pg=0.041, ret=-0.000318, glen=89.8, tlen=250, kl=0.0269, act_lr=8.6e-7, ent=1.24] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.17it/s, pg=0.041, ret=-0.000318, glen=89.8, tlen=250, kl=0.0269, act_lr=8.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.17it/s, pg=-0.0247, ret=0.000236, glen=88.9, tlen=249, kl=0.0204, act_lr=8.6e-7, ent=1.24]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:26,  1.17it/s, pg=-0.0247, ret=0.000236, glen=88.9, tlen=249, kl=0.0204, act_lr=8.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:19<00:26,  1.17it/s, pg=0.107, ret=-0.000949, glen=95.8, tlen=256, kl=0.0189, act_lr=8.6e-7, ent=1.32] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.17it/s, pg=0.107, ret=-0.000949, glen=95.8, tlen=256, kl=0.0189, act_lr=8.6e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.17it/s, pg=0.162, ret=-0.000795, glen=96.4, tlen=257, kl=0.0244, act_lr=8.6e-7, ent=1.3] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:19<00:24,  1.17it/s, pg=0.162, ret=-0.000795, glen=96.4, tlen=257, kl=0.0244, act_lr=8.6e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:24,  1.17it/s, pg=-0.0573, ret=-0.000125, glen=98.4, tlen=259, kl=0.0332, act_lr=8.6e-7, ent=1.33]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:20<00:23,  1.17it/s, pg=-0.0573, ret=-0.000125, glen=98.4, tlen=259, kl=0.0332, act_lr=8.6e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:23,  1.17it/s, pg=-0.0895, ret=0.000607, glen=92.7, tlen=253, kl=0.0314, act_lr=8.6e-7, ent=1.31] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:23,  1.17it/s, pg=-0.0895, ret=0.000607, glen=92.7, tlen=253, kl=0.0314, act_lr=8.6e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:23,  1.17it/s, pg=-0.0975, ret=0.000856, glen=94.3, tlen=255, kl=0.0205, act_lr=8.6e-7, ent=1.54]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.17it/s, pg=-0.0975, ret=0.000856, glen=94.3, tlen=255, kl=0.0205, act_lr=8.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:23<00:22,  1.17it/s, pg=0.0777, ret=-0.000796, glen=90.2, tlen=251, kl=0.0294, act_lr=8.6e-7, ent=1.29]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:23<00:21,  1.17it/s, pg=0.0777, ret=-0.000796, glen=90.2, tlen=251, kl=0.0294, act_lr=8.6e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:21,  1.17it/s, pg=-0.0206, ret=-0.000169, glen=90.7, tlen=251, kl=0.0185, act_lr=8.6e-7, ent=1.31]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:20,  1.18it/s, pg=-0.0206, ret=-0.000169, glen=90.7, tlen=251, kl=0.0185, act_lr=8.6e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:20,  1.18it/s, pg=0.239, ret=-0.000622, glen=99.5, tlen=260, kl=0.0222, act_lr=8.6e-7, ent=1.49]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:22,  1.00it/s, pg=0.239, ret=-0.000622, glen=99.5, tlen=260, kl=0.0222, act_lr=8.6e-7, ent=1.49]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:22,  1.00it/s, pg=0.0613, ret=-5.06e-5, glen=86.3, tlen=247, kl=0.0175, act_lr=8.6e-7, ent=1.26]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:21,  1.04it/s, pg=0.0613, ret=-5.06e-5, glen=86.3, tlen=247, kl=0.0175, act_lr=8.6e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:27<00:21,  1.04it/s, pg=0.0222, ret=-0.000107, glen=84.9, tlen=245, kl=0.0188, act_lr=8.6e-7, ent=1.24]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:19,  1.07it/s, pg=0.0222, ret=-0.000107, glen=84.9, tlen=245, kl=0.0188, act_lr=8.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:28<00:19,  1.07it/s, pg=-0.0121, ret=0.000178, glen=89.8, tlen=250, kl=0.0161, act_lr=8.6e-7, ent=1.31]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:18,  1.10it/s, pg=-0.0121, ret=0.000178, glen=89.8, tlen=250, kl=0.0161, act_lr=8.6e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:29<00:18,  1.10it/s, pg=0.152, ret=-0.000703, glen=84.8, tlen=245, kl=0.0219, act_lr=8.6e-7, ent=1.29] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.12it/s, pg=0.152, ret=-0.000703, glen=84.8, tlen=245, kl=0.0219, act_lr=8.6e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.12it/s, pg=-0.0561, ret=-2.9e-5, glen=85.8, tlen=246, kl=0.0203, act_lr=8.6e-7, ent=1.24]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:29<00:15,  1.14it/s, pg=-0.0561, ret=-2.9e-5, glen=85.8, tlen=246, kl=0.0203, act_lr=8.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.14it/s, pg=-0.061, ret=0.000575, glen=91.6, tlen=252, kl=0.038, act_lr=8.6e-7, ent=1.2]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.15it/s, pg=-0.061, ret=0.000575, glen=91.6, tlen=252, kl=0.038, act_lr=8.6e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.15it/s, pg=0.128, ret=-5.46e-5, glen=96.5, tlen=257, kl=0.0346, act_lr=8.6e-7, ent=1.25]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.16it/s, pg=0.128, ret=-5.46e-5, glen=96.5, tlen=257, kl=0.0346, act_lr=8.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.16it/s, pg=0.0592, ret=-5.36e-5, glen=91.5, tlen=252, kl=0.0247, act_lr=8.6e-7, ent=1.36]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.16it/s, pg=0.0592, ret=-5.36e-5, glen=91.5, tlen=252, kl=0.0247, act_lr=8.6e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:33<00:12,  1.16it/s, pg=-0.104, ret=0.000575, glen=90.2, tlen=251, kl=0.0229, act_lr=8.6e-7, ent=1.3] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:12,  1.16it/s, pg=-0.104, ret=0.000575, glen=90.2, tlen=251, kl=0.0229, act_lr=8.6e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:34<00:12,  1.16it/s, pg=-0.0125, ret=-7.42e-5, glen=84.5, tlen=245, kl=0.0193, act_lr=8.6e-7, ent=1.25]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.17it/s, pg=-0.0125, ret=-7.42e-5, glen=84.5, tlen=245, kl=0.0193, act_lr=8.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.17it/s, pg=-0.0461, ret=0.000209, glen=91.3, tlen=252, kl=0.0245, act_lr=8.6e-7, ent=1.36]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:34<00:10,  1.17it/s, pg=-0.0461, ret=0.000209, glen=91.3, tlen=252, kl=0.0245, act_lr=8.6e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.17it/s, pg=-0.0338, ret=0.000264, glen=90.1, tlen=250, kl=0.0481, act_lr=8.6e-7, ent=1.24]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:35<00:09,  1.17it/s, pg=-0.0338, ret=0.000264, glen=90.1, tlen=250, kl=0.0481, act_lr=8.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.17it/s, pg=-0.104, ret=0.000696, glen=86.3, tlen=247, kl=0.0245, act_lr=8.6e-7, ent=1.31] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:36<00:08,  1.17it/s, pg=-0.104, ret=0.000696, glen=86.3, tlen=247, kl=0.0245, act_lr=8.6e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.17it/s, pg=0.0719, ret=-2.41e-5, glen=90.4, tlen=251, kl=0.0356, act_lr=8.6e-7, ent=1.32]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.17it/s, pg=0.0719, ret=-2.41e-5, glen=90.4, tlen=251, kl=0.0356, act_lr=8.6e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.17it/s, pg=-0.0399, ret=1.73e-5, glen=92.7, tlen=253, kl=0.019, act_lr=8.6e-7, ent=1.37] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.17it/s, pg=-0.0399, ret=1.73e-5, glen=92.7, tlen=253, kl=0.019, act_lr=8.6e-7, ent=1.37]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:39<00:06,  1.17it/s, pg=-0.0355, ret=9.36e-5, glen=86.1, tlen=246, kl=0.0235, act_lr=8.6e-7, ent=1.38]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.17it/s, pg=-0.0355, ret=9.36e-5, glen=86.1, tlen=246, kl=0.0235, act_lr=8.6e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:40<00:05,  1.17it/s, pg=0.121, ret=-6.86e-5, glen=91.2, tlen=252, kl=0.022, act_lr=8.6e-7, ent=1.48]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=0.121, ret=-6.86e-5, glen=91.2, tlen=252, kl=0.022, act_lr=8.6e-7, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=0.0787, ret=-0.000689, glen=96, tlen=257, kl=0.0296, act_lr=8.6e-7, ent=1.31]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:40<00:04,  1.17it/s, pg=0.0787, ret=-0.000689, glen=96, tlen=257, kl=0.0296, act_lr=8.6e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.17it/s, pg=0.0212, ret=-0.00027, glen=92.7, tlen=254, kl=0.018, act_lr=8.6e-7, ent=1.35]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:41<00:03,  1.17it/s, pg=0.0212, ret=-0.00027, glen=92.7, tlen=254, kl=0.018, act_lr=8.6e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=-0.0633, ret=0.000418, glen=87.6, tlen=248, kl=0.0195, act_lr=8.6e-7, ent=1.28]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.18it/s, pg=-0.0633, ret=0.000418, glen=87.6, tlen=248, kl=0.0195, act_lr=8.6e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.18it/s, pg=0.0831, ret=-0.000459, glen=86.3, tlen=247, kl=0.0179, act_lr=8.6e-7, ent=1.24]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.18it/s, pg=0.0831, ret=-0.000459, glen=86.3, tlen=247, kl=0.0179, act_lr=8.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.18it/s, pg=-0.0803, ret=0.000357, glen=91.5, tlen=252, kl=0.0284, act_lr=8.6e-7, ent=1.27]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.16it/s, pg=-0.0803, ret=0.000357, glen=91.5, tlen=252, kl=0.0284, act_lr=8.6e-7, ent=1.27]
2025-07-24 18:44:29.609 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 45.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.16it/s, pg=0.0858, ret=-0.000401, glen=85.2, tlen=245, kl=0.0234, act_lr=8.8e-7, ent=1.27]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.13it/s, pg=0.0858, ret=-0.000401, glen=85.2, tlen=245, kl=0.0234, act_lr=8.8e-7, ent=1.27]
2025-07-24 18:44:30.270 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 18:44:32.602 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.33s
2025-07-24 18:44:32.944 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 48.81s
2025-07-24 18:44:32.951 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0006944032815786508, 'actor_lr': 8.603846157921697e-07, 'clip_ratio': 0.0, 'entropy': 1.300669041963724, 'kl': 0.024448688213641826, 'response_length': 90.26287944500262, 'total_length': 250.76004233727087, 'teacher_total_length': 262.8017704303448, 'return': -2.9650762091757943e-06, 'policy_update_steps': 1.0}

Episode [4/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [16:49<24:20, 182.57s/it][A2025-07-24 18:44:32.997 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:45:46.676 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:45:46.855 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:45:46.856 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 73.86s
2025-07-24 18:45:48.824 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0132,avg_reflection_pattern_score: 0.0057,avg_pass_at_n: 1.0000,avg_num_tokens: 86.4714,std_num_tokens: 49.9608,avg_correct_num_tokens: 85.4765,std_correct_num_tokens: 43.0836,avg_incorrect_num_tokens: 100.8547,std_incorrect_num_tokens: 107.3566
2025-07-24 18:45:49.496 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.64s
2025-07-24 18:45:52.326 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.83s
2025-07-24 18:46:18.289 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 204
2025-07-24 18:46:18.290 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.96s
2025-07-24 18:46:19.651 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.93s
2025-07-24 18:46:19.652 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -6.821791812042505e-05, avg_kl: 0.02996347464767157, avg_response_length: 86.73795748691933, avg_orm_score: 0.0, avg_custom_rewards: -6.821791812042505e-05
2025-07-24 18:46:19.685 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter44_replay_buffer.jsonl
2025-07-24 18:46:21.292 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.61s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:01<?, ?it/s, pg=-0.00146, ret=-0.000289, glen=82.6, tlen=243, kl=0.0262, act_lr=8.8e-7, ent=1.31]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:51,  1.02s/it, pg=-0.00146, ret=-0.000289, glen=82.6, tlen=243, kl=0.0262, act_lr=8.8e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:51,  1.02s/it, pg=0.0942, ret=-0.000595, glen=87, tlen=248, kl=0.0294, act_lr=8.8e-7, ent=1.32]    Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:45,  1.08it/s, pg=0.0942, ret=-0.000595, glen=87, tlen=248, kl=0.0294, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:45,  1.08it/s, pg=0.0249, ret=-0.000446, glen=106, tlen=267, kl=0.0234, act_lr=8.8e-7, ent=1.44]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:44,  1.08it/s, pg=0.0249, ret=-0.000446, glen=106, tlen=267, kl=0.0234, act_lr=8.8e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:44,  1.08it/s, pg=0.0628, ret=-0.000751, glen=83.6, tlen=244, kl=0.0228, act_lr=8.8e-7, ent=1.32]Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:42,  1.12it/s, pg=0.0628, ret=-0.000751, glen=83.6, tlen=244, kl=0.0228, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:42,  1.12it/s, pg=0.0401, ret=-0.000613, glen=84.7, tlen=245, kl=0.034, act_lr=8.8e-7, ent=1.3]  Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.13it/s, pg=0.0401, ret=-0.000613, glen=84.7, tlen=245, kl=0.034, act_lr=8.8e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.13it/s, pg=0.159, ret=-0.000483, glen=83.8, tlen=244, kl=0.0277, act_lr=8.8e-7, ent=1.21]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.15it/s, pg=0.159, ret=-0.000483, glen=83.8, tlen=244, kl=0.0277, act_lr=8.8e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.15it/s, pg=-0.149, ret=0.00058, glen=87.5, tlen=248, kl=0.0255, act_lr=8.8e-7, ent=1.33] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.16it/s, pg=-0.149, ret=0.00058, glen=87.5, tlen=248, kl=0.0255, act_lr=8.8e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.16it/s, pg=0.00577, ret=-0.000115, glen=87.9, tlen=248, kl=0.0279, act_lr=8.8e-7, ent=1.27]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.16it/s, pg=0.00577, ret=-0.000115, glen=87.9, tlen=248, kl=0.0279, act_lr=8.8e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.16it/s, pg=0.12, ret=-0.000906, glen=84.6, tlen=245, kl=0.0327, act_lr=8.8e-7, ent=1.31]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.16it/s, pg=0.12, ret=-0.000906, glen=84.6, tlen=245, kl=0.0327, act_lr=8.8e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.16it/s, pg=-0.0701, ret=0.000324, glen=85.3, tlen=246, kl=0.0374, act_lr=8.8e-7, ent=1.32]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.17it/s, pg=-0.0701, ret=0.000324, glen=85.3, tlen=246, kl=0.0374, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.17it/s, pg=-0.177, ret=0.000912, glen=85.8, tlen=246, kl=0.028, act_lr=8.8e-7, ent=1.26]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.17it/s, pg=-0.177, ret=0.000912, glen=85.8, tlen=246, kl=0.028, act_lr=8.8e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.17it/s, pg=-0.129, ret=0.000906, glen=88.9, tlen=249, kl=0.0343, act_lr=8.8e-7, ent=1.36]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:34,  1.14it/s, pg=-0.129, ret=0.000906, glen=88.9, tlen=249, kl=0.0343, act_lr=8.8e-7, ent=1.36]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:34,  1.14it/s, pg=0.373, ret=-0.000367, glen=106, tlen=266, kl=0.0605, act_lr=8.8e-7, ent=1.87] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:33,  1.14it/s, pg=0.373, ret=-0.000367, glen=106, tlen=266, kl=0.0605, act_lr=8.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:33,  1.14it/s, pg=-0.132, ret=0.000456, glen=89.1, tlen=249, kl=0.032, act_lr=8.8e-7, ent=1.29]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:32,  1.15it/s, pg=-0.132, ret=0.000456, glen=89.1, tlen=249, kl=0.032, act_lr=8.8e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:32,  1.15it/s, pg=-0.0368, ret=-0.000125, glen=81.5, tlen=242, kl=0.024, act_lr=8.8e-7, ent=1.26]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.16it/s, pg=-0.0368, ret=-0.000125, glen=81.5, tlen=242, kl=0.024, act_lr=8.8e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.16it/s, pg=-0.054, ret=0.000342, glen=93.4, tlen=253, kl=0.0266, act_lr=8.8e-7, ent=1.34] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:30,  1.16it/s, pg=-0.054, ret=0.000342, glen=93.4, tlen=253, kl=0.0266, act_lr=8.8e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.16it/s, pg=-0.0758, ret=4.82e-5, glen=83.3, tlen=243, kl=0.0229, act_lr=8.8e-7, ent=1.24]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.17it/s, pg=-0.0758, ret=4.82e-5, glen=83.3, tlen=243, kl=0.0229, act_lr=8.8e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.17it/s, pg=-0.241, ret=0.0012, glen=84.6, tlen=245, kl=0.0308, act_lr=8.8e-7, ent=1.23]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.15it/s, pg=-0.241, ret=0.0012, glen=84.6, tlen=245, kl=0.0308, act_lr=8.8e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.15it/s, pg=0.0698, ret=-0.000159, glen=85.1, tlen=245, kl=0.0241, act_lr=8.8e-7, ent=1.26]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=0.0698, ret=-0.000159, glen=85.1, tlen=245, kl=0.0241, act_lr=8.8e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=-0.0497, ret=9.13e-5, glen=90.5, tlen=251, kl=0.0401, act_lr=8.8e-7, ent=1.38] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.16it/s, pg=-0.0497, ret=9.13e-5, glen=90.5, tlen=251, kl=0.0401, act_lr=8.8e-7, ent=1.38]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.16it/s, pg=0.0739, ret=-0.000532, glen=84, tlen=244, kl=0.0247, act_lr=8.8e-7, ent=1.2]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.0739, ret=-0.000532, glen=84, tlen=244, kl=0.0247, act_lr=8.8e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=-0.0955, ret=0.000534, glen=83.3, tlen=244, kl=0.0266, act_lr=8.8e-7, ent=1.35]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.0955, ret=0.000534, glen=83.3, tlen=244, kl=0.0266, act_lr=8.8e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=0.156, ret=-0.000659, glen=86.1, tlen=246, kl=0.0439, act_lr=8.8e-7, ent=1.28] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:19<00:23,  1.17it/s, pg=0.156, ret=-0.000659, glen=86.1, tlen=246, kl=0.0439, act_lr=8.8e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.0431, ret=0.000162, glen=81.2, tlen=241, kl=0.0233, act_lr=8.8e-7, ent=1.2]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.17it/s, pg=-0.0431, ret=0.000162, glen=81.2, tlen=241, kl=0.0233, act_lr=8.8e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=-0.00308, ret=-4.94e-6, glen=82.2, tlen=243, kl=0.0229, act_lr=8.8e-7, ent=1.27]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.17it/s, pg=-0.00308, ret=-4.94e-6, glen=82.2, tlen=243, kl=0.0229, act_lr=8.8e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.17it/s, pg=0.0669, ret=-0.000442, glen=91.6, tlen=252, kl=0.0338, act_lr=8.8e-7, ent=1.32] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.17it/s, pg=0.0669, ret=-0.000442, glen=91.6, tlen=252, kl=0.0338, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.17it/s, pg=-0.0205, ret=0.000503, glen=86.6, tlen=247, kl=0.0237, act_lr=8.8e-7, ent=1.3] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:20,  1.17it/s, pg=-0.0205, ret=0.000503, glen=86.6, tlen=247, kl=0.0237, act_lr=8.8e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:20,  1.17it/s, pg=-0.138, ret=0.000572, glen=93.1, tlen=254, kl=0.0235, act_lr=8.8e-7, ent=1.32]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:19,  1.17it/s, pg=-0.138, ret=0.000572, glen=93.1, tlen=254, kl=0.0235, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:19,  1.17it/s, pg=0.0161, ret=-0.000187, glen=85.2, tlen=246, kl=0.0233, act_lr=8.8e-7, ent=1.29]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.07it/s, pg=0.0161, ret=-0.000187, glen=85.2, tlen=246, kl=0.0233, act_lr=8.8e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.07it/s, pg=-0.0601, ret=0.000361, glen=87.8, tlen=248, kl=0.0204, act_lr=8.8e-7, ent=1.29]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:19,  1.10it/s, pg=-0.0601, ret=0.000361, glen=87.8, tlen=248, kl=0.0204, act_lr=8.8e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:19,  1.10it/s, pg=-0.0561, ret=0.000348, glen=85.7, tlen=246, kl=0.0353, act_lr=8.8e-7, ent=1.35]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.12it/s, pg=-0.0561, ret=0.000348, glen=85.7, tlen=246, kl=0.0353, act_lr=8.8e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.12it/s, pg=0.0515, ret=-7.07e-5, glen=90.6, tlen=251, kl=0.0422, act_lr=8.8e-7, ent=1.43] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:27<00:16,  1.14it/s, pg=0.0515, ret=-7.07e-5, glen=90.6, tlen=251, kl=0.0422, act_lr=8.8e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=0.00281, ret=-6.07e-5, glen=87, tlen=247, kl=0.0543, act_lr=8.8e-7, ent=1.27] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.15it/s, pg=0.00281, ret=-6.07e-5, glen=87, tlen=247, kl=0.0543, act_lr=8.8e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=0.0739, ret=-0.000403, glen=87, tlen=247, kl=0.0255, act_lr=8.8e-7, ent=1.26]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.16it/s, pg=0.0739, ret=-0.000403, glen=87, tlen=247, kl=0.0255, act_lr=8.8e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=-0.0576, ret=0.000368, glen=79.5, tlen=240, kl=0.0307, act_lr=8.8e-7, ent=1.29]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.16it/s, pg=-0.0576, ret=0.000368, glen=79.5, tlen=240, kl=0.0307, act_lr=8.8e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=0.0194, ret=-4.28e-6, glen=86.3, tlen=247, kl=0.022, act_lr=8.8e-7, ent=1.26]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=0.0194, ret=-4.28e-6, glen=86.3, tlen=247, kl=0.022, act_lr=8.8e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=-0.0122, ret=0.000292, glen=84.6, tlen=245, kl=0.03, act_lr=8.8e-7, ent=1.31]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=-0.0122, ret=0.000292, glen=84.6, tlen=245, kl=0.03, act_lr=8.8e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:11,  1.17it/s, pg=0.081, ret=-0.00104, glen=90.2, tlen=251, kl=0.0212, act_lr=8.8e-7, ent=1.28]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.081, ret=-0.00104, glen=90.2, tlen=251, kl=0.0212, act_lr=8.8e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.1, ret=-0.0005, glen=82.6, tlen=243, kl=0.035, act_lr=8.8e-7, ent=1.32]    Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:33<00:10,  1.17it/s, pg=0.1, ret=-0.0005, glen=82.6, tlen=243, kl=0.035, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-0.0151, ret=2.16e-5, glen=87.8, tlen=248, kl=0.026, act_lr=8.8e-7, ent=1.22]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.17it/s, pg=-0.0151, ret=2.16e-5, glen=87.8, tlen=248, kl=0.026, act_lr=8.8e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.114, ret=0.00079, glen=82.4, tlen=243, kl=0.0324, act_lr=8.8e-7, ent=1.32]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.17it/s, pg=-0.114, ret=0.00079, glen=82.4, tlen=243, kl=0.0324, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=-0.153, ret=0.000902, glen=87.9, tlen=248, kl=0.0243, act_lr=8.8e-7, ent=1.31]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.18it/s, pg=-0.153, ret=0.000902, glen=87.9, tlen=248, kl=0.0243, act_lr=8.8e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.18it/s, pg=-0.0656, ret=9.54e-5, glen=85, tlen=246, kl=0.027, act_lr=8.8e-7, ent=1.32]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.17it/s, pg=-0.0656, ret=9.54e-5, glen=85, tlen=246, kl=0.027, act_lr=8.8e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.17it/s, pg=0.156, ret=-0.000918, glen=83.5, tlen=244, kl=0.0414, act_lr=8.8e-7, ent=1.31]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=0.156, ret=-0.000918, glen=83.5, tlen=244, kl=0.0414, act_lr=8.8e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.17it/s, pg=0.0523, ret=-0.000403, glen=86.1, tlen=247, kl=0.0404, act_lr=8.8e-7, ent=1.34]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.17it/s, pg=0.0523, ret=-0.000403, glen=86.1, tlen=247, kl=0.0404, act_lr=8.8e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.17it/s, pg=-0.141, ret=0.000536, glen=86.1, tlen=247, kl=0.0242, act_lr=8.8e-7, ent=1.35] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:39<00:04,  1.17it/s, pg=-0.141, ret=0.000536, glen=86.1, tlen=247, kl=0.0242, act_lr=8.8e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=-0.0338, ret=0.0003, glen=83.5, tlen=244, kl=0.0282, act_lr=8.8e-7, ent=1.34] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.17it/s, pg=-0.0338, ret=0.0003, glen=83.5, tlen=244, kl=0.0282, act_lr=8.8e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.17it/s, pg=0.0993, ret=-0.00087, glen=82.9, tlen=243, kl=0.0246, act_lr=8.8e-7, ent=1.22]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=0.0993, ret=-0.00087, glen=82.9, tlen=243, kl=0.0246, act_lr=8.8e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=0.00775, ret=4.49e-5, glen=94.5, tlen=255, kl=0.0362, act_lr=8.8e-7, ent=1.33]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=0.00775, ret=4.49e-5, glen=94.5, tlen=255, kl=0.0362, act_lr=8.8e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=0.0831, ret=-0.000241, glen=86.8, tlen=247, kl=0.0245, act_lr=8.8e-7, ent=1.36]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=0.0831, ret=-0.000241, glen=86.8, tlen=247, kl=0.0245, act_lr=8.8e-7, ent=1.36]
2025-07-24 18:47:05.678 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 44.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=-0.0465, ret=0.000292, glen=81.2, tlen=241, kl=0.0263, act_lr=9e-7, ent=1.29]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=-0.0465, ret=0.000292, glen=81.2, tlen=241, kl=0.0263, act_lr=9e-7, ent=1.29]
2025-07-24 18:47:06.358 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 18:47:08.663 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.30s
2025-07-24 18:47:08.992 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 47.64s
2025-07-24 18:47:09.000 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0035704631431429995, 'actor_lr': 8.803921720273673e-07, 'clip_ratio': 0.0, 'entropy': 1.3121638625275855, 'kl': 0.02996347464767157, 'response_length': 86.7379570755304, 'total_length': 247.10017963484222, 'teacher_total_length': 259.0984847124885, 'return': -3.894908991687949e-06, 'policy_update_steps': 1.0}

Episode [4/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [19:25<20:14, 173.55s/it][A2025-07-24 18:47:09.044 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:48:51.278 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:48:51.449 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 18:48:51.450 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 102.41s
2025-07-24 18:48:53.268 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0138,avg_reflection_pattern_score: 0.0049,avg_pass_at_n: 1.0000,avg_num_tokens: 81.0912,std_num_tokens: 68.2098,avg_correct_num_tokens: 80.2064,std_correct_num_tokens: 39.8984,avg_incorrect_num_tokens: 95.4653,std_incorrect_num_tokens: 232.7190
2025-07-24 18:48:53.740 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.29s
2025-07-24 18:48:56.593 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.85s
2025-07-24 18:49:22.175 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 200
2025-07-24 18:49:22.176 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.58s
2025-07-24 18:49:23.577 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.89s
2025-07-24 18:49:23.578 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00025425972780794835, avg_kl: 0.0440081787109375, avg_response_length: 81.92896673202515, avg_orm_score: 0.0, avg_custom_rewards: 0.00025425972780794835
2025-07-24 18:49:23.613 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter45_replay_buffer.jsonl
2025-07-24 18:49:25.150 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.54s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:01<?, ?it/s, pg=-0.0381, ret=-5.36e-5, glen=84.9, tlen=245, kl=0.0404, act_lr=9e-7, ent=1.27]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:49,  1.02s/it, pg=-0.0381, ret=-5.36e-5, glen=84.9, tlen=245, kl=0.0404, act_lr=9e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:49,  1.02s/it, pg=0.127, ret=-0.000935, glen=77, tlen=238, kl=0.0288, act_lr=9e-7, ent=1.2]    Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:44,  1.08it/s, pg=0.127, ret=-0.000935, glen=77, tlen=238, kl=0.0288, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:44,  1.08it/s, pg=-0.123, ret=0.000645, glen=77.4, tlen=238, kl=0.0307, act_lr=9e-7, ent=1.19]Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:42,  1.10it/s, pg=-0.123, ret=0.000645, glen=77.4, tlen=238, kl=0.0307, act_lr=9e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:42,  1.10it/s, pg=-0.0928, ret=1.27e-5, glen=76.8, tlen=237, kl=0.0367, act_lr=9e-7, ent=1.19]Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:41,  1.10it/s, pg=-0.0928, ret=1.27e-5, glen=76.8, tlen=237, kl=0.0367, act_lr=9e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:41,  1.10it/s, pg=0.0257, ret=-0.000167, glen=84, tlen=245, kl=0.032, act_lr=9e-7, ent=1.33]  
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:40,  1.12it/s, pg=0.0257, ret=-0.000167, glen=84, tlen=245, kl=0.032, act_lr=9e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:40,  1.12it/s, pg=0.742, ret=-0.000676, glen=146, tlen=307, kl=0.0262, act_lr=9e-7, ent=1.96]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:39,  1.12it/s, pg=0.742, ret=-0.000676, glen=146, tlen=307, kl=0.0262, act_lr=9e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:39,  1.12it/s, pg=0.011, ret=-0.000175, glen=86.1, tlen=247, kl=0.028, act_lr=9e-7, ent=1.34]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:37,  1.14it/s, pg=0.011, ret=-0.000175, glen=86.1, tlen=247, kl=0.028, act_lr=9e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:37,  1.14it/s, pg=0.203, ret=-0.00129, glen=77.4, tlen=238, kl=0.0485, act_lr=9e-7, ent=1.18]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.15it/s, pg=0.203, ret=-0.00129, glen=77.4, tlen=238, kl=0.0485, act_lr=9e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:08<00:36,  1.15it/s, pg=-0.0774, ret=3.14e-5, glen=80.5, tlen=241, kl=0.0344, act_lr=9e-7, ent=1.2]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=-0.0774, ret=3.14e-5, glen=80.5, tlen=241, kl=0.0344, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=0.0291, ret=-0.000596, glen=79.2, tlen=240, kl=0.0697, act_lr=9e-7, ent=1.2]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.16it/s, pg=0.0291, ret=-0.000596, glen=79.2, tlen=240, kl=0.0697, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.16it/s, pg=0.083, ret=-0.000493, glen=77.4, tlen=238, kl=0.0323, act_lr=9e-7, ent=1.18]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.16it/s, pg=0.083, ret=-0.000493, glen=77.4, tlen=238, kl=0.0323, act_lr=9e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.16it/s, pg=0.0556, ret=-0.000323, glen=81.4, tlen=242, kl=0.0274, act_lr=9e-7, ent=1.23]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:32,  1.17it/s, pg=0.0556, ret=-0.000323, glen=81.4, tlen=242, kl=0.0274, act_lr=9e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:32,  1.17it/s, pg=-0.0772, ret=0.000201, glen=81.1, tlen=241, kl=0.028, act_lr=9e-7, ent=1.24] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:31,  1.17it/s, pg=-0.0772, ret=0.000201, glen=81.1, tlen=241, kl=0.028, act_lr=9e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:31,  1.17it/s, pg=0.0987, ret=-0.000749, glen=80.5, tlen=241, kl=0.032, act_lr=9e-7, ent=1.2] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:30,  1.17it/s, pg=0.0987, ret=-0.000749, glen=80.5, tlen=241, kl=0.032, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:30,  1.17it/s, pg=-0.0823, ret=0.000327, glen=81.2, tlen=242, kl=0.037, act_lr=9e-7, ent=1.22]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:29,  1.17it/s, pg=-0.0823, ret=0.000327, glen=81.2, tlen=242, kl=0.037, act_lr=9e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:29,  1.17it/s, pg=-0.00596, ret=-0.000198, glen=81.2, tlen=241, kl=0.0863, act_lr=9e-7, ent=1.23]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:13<00:29,  1.17it/s, pg=-0.00596, ret=-0.000198, glen=81.2, tlen=241, kl=0.0863, act_lr=9e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.17it/s, pg=-0.0669, ret=0.000174, glen=75.3, tlen=236, kl=0.0302, act_lr=9e-7, ent=1.21]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:14<00:28,  1.17it/s, pg=-0.0669, ret=0.000174, glen=75.3, tlen=236, kl=0.0302, act_lr=9e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.17it/s, pg=-0.169, ret=0.000818, glen=90.5, tlen=251, kl=0.0244, act_lr=9e-7, ent=1.39] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.17it/s, pg=-0.169, ret=0.000818, glen=90.5, tlen=251, kl=0.0244, act_lr=9e-7, ent=1.39]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.17it/s, pg=-0.175, ret=0.000763, glen=83.2, tlen=244, kl=0.0311, act_lr=9e-7, ent=1.21]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:26,  1.17it/s, pg=-0.175, ret=0.000763, glen=83.2, tlen=244, kl=0.0311, act_lr=9e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:26,  1.17it/s, pg=-0.173, ret=0.000573, glen=81.4, tlen=242, kl=0.063, act_lr=9e-7, ent=1.22] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:25,  1.17it/s, pg=-0.173, ret=0.000573, glen=81.4, tlen=242, kl=0.063, act_lr=9e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:25,  1.17it/s, pg=0.0919, ret=-0.000246, glen=82.5, tlen=243, kl=0.0259, act_lr=9e-7, ent=1.23]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:24,  1.17it/s, pg=0.0919, ret=-0.000246, glen=82.5, tlen=243, kl=0.0259, act_lr=9e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:24,  1.17it/s, pg=0.034, ret=-0.000348, glen=82.1, tlen=242, kl=0.0264, act_lr=9e-7, ent=1.27] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:23,  1.17it/s, pg=0.034, ret=-0.000348, glen=82.1, tlen=242, kl=0.0264, act_lr=9e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:23,  1.17it/s, pg=-0.00989, ret=-6.84e-5, glen=84.2, tlen=245, kl=0.028, act_lr=9e-7, ent=1.28]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:19<00:23,  1.16it/s, pg=-0.00989, ret=-6.84e-5, glen=84.2, tlen=245, kl=0.028, act_lr=9e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.16it/s, pg=-0.0599, ret=0.000244, glen=83.9, tlen=245, kl=0.0358, act_lr=9e-7, ent=1.24]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.16it/s, pg=-0.0599, ret=0.000244, glen=83.9, tlen=245, kl=0.0358, act_lr=9e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.16it/s, pg=0.0576, ret=1.87e-6, glen=81.5, tlen=242, kl=0.0301, act_lr=9e-7, ent=1.21]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.16it/s, pg=0.0576, ret=1.87e-6, glen=81.5, tlen=242, kl=0.0301, act_lr=9e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.16it/s, pg=0.00767, ret=-0.000151, glen=81.6, tlen=242, kl=0.0326, act_lr=9e-7, ent=1.24]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.17it/s, pg=0.00767, ret=-0.000151, glen=81.6, tlen=242, kl=0.0326, act_lr=9e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:23<00:20,  1.17it/s, pg=0.144, ret=-0.000604, glen=86.3, tlen=247, kl=0.234, act_lr=9e-7, ent=1.2]    Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:23<00:19,  1.17it/s, pg=0.144, ret=-0.000604, glen=86.3, tlen=247, kl=0.234, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:19,  1.17it/s, pg=-0.0123, ret=-0.00011, glen=81.3, tlen=242, kl=0.0295, act_lr=9e-7, ent=1.19]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:18,  1.17it/s, pg=-0.0123, ret=-0.00011, glen=81.3, tlen=242, kl=0.0295, act_lr=9e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:18,  1.17it/s, pg=-0.142, ret=0.000458, glen=83.8, tlen=244, kl=0.043, act_lr=9e-7, ent=1.21]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:19,  1.07it/s, pg=-0.142, ret=0.000458, glen=83.8, tlen=244, kl=0.043, act_lr=9e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:19,  1.07it/s, pg=0.0902, ret=-0.000621, glen=78.6, tlen=239, kl=0.0523, act_lr=9e-7, ent=1.24]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:18,  1.09it/s, pg=0.0902, ret=-0.000621, glen=78.6, tlen=239, kl=0.0523, act_lr=9e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:18,  1.09it/s, pg=0.0332, ret=2.51e-5, glen=76.1, tlen=236, kl=0.0316, act_lr=9e-7, ent=1.18]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:17,  1.11it/s, pg=0.0332, ret=2.51e-5, glen=76.1, tlen=236, kl=0.0316, act_lr=9e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:17,  1.11it/s, pg=-0.123, ret=0.000611, glen=81.1, tlen=241, kl=0.0269, act_lr=9e-7, ent=1.26]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:27<00:15,  1.13it/s, pg=-0.123, ret=0.000611, glen=81.1, tlen=241, kl=0.0269, act_lr=9e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.13it/s, pg=-0.0344, ret=0.000121, glen=79.3, tlen=240, kl=0.0307, act_lr=9e-7, ent=1.24]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:28<00:14,  1.14it/s, pg=-0.0344, ret=0.000121, glen=79.3, tlen=240, kl=0.0307, act_lr=9e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.14it/s, pg=-0.195, ret=0.00069, glen=81.6, tlen=242, kl=0.0302, act_lr=9e-7, ent=1.28]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:29<00:13,  1.15it/s, pg=-0.195, ret=0.00069, glen=81.6, tlen=242, kl=0.0302, act_lr=9e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:13,  1.15it/s, pg=-0.106, ret=0.00041, glen=76.6, tlen=237, kl=0.0342, act_lr=9e-7, ent=1.18]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:12,  1.16it/s, pg=-0.106, ret=0.00041, glen=76.6, tlen=237, kl=0.0342, act_lr=9e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:12,  1.16it/s, pg=-0.0418, ret=-2.1e-5, glen=79.1, tlen=240, kl=0.0429, act_lr=9e-7, ent=1.26]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.16it/s, pg=-0.0418, ret=-2.1e-5, glen=79.1, tlen=240, kl=0.0429, act_lr=9e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.16it/s, pg=-0.0952, ret=0.00164, glen=80.5, tlen=241, kl=0.0356, act_lr=9e-7, ent=1.23]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.17it/s, pg=-0.0952, ret=0.00164, glen=80.5, tlen=241, kl=0.0356, act_lr=9e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.17it/s, pg=-0.116, ret=0.000339, glen=81.6, tlen=242, kl=0.0244, act_lr=9e-7, ent=1.28]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=-0.116, ret=0.000339, glen=81.6, tlen=242, kl=0.0244, act_lr=9e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=0.00793, ret=-0.000292, glen=77.7, tlen=238, kl=0.0322, act_lr=9e-7, ent=1.2]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:33<00:09,  1.17it/s, pg=0.00793, ret=-0.000292, glen=77.7, tlen=238, kl=0.0322, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=-0.00928, ret=0.000216, glen=82.3, tlen=243, kl=0.0467, act_lr=9e-7, ent=1.22]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:34<00:08,  1.17it/s, pg=-0.00928, ret=0.000216, glen=82.3, tlen=243, kl=0.0467, act_lr=9e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.17it/s, pg=-0.148, ret=0.000507, glen=79.9, tlen=241, kl=0.0381, act_lr=9e-7, ent=1.19]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:35<00:07,  1.17it/s, pg=-0.148, ret=0.000507, glen=79.9, tlen=241, kl=0.0381, act_lr=9e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.17it/s, pg=0.0478, ret=-0.000679, glen=77.6, tlen=238, kl=0.0551, act_lr=9e-7, ent=1.19]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.17it/s, pg=0.0478, ret=-0.000679, glen=77.6, tlen=238, kl=0.0551, act_lr=9e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.17it/s, pg=-0.147, ret=0.000573, glen=76.5, tlen=237, kl=0.0351, act_lr=9e-7, ent=1.2]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:05,  1.17it/s, pg=-0.147, ret=0.000573, glen=76.5, tlen=237, kl=0.0351, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:05,  1.17it/s, pg=-0.188, ret=0.000825, glen=81.7, tlen=242, kl=0.0269, act_lr=9e-7, ent=1.24]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.17it/s, pg=-0.188, ret=0.000825, glen=81.7, tlen=242, kl=0.0269, act_lr=9e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.17it/s, pg=-0.115, ret=0.00042, glen=78.6, tlen=239, kl=0.11, act_lr=9e-7, ent=1.27]   Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=-0.115, ret=0.00042, glen=78.6, tlen=239, kl=0.11, act_lr=9e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=-0.00616, ret=-0.000278, glen=78.4, tlen=239, kl=0.0319, act_lr=9e-7, ent=1.27]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:39<00:03,  1.17it/s, pg=-0.00616, ret=-0.000278, glen=78.4, tlen=239, kl=0.0319, act_lr=9e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=0.00452, ret=-0.000257, glen=81.9, tlen=243, kl=0.172, act_lr=9e-7, ent=1.2]   Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:40<00:02,  1.17it/s, pg=0.00452, ret=-0.000257, glen=81.9, tlen=243, kl=0.172, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.0756, ret=4.57e-5, glen=79.8, tlen=240, kl=0.0281, act_lr=9e-7, ent=1.2] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:41<00:01,  1.15it/s, pg=-0.0756, ret=4.57e-5, glen=79.8, tlen=240, kl=0.0281, act_lr=9e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.15it/s, pg=-0.0775, ret=0.000313, glen=80.2, tlen=240, kl=0.0327, act_lr=9e-7, ent=1.27]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.15it/s, pg=-0.0775, ret=0.000313, glen=80.2, tlen=240, kl=0.0327, act_lr=9e-7, ent=1.27]
2025-07-24 18:50:08.844 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 43.51s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.15it/s, pg=0.062, ret=-0.00054, glen=77.8, tlen=238, kl=0.031, act_lr=9.2e-7, ent=1.21] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.13it/s, pg=0.062, ret=-0.00054, glen=77.8, tlen=238, kl=0.031, act_lr=9.2e-7, ent=1.21]
2025-07-24 18:50:09.696 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 18:50:12.267 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 18:50:12.600 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 47.39s
2025-07-24 18:50:12.607 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01653968811035156, 'actor_lr': 9.003999753076642e-07, 'clip_ratio': 0.0, 'entropy': 1.2453865575790406, 'kl': 0.0440081787109375, 'response_length': 81.92896636962891, 'total_length': 242.5030374145508, 'teacher_total_length': 254.68751678466796, 'return': 2.220075220975559e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [22:28<17:41, 176.84s/it][A2025-07-24 18:50:12.652 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:51:00.772 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:51:00.974 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.20s
2025-07-24 18:51:00.975 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 48.32s
2025-07-24 18:51:03.115 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0135,avg_reflection_pattern_score: 0.0050,avg_pass_at_n: 1.0000,avg_num_tokens: 80.8016,std_num_tokens: 39.3579,avg_correct_num_tokens: 80.4783,std_correct_num_tokens: 39.3368,avg_incorrect_num_tokens: 86.2370,std_incorrect_num_tokens: 39.3143
2025-07-24 18:51:03.540 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.56s
2025-07-24 18:51:06.297 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.76s
2025-07-24 18:51:31.752 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 199
2025-07-24 18:51:31.752 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.45s
2025-07-24 18:51:33.121 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 18:51:33.121 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.2621955698047152e-05, avg_kl: 0.05054263014290201, avg_response_length: 80.93133058020817, avg_orm_score: 0.0, avg_custom_rewards: 1.2621955698047152e-05
2025-07-24 18:51:33.160 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter46_replay_buffer.jsonl
2025-07-24 18:51:34.703 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.54s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:01<?, ?it/s, pg=-0.00336, ret=0.000299, glen=77.7, tlen=239, kl=0.0367, act_lr=9.2e-7, ent=1.28]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:50,  1.03s/it, pg=-0.00336, ret=0.000299, glen=77.7, tlen=239, kl=0.0367, act_lr=9.2e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:50,  1.03s/it, pg=-0.0769, ret=0.000272, glen=80, tlen=241, kl=0.0649, act_lr=9.2e-7, ent=1.21]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:44,  1.07it/s, pg=-0.0769, ret=0.000272, glen=80, tlen=241, kl=0.0649, act_lr=9.2e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:44,  1.07it/s, pg=-0.0924, ret=0.000471, glen=83.6, tlen=244, kl=0.0339, act_lr=9.2e-7, ent=1.29]Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:43,  1.09it/s, pg=-0.0924, ret=0.000471, glen=83.6, tlen=244, kl=0.0339, act_lr=9.2e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:43,  1.09it/s, pg=0.135, ret=-0.000291, glen=79.9, tlen=241, kl=0.0547, act_lr=9.2e-7, ent=1.23] Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:41,  1.12it/s, pg=0.135, ret=-0.000291, glen=79.9, tlen=241, kl=0.0547, act_lr=9.2e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:41,  1.12it/s, pg=0.0237, ret=-3.09e-5, glen=79.9, tlen=240, kl=0.0301, act_lr=9.2e-7, ent=1.16]Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:40,  1.11it/s, pg=0.0237, ret=-3.09e-5, glen=79.9, tlen=240, kl=0.0301, act_lr=9.2e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:40,  1.11it/s, pg=0.18, ret=-0.000799, glen=86.8, tlen=247, kl=0.032, act_lr=9.2e-7, ent=1.18]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:39,  1.13it/s, pg=0.18, ret=-0.000799, glen=86.8, tlen=247, kl=0.032, act_lr=9.2e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:39,  1.13it/s, pg=-0.0494, ret=0.000367, glen=79.3, tlen=240, kl=0.04, act_lr=9.2e-7, ent=1.23]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:37,  1.14it/s, pg=-0.0494, ret=0.000367, glen=79.3, tlen=240, kl=0.04, act_lr=9.2e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:37,  1.14it/s, pg=-0.0439, ret=0.000348, glen=77.4, tlen=238, kl=0.0367, act_lr=9.2e-7, ent=1.2]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.15it/s, pg=-0.0439, ret=0.000348, glen=77.4, tlen=238, kl=0.0367, act_lr=9.2e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:08<00:36,  1.15it/s, pg=0.0239, ret=-2.64e-5, glen=77.2, tlen=238, kl=0.168, act_lr=9.2e-7, ent=1.19] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=0.0239, ret=-2.64e-5, glen=77.2, tlen=238, kl=0.168, act_lr=9.2e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=-0.00946, ret=-9.12e-5, glen=88.4, tlen=249, kl=0.0335, act_lr=9.2e-7, ent=1.33]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.16it/s, pg=-0.00946, ret=-9.12e-5, glen=88.4, tlen=249, kl=0.0335, act_lr=9.2e-7, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.16it/s, pg=-0.0434, ret=0.000305, glen=85.6, tlen=247, kl=0.0485, act_lr=9.2e-7, ent=1.24] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.16it/s, pg=-0.0434, ret=0.000305, glen=85.6, tlen=247, kl=0.0485, act_lr=9.2e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.16it/s, pg=0.0265, ret=0.000128, glen=77.4, tlen=238, kl=0.063, act_lr=9.2e-7, ent=1.26]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:33,  1.14it/s, pg=0.0265, ret=0.000128, glen=77.4, tlen=238, kl=0.063, act_lr=9.2e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:33,  1.14it/s, pg=0.0379, ret=-0.000392, glen=83.3, tlen=243, kl=0.0326, act_lr=9.2e-7, ent=1.31]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:32,  1.15it/s, pg=0.0379, ret=-0.000392, glen=83.3, tlen=243, kl=0.0326, act_lr=9.2e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:32,  1.15it/s, pg=-0.0922, ret=0.000453, glen=82.3, tlen=243, kl=0.0256, act_lr=9.2e-7, ent=1.17]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:31,  1.15it/s, pg=-0.0922, ret=0.000453, glen=82.3, tlen=243, kl=0.0256, act_lr=9.2e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:31,  1.15it/s, pg=-0.152, ret=0.00057, glen=78.3, tlen=239, kl=0.0329, act_lr=9.2e-7, ent=1.23]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:30,  1.16it/s, pg=-0.152, ret=0.00057, glen=78.3, tlen=239, kl=0.0329, act_lr=9.2e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:14<00:30,  1.16it/s, pg=-0.0814, ret=0.000357, glen=83.8, tlen=244, kl=0.0396, act_lr=9.2e-7, ent=1.22]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.16it/s, pg=-0.0814, ret=0.000357, glen=83.8, tlen=244, kl=0.0396, act_lr=9.2e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.16it/s, pg=-0.025, ret=0.000282, glen=81.8, tlen=243, kl=0.0389, act_lr=9.2e-7, ent=1.24] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:14<00:28,  1.17it/s, pg=-0.025, ret=0.000282, glen=81.8, tlen=243, kl=0.0389, act_lr=9.2e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.17it/s, pg=0.0433, ret=-0.000185, glen=83, tlen=244, kl=0.0337, act_lr=9.2e-7, ent=1.24] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.17it/s, pg=0.0433, ret=-0.000185, glen=83, tlen=244, kl=0.0337, act_lr=9.2e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.17it/s, pg=0.0175, ret=-0.000248, glen=82.4, tlen=242, kl=0.0318, act_lr=9.2e-7, ent=1.21]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:26,  1.17it/s, pg=0.0175, ret=-0.000248, glen=82.4, tlen=242, kl=0.0318, act_lr=9.2e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:26,  1.17it/s, pg=0.0152, ret=-0.00019, glen=81.7, tlen=242, kl=0.0378, act_lr=9.2e-7, ent=1.23] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:25,  1.17it/s, pg=0.0152, ret=-0.00019, glen=81.7, tlen=242, kl=0.0378, act_lr=9.2e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:25,  1.17it/s, pg=0.00296, ret=0.000196, glen=83.4, tlen=244, kl=0.214, act_lr=9.2e-7, ent=1.24]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:24,  1.17it/s, pg=0.00296, ret=0.000196, glen=83.4, tlen=244, kl=0.214, act_lr=9.2e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:24,  1.17it/s, pg=-0.163, ret=0.000724, glen=81.5, tlen=241, kl=0.0368, act_lr=9.2e-7, ent=1.17]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:23,  1.17it/s, pg=-0.163, ret=0.000724, glen=81.5, tlen=241, kl=0.0368, act_lr=9.2e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:20<00:23,  1.17it/s, pg=0.198, ret=-0.001, glen=77.9, tlen=239, kl=0.0729, act_lr=9.2e-7, ent=1.22]   Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.17it/s, pg=0.198, ret=-0.001, glen=77.9, tlen=239, kl=0.0729, act_lr=9.2e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.17it/s, pg=-0.126, ret=0.000745, glen=81, tlen=242, kl=0.035, act_lr=9.2e-7, ent=1.21]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.17it/s, pg=-0.126, ret=0.000745, glen=81, tlen=242, kl=0.035, act_lr=9.2e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.17it/s, pg=0.0143, ret=-0.000352, glen=75.8, tlen=236, kl=0.0355, act_lr=9.2e-7, ent=1.19]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.17it/s, pg=0.0143, ret=-0.000352, glen=75.8, tlen=236, kl=0.0355, act_lr=9.2e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.17it/s, pg=0.103, ret=-0.000395, glen=80.8, tlen=241, kl=0.0345, act_lr=9.2e-7, ent=1.18] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.17it/s, pg=0.103, ret=-0.000395, glen=80.8, tlen=241, kl=0.0345, act_lr=9.2e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:23<00:20,  1.17it/s, pg=0.0833, ret=-0.000504, glen=84.7, tlen=245, kl=0.053, act_lr=9.2e-7, ent=1.34]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:23<00:20,  1.15it/s, pg=0.0833, ret=-0.000504, glen=84.7, tlen=245, kl=0.053, act_lr=9.2e-7, ent=1.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:20,  1.15it/s, pg=-0.0743, ret=0.000164, glen=80, tlen=241, kl=0.205, act_lr=9.2e-7, ent=1.22]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:19,  1.15it/s, pg=-0.0743, ret=0.000164, glen=80, tlen=241, kl=0.205, act_lr=9.2e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:19,  1.15it/s, pg=0.0489, ret=-0.000227, glen=80.1, tlen=241, kl=0.185, act_lr=9.2e-7, ent=1.18]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:20,  1.01it/s, pg=0.0489, ret=-0.000227, glen=80.1, tlen=241, kl=0.185, act_lr=9.2e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:20,  1.01it/s, pg=-0.0441, ret=0.000341, glen=80.7, tlen=242, kl=0.0375, act_lr=9.2e-7, ent=1.27]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:19,  1.05it/s, pg=-0.0441, ret=0.000341, glen=80.7, tlen=242, kl=0.0375, act_lr=9.2e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:19,  1.05it/s, pg=0.0634, ret=-0.000506, glen=80.4, tlen=241, kl=0.0309, act_lr=9.2e-7, ent=1.26]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:17,  1.09it/s, pg=0.0634, ret=-0.000506, glen=80.4, tlen=241, kl=0.0309, act_lr=9.2e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:28<00:17,  1.09it/s, pg=0.0769, ret=-0.000321, glen=80.8, tlen=241, kl=0.0375, act_lr=9.2e-7, ent=1.2] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:16,  1.11it/s, pg=0.0769, ret=-0.000321, glen=80.8, tlen=241, kl=0.0375, act_lr=9.2e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:29<00:16,  1.11it/s, pg=0.0229, ret=-5.44e-5, glen=77, tlen=237, kl=0.0384, act_lr=9.2e-7, ent=1.2]   Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:15,  1.13it/s, pg=0.0229, ret=-5.44e-5, glen=77, tlen=237, kl=0.0384, act_lr=9.2e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:15,  1.13it/s, pg=-0.0823, ret=0.000197, glen=78.9, tlen=239, kl=0.035, act_lr=9.2e-7, ent=1.28]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:29<00:14,  1.14it/s, pg=-0.0823, ret=0.000197, glen=78.9, tlen=239, kl=0.035, act_lr=9.2e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:14,  1.14it/s, pg=-0.164, ret=0.000491, glen=85.8, tlen=246, kl=0.0292, act_lr=9.2e-7, ent=1.27]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:13,  1.15it/s, pg=-0.164, ret=0.000491, glen=85.8, tlen=246, kl=0.0292, act_lr=9.2e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:13,  1.15it/s, pg=0.128, ret=-0.000972, glen=83.9, tlen=244, kl=0.0402, act_lr=9.2e-7, ent=1.26]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.16it/s, pg=0.128, ret=-0.000972, glen=83.9, tlen=244, kl=0.0402, act_lr=9.2e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.16it/s, pg=-0.0898, ret=0.000249, glen=78.4, tlen=239, kl=0.0341, act_lr=9.2e-7, ent=1.23]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.16it/s, pg=-0.0898, ret=0.000249, glen=78.4, tlen=239, kl=0.0341, act_lr=9.2e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.16it/s, pg=0.152, ret=-0.000918, glen=76, tlen=236, kl=0.0352, act_lr=9.2e-7, ent=1.14]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=0.152, ret=-0.000918, glen=76, tlen=236, kl=0.0352, act_lr=9.2e-7, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:34<00:10,  1.17it/s, pg=0.218, ret=-0.00045, glen=75.9, tlen=236, kl=0.0378, act_lr=9.2e-7, ent=1.24]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=0.218, ret=-0.00045, glen=75.9, tlen=236, kl=0.0378, act_lr=9.2e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:35<00:09,  1.17it/s, pg=-0.00266, ret=-0.00019, glen=81.8, tlen=243, kl=0.0375, act_lr=9.2e-7, ent=1.22]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.17it/s, pg=-0.00266, ret=-0.00019, glen=81.8, tlen=243, kl=0.0375, act_lr=9.2e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.17it/s, pg=-0.122, ret=0.000503, glen=74.6, tlen=235, kl=0.0352, act_lr=9.2e-7, ent=1.18]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:35<00:07,  1.17it/s, pg=-0.122, ret=0.000503, glen=74.6, tlen=235, kl=0.0352, act_lr=9.2e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.17it/s, pg=-0.0418, ret=0.00014, glen=82.4, tlen=243, kl=0.034, act_lr=9.2e-7, ent=1.22] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.17it/s, pg=-0.0418, ret=0.00014, glen=82.4, tlen=243, kl=0.034, act_lr=9.2e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.17it/s, pg=0.0811, ret=-0.000325, glen=82.1, tlen=242, kl=0.0363, act_lr=9.2e-7, ent=1.3]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:05,  1.17it/s, pg=0.0811, ret=-0.000325, glen=82.1, tlen=242, kl=0.0363, act_lr=9.2e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:05,  1.17it/s, pg=0.0326, ret=8.43e-5, glen=80.9, tlen=241, kl=0.0332, act_lr=9.2e-7, ent=1.2]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.17it/s, pg=0.0326, ret=8.43e-5, glen=80.9, tlen=241, kl=0.0332, act_lr=9.2e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.17it/s, pg=-0.000854, ret=0.000279, glen=81.4, tlen=242, kl=0.0336, act_lr=9.2e-7, ent=1.25]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=-0.000854, ret=0.000279, glen=81.4, tlen=242, kl=0.0336, act_lr=9.2e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:40<00:04,  1.17it/s, pg=0.0761, ret=-0.000234, glen=85.8, tlen=247, kl=0.0314, act_lr=9.2e-7, ent=1.22]  Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=0.0761, ret=-0.000234, glen=85.8, tlen=247, kl=0.0314, act_lr=9.2e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=0.0101, ret=-6.8e-5, glen=85.7, tlen=246, kl=0.0323, act_lr=9.2e-7, ent=1.28]  Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:40<00:02,  1.17it/s, pg=0.0101, ret=-6.8e-5, glen=85.7, tlen=246, kl=0.0323, act_lr=9.2e-7, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.173, ret=0.000741, glen=82.9, tlen=243, kl=0.0364, act_lr=9.2e-7, ent=1.23]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:41<00:01,  1.17it/s, pg=-0.173, ret=0.000741, glen=82.9, tlen=243, kl=0.0364, act_lr=9.2e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.17it/s, pg=0.0644, ret=-0.000372, glen=78.1, tlen=238, kl=0.0375, act_lr=9.2e-7, ent=1.24]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.17it/s, pg=0.0644, ret=-0.000372, glen=78.1, tlen=238, kl=0.0375, act_lr=9.2e-7, ent=1.24]
2025-07-24 18:52:18.556 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 43.67s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.17it/s, pg=-0.0719, ret=0.000385, glen=79.3, tlen=240, kl=0.0336, act_lr=9.4e-7, ent=1.24]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.12it/s, pg=-0.0719, ret=0.000385, glen=79.3, tlen=240, kl=0.0336, act_lr=9.4e-7, ent=1.24]
2025-07-24 18:52:19.399 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 18:52:21.967 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 18:52:22.311 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 47.54s
2025-07-24 18:52:22.318 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.001063995361328125, 'actor_lr': 9.203999911733263e-07, 'clip_ratio': 0.0, 'entropy': 1.2313026905059814, 'kl': 0.0504693603515625, 'response_length': 80.95629867553711, 'total_length': 241.42681030273437, 'teacher_total_length': 253.51202575683592, 'return': -1.1203992471564561e-06, 'policy_update_steps': 1.0}

Episode [4/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [24:38<13:29, 161.84s/it][A2025-07-24 18:52:22.362 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:53:24.951 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:53:25.134 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:53:25.135 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 62.77s
2025-07-24 18:53:26.999 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0137,avg_reflection_pattern_score: 0.0059,avg_pass_at_n: 1.0000,avg_num_tokens: 76.6750,std_num_tokens: 45.1425,avg_correct_num_tokens: 76.0606,std_correct_num_tokens: 44.8729,avg_incorrect_num_tokens: 87.4480,std_incorrect_num_tokens: 48.3804
2025-07-24 18:53:27.375 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.24s
2025-07-24 18:53:30.507 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.13s
2025-07-24 18:53:55.978 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 197
2025-07-24 18:53:55.979 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.47s
2025-07-24 18:53:57.293 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 18:53:57.293 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0004742861906217818, avg_kl: 0.0687460342639594, avg_response_length: 76.84028040697127, avg_orm_score: 0.0, avg_custom_rewards: 0.0004742861906217818
2025-07-24 18:53:57.323 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter47_replay_buffer.jsonl
2025-07-24 18:53:58.816 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.50s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:01<?, ?it/s, pg=-0.101, ret=0.000211, glen=80, tlen=241, kl=0.04, act_lr=9.4e-7, ent=1.21]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:50,  1.02s/it, pg=-0.101, ret=0.000211, glen=80, tlen=241, kl=0.04, act_lr=9.4e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:50,  1.02s/it, pg=0.0072, ret=0.00932, glen=72.7, tlen=232, kl=0.0432, act_lr=9.4e-7, ent=1.11]Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:44,  1.08it/s, pg=0.0072, ret=0.00932, glen=72.7, tlen=232, kl=0.0432, act_lr=9.4e-7, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:44,  1.08it/s, pg=-0.0356, ret=7.2e-5, glen=81.6, tlen=243, kl=0.0595, act_lr=9.4e-7, ent=1.3] Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:42,  1.09it/s, pg=-0.0356, ret=7.2e-5, glen=81.6, tlen=243, kl=0.0595, act_lr=9.4e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:42,  1.09it/s, pg=-0.00528, ret=0.00013, glen=75, tlen=236, kl=0.0447, act_lr=9.4e-7, ent=1.18]Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:41,  1.12it/s, pg=-0.00528, ret=0.00013, glen=75, tlen=236, kl=0.0447, act_lr=9.4e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:41,  1.12it/s, pg=-0.106, ret=0.000429, glen=75, tlen=236, kl=0.0486, act_lr=9.4e-7, ent=1.26] Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:40,  1.12it/s, pg=-0.106, ret=0.000429, glen=75, tlen=236, kl=0.0486, act_lr=9.4e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:40,  1.12it/s, pg=0.1, ret=-0.000347, glen=72.2, tlen=233, kl=0.0433, act_lr=9.4e-7, ent=1.16]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:38,  1.14it/s, pg=0.1, ret=-0.000347, glen=72.2, tlen=233, kl=0.0433, act_lr=9.4e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:38,  1.14it/s, pg=0.0263, ret=-0.00058, glen=78.6, tlen=239, kl=0.0463, act_lr=9.4e-7, ent=1.22]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:38,  1.13it/s, pg=0.0263, ret=-0.00058, glen=78.6, tlen=239, kl=0.0463, act_lr=9.4e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:38,  1.13it/s, pg=0.0745, ret=-6.64e-5, glen=74.7, tlen=236, kl=0.0437, act_lr=9.4e-7, ent=1.19]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.14it/s, pg=0.0745, ret=-6.64e-5, glen=74.7, tlen=236, kl=0.0437, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.14it/s, pg=0.0989, ret=-0.000668, glen=75.8, tlen=236, kl=0.0471, act_lr=9.4e-7, ent=1.21]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:07<00:35,  1.15it/s, pg=0.0989, ret=-0.000668, glen=75.8, tlen=236, kl=0.0471, act_lr=9.4e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=0.136, ret=-0.000444, glen=86, tlen=247, kl=0.041, act_lr=9.4e-7, ent=1.25]    Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.16it/s, pg=0.136, ret=-0.000444, glen=86, tlen=247, kl=0.041, act_lr=9.4e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.16it/s, pg=-0.118, ret=0.000535, glen=74.5, tlen=235, kl=0.0456, act_lr=9.4e-7, ent=1.2]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.16it/s, pg=-0.118, ret=0.000535, glen=74.5, tlen=235, kl=0.0456, act_lr=9.4e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.16it/s, pg=-0.0121, ret=0.000307, glen=74.9, tlen=236, kl=0.0409, act_lr=9.4e-7, ent=1.18]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:32,  1.16it/s, pg=-0.0121, ret=0.000307, glen=74.9, tlen=236, kl=0.0409, act_lr=9.4e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:32,  1.16it/s, pg=0.0193, ret=-0.000116, glen=78, tlen=239, kl=0.048, act_lr=9.4e-7, ent=1.22]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:31,  1.17it/s, pg=0.0193, ret=-0.000116, glen=78, tlen=239, kl=0.048, act_lr=9.4e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:31,  1.17it/s, pg=-0.031, ret=0.00027, glen=71.9, tlen=232, kl=0.0538, act_lr=9.4e-7, ent=1.14]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:30,  1.17it/s, pg=-0.031, ret=0.00027, glen=71.9, tlen=232, kl=0.0538, act_lr=9.4e-7, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:30,  1.17it/s, pg=0.0721, ret=-0.000136, glen=76.6, tlen=237, kl=0.0491, act_lr=9.4e-7, ent=1.24]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:30,  1.15it/s, pg=0.0721, ret=-0.000136, glen=76.6, tlen=237, kl=0.0491, act_lr=9.4e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:14<00:30,  1.15it/s, pg=0.0117, ret=-0.000155, glen=77.5, tlen=238, kl=0.118, act_lr=9.4e-7, ent=1.24] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.15it/s, pg=0.0117, ret=-0.000155, glen=77.5, tlen=238, kl=0.118, act_lr=9.4e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.15it/s, pg=-0.0436, ret=0.000238, glen=76, tlen=237, kl=0.0436, act_lr=9.4e-7, ent=1.19] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:14<00:28,  1.16it/s, pg=-0.0436, ret=0.000238, glen=76, tlen=237, kl=0.0436, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.16it/s, pg=-0.123, ret=0.000508, glen=78.9, tlen=240, kl=0.0426, act_lr=9.4e-7, ent=1.2]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.16it/s, pg=-0.123, ret=0.000508, glen=78.9, tlen=240, kl=0.0426, act_lr=9.4e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.16it/s, pg=0.0786, ret=-0.000449, glen=75.4, tlen=236, kl=0.0413, act_lr=9.4e-7, ent=1.18]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:26,  1.17it/s, pg=0.0786, ret=-0.000449, glen=75.4, tlen=236, kl=0.0413, act_lr=9.4e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:26,  1.17it/s, pg=0.169, ret=-0.000328, glen=76.8, tlen=237, kl=0.0489, act_lr=9.4e-7, ent=1.18] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:25,  1.17it/s, pg=0.169, ret=-0.000328, glen=76.8, tlen=237, kl=0.0489, act_lr=9.4e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:25,  1.17it/s, pg=-0.12, ret=0.000537, glen=75.2, tlen=236, kl=0.0556, act_lr=9.4e-7, ent=1.2]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:25,  1.14it/s, pg=-0.12, ret=0.000537, glen=75.2, tlen=236, kl=0.0556, act_lr=9.4e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:25,  1.14it/s, pg=-0.0586, ret=0.000315, glen=79, tlen=240, kl=0.046, act_lr=9.4e-7, ent=1.24]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:24,  1.15it/s, pg=-0.0586, ret=0.000315, glen=79, tlen=240, kl=0.046, act_lr=9.4e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:20<00:24,  1.15it/s, pg=0.0864, ret=-9.18e-5, glen=75.1, tlen=236, kl=0.0381, act_lr=9.4e-7, ent=1.17]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.16it/s, pg=0.0864, ret=-9.18e-5, glen=75.1, tlen=236, kl=0.0381, act_lr=9.4e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.16it/s, pg=-0.028, ret=8.18e-5, glen=79.5, tlen=240, kl=0.0387, act_lr=9.4e-7, ent=1.19] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.16it/s, pg=-0.028, ret=8.18e-5, glen=79.5, tlen=240, kl=0.0387, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.16it/s, pg=-0.0703, ret=0.000234, glen=76.2, tlen=236, kl=0.0412, act_lr=9.4e-7, ent=1.15]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.16it/s, pg=-0.0703, ret=0.000234, glen=76.2, tlen=236, kl=0.0412, act_lr=9.4e-7, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.16it/s, pg=0.0309, ret=0.000228, glen=75.2, tlen=236, kl=0.0877, act_lr=9.4e-7, ent=1.16] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.16it/s, pg=0.0309, ret=0.000228, glen=75.2, tlen=236, kl=0.0877, act_lr=9.4e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:23<00:20,  1.16it/s, pg=-0.152, ret=0.000437, glen=77.7, tlen=238, kl=0.306, act_lr=9.4e-7, ent=1.32] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:23<00:19,  1.17it/s, pg=-0.152, ret=0.000437, glen=77.7, tlen=238, kl=0.306, act_lr=9.4e-7, ent=1.32]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:19,  1.17it/s, pg=-0.0344, ret=3.41e-5, glen=77.3, tlen=238, kl=0.0532, act_lr=9.4e-7, ent=1.27]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:18,  1.17it/s, pg=-0.0344, ret=3.41e-5, glen=77.3, tlen=238, kl=0.0532, act_lr=9.4e-7, ent=1.27]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:18,  1.17it/s, pg=0.0151, ret=-0.000124, glen=75.6, tlen=236, kl=0.0659, act_lr=9.4e-7, ent=1.23]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:19,  1.07it/s, pg=0.0151, ret=-0.000124, glen=75.6, tlen=236, kl=0.0659, act_lr=9.4e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:19,  1.07it/s, pg=-0.062, ret=0.000263, glen=76.1, tlen=236, kl=0.037, act_lr=9.4e-7, ent=1.22]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:18,  1.10it/s, pg=-0.062, ret=0.000263, glen=76.1, tlen=236, kl=0.037, act_lr=9.4e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:18,  1.10it/s, pg=-0.00256, ret=-0.00013, glen=75.5, tlen=236, kl=0.04, act_lr=9.4e-7, ent=1.19]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:16,  1.12it/s, pg=-0.00256, ret=-0.00013, glen=75.5, tlen=236, kl=0.04, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:28<00:16,  1.12it/s, pg=0.0668, ret=-0.00021, glen=81.2, tlen=243, kl=0.0728, act_lr=9.4e-7, ent=1.26]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.13it/s, pg=0.0668, ret=-0.00021, glen=81.2, tlen=243, kl=0.0728, act_lr=9.4e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.13it/s, pg=0.0373, ret=-0.000487, glen=71.5, tlen=232, kl=0.0415, act_lr=9.4e-7, ent=1.15]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:28<00:14,  1.15it/s, pg=0.0373, ret=-0.000487, glen=71.5, tlen=232, kl=0.0415, act_lr=9.4e-7, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.15it/s, pg=0.00208, ret=-0.000115, glen=78.3, tlen=239, kl=0.0335, act_lr=9.4e-7, ent=1.19]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:29<00:13,  1.15it/s, pg=0.00208, ret=-0.000115, glen=78.3, tlen=239, kl=0.0335, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:13,  1.15it/s, pg=0.0831, ret=-0.000338, glen=76.2, tlen=237, kl=0.0385, act_lr=9.4e-7, ent=1.19] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:12,  1.16it/s, pg=0.0831, ret=-0.000338, glen=76.2, tlen=237, kl=0.0385, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:12,  1.16it/s, pg=-0.0242, ret=0.000119, glen=73.3, tlen=234, kl=0.0357, act_lr=9.4e-7, ent=1.21]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.16it/s, pg=-0.0242, ret=0.000119, glen=73.3, tlen=234, kl=0.0357, act_lr=9.4e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.16it/s, pg=-0.0214, ret=0.000599, glen=78.7, tlen=239, kl=0.0601, act_lr=9.4e-7, ent=1.18]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.17it/s, pg=-0.0214, ret=0.000599, glen=78.7, tlen=239, kl=0.0601, act_lr=9.4e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.17it/s, pg=0.106, ret=-0.000468, glen=73.9, tlen=235, kl=0.0521, act_lr=9.4e-7, ent=1.19] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=0.106, ret=-0.000468, glen=73.9, tlen=235, kl=0.0521, act_lr=9.4e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=0.0776, ret=-0.000357, glen=77.8, tlen=239, kl=0.0497, act_lr=9.4e-7, ent=1.22]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:33<00:09,  1.17it/s, pg=0.0776, ret=-0.000357, glen=77.8, tlen=239, kl=0.0497, act_lr=9.4e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=-0.0501, ret=-0.000173, glen=73.3, tlen=234, kl=0.45, act_lr=9.4e-7, ent=1.23] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:34<00:08,  1.17it/s, pg=-0.0501, ret=-0.000173, glen=73.3, tlen=234, kl=0.45, act_lr=9.4e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.17it/s, pg=-0.0479, ret=-8.88e-5, glen=78.8, tlen=240, kl=0.0392, act_lr=9.4e-7, ent=1.24]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:35<00:07,  1.17it/s, pg=-0.0479, ret=-8.88e-5, glen=78.8, tlen=240, kl=0.0392, act_lr=9.4e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.17it/s, pg=0.0779, ret=-0.000161, glen=72.7, tlen=233, kl=0.37, act_lr=9.4e-7, ent=1.17]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.17it/s, pg=0.0779, ret=-0.000161, glen=72.7, tlen=233, kl=0.37, act_lr=9.4e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.17it/s, pg=-0.0233, ret=-0.000201, glen=76.2, tlen=237, kl=0.0413, act_lr=9.4e-7, ent=1.18]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:05,  1.17it/s, pg=-0.0233, ret=-0.000201, glen=76.2, tlen=237, kl=0.0413, act_lr=9.4e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:05,  1.17it/s, pg=-0.0294, ret=0.000148, glen=71.9, tlen=232, kl=0.0425, act_lr=9.4e-7, ent=1.15] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.18it/s, pg=-0.0294, ret=0.000148, glen=71.9, tlen=232, kl=0.0425, act_lr=9.4e-7, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.18it/s, pg=-0.0917, ret=0.00043, glen=95.9, tlen=257, kl=0.056, act_lr=9.4e-7, ent=1.63]  Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=-0.0917, ret=0.00043, glen=95.9, tlen=257, kl=0.056, act_lr=9.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=-0.069, ret=0.000337, glen=75, tlen=236, kl=0.0557, act_lr=9.4e-7, ent=1.17] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:39<00:03,  1.17it/s, pg=-0.069, ret=0.000337, glen=75, tlen=236, kl=0.0557, act_lr=9.4e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=0.0419, ret=1.69e-5, glen=76.2, tlen=237, kl=0.0387, act_lr=9.4e-7, ent=1.23]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:40<00:02,  1.17it/s, pg=0.0419, ret=1.69e-5, glen=76.2, tlen=237, kl=0.0387, act_lr=9.4e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=0.0828, ret=-0.000393, glen=77.1, tlen=238, kl=0.0387, act_lr=9.4e-7, ent=1.29]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:41<00:01,  1.17it/s, pg=0.0828, ret=-0.000393, glen=77.1, tlen=238, kl=0.0387, act_lr=9.4e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.17it/s, pg=0.0152, ret=0.000249, glen=76.5, tlen=237, kl=0.0552, act_lr=9.4e-7, ent=1.19] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.18it/s, pg=0.0152, ret=0.000249, glen=76.5, tlen=237, kl=0.0552, act_lr=9.4e-7, ent=1.19]
2025-07-24 18:54:42.511 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 43.47s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.18it/s, pg=-0.131, ret=0.00061, glen=80.4, tlen=241, kl=0.0435, act_lr=9.6e-7, ent=1.19] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.13it/s, pg=-0.131, ret=0.00061, glen=80.4, tlen=241, kl=0.0435, act_lr=9.6e-7, ent=1.19]
2025-07-24 18:54:43.293 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.73s
2025-07-24 18:54:45.840 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.55s
2025-07-24 18:54:46.179 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 47.26s
2025-07-24 18:54:46.186 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00149078369140625, 'actor_lr': 9.4040000590212e-07, 'clip_ratio': 0.0, 'entropy': 1.2129574131965637, 'kl': 0.06825439453125, 'response_length': 76.79224182128907, 'total_length': 237.4874755859375, 'teacher_total_length': 249.6303335571289, 'return': 0.00020064307580469176, 'policy_update_steps': 1.0}

Episode [4/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [27:02<10:24, 156.22s/it][A2025-07-24 18:54:46.231 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:55:29.552 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:55:29.729 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 18:55:29.730 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 43.50s
2025-07-24 18:55:31.905 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0138,avg_reflection_pattern_score: 0.0060,avg_pass_at_n: 1.0000,avg_num_tokens: 72.7683,std_num_tokens: 31.4793,avg_correct_num_tokens: 72.3505,std_correct_num_tokens: 31.1974,avg_incorrect_num_tokens: 82.9136,std_incorrect_num_tokens: 36.2340
2025-07-24 18:55:32.219 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.49s
2025-07-24 18:55:34.948 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.72s
2025-07-24 18:55:59.965 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 193
2025-07-24 18:55:59.966 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.02s
2025-07-24 18:56:01.356 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.88s
2025-07-24 18:56:01.357 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -5.237617013599588e-05, avg_kl: 0.14868859800032383, avg_response_length: 72.837862597846, avg_orm_score: 0.0, avg_custom_rewards: -5.237617013599588e-05
2025-07-24 18:56:01.390 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter48_replay_buffer.jsonl
2025-07-24 18:56:02.812 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.43s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:01<?, ?it/s, pg=-0.0795, ret=0.000101, glen=70, tlen=231, kl=0.0543, act_lr=9.6e-7, ent=1.19]Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:01<00:48,  1.02s/it, pg=-0.0795, ret=0.000101, glen=70, tlen=231, kl=0.0543, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:01<00:48,  1.02s/it, pg=-0.0563, ret=0.000172, glen=70.9, tlen=231, kl=0.0448, act_lr=9.6e-7, ent=1.22]Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:01<00:43,  1.08it/s, pg=-0.0563, ret=0.000172, glen=70.9, tlen=231, kl=0.0448, act_lr=9.6e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:02<00:43,  1.08it/s, pg=-0.011, ret=0.000127, glen=72.5, tlen=233, kl=0.0406, act_lr=9.6e-7, ent=1.18] Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:02<00:41,  1.10it/s, pg=-0.011, ret=0.000127, glen=72.5, tlen=233, kl=0.0406, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:03<00:41,  1.10it/s, pg=-0.0507, ret=9.26e-6, glen=71.2, tlen=231, kl=0.0549, act_lr=9.6e-7, ent=1.16]Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:03<00:40,  1.11it/s, pg=-0.0507, ret=9.26e-6, glen=71.2, tlen=231, kl=0.0549, act_lr=9.6e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:04<00:40,  1.11it/s, pg=-0.0343, ret=0.000215, glen=76.1, tlen=236, kl=0.0434, act_lr=9.6e-7, ent=1.24]Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:04<00:39,  1.13it/s, pg=-0.0343, ret=0.000215, glen=76.1, tlen=236, kl=0.0434, act_lr=9.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:05<00:39,  1.13it/s, pg=-0.0527, ret=0.000255, glen=73.8, tlen=234, kl=0.0498, act_lr=9.6e-7, ent=1.21]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:05<00:37,  1.14it/s, pg=-0.0527, ret=0.000255, glen=73.8, tlen=234, kl=0.0498, act_lr=9.6e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:06<00:37,  1.14it/s, pg=-0.015, ret=-2.19e-5, glen=71.5, tlen=233, kl=0.0576, act_lr=9.6e-7, ent=1.17] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:06<00:36,  1.15it/s, pg=-0.015, ret=-2.19e-5, glen=71.5, tlen=233, kl=0.0576, act_lr=9.6e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:07<00:36,  1.15it/s, pg=-0.068, ret=0.000467, glen=71, tlen=231, kl=0.129, act_lr=9.6e-7, ent=1.22]   Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:36,  1.13it/s, pg=-0.068, ret=0.000467, glen=71, tlen=231, kl=0.129, act_lr=9.6e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:08<00:36,  1.13it/s, pg=0.181, ret=-0.000659, glen=75.1, tlen=235, kl=0.0757, act_lr=9.6e-7, ent=1.21]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:35,  1.14it/s, pg=0.181, ret=-0.000659, glen=75.1, tlen=235, kl=0.0757, act_lr=9.6e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:35,  1.14it/s, pg=0.0881, ret=-0.000338, glen=69, tlen=229, kl=0.0742, act_lr=9.6e-7, ent=1.2]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:08<00:34,  1.15it/s, pg=0.0881, ret=-0.000338, glen=69, tlen=229, kl=0.0742, act_lr=9.6e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:09<00:34,  1.15it/s, pg=0.0523, ret=-7.43e-5, glen=73.5, tlen=234, kl=0.0446, act_lr=9.6e-7, ent=1.2]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:09<00:32,  1.15it/s, pg=0.0523, ret=-7.43e-5, glen=73.5, tlen=234, kl=0.0446, act_lr=9.6e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:10<00:32,  1.15it/s, pg=-0.0633, ret=0.00021, glen=71.7, tlen=232, kl=0.0519, act_lr=9.6e-7, ent=1.23]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:10<00:31,  1.16it/s, pg=-0.0633, ret=0.00021, glen=71.7, tlen=232, kl=0.0519, act_lr=9.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:11<00:31,  1.16it/s, pg=0.045, ret=-0.000389, glen=75.3, tlen=236, kl=0.0422, act_lr=9.6e-7, ent=1.22]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:11<00:30,  1.16it/s, pg=0.045, ret=-0.000389, glen=75.3, tlen=236, kl=0.0422, act_lr=9.6e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:12<00:30,  1.16it/s, pg=-0.121, ret=0.000298, glen=73.7, tlen=235, kl=0.0519, act_lr=9.6e-7, ent=1.24]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:12<00:29,  1.17it/s, pg=-0.121, ret=0.000298, glen=73.7, tlen=235, kl=0.0519, act_lr=9.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:13<00:29,  1.17it/s, pg=0.0574, ret=-0.000157, glen=69.2, tlen=230, kl=0.05, act_lr=9.6e-7, ent=1.25] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:13<00:29,  1.17it/s, pg=0.0574, ret=-0.000157, glen=69.2, tlen=230, kl=0.05, act_lr=9.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:14<00:29,  1.17it/s, pg=0.109, ret=-0.00032, glen=72.2, tlen=233, kl=0.036, act_lr=9.6e-7, ent=1.23] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.15it/s, pg=0.109, ret=-0.00032, glen=72.2, tlen=233, kl=0.036, act_lr=9.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.15it/s, pg=-0.117, ret=0.000375, glen=72, tlen=233, kl=0.062, act_lr=9.6e-7, ent=1.19] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:14<00:27,  1.15it/s, pg=-0.117, ret=0.000375, glen=72, tlen=233, kl=0.062, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:15<00:27,  1.15it/s, pg=0.0307, ret=-0.000258, glen=75.8, tlen=236, kl=0.0778, act_lr=9.6e-7, ent=1.23]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:15<00:27,  1.14it/s, pg=0.0307, ret=-0.000258, glen=75.8, tlen=236, kl=0.0778, act_lr=9.6e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:16<00:27,  1.14it/s, pg=0.0109, ret=0.000166, glen=75.6, tlen=236, kl=0.0392, act_lr=9.6e-7, ent=1.18] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:16<00:26,  1.15it/s, pg=0.0109, ret=0.000166, glen=75.6, tlen=236, kl=0.0392, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:17<00:26,  1.15it/s, pg=0.0482, ret=2.01e-5, glen=71.3, tlen=231, kl=0.062, act_lr=9.6e-7, ent=1.21]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:17<00:25,  1.14it/s, pg=0.0482, ret=2.01e-5, glen=71.3, tlen=231, kl=0.062, act_lr=9.6e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:18<00:25,  1.14it/s, pg=-0.0426, ret=0.000213, glen=76.6, tlen=237, kl=0.0423, act_lr=9.6e-7, ent=1.22]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:18<00:24,  1.12it/s, pg=-0.0426, ret=0.000213, glen=76.6, tlen=237, kl=0.0423, act_lr=9.6e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:19<00:24,  1.12it/s, pg=0.0594, ret=1.44e-5, glen=70.7, tlen=231, kl=0.0518, act_lr=9.6e-7, ent=1.16]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:19<00:23,  1.14it/s, pg=0.0594, ret=1.44e-5, glen=70.7, tlen=231, kl=0.0518, act_lr=9.6e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:20<00:23,  1.14it/s, pg=-0.0206, ret=0.000225, glen=73.8, tlen=234, kl=0.0468, act_lr=9.6e-7, ent=1.25]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.15it/s, pg=-0.0206, ret=0.000225, glen=73.8, tlen=234, kl=0.0468, act_lr=9.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:21<00:22,  1.15it/s, pg=0.0477, ret=-0.000241, glen=72.2, tlen=233, kl=0.0638, act_lr=9.6e-7, ent=1.21]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:21<00:21,  1.15it/s, pg=0.0477, ret=-0.000241, glen=72.2, tlen=233, kl=0.0638, act_lr=9.6e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:21<00:21,  1.15it/s, pg=0.0383, ret=-0.000236, glen=71.8, tlen=232, kl=4.44, act_lr=9.6e-7, ent=1.19]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:21<00:20,  1.16it/s, pg=0.0383, ret=-0.000236, glen=71.8, tlen=232, kl=4.44, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:22<00:20,  1.16it/s, pg=-0.0546, ret=-0.000184, glen=72.1, tlen=233, kl=0.0475, act_lr=9.6e-7, ent=1.26]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:22<00:19,  1.16it/s, pg=-0.0546, ret=-0.000184, glen=72.1, tlen=233, kl=0.0475, act_lr=9.6e-7, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:23<00:19,  1.16it/s, pg=-0.0859, ret=0.000364, glen=77.7, tlen=238, kl=0.0392, act_lr=9.6e-7, ent=1.25] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:23<00:18,  1.16it/s, pg=-0.0859, ret=0.000364, glen=77.7, tlen=238, kl=0.0392, act_lr=9.6e-7, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:18,  1.16it/s, pg=-0.15, ret=0.000524, glen=73.3, tlen=234, kl=0.0429, act_lr=9.6e-7, ent=1.22]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:24<00:18,  1.17it/s, pg=-0.15, ret=0.000524, glen=73.3, tlen=234, kl=0.0429, act_lr=9.6e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:25<00:18,  1.17it/s, pg=-0.00519, ret=-0.000109, glen=76.1, tlen=237, kl=0.0589, act_lr=9.6e-7, ent=1.21]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:25<00:19,  1.01it/s, pg=-0.00519, ret=-0.000109, glen=76.1, tlen=237, kl=0.0589, act_lr=9.6e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:26<00:19,  1.01it/s, pg=-0.0523, ret=0.000194, glen=70.9, tlen=232, kl=0.0609, act_lr=9.6e-7, ent=1.17]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:26<00:18,  1.05it/s, pg=-0.0523, ret=0.000194, glen=70.9, tlen=232, kl=0.0609, act_lr=9.6e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:27<00:18,  1.05it/s, pg=0.0789, ret=-0.00035, glen=75.7, tlen=236, kl=0.0638, act_lr=9.6e-7, ent=1.24] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:27<00:16,  1.08it/s, pg=0.0789, ret=-0.00035, glen=75.7, tlen=236, kl=0.0638, act_lr=9.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:28<00:16,  1.08it/s, pg=-0.0577, ret=0.000171, glen=72.4, tlen=233, kl=0.0461, act_lr=9.6e-7, ent=1.18]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:28<00:15,  1.11it/s, pg=-0.0577, ret=0.000171, glen=72.4, tlen=233, kl=0.0461, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:29<00:15,  1.11it/s, pg=0.101, ret=-5.07e-5, glen=76.8, tlen=237, kl=0.0513, act_lr=9.6e-7, ent=1.24]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:14,  1.13it/s, pg=0.101, ret=-5.07e-5, glen=76.8, tlen=237, kl=0.0513, act_lr=9.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:30<00:14,  1.13it/s, pg=0.141, ret=-0.000538, glen=73.3, tlen=234, kl=0.0447, act_lr=9.6e-7, ent=1.17]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:13,  1.14it/s, pg=0.141, ret=-0.000538, glen=73.3, tlen=234, kl=0.0447, act_lr=9.6e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:13,  1.14it/s, pg=-0.0366, ret=0.000198, glen=75.8, tlen=237, kl=0.222, act_lr=9.6e-7, ent=1.18]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:30<00:12,  1.15it/s, pg=-0.0366, ret=0.000198, glen=75.8, tlen=237, kl=0.222, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:31<00:12,  1.15it/s, pg=0.119, ret=-0.000198, glen=71, tlen=232, kl=0.0558, act_lr=9.6e-7, ent=1.17]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:31<00:11,  1.15it/s, pg=0.119, ret=-0.000198, glen=71, tlen=232, kl=0.0558, act_lr=9.6e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:32<00:11,  1.15it/s, pg=0.111, ret=-0.000521, glen=75.9, tlen=236, kl=0.0607, act_lr=9.6e-7, ent=1.22]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:32<00:10,  1.14it/s, pg=0.111, ret=-0.000521, glen=75.9, tlen=236, kl=0.0607, act_lr=9.6e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:33<00:10,  1.14it/s, pg=-0.0563, ret=0.000309, glen=73.4, tlen=234, kl=0.0497, act_lr=9.6e-7, ent=1.18]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:33<00:09,  1.14it/s, pg=-0.0563, ret=0.000309, glen=73.4, tlen=234, kl=0.0497, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:34<00:09,  1.14it/s, pg=-0.0814, ret=0.000253, glen=69.3, tlen=230, kl=0.045, act_lr=9.6e-7, ent=1.19] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:34<00:08,  1.15it/s, pg=-0.0814, ret=0.000253, glen=69.3, tlen=230, kl=0.045, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:35<00:08,  1.15it/s, pg=-0.151, ret=0.000491, glen=71.8, tlen=232, kl=0.0541, act_lr=9.6e-7, ent=1.19]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.16it/s, pg=-0.151, ret=0.000491, glen=71.8, tlen=232, kl=0.0541, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:36<00:07,  1.16it/s, pg=-0.0865, ret=0.000394, glen=76.6, tlen=237, kl=0.0638, act_lr=9.6e-7, ent=1.21]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.16it/s, pg=-0.0865, ret=0.000394, glen=76.6, tlen=237, kl=0.0638, act_lr=9.6e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.16it/s, pg=0.0889, ret=-0.000373, glen=72.4, tlen=233, kl=0.0563, act_lr=9.6e-7, ent=1.19]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:36<00:06,  1.17it/s, pg=0.0889, ret=-0.000373, glen=72.4, tlen=233, kl=0.0563, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:37<00:06,  1.17it/s, pg=0.00354, ret=4.63e-5, glen=70.9, tlen=232, kl=0.0395, act_lr=9.6e-7, ent=1.19] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:37<00:05,  1.17it/s, pg=0.00354, ret=4.63e-5, glen=70.9, tlen=232, kl=0.0395, act_lr=9.6e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:38<00:05,  1.17it/s, pg=0.0161, ret=-0.000145, glen=71.3, tlen=232, kl=0.0428, act_lr=9.6e-7, ent=1.18]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:38<00:04,  1.17it/s, pg=0.0161, ret=-0.000145, glen=71.3, tlen=232, kl=0.0428, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:39<00:04,  1.17it/s, pg=0.136, ret=-0.000464, glen=70.6, tlen=231, kl=0.0468, act_lr=9.6e-7, ent=1.17] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:39<00:03,  1.17it/s, pg=0.136, ret=-0.000464, glen=70.6, tlen=231, kl=0.0468, act_lr=9.6e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:40<00:03,  1.17it/s, pg=-0.00769, ret=-0.000109, glen=69.8, tlen=230, kl=0.0689, act_lr=9.6e-7, ent=1.18]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:40<00:02,  1.17it/s, pg=-0.00769, ret=-0.000109, glen=69.8, tlen=230, kl=0.0689, act_lr=9.6e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:41<00:02,  1.17it/s, pg=0.085, ret=-0.000408, glen=74.8, tlen=236, kl=0.0492, act_lr=9.6e-7, ent=1.3]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=0.085, ret=-0.000408, glen=74.8, tlen=236, kl=0.0492, act_lr=9.6e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:42<00:01,  1.17it/s, pg=-0.0646, ret=0.000106, glen=70.1, tlen=231, kl=0.0552, act_lr=9.6e-7, ent=1.24]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.17it/s, pg=-0.0646, ret=0.000106, glen=70.1, tlen=231, kl=0.0552, act_lr=9.6e-7, ent=1.24]
2025-07-24 18:56:45.971 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 42.98s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.17it/s, pg=-0.0293, ret=0.00017, glen=73, tlen=233, kl=0.0519, act_lr=9.8e-7, ent=1.2]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.12it/s, pg=-0.0293, ret=0.00017, glen=73, tlen=233, kl=0.0519, act_lr=9.8e-7, ent=1.2]
2025-07-24 18:56:46.772 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 18:56:49.311 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 18:56:49.661 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 46.79s
2025-07-24 18:56:49.667 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -7.462014957350128e-05, 'actor_lr': 9.604081293330136e-07, 'clip_ratio': 0.0, 'entropy': 1.206764783178057, 'kl': 0.1470405422911352, 'response_length': 72.8838328147421, 'total_length': 233.41868124202807, 'teacher_total_length': 245.41330734564335, 'return': -1.128311108617226e-06, 'policy_update_steps': 1.0}

Episode [4/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [29:05<07:18, 146.11s/it][A2025-07-24 18:56:49.710 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:57:32.677 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:57:32.848 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 18:57:32.849 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 43.14s
2025-07-24 18:57:34.716 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0140,avg_reflection_pattern_score: 0.0039,avg_pass_at_n: 1.0000,avg_num_tokens: 70.2402,std_num_tokens: 35.7611,avg_correct_num_tokens: 69.7164,std_correct_num_tokens: 35.4190,avg_incorrect_num_tokens: 79.9333,std_incorrect_num_tokens: 40.3780
2025-07-24 18:57:35.004 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.15s
2025-07-24 18:57:37.955 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.95s
2025-07-24 18:58:02.519 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 191
2025-07-24 18:58:02.519 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.56s
2025-07-24 18:58:03.722 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 18:58:03.723 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00013420360677275826, avg_kl: 0.1150324157395288, avg_response_length: 70.27558523447726, avg_orm_score: 0.0, avg_custom_rewards: 0.00013420360677275826
2025-07-24 18:58:03.753 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter49_replay_buffer.jsonl
2025-07-24 18:58:05.129 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.38s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/48 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/48 [00:00<?, ?it/s, pg=-0.168, ret=0.00168, glen=68.1, tlen=229, kl=0.0582, act_lr=9.8e-7, ent=1.21]Actor Train epoch [1/1]:   2%|‚ñè         | 1/48 [00:00<00:46,  1.01it/s, pg=-0.168, ret=0.00168, glen=68.1, tlen=229, kl=0.0582, act_lr=9.8e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/48 [00:01<00:46,  1.01it/s, pg=0.014, ret=-0.000188, glen=75.4, tlen=235, kl=0.0681, act_lr=9.8e-7, ent=1.15]Actor Train epoch [1/1]:   4%|‚ñç         | 2/48 [00:01<00:42,  1.09it/s, pg=0.014, ret=-0.000188, glen=75.4, tlen=235, kl=0.0681, act_lr=9.8e-7, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/48 [00:02<00:42,  1.09it/s, pg=-0.174, ret=0.000545, glen=70.6, tlen=231, kl=0.0582, act_lr=9.8e-7, ent=1.19]Actor Train epoch [1/1]:   6%|‚ñã         | 3/48 [00:02<00:39,  1.13it/s, pg=-0.174, ret=0.000545, glen=70.6, tlen=231, kl=0.0582, act_lr=9.8e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/48 [00:03<00:39,  1.13it/s, pg=-0.0416, ret=8.66e-5, glen=67.1, tlen=227, kl=0.0909, act_lr=9.8e-7, ent=1.21]Actor Train epoch [1/1]:   8%|‚ñä         | 4/48 [00:03<00:38,  1.14it/s, pg=-0.0416, ret=8.66e-5, glen=67.1, tlen=227, kl=0.0909, act_lr=9.8e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/48 [00:04<00:38,  1.14it/s, pg=0.0706, ret=-0.000422, glen=69.4, tlen=230, kl=0.135, act_lr=9.8e-7, ent=1.2] Actor Train epoch [1/1]:  10%|‚ñà         | 5/48 [00:04<00:38,  1.12it/s, pg=0.0706, ret=-0.000422, glen=69.4, tlen=230, kl=0.135, act_lr=9.8e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/48 [00:05<00:38,  1.12it/s, pg=0.0859, ret=-2.57e-5, glen=71.9, tlen=232, kl=0.0568, act_lr=9.8e-7, ent=1.18]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 6/48 [00:05<00:36,  1.14it/s, pg=0.0859, ret=-2.57e-5, glen=71.9, tlen=232, kl=0.0568, act_lr=9.8e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 6/48 [00:06<00:36,  1.14it/s, pg=-0.0747, ret=0.000286, glen=67.9, tlen=228, kl=0.0456, act_lr=9.8e-7, ent=1.23]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/48 [00:06<00:35,  1.15it/s, pg=-0.0747, ret=0.000286, glen=67.9, tlen=228, kl=0.0456, act_lr=9.8e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/48 [00:07<00:35,  1.15it/s, pg=-0.0451, ret=0.000264, glen=73.4, tlen=234, kl=0.0452, act_lr=9.8e-7, ent=1.17]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/48 [00:07<00:34,  1.16it/s, pg=-0.0451, ret=0.000264, glen=73.4, tlen=234, kl=0.0452, act_lr=9.8e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/48 [00:07<00:34,  1.16it/s, pg=-0.037, ret=0.000145, glen=70.3, tlen=231, kl=0.0701, act_lr=9.8e-7, ent=1.17] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/48 [00:07<00:33,  1.16it/s, pg=-0.037, ret=0.000145, glen=70.3, tlen=231, kl=0.0701, act_lr=9.8e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/48 [00:08<00:33,  1.16it/s, pg=0.0349, ret=1.66e-5, glen=66.3, tlen=227, kl=0.0892, act_lr=9.8e-7, ent=1.2]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 10/48 [00:08<00:32,  1.16it/s, pg=0.0349, ret=1.66e-5, glen=66.3, tlen=227, kl=0.0892, act_lr=9.8e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 10/48 [00:09<00:32,  1.16it/s, pg=0.0271, ret=6.56e-5, glen=71.1, tlen=231, kl=0.284, act_lr=9.8e-7, ent=1.19]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/48 [00:09<00:31,  1.17it/s, pg=0.0271, ret=6.56e-5, glen=71.1, tlen=231, kl=0.284, act_lr=9.8e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/48 [00:10<00:31,  1.17it/s, pg=0.0869, ret=-0.00043, glen=68, tlen=228, kl=0.138, act_lr=9.8e-7, ent=1.17] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 12/48 [00:10<00:30,  1.17it/s, pg=0.0869, ret=-0.00043, glen=68, tlen=228, kl=0.138, act_lr=9.8e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 12/48 [00:11<00:30,  1.17it/s, pg=-0.0659, ret=0.000167, glen=70.2, tlen=230, kl=0.0721, act_lr=9.8e-7, ent=1.23]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/48 [00:11<00:30,  1.14it/s, pg=-0.0659, ret=0.000167, glen=70.2, tlen=230, kl=0.0721, act_lr=9.8e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/48 [00:12<00:30,  1.14it/s, pg=-0.0251, ret=0.000346, glen=70.2, tlen=230, kl=0.0718, act_lr=9.8e-7, ent=1.22]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 14/48 [00:12<00:29,  1.15it/s, pg=-0.0251, ret=0.000346, glen=70.2, tlen=230, kl=0.0718, act_lr=9.8e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 14/48 [00:13<00:29,  1.15it/s, pg=-0.0767, ret=0.000177, glen=69.2, tlen=230, kl=0.0554, act_lr=9.8e-7, ent=1.16]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [00:13<00:28,  1.16it/s, pg=-0.0767, ret=0.000177, glen=69.2, tlen=230, kl=0.0554, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [00:13<00:28,  1.16it/s, pg=0.0593, ret=7.71e-6, glen=68.5, tlen=229, kl=0.0952, act_lr=9.8e-7, ent=1.16]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [00:13<00:27,  1.16it/s, pg=0.0593, ret=7.71e-6, glen=68.5, tlen=229, kl=0.0952, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [00:14<00:27,  1.16it/s, pg=-0.185, ret=0.000692, glen=70.2, tlen=230, kl=0.129, act_lr=9.8e-7, ent=1.2] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [00:14<00:26,  1.15it/s, pg=-0.185, ret=0.000692, glen=70.2, tlen=230, kl=0.129, act_lr=9.8e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [00:15<00:26,  1.15it/s, pg=-0.0367, ret=0.00014, glen=70.3, tlen=231, kl=0.376, act_lr=9.8e-7, ent=1.29]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [00:15<00:25,  1.15it/s, pg=-0.0367, ret=0.00014, glen=70.3, tlen=231, kl=0.376, act_lr=9.8e-7, ent=1.29]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [00:16<00:25,  1.15it/s, pg=-0.014, ret=0.000103, glen=72.4, tlen=233, kl=0.442, act_lr=9.8e-7, ent=1.19]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [00:16<00:24,  1.16it/s, pg=-0.014, ret=0.000103, glen=72.4, tlen=233, kl=0.442, act_lr=9.8e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [00:17<00:24,  1.16it/s, pg=0.0359, ret=-4.55e-6, glen=77, tlen=237, kl=0.207, act_lr=9.8e-7, ent=1.31]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20/48 [00:17<00:24,  1.16it/s, pg=0.0359, ret=-4.55e-6, glen=77, tlen=237, kl=0.207, act_lr=9.8e-7, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20/48 [00:18<00:24,  1.16it/s, pg=0.0706, ret=-0.000167, glen=72, tlen=232, kl=0.0488, act_lr=9.8e-7, ent=1.15]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [00:18<00:23,  1.17it/s, pg=0.0706, ret=-0.000167, glen=72, tlen=232, kl=0.0488, act_lr=9.8e-7, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [00:19<00:23,  1.17it/s, pg=0.103, ret=-5.72e-5, glen=68.4, tlen=228, kl=0.0486, act_lr=9.8e-7, ent=1.13]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [00:19<00:22,  1.17it/s, pg=0.103, ret=-5.72e-5, glen=68.4, tlen=228, kl=0.0486, act_lr=9.8e-7, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [00:19<00:22,  1.17it/s, pg=0.00533, ret=-0.000137, glen=69.3, tlen=230, kl=0.635, act_lr=9.8e-7, ent=1.15]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 23/48 [00:19<00:21,  1.17it/s, pg=0.00533, ret=-0.000137, glen=69.3, tlen=230, kl=0.635, act_lr=9.8e-7, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 23/48 [00:20<00:21,  1.17it/s, pg=0.148, ret=-0.000617, glen=70.1, tlen=231, kl=0.1, act_lr=9.8e-7, ent=1.16]    Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/48 [00:20<00:20,  1.17it/s, pg=0.148, ret=-0.000617, glen=70.1, tlen=231, kl=0.1, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/48 [00:21<00:20,  1.17it/s, pg=0.0398, ret=-0.00015, glen=70, tlen=230, kl=0.11, act_lr=9.8e-7, ent=1.17] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25/48 [00:21<00:19,  1.17it/s, pg=0.0398, ret=-0.00015, glen=70, tlen=230, kl=0.11, act_lr=9.8e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25/48 [00:22<00:19,  1.17it/s, pg=0.0144, ret=-3.06e-5, glen=68.7, tlen=229, kl=0.0639, act_lr=9.8e-7, ent=1.17]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26/48 [00:22<00:18,  1.17it/s, pg=0.0144, ret=-3.06e-5, glen=68.7, tlen=229, kl=0.0639, act_lr=9.8e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26/48 [00:23<00:18,  1.17it/s, pg=-0.0847, ret=0.000142, glen=73.2, tlen=234, kl=0.0458, act_lr=9.8e-7, ent=1.24]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/48 [00:23<00:17,  1.17it/s, pg=-0.0847, ret=0.000142, glen=73.2, tlen=234, kl=0.0458, act_lr=9.8e-7, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/48 [00:24<00:17,  1.17it/s, pg=0.0106, ret=-0.000188, glen=68.9, tlen=229, kl=0.079, act_lr=9.8e-7, ent=1.18] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 28/48 [00:24<00:17,  1.18it/s, pg=0.0106, ret=-0.000188, glen=68.9, tlen=229, kl=0.079, act_lr=9.8e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 28/48 [00:25<00:17,  1.18it/s, pg=-0.0366, ret=9.92e-5, glen=68.3, tlen=228, kl=0.061, act_lr=9.8e-7, ent=1.16] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 29/48 [00:25<00:17,  1.07it/s, pg=-0.0366, ret=9.92e-5, glen=68.3, tlen=228, kl=0.061, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 29/48 [00:26<00:17,  1.07it/s, pg=0.176, ret=-0.000587, glen=69.6, tlen=230, kl=0.0864, act_lr=9.8e-7, ent=1.17]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 30/48 [00:26<00:16,  1.10it/s, pg=0.176, ret=-0.000587, glen=69.6, tlen=230, kl=0.0864, act_lr=9.8e-7, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 30/48 [00:27<00:16,  1.10it/s, pg=-0.105, ret=0.000546, glen=69.9, tlen=230, kl=0.0848, act_lr=9.8e-7, ent=1.19]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 31/48 [00:27<00:15,  1.12it/s, pg=-0.105, ret=0.000546, glen=69.9, tlen=230, kl=0.0848, act_lr=9.8e-7, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 31/48 [00:27<00:15,  1.12it/s, pg=-0.107, ret=0.000184, glen=71.7, tlen=232, kl=0.0536, act_lr=9.8e-7, ent=1.2] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 32/48 [00:27<00:14,  1.14it/s, pg=-0.107, ret=0.000184, glen=71.7, tlen=232, kl=0.0536, act_lr=9.8e-7, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 32/48 [00:28<00:14,  1.14it/s, pg=0.0162, ret=-1.14e-5, glen=69.7, tlen=230, kl=0.0599, act_lr=9.8e-7, ent=1.16]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 33/48 [00:28<00:13,  1.15it/s, pg=0.0162, ret=-1.14e-5, glen=69.7, tlen=230, kl=0.0599, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 33/48 [00:29<00:13,  1.15it/s, pg=0.00369, ret=-0.000448, glen=69.3, tlen=230, kl=0.0594, act_lr=9.8e-7, ent=1.22]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 34/48 [00:29<00:12,  1.16it/s, pg=0.00369, ret=-0.000448, glen=69.3, tlen=230, kl=0.0594, act_lr=9.8e-7, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 34/48 [00:30<00:12,  1.16it/s, pg=0.124, ret=-0.000372, glen=70, tlen=231, kl=0.0627, act_lr=9.8e-7, ent=1.23]    Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 35/48 [00:30<00:11,  1.16it/s, pg=0.124, ret=-0.000372, glen=70, tlen=231, kl=0.0627, act_lr=9.8e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 35/48 [00:31<00:11,  1.16it/s, pg=0.14, ret=-0.000195, glen=69.7, tlen=230, kl=0.379, act_lr=9.8e-7, ent=1.16]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 36/48 [00:31<00:10,  1.17it/s, pg=0.14, ret=-0.000195, glen=69.7, tlen=230, kl=0.379, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 36/48 [00:32<00:10,  1.17it/s, pg=-0.0352, ret=0.000158, glen=73.5, tlen=234, kl=0.0622, act_lr=9.8e-7, ent=1.21]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 37/48 [00:32<00:09,  1.17it/s, pg=-0.0352, ret=0.000158, glen=73.5, tlen=234, kl=0.0622, act_lr=9.8e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 37/48 [00:32<00:09,  1.17it/s, pg=0.154, ret=-0.000674, glen=74.2, tlen=235, kl=0.0496, act_lr=9.8e-7, ent=1.23] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 38/48 [00:32<00:08,  1.17it/s, pg=0.154, ret=-0.000674, glen=74.2, tlen=235, kl=0.0496, act_lr=9.8e-7, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 38/48 [00:33<00:08,  1.17it/s, pg=0.101, ret=-0.000193, glen=71.6, tlen=232, kl=0.0594, act_lr=9.8e-7, ent=1.18]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [00:33<00:07,  1.17it/s, pg=0.101, ret=-0.000193, glen=71.6, tlen=232, kl=0.0594, act_lr=9.8e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [00:34<00:07,  1.17it/s, pg=-0.0863, ret=0.000209, glen=72, tlen=232, kl=0.0647, act_lr=9.8e-7, ent=1.13] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 40/48 [00:34<00:06,  1.17it/s, pg=-0.0863, ret=0.000209, glen=72, tlen=232, kl=0.0647, act_lr=9.8e-7, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 40/48 [00:35<00:06,  1.17it/s, pg=0.0358, ret=-0.000227, glen=68.4, tlen=229, kl=0.0739, act_lr=9.8e-7, ent=1.21]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 41/48 [00:35<00:05,  1.17it/s, pg=0.0358, ret=-0.000227, glen=68.4, tlen=229, kl=0.0739, act_lr=9.8e-7, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 41/48 [00:36<00:05,  1.17it/s, pg=-0.134, ret=0.000592, glen=71.5, tlen=232, kl=0.0501, act_lr=9.8e-7, ent=1.16] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 42/48 [00:36<00:05,  1.18it/s, pg=-0.134, ret=0.000592, glen=71.5, tlen=232, kl=0.0501, act_lr=9.8e-7, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 42/48 [00:37<00:05,  1.18it/s, pg=0.0587, ret=-0.000206, glen=67.4, tlen=228, kl=0.182, act_lr=9.8e-7, ent=1.18]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 43/48 [00:37<00:04,  1.18it/s, pg=0.0587, ret=-0.000206, glen=67.4, tlen=228, kl=0.182, act_lr=9.8e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 43/48 [00:38<00:04,  1.18it/s, pg=0.0431, ret=-0.000443, glen=67.1, tlen=227, kl=0.0724, act_lr=9.8e-7, ent=1.14]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/48 [00:38<00:03,  1.18it/s, pg=0.0431, ret=-0.000443, glen=67.1, tlen=227, kl=0.0724, act_lr=9.8e-7, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/48 [00:38<00:03,  1.18it/s, pg=-0.0876, ret=0.000546, glen=75, tlen=235, kl=0.0489, act_lr=9.8e-7, ent=1.35]  Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [00:38<00:02,  1.17it/s, pg=-0.0876, ret=0.000546, glen=75, tlen=235, kl=0.0489, act_lr=9.8e-7, ent=1.35]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [00:39<00:02,  1.17it/s, pg=0.032, ret=-0.000383, glen=69.4, tlen=230, kl=0.14, act_lr=9.8e-7, ent=1.18] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 46/48 [00:39<00:01,  1.17it/s, pg=0.032, ret=-0.000383, glen=69.4, tlen=230, kl=0.14, act_lr=9.8e-7, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 46/48 [00:40<00:01,  1.17it/s, pg=-0.198, ret=0.000809, glen=66, tlen=227, kl=0.109, act_lr=9.8e-7, ent=1.14] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:40<00:00,  1.17it/s, pg=-0.198, ret=0.000809, glen=66, tlen=227, kl=0.109, act_lr=9.8e-7, ent=1.14]
2025-07-24 18:58:46.892 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 41.58s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:41<00:00,  1.17it/s, pg=0.0541, ret=-0.000571, glen=70.2, tlen=231, kl=0.062, act_lr=1e-6, ent=1.22]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:41<00:00,  1.13it/s, pg=0.0541, ret=-0.000571, glen=70.2, tlen=231, kl=0.062, act_lr=1e-6, ent=1.22]
2025-07-24 18:58:47.738 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 18:58:50.320 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 18:58:50.650 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 45.47s
2025-07-24 18:58:50.654 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0015444755554199219, 'actor_lr': 9.804166486067818e-07, 'clip_ratio': 0.0, 'entropy': 1.191605729361375, 'kl': 0.11535390218098958, 'response_length': 70.26408783594768, 'total_length': 230.6802854537964, 'teacher_total_length': 242.68953482309976, 'return': 2.6695139789959892e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [31:06<04:36, 138.42s/it][A2025-07-24 18:58:56.781 | INFO     | orz.ppo.trainer:train:183 - Successfully save model weights, training continue.
2025-07-24 18:58:56.783 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   1%|          | 1/172 [00:00<01:38,  1.74it/s, est. speed input: 305.48 toks/s, output: 34.71 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  26%|‚ñà‚ñà‚ñå       | 44/172 [00:01<00:02, 50.02it/s, est. speed input: 5452.33 toks/s, output: 1055.30 toks/s]Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 50/172 [00:01<00:02, 52.41it/s, est. speed input: 5788.39 toks/s, output: 1176.68 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:02<00:00, 75.64it/s, est. speed input: 10213.24 toks/s, output: 3326.81 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
2025-07-24 18:59:02.660 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 303.2591,strategyqa_test/accuracy: 0.5269,eval_accuracy: 0.5269
2025-07-24 18:59:02.957 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 18:59:43.379 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 18:59:43.567 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 18:59:43.568 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 40.61s
2025-07-24 18:59:45.604 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0042,avg_pass_at_n: 1.0000,avg_num_tokens: 67.3611,std_num_tokens: 29.4775,avg_correct_num_tokens: 66.9133,std_correct_num_tokens: 29.1514,avg_incorrect_num_tokens: 78.0303,std_incorrect_num_tokens: 34.7237
2025-07-24 18:59:46.042 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.47s
2025-07-24 18:59:48.743 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.70s
2025-07-24 19:00:13.339 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 189
2025-07-24 19:00:13.340 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.60s
2025-07-24 19:00:14.564 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 19:00:14.565 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00018582318351411867, avg_kl: 0.42463288483796297, avg_response_length: 67.50164950335467, avg_orm_score: 0.0, avg_custom_rewards: 0.00018582318351411867
2025-07-24 19:00:14.598 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter50_replay_buffer.jsonl
2025-07-24 19:00:15.941 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.35s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/48 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:02<00:00, 88.85it/s, est. speed input: 9806.12 toks/s, output: 3101.24 toks/s][32m [repeated 70x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:05<00:00, 34.21it/s, est. speed input: 6215.72 toks/s, output: 2378.96 toks/s][32m [repeated 10x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/48 [00:01<?, ?it/s, pg=0.0594, ret=-9.86e-5, glen=68.3, tlen=229, kl=0.0397, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:   2%|‚ñè         | 1/48 [00:01<00:48,  1.03s/it, pg=0.0594, ret=-9.86e-5, glen=68.3, tlen=229, kl=0.0397, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/48 [00:01<00:48,  1.03s/it, pg=-0.0753, ret=0.000164, glen=75.6, tlen=236, kl=0.0616, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:   4%|‚ñç         | 2/48 [00:01<00:42,  1.07it/s, pg=-0.0753, ret=0.000164, glen=75.6, tlen=236, kl=0.0616, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/48 [00:02<00:42,  1.07it/s, pg=-0.164, ret=0.000572, glen=72, tlen=233, kl=0.0756, act_lr=1e-6, ent=1.2]    Actor Train epoch [1/1]:   6%|‚ñã         | 3/48 [00:02<00:41,  1.09it/s, pg=-0.164, ret=0.000572, glen=72, tlen=233, kl=0.0756, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/48 [00:03<00:41,  1.09it/s, pg=-0.00924, ret=-1.61e-5, glen=67.2, tlen=227, kl=0.0579, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:   8%|‚ñä         | 4/48 [00:03<00:39,  1.12it/s, pg=-0.00924, ret=-1.61e-5, glen=67.2, tlen=227, kl=0.0579, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/48 [00:04<00:39,  1.12it/s, pg=-0.0564, ret=6.54e-5, glen=67, tlen=228, kl=0.063, act_lr=1e-6, ent=1.15]     Actor Train epoch [1/1]:  10%|‚ñà         | 5/48 [00:04<00:37,  1.14it/s, pg=-0.0564, ret=6.54e-5, glen=67, tlen=228, kl=0.063, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/48 [00:05<00:37,  1.14it/s, pg=0.107, ret=-0.000424, glen=69.8, tlen=230, kl=0.0557, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 6/48 [00:05<00:36,  1.15it/s, pg=0.107, ret=-0.000424, glen=69.8, tlen=230, kl=0.0557, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 6/48 [00:06<00:36,  1.15it/s, pg=0.00577, ret=-0.000293, glen=68.7, tlen=229, kl=0.543, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/48 [00:06<00:35,  1.16it/s, pg=0.00577, ret=-0.000293, glen=68.7, tlen=229, kl=0.543, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/48 [00:07<00:35,  1.16it/s, pg=0.0142, ret=-0.000203, glen=69.7, tlen=230, kl=0.144, act_lr=1e-6, ent=1.18] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/48 [00:07<00:34,  1.16it/s, pg=0.0142, ret=-0.000203, glen=69.7, tlen=230, kl=0.144, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/48 [00:07<00:34,  1.16it/s, pg=-0.124, ret=0.000285, glen=63.1, tlen=224, kl=0.0566, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/48 [00:07<00:33,  1.17it/s, pg=-0.124, ret=0.000285, glen=63.1, tlen=224, kl=0.0566, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/48 [00:08<00:33,  1.17it/s, pg=0.00623, ret=-5.79e-5, glen=66.1, tlen=227, kl=0.0994, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 10/48 [00:08<00:32,  1.17it/s, pg=0.00623, ret=-5.79e-5, glen=66.1, tlen=227, kl=0.0994, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 10/48 [00:09<00:32,  1.17it/s, pg=0.0953, ret=-0.000194, glen=64.7, tlen=225, kl=1.46, act_lr=1e-6, ent=1.14]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/48 [00:09<00:31,  1.17it/s, pg=0.0953, ret=-0.000194, glen=64.7, tlen=225, kl=1.46, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/48 [00:10<00:31,  1.17it/s, pg=-0.0637, ret=0.000127, glen=65.2, tlen=226, kl=0.134, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 12/48 [00:10<00:30,  1.17it/s, pg=-0.0637, ret=0.000127, glen=65.2, tlen=226, kl=0.134, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 12/48 [00:11<00:30,  1.17it/s, pg=0.2, ret=-0.000611, glen=66.3, tlen=227, kl=0.301, act_lr=1e-6, ent=1.14]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/48 [00:11<00:29,  1.17it/s, pg=0.2, ret=-0.000611, glen=66.3, tlen=227, kl=0.301, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/48 [00:12<00:29,  1.17it/s, pg=-0.03, ret=0.0013, glen=68.4, tlen=229, kl=8.62, act_lr=1e-6, ent=1.18]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 14/48 [00:12<00:28,  1.17it/s, pg=-0.03, ret=0.0013, glen=68.4, tlen=229, kl=8.62, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 14/48 [00:13<00:28,  1.17it/s, pg=-0.13, ret=0.000388, glen=66, tlen=227, kl=0.145, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [00:13<00:28,  1.15it/s, pg=-0.13, ret=0.000388, glen=66, tlen=227, kl=0.145, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [00:13<00:28,  1.15it/s, pg=0.0649, ret=-0.000293, glen=66.4, tlen=227, kl=0.0771, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [00:13<00:27,  1.16it/s, pg=0.0649, ret=-0.000293, glen=66.4, tlen=227, kl=0.0771, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [00:14<00:27,  1.16it/s, pg=-0.16, ret=0.000498, glen=68.9, tlen=230, kl=0.0615, act_lr=1e-6, ent=1.18]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [00:14<00:26,  1.16it/s, pg=-0.16, ret=0.000498, glen=68.9, tlen=230, kl=0.0615, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [00:15<00:26,  1.16it/s, pg=0.104, ret=-0.000327, glen=67.2, tlen=228, kl=0.0889, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [00:15<00:25,  1.17it/s, pg=0.104, ret=-0.000327, glen=67.2, tlen=228, kl=0.0889, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [00:16<00:25,  1.17it/s, pg=0.124, ret=-0.000202, glen=68.5, tlen=229, kl=0.0919, act_lr=1e-6, ent=1.2] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [00:16<00:24,  1.17it/s, pg=0.124, ret=-0.000202, glen=68.5, tlen=229, kl=0.0919, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [00:17<00:24,  1.17it/s, pg=0.182, ret=-0.000585, glen=66.1, tlen=227, kl=0.0738, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20/48 [00:17<00:23,  1.17it/s, pg=0.182, ret=-0.000585, glen=66.1, tlen=227, kl=0.0738, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20/48 [00:18<00:23,  1.17it/s, pg=0.0232, ret=-6.66e-5, glen=68.1, tlen=229, kl=0.0806, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [00:18<00:23,  1.17it/s, pg=0.0232, ret=-6.66e-5, glen=68.1, tlen=229, kl=0.0806, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [00:19<00:23,  1.17it/s, pg=-0.0859, ret=0.000252, glen=67.2, tlen=228, kl=0.0765, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [00:19<00:22,  1.15it/s, pg=-0.0859, ret=0.000252, glen=67.2, tlen=228, kl=0.0765, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [00:19<00:22,  1.15it/s, pg=-0.094, ret=0.000261, glen=65.7, tlen=226, kl=0.0755, act_lr=1e-6, ent=1.18] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 23/48 [00:19<00:21,  1.16it/s, pg=-0.094, ret=0.000261, glen=65.7, tlen=226, kl=0.0755, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 23/48 [00:20<00:21,  1.16it/s, pg=0.163, ret=-0.000413, glen=67, tlen=228, kl=0.119, act_lr=1e-6, ent=1.11]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/48 [00:20<00:20,  1.15it/s, pg=0.163, ret=-0.000413, glen=67, tlen=228, kl=0.119, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/48 [00:21<00:20,  1.15it/s, pg=0.0855, ret=-0.000337, glen=63, tlen=223, kl=2.13, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25/48 [00:21<00:19,  1.16it/s, pg=0.0855, ret=-0.000337, glen=63, tlen=223, kl=2.13, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25/48 [00:22<00:19,  1.16it/s, pg=0.0432, ret=-0.000258, glen=70.6, tlen=231, kl=0.133, act_lr=1e-6, ent=1.19]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26/48 [00:22<00:18,  1.16it/s, pg=0.0432, ret=-0.000258, glen=70.6, tlen=231, kl=0.133, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26/48 [00:23<00:18,  1.16it/s, pg=0.156, ret=-0.000317, glen=70.5, tlen=231, kl=1.43, act_lr=1e-6, ent=1.2]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/48 [00:23<00:17,  1.17it/s, pg=0.156, ret=-0.000317, glen=70.5, tlen=231, kl=1.43, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/48 [00:24<00:17,  1.17it/s, pg=-0.101, ret=0.000188, glen=66.8, tlen=227, kl=0.0708, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 28/48 [00:24<00:17,  1.17it/s, pg=-0.101, ret=0.000188, glen=66.8, tlen=227, kl=0.0708, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 28/48 [00:25<00:17,  1.17it/s, pg=-0.0323, ret=7.17e-5, glen=64.6, tlen=225, kl=1.79, act_lr=1e-6, ent=1.18]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 29/48 [00:25<00:18,  1.02it/s, pg=-0.0323, ret=7.17e-5, glen=64.6, tlen=225, kl=1.79, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 29/48 [00:26<00:18,  1.02it/s, pg=0.0476, ret=-0.000346, glen=62.7, tlen=224, kl=0.0992, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 30/48 [00:26<00:16,  1.06it/s, pg=0.0476, ret=-0.000346, glen=62.7, tlen=224, kl=0.0992, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 30/48 [00:27<00:16,  1.06it/s, pg=-0.119, ret=0.000364, glen=66.8, tlen=228, kl=0.0767, act_lr=1e-6, ent=1.15] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 31/48 [00:27<00:15,  1.09it/s, pg=-0.119, ret=0.000364, glen=66.8, tlen=228, kl=0.0767, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 31/48 [00:28<00:15,  1.09it/s, pg=-0.177, ret=0.000741, glen=68, tlen=229, kl=0.444, act_lr=1e-6, ent=1.12]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 32/48 [00:28<00:14,  1.12it/s, pg=-0.177, ret=0.000741, glen=68, tlen=229, kl=0.444, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 32/48 [00:28<00:14,  1.12it/s, pg=-0.0385, ret=0.000166, glen=69.1, tlen=230, kl=0.0812, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 33/48 [00:28<00:13,  1.13it/s, pg=-0.0385, ret=0.000166, glen=69.1, tlen=230, kl=0.0812, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 33/48 [00:29<00:13,  1.13it/s, pg=-0.0306, ret=1.22e-5, glen=64.6, tlen=225, kl=0.0931, act_lr=1e-6, ent=1.19] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 34/48 [00:29<00:12,  1.15it/s, pg=-0.0306, ret=1.22e-5, glen=64.6, tlen=225, kl=0.0931, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 34/48 [00:30<00:12,  1.15it/s, pg=-0.0101, ret=0.00022, glen=68.3, tlen=229, kl=0.0852, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 35/48 [00:30<00:11,  1.16it/s, pg=-0.0101, ret=0.00022, glen=68.3, tlen=229, kl=0.0852, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 35/48 [00:31<00:11,  1.16it/s, pg=0.0292, ret=7.33e-6, glen=66.5, tlen=227, kl=0.0813, act_lr=1e-6, ent=1.17] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 36/48 [00:31<00:10,  1.16it/s, pg=0.0292, ret=7.33e-6, glen=66.5, tlen=227, kl=0.0813, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 36/48 [00:32<00:10,  1.16it/s, pg=-0.103, ret=0.000231, glen=67.8, tlen=229, kl=0.0624, act_lr=1e-6, ent=1.19]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 37/48 [00:32<00:09,  1.17it/s, pg=-0.103, ret=0.000231, glen=67.8, tlen=229, kl=0.0624, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 37/48 [00:33<00:09,  1.17it/s, pg=-0.0613, ret=0.000216, glen=66.5, tlen=227, kl=0.0621, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 38/48 [00:33<00:08,  1.17it/s, pg=-0.0613, ret=0.000216, glen=66.5, tlen=227, kl=0.0621, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 38/48 [00:33<00:08,  1.17it/s, pg=0.0757, ret=-0.000216, glen=69.5, tlen=230, kl=0.072, act_lr=1e-6, ent=1.11] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [00:33<00:07,  1.17it/s, pg=0.0757, ret=-0.000216, glen=69.5, tlen=230, kl=0.072, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [00:34<00:07,  1.17it/s, pg=0.00781, ret=-9.54e-5, glen=69.3, tlen=230, kl=0.0759, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 40/48 [00:34<00:06,  1.17it/s, pg=0.00781, ret=-9.54e-5, glen=69.3, tlen=230, kl=0.0759, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 40/48 [00:35<00:06,  1.17it/s, pg=-0.0471, ret=0.000243, glen=66.7, tlen=228, kl=0.0799, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 41/48 [00:35<00:05,  1.17it/s, pg=-0.0471, ret=0.000243, glen=66.7, tlen=228, kl=0.0799, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 41/48 [00:36<00:05,  1.17it/s, pg=-0.0494, ret=0.000171, glen=65.2, tlen=226, kl=0.0614, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 42/48 [00:36<00:05,  1.17it/s, pg=-0.0494, ret=0.000171, glen=65.2, tlen=226, kl=0.0614, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 42/48 [00:37<00:05,  1.17it/s, pg=-0.0516, ret=0.000395, glen=71.7, tlen=232, kl=0.233, act_lr=1e-6, ent=1.19] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 43/48 [00:37<00:04,  1.18it/s, pg=-0.0516, ret=0.000395, glen=71.7, tlen=232, kl=0.233, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 43/48 [00:38<00:04,  1.18it/s, pg=-0.0859, ret=0.000179, glen=67.6, tlen=228, kl=0.0792, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/48 [00:38<00:03,  1.17it/s, pg=-0.0859, ret=0.000179, glen=67.6, tlen=228, kl=0.0792, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/48 [00:39<00:03,  1.17it/s, pg=0.0938, ret=-0.000455, glen=70, tlen=231, kl=0.0782, act_lr=1e-6, ent=1.25]  Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [00:39<00:02,  1.18it/s, pg=0.0938, ret=-0.000455, glen=70, tlen=231, kl=0.0782, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [00:39<00:02,  1.18it/s, pg=0.0966, ret=-0.000599, glen=66.5, tlen=227, kl=0.0546, act_lr=1e-6, ent=1.19]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 46/48 [00:39<00:01,  1.17it/s, pg=0.0966, ret=-0.000599, glen=66.5, tlen=227, kl=0.0546, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 46/48 [00:40<00:01,  1.17it/s, pg=-0.026, ret=0.000321, glen=69, tlen=230, kl=0.0798, act_lr=1e-6, ent=1.16]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:40<00:00,  1.17it/s, pg=-0.026, ret=0.000321, glen=69, tlen=230, kl=0.0798, act_lr=1e-6, ent=1.16]
2025-07-24 19:00:57.873 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 41.76s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:41<00:00,  1.17it/s, pg=-0.00171, ret=0.000111, glen=67.6, tlen=228, kl=0.0744, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:41<00:00,  1.13it/s, pg=-0.00171, ret=0.000111, glen=67.6, tlen=228, kl=0.0744, act_lr=1e-6, ent=1.16]
2025-07-24 19:00:58.684 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 19:01:01.361 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.68s
2025-07-24 19:01:01.692 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 45.69s
2025-07-24 19:01:01.698 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.002956092357635498, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.1637052322427432, 'kl': 0.4191907246907552, 'response_length': 67.54241315523784, 'total_length': 228.24781131744385, 'teacher_total_length': 239.964893023173, 'return': 2.370517550313404e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [33:17<02:16, 136.18s/it][A2025-07-24 19:01:01.704 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:31,  1.87it/s, est. speed input: 332.57 toks/s, output: 33.63 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/172 [00:02<00:00, 66.90it/s, est. speed input: 10983.32 toks/s, output: 3152.31 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 152/172 [00:02<00:00, 65.68it/s, est. speed input: 9480.94 toks/s, output: 2810.87 toks/s][32m [repeated 68x across cluster][0m
2025-07-24 19:01:09.831 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 272.8967,strategyqa_test/accuracy: 0.5080,eval_accuracy: 0.5080
2025-07-24 19:01:10.084 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:01:37.097 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:01:37.273 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:01:37.273 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 27.19s
2025-07-24 19:01:38.399 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0051,avg_pass_at_n: 1.0000,avg_num_tokens: 66.2498,std_num_tokens: 29.6448,avg_correct_num_tokens: 65.8097,std_correct_num_tokens: 29.3551,avg_incorrect_num_tokens: 80.6693,std_incorrect_num_tokens: 34.9931
2025-07-24 19:01:38.562 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.29s
2025-07-24 19:01:39.804 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.24s
2025-07-24 19:01:52.873 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 98
2025-07-24 19:01:52.873 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 13.07s
2025-07-24 19:01:53.771 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.52s
2025-07-24 19:01:53.771 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.00010016235957701444, avg_kl: 1.0879379583864797, avg_response_length: 66.326593087644, avg_orm_score: 0.0, avg_custom_rewards: -0.00010016235957701444
2025-07-24 19:01:53.792 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter51_replay_buffer.jsonl
2025-07-24 19:01:54.488 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.70s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/25 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:07<00:00, 23.72it/s, est. speed input: 4310.43 toks/s, output: 1491.28 toks/s][32m [repeated 11x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/25 [00:00<?, ?it/s, pg=-0.119, ret=0.000306, glen=67.8, tlen=228, kl=0.844, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:   4%|‚ñç         | 1/25 [00:00<00:23,  1.03it/s, pg=-0.119, ret=0.000306, glen=67.8, tlen=228, kl=0.844, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/25 [00:01<00:23,  1.03it/s, pg=0.0456, ret=-0.00015, glen=66, tlen=227, kl=0.0974, act_lr=1e-6, ent=1.21] Actor Train epoch [1/1]:   8%|‚ñä         | 2/25 [00:01<00:20,  1.10it/s, pg=0.0456, ret=-0.00015, glen=66, tlen=227, kl=0.0974, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 2/25 [00:02<00:20,  1.10it/s, pg=-0.0573, ret=8.13e-5, glen=64.4, tlen=225, kl=0.0955, act_lr=1e-6, ent=1.21]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 3/25 [00:02<00:20,  1.10it/s, pg=-0.0573, ret=8.13e-5, glen=64.4, tlen=225, kl=0.0955, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 3/25 [00:03<00:20,  1.10it/s, pg=0.116, ret=-0.000192, glen=67.1, tlen=228, kl=0.306, act_lr=1e-6, ent=1.22] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 4/25 [00:03<00:18,  1.13it/s, pg=0.116, ret=-0.000192, glen=67.1, tlen=228, kl=0.306, act_lr=1e-6, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 4/25 [00:04<00:18,  1.13it/s, pg=0.112, ret=-0.000445, glen=65.9, tlen=227, kl=0.102, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 5/25 [00:04<00:17,  1.14it/s, pg=0.112, ret=-0.000445, glen=65.9, tlen=227, kl=0.102, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 5/25 [00:05<00:17,  1.14it/s, pg=-0.033, ret=0.000194, glen=66, tlen=227, kl=0.117, act_lr=1e-6, ent=1.14]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:05<00:16,  1.13it/s, pg=-0.033, ret=0.000194, glen=66, tlen=227, kl=0.117, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:06<00:16,  1.13it/s, pg=0.0413, ret=0.000158, glen=68.6, tlen=229, kl=0.179, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:06<00:15,  1.14it/s, pg=0.0413, ret=0.000158, glen=68.6, tlen=229, kl=0.179, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:07<00:15,  1.14it/s, pg=-0.096, ret=0.000343, glen=67, tlen=228, kl=0.232, act_lr=1e-6, ent=1.21]  Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:07<00:14,  1.15it/s, pg=-0.096, ret=0.000343, glen=67, tlen=228, kl=0.232, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:07<00:14,  1.15it/s, pg=0.0696, ret=-0.000303, glen=65.4, tlen=226, kl=0.078, act_lr=1e-6, ent=1.23]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:07<00:13,  1.16it/s, pg=0.0696, ret=-0.000303, glen=65.4, tlen=226, kl=0.078, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:08<00:13,  1.16it/s, pg=0.00549, ret=-6.23e-6, glen=66.1, tlen=227, kl=0.0712, act_lr=1e-6, ent=1.19]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [00:08<00:12,  1.17it/s, pg=0.00549, ret=-6.23e-6, glen=66.1, tlen=227, kl=0.0712, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [00:09<00:12,  1.17it/s, pg=0.0488, ret=-0.000151, glen=64.7, tlen=225, kl=0.0751, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:09<00:11,  1.17it/s, pg=0.0488, ret=-0.000151, glen=64.7, tlen=225, kl=0.0751, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:10<00:11,  1.17it/s, pg=-0.0514, ret=-1.96e-5, glen=66.1, tlen=227, kl=9.52, act_lr=1e-6, ent=1.18]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [00:10<00:11,  1.17it/s, pg=-0.0514, ret=-1.96e-5, glen=66.1, tlen=227, kl=9.52, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [00:11<00:11,  1.17it/s, pg=0.0697, ret=-9.92e-5, glen=67.6, tlen=228, kl=0.194, act_lr=1e-6, ent=1.21]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:11<00:10,  1.17it/s, pg=0.0697, ret=-9.92e-5, glen=67.6, tlen=228, kl=0.194, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:12<00:10,  1.17it/s, pg=0.236, ret=-0.00066, glen=67.4, tlen=228, kl=0.136, act_lr=1e-6, ent=1.16] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 14/25 [00:12<00:09,  1.17it/s, pg=0.236, ret=-0.00066, glen=67.4, tlen=228, kl=0.136, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 14/25 [00:13<00:09,  1.17it/s, pg=-0.0853, ret=0.000344, glen=67.3, tlen=228, kl=0.105, act_lr=1e-6, ent=1.24]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:13<00:08,  1.17it/s, pg=-0.0853, ret=0.000344, glen=67.3, tlen=228, kl=0.105, act_lr=1e-6, ent=1.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:13<00:08,  1.17it/s, pg=-0.0826, ret=0.000333, glen=65.8, tlen=227, kl=0.0848, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [00:13<00:07,  1.17it/s, pg=-0.0826, ret=0.000333, glen=65.8, tlen=227, kl=0.0848, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [00:14<00:07,  1.17it/s, pg=-0.0682, ret=6.86e-5, glen=67.9, tlen=228, kl=0.0858, act_lr=1e-6, ent=1.18] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:14<00:06,  1.17it/s, pg=-0.0682, ret=6.86e-5, glen=67.9, tlen=228, kl=0.0858, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:15<00:06,  1.17it/s, pg=-0.177, ret=0.000599, glen=64.1, tlen=225, kl=0.969, act_lr=1e-6, ent=1.16] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 18/25 [00:15<00:05,  1.17it/s, pg=-0.177, ret=0.000599, glen=64.1, tlen=225, kl=0.969, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 18/25 [00:16<00:05,  1.17it/s, pg=0.118, ret=-0.000432, glen=66.9, tlen=227, kl=0.0963, act_lr=1e-6, ent=1.25]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:16<00:05,  1.17it/s, pg=0.118, ret=-0.000432, glen=66.9, tlen=227, kl=0.0963, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:17<00:05,  1.17it/s, pg=0.157, ret=-0.000594, glen=66.6, tlen=227, kl=0.0549, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [00:17<00:04,  1.18it/s, pg=0.157, ret=-0.000594, glen=66.6, tlen=227, kl=0.0549, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [00:18<00:04,  1.18it/s, pg=-0.0653, ret=-7.8e-5, glen=69.8, tlen=231, kl=0.0764, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:18<00:03,  1.17it/s, pg=-0.0653, ret=-7.8e-5, glen=69.8, tlen=231, kl=0.0764, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:18<00:03,  1.17it/s, pg=-0.0785, ret=0.000289, glen=64.9, tlen=225, kl=0.576, act_lr=1e-6, ent=1.21]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [00:18<00:02,  1.18it/s, pg=-0.0785, ret=0.000289, glen=64.9, tlen=225, kl=0.576, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [00:19<00:02,  1.18it/s, pg=-0.0482, ret=0.000138, glen=65.6, tlen=227, kl=12.2, act_lr=1e-6, ent=1.18] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:19<00:01,  1.18it/s, pg=-0.0482, ret=0.000138, glen=65.6, tlen=227, kl=12.2, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:20<00:01,  1.18it/s, pg=-0.0281, ret=-8.29e-6, glen=63.4, tlen=224, kl=0.169, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [00:20<00:00,  1.18it/s, pg=-0.0281, ret=-8.29e-6, glen=63.4, tlen=224, kl=0.169, act_lr=1e-6, ent=1.26]
2025-07-24 19:02:16.279 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 21.60s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [00:21<00:00,  1.18it/s, pg=-0.0858, ret=0.000316, glen=65.3, tlen=227, kl=0.294, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [00:21<00:00,  1.11it/s, pg=-0.0858, ret=0.000316, glen=65.3, tlen=227, kl=0.294, act_lr=1e-6, ent=1.18]
2025-07-24 19:02:16.961 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:02:19.197 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.24s
2025-07-24 19:02:19.545 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 24.99s
2025-07-24 19:02:19.548 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.002293968200683594, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.1948195171356202, 'kl': 1.0712939453125, 'response_length': 66.3034715270996, 'total_length': 227.01928466796875, 'teacher_total_length': 238.92864807128908, 'return': 1.3313835370354354e-06, 'policy_update_steps': 1.0}

Episode [4/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [34:35<00:00, 118.51s/it][A
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,717] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.48 GB, percent = 21.7%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7cc3ddc40>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,718] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,719] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 18:27:43,720] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }

2025-07-24 19:02:28.356 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 18:27:43,210] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:35:36,534] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:39:33,841] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:42:02,494] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:44:29,602] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:47:05,671] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:50:08,837] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:52:18,549] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 18:56:45,964] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:00:57,866] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:02:16,273] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:26,149] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:26,347] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1695, num_elems = 8.89B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:27,872] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:27,873] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:27,880] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:27,881] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,147] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,147] [INFO] [utils.py:782:see_memory_usage] MA 3.68 GB         Max_MA 8.64 GB         CA 4.7 GB         Max_CA 41 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,148] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.58 GB, percent = 21.8%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [5/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [4/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [34:44<00:00, 160.36s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,350] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,351] [INFO] [utils.py:782:see_memory_usage] MA 3.68 GB         Max_MA 3.68 GB         CA 4.7 GB         Max_CA 5 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,351] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.56 GB, percent = 21.8%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7c3591400>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 19:02:28.674 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:03:05.242 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:03:05.431 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:03:05.432 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.76s
2025-07-24 19:03:07.408 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0045,avg_pass_at_n: 1.0000,avg_num_tokens: 59.6565,std_num_tokens: 25.3077,avg_correct_num_tokens: 59.3711,std_correct_num_tokens: 25.2086,avg_incorrect_num_tokens: 67.4602,std_incorrect_num_tokens: 26.7248
2025-07-24 19:03:07.800 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.37s
2025-07-24 19:03:10.297 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.49s
2025-07-24 19:03:34.389 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 19:03:34.390 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.09s
2025-07-24 19:03:35.606 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 19:03:35.607 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.5512480596233938e-05, avg_kl: 0.0, avg_response_length: 59.73050823840466, avg_orm_score: 0.0, avg_custom_rewards: 1.5512480596233938e-05
2025-07-24 19:03:35.634 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter52_replay_buffer.jsonl
2025-07-24 19:03:36.881 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.25s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=0.103, ret=-0.000274, glen=61.7, tlen=222, kl=0, act_lr=1e-6, ent=1.23]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.00it/s, pg=0.103, ret=-0.000274, glen=61.7, tlen=222, kl=0, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.00it/s, pg=-0.0251, ret=0.000148, glen=60.3, tlen=220, kl=0, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0251, ret=0.000148, glen=60.3, tlen=220, kl=0, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0676, ret=0.000264, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.13it/s, pg=-0.0676, ret=0.000264, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.13it/s, pg=-0.0644, ret=0.000164, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=1.16]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:38,  1.10it/s, pg=-0.0644, ret=0.000164, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:38,  1.10it/s, pg=0.122, ret=-0.000284, glen=60.3, tlen=221, kl=0, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=0.122, ret=-0.000284, glen=60.3, tlen=221, kl=0, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=-0.045, ret=0.000263, glen=59.4, tlen=220, kl=0, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.12it/s, pg=-0.045, ret=0.000263, glen=59.4, tlen=220, kl=0, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.12it/s, pg=0.0191, ret=-0.000293, glen=59.1, tlen=220, kl=0, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=0.0191, ret=-0.000293, glen=59.1, tlen=220, kl=0, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=0.00647, ret=3.43e-5, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=0.00647, ret=3.43e-5, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.13it/s, pg=-0.00119, ret=-0.00011, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:33,  1.12it/s, pg=-0.00119, ret=-0.00011, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:33,  1.12it/s, pg=-0.0364, ret=0.00012, glen=64.1, tlen=224, kl=0, act_lr=1e-6, ent=1.22]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.13it/s, pg=-0.0364, ret=0.00012, glen=64.1, tlen=224, kl=0, act_lr=1e-6, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.13it/s, pg=0.0246, ret=-4.63e-5, glen=58.3, tlen=219, kl=0, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=0.0246, ret=-4.63e-5, glen=58.3, tlen=219, kl=0, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.027, ret=0.000115, glen=59.3, tlen=220, kl=0, act_lr=1e-6, ent=1.19]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=-0.027, ret=0.000115, glen=59.3, tlen=220, kl=0, act_lr=1e-6, ent=1.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=0.21, ret=-0.000856, glen=59.8, tlen=221, kl=0, act_lr=1e-6, ent=1.2]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:29,  1.14it/s, pg=0.21, ret=-0.000856, glen=59.8, tlen=221, kl=0, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:29,  1.14it/s, pg=0.0267, ret=5.46e-5, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.15it/s, pg=0.0267, ret=5.46e-5, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.15it/s, pg=-0.0709, ret=0.000139, glen=57.9, tlen=218, kl=0, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.16it/s, pg=-0.0709, ret=0.000139, glen=57.9, tlen=218, kl=0, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.16it/s, pg=-0.00671, ret=0.000183, glen=57.5, tlen=218, kl=0, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=-0.00671, ret=0.000183, glen=57.5, tlen=218, kl=0, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=-0.0484, ret=0.000287, glen=59.1, tlen=220, kl=0, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0484, ret=0.000287, glen=59.1, tlen=220, kl=0, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.00714, ret=-2.7e-5, glen=60.3, tlen=221, kl=0, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.00714, ret=-2.7e-5, glen=60.3, tlen=221, kl=0, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0502, ret=0.000147, glen=57.8, tlen=218, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0502, ret=0.000147, glen=57.8, tlen=218, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=0.0211, ret=4.39e-5, glen=56.8, tlen=217, kl=0, act_lr=1e-6, ent=1.14]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.0211, ret=4.39e-5, glen=56.8, tlen=217, kl=0, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0262, ret=-0.000111, glen=58, tlen=219, kl=0, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0262, ret=-0.000111, glen=58, tlen=219, kl=0, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.12, ret=-0.000407, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.12, ret=-0.000407, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.17it/s, pg=-0.116, ret=0.000388, glen=58.1, tlen=219, kl=0, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.116, ret=0.000388, glen=58.1, tlen=219, kl=0, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.11, ret=0.000243, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=1.15]   Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.11, ret=0.000243, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.0166, ret=-0.000121, glen=60.2, tlen=221, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=0.0166, ret=-0.000121, glen=60.2, tlen=221, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=0.0454, ret=-0.000264, glen=56.3, tlen=217, kl=0, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.0454, ret=-0.000264, glen=56.3, tlen=217, kl=0, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0828, ret=0.000401, glen=62.4, tlen=223, kl=0, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0828, ret=0.000401, glen=62.4, tlen=223, kl=0, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=0.0931, ret=-0.000407, glen=57.2, tlen=218, kl=0, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=0.0931, ret=-0.000407, glen=57.2, tlen=218, kl=0, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.162, ret=0.000619, glen=59.7, tlen=220, kl=0, act_lr=1e-6, ent=1.17] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.162, ret=0.000619, glen=59.7, tlen=220, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0736, ret=0.000259, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0736, ret=0.000259, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=0.0662, ret=-0.000408, glen=57.9, tlen=219, kl=0, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.0662, ret=-0.000408, glen=57.9, tlen=219, kl=0, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.142, ret=-0.000406, glen=59.3, tlen=220, kl=0, act_lr=1e-6, ent=1.14] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=0.142, ret=-0.000406, glen=59.3, tlen=220, kl=0, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0352, ret=0.000158, glen=61.9, tlen=223, kl=0, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0352, ret=0.000158, glen=61.9, tlen=223, kl=0, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0975, ret=0.000346, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0975, ret=0.000346, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=0.168, ret=-0.000752, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=1.17]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.168, ret=-0.000752, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.067, ret=0.000327, glen=60, tlen=220, kl=0, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.067, ret=0.000327, glen=60, tlen=220, kl=0, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=0.112, ret=-0.000393, glen=62.1, tlen=223, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=0.112, ret=-0.000393, glen=62.1, tlen=223, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=-0.04, ret=0.000211, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=1.21] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.04, ret=0.000211, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.201, ret=0.000678, glen=59.8, tlen=221, kl=0, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.201, ret=0.000678, glen=59.8, tlen=221, kl=0, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0944, ret=0.000415, glen=59.7, tlen=220, kl=0, act_lr=1e-6, ent=1.18]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0944, ret=0.000415, glen=59.7, tlen=220, kl=0, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.121, ret=0.000301, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=1.18] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.121, ret=0.000301, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.166, ret=-0.000476, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=1.17]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.18it/s, pg=0.166, ret=-0.000476, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=1.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.18it/s, pg=0.0841, ret=-0.000258, glen=58.3, tlen=219, kl=0, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=0.0841, ret=-0.000258, glen=58.3, tlen=219, kl=0, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.18it/s, pg=-0.0397, ret=0.000119, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=1.23]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0397, ret=0.000119, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=0.0364, ret=-0.000283, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=0.0364, ret=-0.000283, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=1.07]
2025-07-24 19:04:17.043 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.99s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0413, ret=9.64e-5, glen=60.1, tlen=221, kl=0, act_lr=1e-6, ent=1.17] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0413, ret=9.64e-5, glen=60.1, tlen=221, kl=0, act_lr=1e-6, ent=1.17]
2025-07-24 19:04:17.900 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 19:04:20.465 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 19:04:20.869 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.94s
2025-07-24 19:04:20.876 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0026495560355808425, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.155879891437033, 'kl': 0.0, 'response_length': 59.69598438428796, 'total_length': 220.2140894350798, 'teacher_total_length': 232.3579522837763, 'return': 7.569023775589733e-06, 'policy_update_steps': 1.0}
Episode [5/20]:   8%|‚ñä         | 1/13 [01:52<22:29, 112.44s/it]2025-07-24 19:04:20.920 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:04:56.613 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:04:56.794 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:04:56.795 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.87s
2025-07-24 19:04:58.481 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0033,avg_pass_at_n: 1.0000,avg_num_tokens: 58.3918,std_num_tokens: 24.4283,avg_correct_num_tokens: 58.1632,std_correct_num_tokens: 24.1786,avg_incorrect_num_tokens: 64.8050,std_incorrect_num_tokens: 29.9099
2025-07-24 19:04:58.939 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.14s
2025-07-24 19:05:01.604 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.66s
2025-07-24 19:05:25.305 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 181
2025-07-24 19:05:25.306 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.70s
2025-07-24 19:05:26.607 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.88s
2025-07-24 19:05:26.607 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00022463347439304282, avg_kl: 0.0019196568273049032, avg_response_length: 58.47505957250437, avg_orm_score: 0.0, avg_custom_rewards: 0.00022463347439304282
2025-07-24 19:05:26.637 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter53_replay_buffer.jsonl
2025-07-24 19:05:27.850 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.0228, ret=-0.00017, glen=59.4, tlen=220, kl=0.00214, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:46,  1.03s/it, pg=0.0228, ret=-0.00017, glen=59.4, tlen=220, kl=0.00214, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:46,  1.03s/it, pg=0.0398, ret=-0.000291, glen=60.4, tlen=221, kl=0.00205, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.07it/s, pg=0.0398, ret=-0.000291, glen=60.4, tlen=221, kl=0.00205, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.07it/s, pg=-0.00552, ret=-7.63e-5, glen=58.2, tlen=218, kl=0.00214, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.00552, ret=-7.63e-5, glen=58.2, tlen=218, kl=0.00214, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=0.0181, ret=0.000207, glen=59.4, tlen=219, kl=0.00163, act_lr=1e-6, ent=1.14]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:36,  1.14it/s, pg=0.0181, ret=0.000207, glen=59.4, tlen=219, kl=0.00163, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:36,  1.14it/s, pg=-0.02, ret=-4.69e-5, glen=56.7, tlen=217, kl=0.0017, act_lr=1e-6, ent=1.12]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=-0.02, ret=-4.69e-5, glen=56.7, tlen=217, kl=0.0017, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=-0.0831, ret=0.000331, glen=58.4, tlen=218, kl=0.00218, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.14it/s, pg=-0.0831, ret=0.000331, glen=58.4, tlen=218, kl=0.00218, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.14it/s, pg=0.0831, ret=-0.000201, glen=59.2, tlen=219, kl=0.00168, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.15it/s, pg=0.0831, ret=-0.000201, glen=59.2, tlen=219, kl=0.00168, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.15it/s, pg=-0.0362, ret=0.00031, glen=56.7, tlen=217, kl=0.00196, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0362, ret=0.00031, glen=56.7, tlen=217, kl=0.00196, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.078, ret=4.03e-5, glen=60.5, tlen=220, kl=0.0018, act_lr=1e-6, ent=1.12] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.17it/s, pg=-0.078, ret=4.03e-5, glen=60.5, tlen=220, kl=0.0018, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.17it/s, pg=0.00235, ret=0.0001, glen=59.7, tlen=220, kl=0.00217, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.17it/s, pg=0.00235, ret=0.0001, glen=59.7, tlen=220, kl=0.00217, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.17it/s, pg=-0.0219, ret=0.00205, glen=60.5, tlen=221, kl=0.00189, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:29,  1.17it/s, pg=-0.0219, ret=0.00205, glen=60.5, tlen=221, kl=0.00189, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:29,  1.17it/s, pg=0.0233, ret=-7.11e-5, glen=55.9, tlen=216, kl=0.0018, act_lr=1e-6, ent=1.14] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.17it/s, pg=0.0233, ret=-7.11e-5, glen=55.9, tlen=216, kl=0.0018, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.17it/s, pg=0.0606, ret=8.29e-5, glen=60.1, tlen=220, kl=0.00193, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=0.0606, ret=8.29e-5, glen=60.1, tlen=220, kl=0.00193, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=0.0104, ret=-0.000111, glen=58.9, tlen=219, kl=0.002, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=0.0104, ret=-0.000111, glen=58.9, tlen=219, kl=0.002, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=-0.00739, ret=-0.000326, glen=56.3, tlen=216, kl=0.00187, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:12<00:26,  1.17it/s, pg=-0.00739, ret=-0.000326, glen=56.3, tlen=216, kl=0.00187, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0938, ret=0.000235, glen=54.3, tlen=214, kl=0.00175, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.15it/s, pg=-0.0938, ret=0.000235, glen=54.3, tlen=214, kl=0.00175, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.15it/s, pg=-0.0785, ret=0.0003, glen=58.7, tlen=218, kl=0.00185, act_lr=1e-6, ent=1.13]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:25,  1.16it/s, pg=-0.0785, ret=0.0003, glen=58.7, tlen=218, kl=0.00185, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.16it/s, pg=0.123, ret=-0.00045, glen=58.3, tlen=218, kl=0.00204, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=0.123, ret=-0.00045, glen=58.3, tlen=218, kl=0.00204, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=-0.0311, ret=0.000168, glen=59.8, tlen=220, kl=0.00187, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0311, ret=0.000168, glen=59.8, tlen=220, kl=0.00187, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=0.0276, ret=-0.00024, glen=61.3, tlen=221, kl=0.00169, act_lr=1e-6, ent=1.11] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.0276, ret=-0.00024, glen=61.3, tlen=221, kl=0.00169, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0423, ret=-0.0002, glen=57.2, tlen=217, kl=0.00179, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0423, ret=-0.0002, glen=57.2, tlen=217, kl=0.00179, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=-0.0318, ret=4.12e-5, glen=58.9, tlen=218, kl=0.00183, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0318, ret=4.12e-5, glen=58.9, tlen=218, kl=0.00183, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0452, ret=8.08e-5, glen=57.4, tlen=218, kl=0.00167, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.17it/s, pg=-0.0452, ret=8.08e-5, glen=57.4, tlen=218, kl=0.00167, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0183, ret=0.000135, glen=61.1, tlen=221, kl=0.0019, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0183, ret=0.000135, glen=61.1, tlen=221, kl=0.0019, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.0779, ret=-0.000209, glen=57.7, tlen=218, kl=0.00205, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=0.0779, ret=-0.000209, glen=57.7, tlen=218, kl=0.00205, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=0.0149, ret=-5.89e-5, glen=60.4, tlen=220, kl=0.00184, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.0149, ret=-5.89e-5, glen=60.4, tlen=220, kl=0.00184, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.0533, ret=-0.000202, glen=57.3, tlen=217, kl=0.00181, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.0533, ret=-0.000202, glen=57.3, tlen=217, kl=0.00181, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0948, ret=0.000342, glen=58, tlen=218, kl=0.0021, act_lr=1e-6, ent=1.15]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.18it/s, pg=-0.0948, ret=0.000342, glen=58, tlen=218, kl=0.0021, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.18it/s, pg=-0.0269, ret=-3.88e-5, glen=56.4, tlen=216, kl=0.00182, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0269, ret=-3.88e-5, glen=56.4, tlen=216, kl=0.00182, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.0749, ret=-0.000124, glen=56.8, tlen=217, kl=0.00218, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.0749, ret=-0.000124, glen=56.8, tlen=217, kl=0.00218, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.0903, ret=-0.000156, glen=60.1, tlen=220, kl=0.00188, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:26<00:13,  1.12it/s, pg=0.0903, ret=-0.000156, glen=60.1, tlen=220, kl=0.00188, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.124, ret=0.000343, glen=56, tlen=216, kl=0.00206, act_lr=1e-6, ent=1.12]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=-0.124, ret=0.000343, glen=56, tlen=216, kl=0.00206, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.012, ret=-2.21e-5, glen=58, tlen=218, kl=0.00187, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.012, ret=-2.21e-5, glen=58, tlen=218, kl=0.00187, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0629, ret=0.000305, glen=59.4, tlen=220, kl=0.0022, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0629, ret=0.000305, glen=59.4, tlen=220, kl=0.0022, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=-0.0472, ret=0.000217, glen=58.6, tlen=218, kl=0.0019, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0472, ret=0.000217, glen=58.6, tlen=218, kl=0.0019, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.012, ret=0.000115, glen=61.6, tlen=222, kl=0.00199, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.012, ret=0.000115, glen=61.6, tlen=222, kl=0.00199, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.118, ret=0.000448, glen=58.9, tlen=219, kl=0.00167, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.118, ret=0.000448, glen=58.9, tlen=219, kl=0.00167, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=0.18, ret=-0.000514, glen=57, tlen=217, kl=0.00193, act_lr=1e-6, ent=1.13]   Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:32<00:06,  1.17it/s, pg=0.18, ret=-0.000514, glen=57, tlen=217, kl=0.00193, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0136, ret=0.000106, glen=58.7, tlen=219, kl=0.00184, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0136, ret=0.000106, glen=58.7, tlen=219, kl=0.00184, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0562, ret=0.000154, glen=58.8, tlen=219, kl=0.00219, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0562, ret=0.000154, glen=58.8, tlen=219, kl=0.00219, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.09, ret=-0.000415, glen=57.7, tlen=217, kl=0.00176, act_lr=1e-6, ent=1.1]   Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.09, ret=-0.000415, glen=57.7, tlen=217, kl=0.00176, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0534, ret=0.000143, glen=57.3, tlen=217, kl=0.00188, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0534, ret=0.000143, glen=57.3, tlen=217, kl=0.00188, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0369, ret=-7.92e-5, glen=60.9, tlen=221, kl=0.00186, act_lr=1e-6, ent=1.11] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0369, ret=-7.92e-5, glen=60.9, tlen=221, kl=0.00186, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.0256, ret=-9.03e-5, glen=56.2, tlen=216, kl=0.00198, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.0256, ret=-9.03e-5, glen=56.2, tlen=216, kl=0.00198, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0468, ret=8.24e-5, glen=59.6, tlen=219, kl=0.00208, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.17it/s, pg=-0.0468, ret=8.24e-5, glen=59.6, tlen=219, kl=0.00208, act_lr=1e-6, ent=1.13]
2025-07-24 19:06:07.856 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.81s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0125, ret=0.000173, glen=56.6, tlen=216, kl=0.00203, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0125, ret=0.000173, glen=56.6, tlen=216, kl=0.00203, act_lr=1e-6, ent=1.08]
2025-07-24 19:06:08.551 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 19:06:10.616 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.06s
2025-07-24 19:06:10.966 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.04s
2025-07-24 19:06:10.972 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0029282362564750338, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.1121161320935125, 'kl': 0.0019183366195015285, 'response_length': 58.461181806481406, 'total_length': 218.4082027932872, 'teacher_total_length': 230.45387268066406, 'return': 5.243863486849359e-05, 'policy_update_steps': 1.0}
Episode [5/20]:  15%|‚ñà‚ñå        | 2/13 [03:42<20:21, 111.06s/it]2025-07-24 19:06:11.018 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:06:45.273 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:06:45.439 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 19:06:45.439 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.42s
2025-07-24 19:06:47.161 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0026,avg_pass_at_n: 1.0000,avg_num_tokens: 56.1400,std_num_tokens: 24.0354,avg_correct_num_tokens: 55.9502,std_correct_num_tokens: 23.7980,avg_incorrect_num_tokens: 62.4832,std_incorrect_num_tokens: 30.2616
2025-07-24 19:06:47.610 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.17s
2025-07-24 19:06:50.257 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.64s
2025-07-24 19:07:13.421 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 19:07:13.421 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.16s
2025-07-24 19:07:14.619 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 19:07:14.619 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 4.824991442013386e-05, avg_kl: 0.004328466660483589, avg_response_length: 56.18804884756078, avg_orm_score: 0.0, avg_custom_rewards: 4.824991442013386e-05
2025-07-24 19:07:14.646 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter54_replay_buffer.jsonl
2025-07-24 19:07:15.858 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.107, ret=0.000296, glen=58.7, tlen=219, kl=0.00534, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.107, ret=0.000296, glen=58.7, tlen=219, kl=0.00534, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.00665, ret=0.000167, glen=58.6, tlen=219, kl=0.00603, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=-0.00665, ret=0.000167, glen=58.6, tlen=219, kl=0.00603, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=0.0628, ret=-0.000102, glen=57.3, tlen=218, kl=0.00427, act_lr=1e-6, ent=1.12] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.10it/s, pg=0.0628, ret=-0.000102, glen=57.3, tlen=218, kl=0.00427, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.10it/s, pg=0.0954, ret=-0.000339, glen=54.9, tlen=215, kl=0.00451, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.11it/s, pg=0.0954, ret=-0.000339, glen=54.9, tlen=215, kl=0.00451, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.11it/s, pg=-0.0858, ret=0.000222, glen=55.1, tlen=216, kl=0.0052, act_lr=1e-6, ent=1.09] Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.11it/s, pg=-0.0858, ret=0.000222, glen=55.1, tlen=216, kl=0.0052, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.11it/s, pg=-0.0407, ret=9.58e-5, glen=53.1, tlen=213, kl=0.00817, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.13it/s, pg=-0.0407, ret=9.58e-5, glen=53.1, tlen=213, kl=0.00817, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.13it/s, pg=0.0178, ret=-0.000106, glen=55.8, tlen=216, kl=0.00614, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=0.0178, ret=-0.000106, glen=55.8, tlen=216, kl=0.00614, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=-0.0633, ret=0.000193, glen=57.7, tlen=218, kl=0.00322, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.0633, ret=0.000193, glen=57.7, tlen=218, kl=0.00322, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.133, ret=0.000454, glen=56.6, tlen=217, kl=0.00534, act_lr=1e-6, ent=1.15] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.16it/s, pg=-0.133, ret=0.000454, glen=56.6, tlen=217, kl=0.00534, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.16it/s, pg=0.0968, ret=-7.78e-5, glen=57.7, tlen=218, kl=0.00455, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.14it/s, pg=0.0968, ret=-7.78e-5, glen=57.7, tlen=218, kl=0.00455, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.14it/s, pg=0.0288, ret=-0.000257, glen=55.7, tlen=216, kl=0.00586, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=0.0288, ret=-0.000257, glen=55.7, tlen=216, kl=0.00586, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=0.0137, ret=-7.98e-5, glen=54.6, tlen=215, kl=0.00517, act_lr=1e-6, ent=1.13] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:29,  1.14it/s, pg=0.0137, ret=-7.98e-5, glen=54.6, tlen=215, kl=0.00517, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:29,  1.14it/s, pg=0.0592, ret=-8.08e-5, glen=55.5, tlen=216, kl=0.0045, act_lr=1e-6, ent=1.07] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=0.0592, ret=-8.08e-5, glen=55.5, tlen=216, kl=0.0045, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=-0.0384, ret=7.84e-5, glen=56.3, tlen=216, kl=0.00442, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:27,  1.14it/s, pg=-0.0384, ret=7.84e-5, glen=56.3, tlen=216, kl=0.00442, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:27,  1.14it/s, pg=-0.00299, ret=-6.65e-5, glen=55.2, tlen=216, kl=0.00282, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.15it/s, pg=-0.00299, ret=-6.65e-5, glen=55.2, tlen=216, kl=0.00282, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:26,  1.15it/s, pg=0.0944, ret=-0.000202, glen=55.9, tlen=216, kl=0.00508, act_lr=1e-6, ent=1.09] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.15it/s, pg=0.0944, ret=-0.000202, glen=55.9, tlen=216, kl=0.00508, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.15it/s, pg=-0.0976, ret=0.000113, glen=56.2, tlen=216, kl=0.00324, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.16it/s, pg=-0.0976, ret=0.000113, glen=56.2, tlen=216, kl=0.00324, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.16it/s, pg=-0.0356, ret=0.000146, glen=57.2, tlen=218, kl=0.00317, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.16it/s, pg=-0.0356, ret=0.000146, glen=57.2, tlen=218, kl=0.00317, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.16it/s, pg=-0.0309, ret=0.000126, glen=56.3, tlen=217, kl=0.00333, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.0309, ret=0.000126, glen=56.3, tlen=217, kl=0.00333, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=0.2, ret=-0.000775, glen=57.4, tlen=218, kl=0.00434, act_lr=1e-6, ent=1.08]   Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=0.2, ret=-0.000775, glen=57.4, tlen=218, kl=0.00434, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.111, ret=0.000294, glen=53.9, tlen=214, kl=0.00348, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.111, ret=0.000294, glen=53.9, tlen=214, kl=0.00348, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=0.11, ret=-0.000257, glen=58.9, tlen=219, kl=0.00399, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.15it/s, pg=0.11, ret=-0.000257, glen=58.9, tlen=219, kl=0.00399, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.15it/s, pg=0.0426, ret=4.87e-5, glen=53.1, tlen=214, kl=0.00353, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=0.0426, ret=4.87e-5, glen=53.1, tlen=214, kl=0.00353, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=-0.000103, ret=-9.61e-5, glen=53.2, tlen=214, kl=0.00313, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.16it/s, pg=-0.000103, ret=-9.61e-5, glen=53.2, tlen=214, kl=0.00313, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.16it/s, pg=0.0541, ret=-0.000292, glen=59.4, tlen=220, kl=0.0037, act_lr=1e-6, ent=1.11]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.0541, ret=-0.000292, glen=59.4, tlen=220, kl=0.0037, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.078, ret=2.77e-5, glen=55.3, tlen=216, kl=0.00327, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.078, ret=2.77e-5, glen=55.3, tlen=216, kl=0.00327, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.0179, ret=-1.71e-5, glen=56.5, tlen=217, kl=0.00508, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=0.0179, ret=-1.71e-5, glen=56.5, tlen=217, kl=0.00508, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0311, ret=0.000208, glen=57, tlen=218, kl=0.00447, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0311, ret=0.000208, glen=57, tlen=218, kl=0.00447, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.182, ret=-0.000793, glen=58.9, tlen=220, kl=0.00628, act_lr=1e-6, ent=1.15]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.07it/s, pg=0.182, ret=-0.000793, glen=58.9, tlen=220, kl=0.00628, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.07it/s, pg=-0.0402, ret=0.000121, glen=56.1, tlen=217, kl=0.00346, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0402, ret=0.000121, glen=56.1, tlen=217, kl=0.00346, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.138, ret=0.000515, glen=55.2, tlen=216, kl=0.00531, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.138, ret=0.000515, glen=55.2, tlen=216, kl=0.00531, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=0.249, ret=-0.000707, glen=58.9, tlen=220, kl=0.00403, act_lr=1e-6, ent=1.16]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.249, ret=-0.000707, glen=58.9, tlen=220, kl=0.00403, act_lr=1e-6, ent=1.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.0503, ret=-0.000133, glen=56.6, tlen=217, kl=0.00375, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=0.0503, ret=-0.000133, glen=56.6, tlen=217, kl=0.00375, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.125, ret=-0.000266, glen=57.2, tlen=218, kl=0.00315, act_lr=1e-6, ent=1.14] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.125, ret=-0.000266, glen=57.2, tlen=218, kl=0.00315, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0462, ret=0.000209, glen=54, tlen=214, kl=0.00358, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0462, ret=0.000209, glen=54, tlen=214, kl=0.00358, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.101, ret=0.000362, glen=58, tlen=218, kl=0.00367, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.101, ret=0.000362, glen=58, tlen=218, kl=0.00367, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=-0.0105, ret=-2.98e-5, glen=54, tlen=214, kl=0.00511, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0105, ret=-2.98e-5, glen=54, tlen=214, kl=0.00511, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.147, ret=0.000426, glen=59.1, tlen=220, kl=0.00522, act_lr=1e-6, ent=1.21]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.147, ret=0.000426, glen=59.1, tlen=220, kl=0.00522, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0391, ret=4.77e-5, glen=55.9, tlen=216, kl=0.00354, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0391, ret=4.77e-5, glen=55.9, tlen=216, kl=0.00354, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0314, ret=8.49e-5, glen=55.1, tlen=216, kl=0.00411, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0314, ret=8.49e-5, glen=55.1, tlen=216, kl=0.00411, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0131, ret=7.48e-5, glen=54.4, tlen=215, kl=0.00452, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.15it/s, pg=-0.0131, ret=7.48e-5, glen=54.4, tlen=215, kl=0.00452, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.15it/s, pg=-0.0291, ret=0.000121, glen=54.3, tlen=215, kl=0.00258, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.15it/s, pg=-0.0291, ret=0.000121, glen=54.3, tlen=215, kl=0.00258, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.15it/s, pg=0.0204, ret=-3.44e-5, glen=57.6, tlen=218, kl=0.00314, act_lr=1e-6, ent=1.09] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.16it/s, pg=0.0204, ret=-3.44e-5, glen=57.6, tlen=218, kl=0.00314, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.16it/s, pg=-0.119, ret=0.000408, glen=55.5, tlen=216, kl=0.0044, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.16it/s, pg=-0.119, ret=0.000408, glen=55.5, tlen=216, kl=0.0044, act_lr=1e-6, ent=1.1]
2025-07-24 19:07:55.365 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.27s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.16it/s, pg=0.0183, ret=-0.000168, glen=55.5, tlen=216, kl=0.00448, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=0.0183, ret=-0.000168, glen=55.5, tlen=216, kl=0.00448, act_lr=1e-6, ent=1.1]
2025-07-24 19:07:56.029 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 19:07:58.351 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-24 19:07:58.701 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.73s
2025-07-24 19:07:58.707 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0008447435167100695, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.104433666335212, 'kl': 0.004348500564487444, 'response_length': 56.21700422498915, 'total_length': 216.63660380045573, 'teacher_total_length': 228.67666625976562, 'return': -9.094274395869838e-07, 'policy_update_steps': 1.0}
Episode [5/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:30<18:15, 109.54s/it]2025-07-24 19:07:58.751 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:08:38.792 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:08:38.982 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:08:38.983 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 40.23s
2025-07-24 19:08:40.679 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0027,avg_pass_at_n: 1.0000,avg_num_tokens: 53.3037,std_num_tokens: 23.1300,avg_correct_num_tokens: 53.2837,std_correct_num_tokens: 23.1391,avg_incorrect_num_tokens: 53.8800,std_incorrect_num_tokens: 22.8589
2025-07-24 19:08:41.110 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.13s
2025-07-24 19:08:43.768 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.65s
2025-07-24 19:09:06.362 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 176
2025-07-24 19:09:06.362 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.59s
2025-07-24 19:09:07.569 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 19:09:07.569 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 7.849076692681676e-06, avg_kl: 0.009420394897460938, avg_response_length: 53.357605478980325, avg_orm_score: 0.0, avg_custom_rewards: 7.849076692681676e-06
2025-07-24 19:09:07.596 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter55_replay_buffer.jsonl
2025-07-24 19:09:08.736 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.14s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=0.0834, ret=-0.000179, glen=58.1, tlen=218, kl=0.0165, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.00it/s, pg=0.0834, ret=-0.000179, glen=58.1, tlen=218, kl=0.0165, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.00it/s, pg=-0.105, ret=0.000374, glen=53, tlen=213, kl=0.0108, act_lr=1e-6, ent=1.07]   Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=-0.105, ret=0.000374, glen=53, tlen=213, kl=0.0108, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=0.0893, ret=-6.17e-5, glen=52.6, tlen=213, kl=0.00649, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=0.0893, ret=-6.17e-5, glen=52.6, tlen=213, kl=0.00649, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=-0.0358, ret=0.000277, glen=53.2, tlen=213, kl=0.0102, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.13it/s, pg=-0.0358, ret=0.000277, glen=53.2, tlen=213, kl=0.0102, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.13it/s, pg=-0.0793, ret=0.000307, glen=54, tlen=214, kl=0.007, act_lr=1e-6, ent=1.09]   Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.12it/s, pg=-0.0793, ret=0.000307, glen=54, tlen=214, kl=0.007, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.12it/s, pg=-0.064, ret=0.000262, glen=53.7, tlen=214, kl=0.0126, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.064, ret=0.000262, glen=53.7, tlen=214, kl=0.0126, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=-0.0598, ret=0.000236, glen=53, tlen=213, kl=0.00983, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.15it/s, pg=-0.0598, ret=0.000236, glen=53, tlen=213, kl=0.00983, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.15it/s, pg=0.0148, ret=-0.000166, glen=54.8, tlen=215, kl=0.00694, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.16it/s, pg=0.0148, ret=-0.000166, glen=54.8, tlen=215, kl=0.00694, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.16it/s, pg=0.0189, ret=-0.000308, glen=52.2, tlen=212, kl=0.0211, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.14it/s, pg=0.0189, ret=-0.000308, glen=52.2, tlen=212, kl=0.0211, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=-0.0878, ret=0.000299, glen=51.7, tlen=212, kl=0.00803, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=-0.0878, ret=0.000299, glen=51.7, tlen=212, kl=0.00803, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=0.0555, ret=-0.00035, glen=52.6, tlen=212, kl=0.014, act_lr=1e-6, ent=1.07]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=0.0555, ret=-0.00035, glen=52.6, tlen=212, kl=0.014, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=-0.0587, ret=0.000136, glen=54.1, tlen=214, kl=0.00493, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=-0.0587, ret=0.000136, glen=54.1, tlen=214, kl=0.00493, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.12, ret=0.000325, glen=53, tlen=213, kl=0.0107, act_lr=1e-6, ent=1.08]    Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=-0.12, ret=0.000325, glen=53, tlen=213, kl=0.0107, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=0.0858, ret=-0.000348, glen=52, tlen=212, kl=0.0099, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=0.0858, ret=-0.000348, glen=52, tlen=212, kl=0.0099, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=0.0502, ret=-0.000262, glen=53.5, tlen=213, kl=0.00835, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.15it/s, pg=0.0502, ret=-0.000262, glen=53.5, tlen=213, kl=0.00835, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.15it/s, pg=-0.112, ret=0.000336, glen=53.2, tlen=214, kl=0.00716, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:24,  1.15it/s, pg=-0.112, ret=0.000336, glen=53.2, tlen=214, kl=0.00716, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.15it/s, pg=-0.0871, ret=0.000301, glen=51, tlen=211, kl=0.0218, act_lr=1e-6, ent=1.06]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.16it/s, pg=-0.0871, ret=0.000301, glen=51, tlen=211, kl=0.0218, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=0.0431, ret=-0.000229, glen=53.9, tlen=214, kl=0.00762, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=0.0431, ret=-0.000229, glen=53.9, tlen=214, kl=0.00762, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=-0.0388, ret=6.57e-5, glen=52.8, tlen=213, kl=0.00609, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.0388, ret=6.57e-5, glen=52.8, tlen=213, kl=0.00609, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0977, ret=0.000345, glen=52.5, tlen=213, kl=0.00652, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0977, ret=0.000345, glen=52.5, tlen=213, kl=0.00652, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=0.0724, ret=-0.000321, glen=53.7, tlen=214, kl=0.00585, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=0.0724, ret=-0.000321, glen=53.7, tlen=214, kl=0.00585, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=-0.0885, ret=0.000319, glen=51.9, tlen=212, kl=0.00677, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.0885, ret=0.000319, glen=51.9, tlen=212, kl=0.00677, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.0355, ret=2.13e-5, glen=52.2, tlen=212, kl=0.0107, act_lr=1e-6, ent=1.07]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.17it/s, pg=-0.0355, ret=2.13e-5, glen=52.2, tlen=212, kl=0.0107, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=0.0271, ret=-0.000234, glen=52.2, tlen=212, kl=0.0112, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.0271, ret=-0.000234, glen=52.2, tlen=212, kl=0.0112, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.0751, ret=0.00031, glen=52.8, tlen=213, kl=0.00782, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.0751, ret=0.00031, glen=52.8, tlen=213, kl=0.00782, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.253, ret=-0.000704, glen=55.2, tlen=215, kl=0.00654, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.253, ret=-0.000704, glen=55.2, tlen=215, kl=0.00654, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=-0.0664, ret=2.68e-5, glen=54.7, tlen=215, kl=0.00712, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=-0.0664, ret=2.68e-5, glen=54.7, tlen=215, kl=0.00712, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.063, ret=0.000324, glen=55.1, tlen=216, kl=0.0079, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.063, ret=0.000324, glen=55.1, tlen=216, kl=0.0079, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.107, ret=0.000488, glen=54.3, tlen=215, kl=0.0107, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:13,  1.07it/s, pg=-0.107, ret=0.000488, glen=54.3, tlen=215, kl=0.0107, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:13,  1.07it/s, pg=0.112, ret=-0.000396, glen=54.7, tlen=214, kl=0.00856, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.112, ret=-0.000396, glen=54.7, tlen=214, kl=0.00856, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=0.115, ret=-0.000424, glen=54.2, tlen=214, kl=0.00642, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.115, ret=-0.000424, glen=54.2, tlen=214, kl=0.00642, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.00995, ret=7.75e-5, glen=52.9, tlen=213, kl=0.0126, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=-0.00995, ret=7.75e-5, glen=52.9, tlen=213, kl=0.0126, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.112, ret=-0.000272, glen=51.4, tlen=211, kl=0.00777, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.112, ret=-0.000272, glen=51.4, tlen=211, kl=0.00777, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.0766, ret=0.000312, glen=52.3, tlen=212, kl=0.00597, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=-0.0766, ret=0.000312, glen=52.3, tlen=212, kl=0.00597, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=-0.0674, ret=0.000211, glen=52.5, tlen=213, kl=0.0163, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0674, ret=0.000211, glen=52.5, tlen=213, kl=0.0163, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0849, ret=0.000369, glen=55.4, tlen=216, kl=0.00704, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=-0.0849, ret=0.000369, glen=55.4, tlen=216, kl=0.00704, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=0.0605, ret=-0.000155, glen=55.9, tlen=216, kl=0.00775, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=0.0605, ret=-0.000155, glen=55.9, tlen=216, kl=0.00775, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=0.0976, ret=-0.000241, glen=55.3, tlen=216, kl=0.00735, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:32<00:05,  1.17it/s, pg=0.0976, ret=-0.000241, glen=55.3, tlen=216, kl=0.00735, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0632, ret=-0.000351, glen=52.1, tlen=212, kl=0.00825, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=0.0632, ret=-0.000351, glen=52.1, tlen=212, kl=0.00825, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0145, ret=7.91e-5, glen=55.4, tlen=216, kl=0.00742, act_lr=1e-6, ent=1.13] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.0145, ret=7.91e-5, glen=55.4, tlen=216, kl=0.00742, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.0172, ret=5.51e-5, glen=50.6, tlen=211, kl=0.00874, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=0.0172, ret=5.51e-5, glen=50.6, tlen=211, kl=0.00874, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.167, ret=-0.000366, glen=52.4, tlen=213, kl=0.0125, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.167, ret=-0.000366, glen=52.4, tlen=213, kl=0.0125, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.0486, ret=-0.000231, glen=53.2, tlen=213, kl=0.00741, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=0.0486, ret=-0.000231, glen=53.2, tlen=213, kl=0.00741, act_lr=1e-6, ent=1.09]
2025-07-24 19:09:47.100 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=0.0398, ret=-0.000246, glen=52.4, tlen=212, kl=0.00909, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=0.0398, ret=-0.000246, glen=52.4, tlen=212, kl=0.00909, act_lr=1e-6, ent=1.07]
2025-07-24 19:09:47.913 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 19:09:50.528 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.61s
2025-07-24 19:09:50.857 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.07s
2025-07-24 19:09:50.864 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0001867467706853693, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.0883013118397107, 'kl': 0.009420394897460938, 'response_length': 53.357605240561746, 'total_length': 213.43730198253286, 'teacher_total_length': 225.39523870294744, 'return': 2.994843684560196e-07, 'policy_update_steps': 1.0}
Episode [5/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [07:22<16:35, 110.57s/it]2025-07-24 19:09:50.914 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:10:22.675 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:10:22.853 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:10:22.854 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.94s
2025-07-24 19:10:24.544 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 51.3036,std_num_tokens: 20.2148,avg_correct_num_tokens: 51.1935,std_correct_num_tokens: 20.1408,avg_incorrect_num_tokens: 55.4481,std_incorrect_num_tokens: 22.4382
2025-07-24 19:10:24.932 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.08s
2025-07-24 19:10:27.350 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.42s
2025-07-24 19:10:49.817 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 19:10:49.817 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.46s
2025-07-24 19:10:51.029 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 19:10:51.030 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -7.694099198228546e-05, avg_kl: 0.017133614676339285, avg_response_length: 51.32539701189314, avg_orm_score: 0.0, avg_custom_rewards: -7.694099198228546e-05
2025-07-24 19:10:51.059 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter56_replay_buffer.jsonl
2025-07-24 19:10:52.177 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.12s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.115, ret=-0.00013, glen=51.3, tlen=212, kl=0.0161, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=0.115, ret=-0.00013, glen=51.3, tlen=212, kl=0.0161, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=0.0287, ret=-0.00034, glen=49.6, tlen=211, kl=0.0349, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=0.0287, ret=-0.00034, glen=49.6, tlen=211, kl=0.0349, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=-0.0845, ret=0.000228, glen=51.9, tlen=212, kl=0.0122, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.13it/s, pg=-0.0845, ret=0.000228, glen=51.9, tlen=212, kl=0.0122, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.13it/s, pg=0.0988, ret=-0.000365, glen=50.9, tlen=212, kl=0.0382, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:34,  1.15it/s, pg=0.0988, ret=-0.000365, glen=50.9, tlen=212, kl=0.0382, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:34,  1.15it/s, pg=0.0832, ret=-0.000282, glen=53.3, tlen=214, kl=0.0148, act_lr=1e-6, ent=1.14]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:33,  1.16it/s, pg=0.0832, ret=-0.000282, glen=53.3, tlen=214, kl=0.0148, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:33,  1.16it/s, pg=-0.0698, ret=0.000251, glen=50.1, tlen=211, kl=0.0126, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:32,  1.16it/s, pg=-0.0698, ret=0.000251, glen=50.1, tlen=211, kl=0.0126, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:32,  1.16it/s, pg=-0.151, ret=0.000449, glen=52.5, tlen=213, kl=0.0104, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:31,  1.17it/s, pg=-0.151, ret=0.000449, glen=52.5, tlen=213, kl=0.0104, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:31,  1.17it/s, pg=-0.0923, ret=0.000274, glen=51.9, tlen=212, kl=0.0104, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:06<00:30,  1.17it/s, pg=-0.0923, ret=0.000274, glen=51.9, tlen=212, kl=0.0104, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:30,  1.17it/s, pg=0.00568, ret=0.000155, glen=51.8, tlen=213, kl=0.0103, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=0.00568, ret=0.000155, glen=51.8, tlen=213, kl=0.0103, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.0805, ret=-0.000253, glen=49.6, tlen=210, kl=0.0109, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=0.0805, ret=-0.000253, glen=49.6, tlen=210, kl=0.0109, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=-0.049, ret=0.000151, glen=51.5, tlen=212, kl=0.0321, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.049, ret=0.000151, glen=51.5, tlen=212, kl=0.0321, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=-0.116, ret=0.000376, glen=51.1, tlen=211, kl=0.0214, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.17it/s, pg=-0.116, ret=0.000376, glen=51.1, tlen=211, kl=0.0214, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.17it/s, pg=-0.0116, ret=8.86e-5, glen=51, tlen=211, kl=0.0137, act_lr=1e-6, ent=1.12] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:27,  1.11it/s, pg=-0.0116, ret=8.86e-5, glen=51, tlen=211, kl=0.0137, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:27,  1.11it/s, pg=0.0136, ret=-0.000146, glen=52.1, tlen=212, kl=0.0271, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:27,  1.11it/s, pg=0.0136, ret=-0.000146, glen=52.1, tlen=212, kl=0.0271, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:27,  1.11it/s, pg=0.00342, ret=-5.31e-5, glen=47.6, tlen=208, kl=0.0101, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.12it/s, pg=0.00342, ret=-5.31e-5, glen=47.6, tlen=208, kl=0.0101, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.12it/s, pg=0.0482, ret=-0.000188, glen=52.8, tlen=213, kl=0.0278, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.14it/s, pg=0.0482, ret=-0.000188, glen=52.8, tlen=213, kl=0.0278, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.14it/s, pg=-0.149, ret=0.0004, glen=50.8, tlen=211, kl=0.0215, act_lr=1e-6, ent=1.03]   Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.15it/s, pg=-0.149, ret=0.0004, glen=50.8, tlen=211, kl=0.0215, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.15it/s, pg=-0.0732, ret=0.000263, glen=51, tlen=212, kl=0.0124, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=-0.0732, ret=0.000263, glen=51, tlen=212, kl=0.0124, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=0.101, ret=-0.000319, glen=51.3, tlen=212, kl=0.0153, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=0.101, ret=-0.000319, glen=51.3, tlen=212, kl=0.0153, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=-0.0519, ret=0.000122, glen=52.4, tlen=213, kl=0.0118, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0519, ret=0.000122, glen=52.4, tlen=213, kl=0.0118, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0834, ret=0.000175, glen=51.9, tlen=213, kl=0.0116, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0834, ret=0.000175, glen=51.9, tlen=213, kl=0.0116, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.118, ret=-0.000256, glen=51.3, tlen=212, kl=0.0196, act_lr=1e-6, ent=1.07] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.118, ret=-0.000256, glen=51.3, tlen=212, kl=0.0196, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.0209, ret=0.000175, glen=50.9, tlen=212, kl=0.0424, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.18it/s, pg=-0.0209, ret=0.000175, glen=50.9, tlen=212, kl=0.0424, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=0.0901, ret=-0.000305, glen=51.4, tlen=212, kl=0.0241, act_lr=1e-6, ent=1.12]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.18it/s, pg=0.0901, ret=-0.000305, glen=51.4, tlen=212, kl=0.0241, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.18it/s, pg=-0.0369, ret=2.32e-5, glen=52.7, tlen=213, kl=0.00931, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.18it/s, pg=-0.0369, ret=2.32e-5, glen=52.7, tlen=213, kl=0.00931, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.18it/s, pg=-0.0176, ret=9.05e-5, glen=49.5, tlen=210, kl=0.0275, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.18it/s, pg=-0.0176, ret=9.05e-5, glen=49.5, tlen=210, kl=0.0275, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.18it/s, pg=0.000574, ret=6.72e-5, glen=50.1, tlen=211, kl=0.0173, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.18it/s, pg=0.000574, ret=6.72e-5, glen=50.1, tlen=211, kl=0.0173, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.18it/s, pg=-0.116, ret=0.000339, glen=50.4, tlen=211, kl=0.0123, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.18it/s, pg=-0.116, ret=0.000339, glen=50.4, tlen=211, kl=0.0123, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.18it/s, pg=-0.0496, ret=-6.72e-5, glen=52.1, tlen=213, kl=0.0101, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=-0.0496, ret=-6.72e-5, glen=52.1, tlen=213, kl=0.0101, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.0112, ret=2.37e-5, glen=53.1, tlen=213, kl=0.011, act_lr=1e-6, ent=1.09]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.0112, ret=2.37e-5, glen=53.1, tlen=213, kl=0.011, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=0.0819, ret=-0.000182, glen=53.8, tlen=215, kl=0.0118, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.0819, ret=-0.000182, glen=53.8, tlen=215, kl=0.0118, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0671, ret=0.00021, glen=50.8, tlen=211, kl=0.0124, act_lr=1e-6, ent=1.09] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=-0.0671, ret=0.00021, glen=50.8, tlen=211, kl=0.0124, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=-0.079, ret=0.000312, glen=51.2, tlen=212, kl=0.0152, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=-0.079, ret=0.000312, glen=51.2, tlen=212, kl=0.0152, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=0.208, ret=-0.00052, glen=50.6, tlen=211, kl=0.0295, act_lr=1e-6, ent=1.15] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=0.208, ret=-0.00052, glen=50.6, tlen=211, kl=0.0295, act_lr=1e-6, ent=1.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=-0.0815, ret=0.000179, glen=52.6, tlen=213, kl=0.0166, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0815, ret=0.000179, glen=52.6, tlen=213, kl=0.0166, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0339, ret=-7.79e-5, glen=49, tlen=209, kl=0.024, act_lr=1e-6, ent=1.06]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.0339, ret=-7.79e-5, glen=49, tlen=209, kl=0.024, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=0.0437, ret=2.52e-5, glen=54, tlen=215, kl=0.0117, act_lr=1e-6, ent=1.12] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=0.0437, ret=2.52e-5, glen=54, tlen=215, kl=0.0117, act_lr=1e-6, ent=1.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=-0.0319, ret=5.8e-5, glen=51.9, tlen=213, kl=0.00889, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0319, ret=5.8e-5, glen=51.9, tlen=213, kl=0.00889, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0918, ret=-0.000123, glen=50.4, tlen=211, kl=0.0209, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=0.0918, ret=-0.000123, glen=50.4, tlen=211, kl=0.0209, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.15, ret=-0.000368, glen=49.4, tlen=210, kl=0.0133, act_lr=1e-6, ent=1.07]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.15, ret=-0.000368, glen=49.4, tlen=210, kl=0.0133, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0386, ret=-9.94e-5, glen=52.7, tlen=213, kl=0.0105, act_lr=1e-6, ent=1.11]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0386, ret=-9.94e-5, glen=52.7, tlen=213, kl=0.0105, act_lr=1e-6, ent=1.11]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.0327, ret=0.000217, glen=50.2, tlen=210, kl=0.01, act_lr=1e-6, ent=1.06]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=-0.0327, ret=0.000217, glen=50.2, tlen=210, kl=0.01, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=-0.0255, ret=-0.000103, glen=51.8, tlen=212, kl=0.01, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=-0.0255, ret=-0.000103, glen=51.8, tlen=212, kl=0.01, act_lr=1e-6, ent=1.08]
2025-07-24 19:11:30.582 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=0.145, ret=-0.000478, glen=51.3, tlen=212, kl=0.0105, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=0.145, ret=-0.000478, glen=51.3, tlen=212, kl=0.0105, act_lr=1e-6, ent=1.06]
2025-07-24 19:11:31.262 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:11:33.589 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.33s
2025-07-24 19:11:33.921 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.68s
2025-07-24 19:11:33.927 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0010180690071799538, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.082818402485414, 'kl': 0.01715001192959872, 'response_length': 51.310527541420676, 'total_length': 211.92941665649414, 'teacher_total_length': 223.9316617792303, 'return': -3.780521431260488e-08, 'policy_update_steps': 1.0}
Episode [5/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [09:05<14:22, 107.87s/it]2025-07-24 19:11:33.970 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:12:05.009 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:12:05.185 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:12:05.185 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.22s
2025-07-24 19:12:07.166 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0164,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 49.4729,std_num_tokens: 18.9614,avg_correct_num_tokens: 49.3154,std_correct_num_tokens: 18.7884,avg_incorrect_num_tokens: 55.0489,std_incorrect_num_tokens: 23.6394
2025-07-24 19:12:07.588 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.40s
2025-07-24 19:12:10.042 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.45s
2025-07-24 19:12:32.790 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 19:12:32.794 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.75s
2025-07-24 19:12:33.974 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 19:12:33.974 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.0409328729213995e-06, avg_kl: 0.03581956885326868, avg_response_length: 49.538733186392946, avg_orm_score: 0.0, avg_custom_rewards: 2.0409328729213995e-06
2025-07-24 19:12:34.001 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter57_replay_buffer.jsonl
2025-07-24 19:12:35.088 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.09s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.144, ret=-0.000447, glen=48.6, tlen=210, kl=0.0143, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:45,  1.07s/it, pg=0.144, ret=-0.000447, glen=48.6, tlen=210, kl=0.0143, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:45,  1.07s/it, pg=-0.121, ret=0.000295, glen=50.1, tlen=210, kl=0.0977, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:40,  1.05it/s, pg=-0.121, ret=0.000295, glen=50.1, tlen=210, kl=0.0977, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:40,  1.05it/s, pg=0.0743, ret=-0.000123, glen=50.2, tlen=211, kl=0.0512, act_lr=1e-6, ent=1]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:37,  1.10it/s, pg=0.0743, ret=-0.000123, glen=50.2, tlen=211, kl=0.0512, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:37,  1.10it/s, pg=-0.144, ret=0.000352, glen=48.9, tlen=209, kl=0.0181, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:36,  1.10it/s, pg=-0.144, ret=0.000352, glen=48.9, tlen=209, kl=0.0181, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:36,  1.10it/s, pg=-0.0488, ret=0.000139, glen=50.3, tlen=211, kl=0.0168, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.12it/s, pg=-0.0488, ret=0.000139, glen=50.3, tlen=211, kl=0.0168, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.12it/s, pg=-0.0222, ret=0.000197, glen=48.3, tlen=209, kl=0.0145, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.0222, ret=0.000197, glen=48.3, tlen=209, kl=0.0145, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=-0.0747, ret=9.55e-5, glen=49.6, tlen=210, kl=0.0687, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.15it/s, pg=-0.0747, ret=9.55e-5, glen=49.6, tlen=210, kl=0.0687, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.15it/s, pg=0.00527, ret=-1.41e-5, glen=50.5, tlen=212, kl=0.0505, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.16it/s, pg=0.00527, ret=-1.41e-5, glen=50.5, tlen=212, kl=0.0505, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.16it/s, pg=0.0867, ret=-0.00025, glen=47.4, tlen=208, kl=0.017, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.16it/s, pg=0.0867, ret=-0.00025, glen=47.4, tlen=208, kl=0.017, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.16it/s, pg=0.0356, ret=7.97e-5, glen=52.2, tlen=213, kl=0.0798, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.17it/s, pg=0.0356, ret=7.97e-5, glen=52.2, tlen=213, kl=0.0798, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.17it/s, pg=-0.0293, ret=-1.47e-5, glen=50.3, tlen=211, kl=0.0171, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.17it/s, pg=-0.0293, ret=-1.47e-5, glen=50.3, tlen=211, kl=0.0171, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.17it/s, pg=0.00696, ret=8.29e-5, glen=49.6, tlen=210, kl=0.0161, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.17it/s, pg=0.00696, ret=8.29e-5, glen=49.6, tlen=210, kl=0.0161, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.17it/s, pg=0.076, ret=-0.00037, glen=50.5, tlen=212, kl=0.0433, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.076, ret=-0.00037, glen=50.5, tlen=212, kl=0.0433, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=-0.0389, ret=3.74e-5, glen=49.9, tlen=211, kl=0.0222, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=-0.0389, ret=3.74e-5, glen=49.9, tlen=211, kl=0.0222, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=0.0381, ret=9.95e-6, glen=49.4, tlen=210, kl=0.0754, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=0.0381, ret=9.95e-6, glen=49.4, tlen=210, kl=0.0754, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=0.0684, ret=-4.57e-5, glen=47.7, tlen=208, kl=0.0138, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:24,  1.16it/s, pg=0.0684, ret=-4.57e-5, glen=47.7, tlen=208, kl=0.0138, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.0286, ret=7.08e-5, glen=51, tlen=212, kl=0.0161, act_lr=1e-6, ent=1.07]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.14it/s, pg=-0.0286, ret=7.08e-5, glen=51, tlen=212, kl=0.0161, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.14it/s, pg=0.000305, ret=4.53e-5, glen=48.3, tlen=209, kl=0.0148, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.15it/s, pg=0.000305, ret=4.53e-5, glen=48.3, tlen=209, kl=0.0148, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.15it/s, pg=-0.0585, ret=0.000109, glen=47.9, tlen=209, kl=0.0129, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=-0.0585, ret=0.000109, glen=47.9, tlen=209, kl=0.0129, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=0.0263, ret=3.7e-6, glen=48.9, tlen=209, kl=0.0233, act_lr=1e-6, ent=1.05]   Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.16it/s, pg=0.0263, ret=3.7e-6, glen=48.9, tlen=209, kl=0.0233, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.16it/s, pg=0.0194, ret=3.38e-5, glen=47.9, tlen=209, kl=0.121, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.16it/s, pg=0.0194, ret=3.38e-5, glen=47.9, tlen=209, kl=0.121, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.16it/s, pg=-0.0101, ret=5.25e-6, glen=49, tlen=210, kl=0.0128, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.16it/s, pg=-0.0101, ret=5.25e-6, glen=49, tlen=210, kl=0.0128, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.16it/s, pg=-0.0487, ret=1.77e-5, glen=51.5, tlen=213, kl=0.14, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.17it/s, pg=-0.0487, ret=1.77e-5, glen=51.5, tlen=213, kl=0.14, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.17it/s, pg=0.0166, ret=-3.58e-5, glen=51.2, tlen=212, kl=0.0176, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.0166, ret=-3.58e-5, glen=51.2, tlen=212, kl=0.0176, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=0.0242, ret=-2.48e-5, glen=51.6, tlen=212, kl=0.134, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=0.0242, ret=-2.48e-5, glen=51.6, tlen=212, kl=0.134, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.0186, ret=-9.54e-5, glen=48.4, tlen=209, kl=0.0143, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.0186, ret=-9.54e-5, glen=48.4, tlen=209, kl=0.0143, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.095, ret=-0.000225, glen=50.6, tlen=211, kl=0.012, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.095, ret=-0.000225, glen=50.6, tlen=211, kl=0.012, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.0677, ret=9.15e-5, glen=49.9, tlen=210, kl=0.0154, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.0677, ret=9.15e-5, glen=49.9, tlen=210, kl=0.0154, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.00903, ret=-3.6e-5, glen=51.4, tlen=212, kl=0.0364, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.01it/s, pg=-0.00903, ret=-3.6e-5, glen=51.4, tlen=212, kl=0.0364, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.01it/s, pg=-0.099, ret=0.00038, glen=47.7, tlen=208, kl=0.0139, act_lr=1e-6, ent=1.05]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:13,  1.06it/s, pg=-0.099, ret=0.00038, glen=47.7, tlen=208, kl=0.0139, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:13,  1.06it/s, pg=-0.0396, ret=9.67e-6, glen=46.9, tlen=207, kl=0.015, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.09it/s, pg=-0.0396, ret=9.67e-6, glen=46.9, tlen=207, kl=0.015, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.09it/s, pg=0.103, ret=-0.000351, glen=49.5, tlen=210, kl=0.0124, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.11it/s, pg=0.103, ret=-0.000351, glen=49.5, tlen=210, kl=0.0124, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.11it/s, pg=-0.0771, ret=0.000301, glen=48.7, tlen=209, kl=0.0179, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.13it/s, pg=-0.0771, ret=0.000301, glen=48.7, tlen=209, kl=0.0179, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.13it/s, pg=-0.0102, ret=0.00016, glen=49, tlen=210, kl=0.0215, act_lr=1e-6, ent=1.04]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.0102, ret=0.00016, glen=49, tlen=210, kl=0.0215, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=0.00226, ret=-8.28e-5, glen=51.8, tlen=212, kl=0.0876, act_lr=1e-6, ent=1]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.00226, ret=-8.28e-5, glen=51.8, tlen=212, kl=0.0876, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.102, ret=0.000263, glen=48.9, tlen=210, kl=0.016, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.102, ret=0.000263, glen=48.9, tlen=210, kl=0.016, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.00165, ret=-0.000157, glen=50.9, tlen=212, kl=0.0228, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.00165, ret=-0.000157, glen=50.9, tlen=212, kl=0.0228, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=0.0324, ret=3.76e-5, glen=49.9, tlen=210, kl=0.0146, act_lr=1e-6, ent=1.05]    Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0324, ret=3.76e-5, glen=49.9, tlen=210, kl=0.0146, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=0.00531, ret=1.57e-5, glen=48.5, tlen=210, kl=0.0551, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.00531, ret=1.57e-5, glen=48.5, tlen=210, kl=0.0551, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0258, ret=0.000121, glen=48.2, tlen=209, kl=0.0143, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0258, ret=0.000121, glen=48.2, tlen=209, kl=0.0143, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.0987, ret=-0.000341, glen=48.2, tlen=209, kl=0.0273, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=0.0987, ret=-0.000341, glen=48.2, tlen=209, kl=0.0273, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.143, ret=-0.000563, glen=47.5, tlen=209, kl=0.0212, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.143, ret=-0.000563, glen=47.5, tlen=209, kl=0.0212, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.00793, ret=-7.43e-5, glen=51.3, tlen=212, kl=0.0247, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=0.00793, ret=-7.43e-5, glen=51.3, tlen=212, kl=0.0247, act_lr=1e-6, ent=1.02]
2025-07-24 19:13:13.695 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.45s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=-0.107, ret=0.000293, glen=50.1, tlen=211, kl=0.0163, act_lr=1e-6, ent=1.08] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.107, ret=0.000293, glen=50.1, tlen=211, kl=0.0163, act_lr=1e-6, ent=1.08]
2025-07-24 19:13:14.376 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 19:13:16.401 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.03s
2025-07-24 19:13:16.744 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.61s
2025-07-24 19:13:16.750 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00036949867551976985, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.0344691412015394, 'kl': 0.035625457763671875, 'response_length': 49.50538591905074, 'total_length': 210.2055892944336, 'teacher_total_length': 222.20326683738014, 'return': -4.604817149811424e-08, 'policy_update_steps': 1.0}
Episode [5/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [10:48<12:23, 106.15s/it]2025-07-24 19:13:16.794 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:13:46.922 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:13:47.105 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:13:47.106 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.31s
2025-07-24 19:13:48.667 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0168,avg_reflection_pattern_score: 0.0022,avg_pass_at_n: 1.0000,avg_num_tokens: 47.9336,std_num_tokens: 18.7842,avg_correct_num_tokens: 47.7879,std_correct_num_tokens: 17.7357,avg_incorrect_num_tokens: 53.8477,std_incorrect_num_tokens: 43.2544
2025-07-24 19:13:49.071 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.96s
2025-07-24 19:13:51.482 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.41s
2025-07-24 19:14:13.546 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 172
2025-07-24 19:14:13.546 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.06s
2025-07-24 19:14:14.905 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 19:14:14.905 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -4.733867654279091e-05, avg_kl: 0.11534863849018895, avg_response_length: 47.97785213381745, avg_orm_score: 0.0, avg_custom_rewards: -4.733867654279091e-05
2025-07-24 19:14:14.932 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter58_replay_buffer.jsonl
2025-07-24 19:14:15.996 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.07s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s, pg=0.0245, ret=7.34e-5, glen=47.5, tlen=208, kl=0.0209, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:00<00:41,  1.02it/s, pg=0.0245, ret=7.34e-5, glen=47.5, tlen=208, kl=0.0209, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:41,  1.02it/s, pg=-0.132, ret=0.000376, glen=46.9, tlen=208, kl=0.0227, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.10it/s, pg=-0.132, ret=0.000376, glen=46.9, tlen=208, kl=0.0227, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.10it/s, pg=0.238, ret=-0.000865, glen=50.4, tlen=211, kl=0.0458, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:36,  1.11it/s, pg=0.238, ret=-0.000865, glen=50.4, tlen=211, kl=0.0458, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:36,  1.11it/s, pg=0.115, ret=-0.000285, glen=47.5, tlen=208, kl=0.211, act_lr=1e-6, ent=1.07] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:35,  1.11it/s, pg=0.115, ret=-0.000285, glen=47.5, tlen=208, kl=0.211, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:35,  1.11it/s, pg=0.0808, ret=-0.000292, glen=46.3, tlen=206, kl=0.0212, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.13it/s, pg=0.0808, ret=-0.000292, glen=46.3, tlen=206, kl=0.0212, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.13it/s, pg=-0.0779, ret=0.000176, glen=48.3, tlen=209, kl=0.021, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.15it/s, pg=-0.0779, ret=0.000176, glen=48.3, tlen=209, kl=0.021, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.15it/s, pg=0.103, ret=-0.000338, glen=48.1, tlen=208, kl=0.0165, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.16it/s, pg=0.103, ret=-0.000338, glen=48.1, tlen=208, kl=0.0165, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.16it/s, pg=-0.0651, ret=0.000193, glen=47.8, tlen=208, kl=0.0179, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.0651, ret=0.000193, glen=47.8, tlen=208, kl=0.0179, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.0493, ret=0.000101, glen=48.3, tlen=209, kl=0.495, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.17it/s, pg=-0.0493, ret=0.000101, glen=48.3, tlen=209, kl=0.495, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.17it/s, pg=-0.102, ret=0.000202, glen=48.7, tlen=209, kl=0.0192, act_lr=1e-6, ent=1.1] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.14it/s, pg=-0.102, ret=0.000202, glen=48.7, tlen=209, kl=0.0192, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.14it/s, pg=-0.0144, ret=7.79e-5, glen=46.3, tlen=207, kl=0.324, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:28,  1.13it/s, pg=-0.0144, ret=7.79e-5, glen=46.3, tlen=207, kl=0.324, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:28,  1.13it/s, pg=-0.0682, ret=0.00016, glen=47.1, tlen=207, kl=0.016, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:27,  1.14it/s, pg=-0.0682, ret=0.00016, glen=47.1, tlen=207, kl=0.016, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:27,  1.14it/s, pg=-0.105, ret=0.000324, glen=49.1, tlen=210, kl=0.0602, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:26,  1.15it/s, pg=-0.105, ret=0.000324, glen=49.1, tlen=210, kl=0.0602, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:26,  1.15it/s, pg=0.0788, ret=-0.000256, glen=46.4, tlen=207, kl=0.705, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:25,  1.16it/s, pg=0.0788, ret=-0.000256, glen=46.4, tlen=207, kl=0.705, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:25,  1.16it/s, pg=-0.00928, ret=-0.000173, glen=48.2, tlen=209, kl=0.16, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.00928, ret=-0.000173, glen=48.2, tlen=209, kl=0.16, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:14<00:24,  1.15it/s, pg=0.0552, ret=-0.000173, glen=48.9, tlen=210, kl=0.052, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.15it/s, pg=0.0552, ret=-0.000173, glen=48.9, tlen=210, kl=0.052, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.15it/s, pg=-0.135, ret=0.000358, glen=50.5, tlen=211, kl=0.0254, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.16it/s, pg=-0.135, ret=0.000358, glen=50.5, tlen=211, kl=0.0254, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.16it/s, pg=0.00732, ret=0.000102, glen=51.4, tlen=212, kl=0.243, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=0.00732, ret=0.000102, glen=51.4, tlen=212, kl=0.243, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=0.0595, ret=-0.000178, glen=48.9, tlen=209, kl=0.413, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=0.0595, ret=-0.000178, glen=48.9, tlen=209, kl=0.413, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=0.0508, ret=-7.46e-5, glen=47.4, tlen=207, kl=0.0162, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=0.0508, ret=-7.46e-5, glen=47.4, tlen=207, kl=0.0162, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=0.0142, ret=1.36e-5, glen=46.2, tlen=207, kl=0.0183, act_lr=1e-6, ent=1.03]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=0.0142, ret=1.36e-5, glen=46.2, tlen=207, kl=0.0183, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.17it/s, pg=-0.0706, ret=8.48e-5, glen=47.3, tlen=208, kl=0.0163, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.0706, ret=8.48e-5, glen=47.3, tlen=208, kl=0.0163, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.00151, ret=-8.65e-5, glen=48.9, tlen=209, kl=0.0281, act_lr=1e-6, ent=1.09]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.17it/s, pg=-0.00151, ret=-8.65e-5, glen=48.9, tlen=209, kl=0.0281, act_lr=1e-6, ent=1.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=-0.0476, ret=7.5e-6, glen=46.5, tlen=206, kl=0.019, act_lr=1e-6, ent=0.966]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.18it/s, pg=-0.0476, ret=7.5e-6, glen=46.5, tlen=206, kl=0.019, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.18it/s, pg=0.00256, ret=-0.000122, glen=47.3, tlen=207, kl=0.0193, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.17it/s, pg=0.00256, ret=-0.000122, glen=47.3, tlen=207, kl=0.0193, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.17it/s, pg=-0.0213, ret=-3.78e-5, glen=47.4, tlen=208, kl=0.0243, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.18it/s, pg=-0.0213, ret=-3.78e-5, glen=47.4, tlen=208, kl=0.0243, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.18it/s, pg=-0.0346, ret=6.53e-5, glen=46.5, tlen=206, kl=0.404, act_lr=1e-6, ent=1.01]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.18it/s, pg=-0.0346, ret=6.53e-5, glen=46.5, tlen=206, kl=0.404, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.18it/s, pg=-0.107, ret=0.000269, glen=48.8, tlen=210, kl=0.0237, act_lr=1e-6, ent=1]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.18it/s, pg=-0.107, ret=0.000269, glen=48.8, tlen=210, kl=0.0237, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.18it/s, pg=-0.0687, ret=0.00011, glen=47, tlen=208, kl=0.0187, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.07it/s, pg=-0.0687, ret=0.00011, glen=47, tlen=208, kl=0.0187, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.07it/s, pg=-0.132, ret=0.000356, glen=48.7, tlen=209, kl=0.102, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.10it/s, pg=-0.132, ret=0.000356, glen=48.7, tlen=209, kl=0.102, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.10it/s, pg=-0.0248, ret=6.49e-5, glen=49.4, tlen=210, kl=0.0453, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.0248, ret=6.49e-5, glen=49.4, tlen=210, kl=0.0453, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=0.122, ret=-0.000301, glen=50.2, tlen=210, kl=0.215, act_lr=1e-6, ent=1.03] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.14it/s, pg=0.122, ret=-0.000301, glen=50.2, tlen=210, kl=0.215, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=0.296, ret=-6.1e-5, glen=49.7, tlen=210, kl=0.0736, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=0.296, ret=-6.1e-5, glen=49.7, tlen=210, kl=0.0736, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=-0.00928, ret=0.000161, glen=48, tlen=208, kl=0.103, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.16it/s, pg=-0.00928, ret=0.000161, glen=48, tlen=208, kl=0.103, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.16it/s, pg=0.195, ret=-0.000602, glen=49.7, tlen=210, kl=0.0853, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=0.195, ret=-0.000602, glen=49.7, tlen=210, kl=0.0853, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=-0.0813, ret=0.000335, glen=45.6, tlen=206, kl=0.571, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.17it/s, pg=-0.0813, ret=0.000335, glen=45.6, tlen=206, kl=0.571, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.17it/s, pg=-0.0762, ret=0.000149, glen=47.8, tlen=208, kl=0.021, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.0762, ret=0.000149, glen=47.8, tlen=208, kl=0.021, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=-0.0782, ret=0.000188, glen=48, tlen=208, kl=0.0953, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=-0.0782, ret=0.000188, glen=48, tlen=208, kl=0.0953, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=-0.0456, ret=0.000151, glen=47.6, tlen=208, kl=0.0222, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.17it/s, pg=-0.0456, ret=0.000151, glen=47.6, tlen=208, kl=0.0222, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.083, ret=0.000291, glen=47.1, tlen=208, kl=0.0204, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=-0.083, ret=0.000291, glen=47.1, tlen=208, kl=0.0204, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.00269, ret=-1.68e-5, glen=47.7, tlen=208, kl=0.0276, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.15it/s, pg=-0.00269, ret=-1.68e-5, glen=47.7, tlen=208, kl=0.0276, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.15it/s, pg=0.0231, ret=-0.000126, glen=46.4, tlen=207, kl=0.08, act_lr=1e-6, ent=1.02]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.16it/s, pg=0.0231, ret=-0.000126, glen=46.4, tlen=207, kl=0.08, act_lr=1e-6, ent=1.02]
2025-07-24 19:14:53.569 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.40s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.16it/s, pg=0.108, ret=-0.0005, glen=47.1, tlen=208, kl=0.0196, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=0.108, ret=-0.0005, glen=47.1, tlen=208, kl=0.0196, act_lr=1e-6, ent=1.02]
2025-07-24 19:14:54.248 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:14:56.662 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.41s
2025-07-24 19:14:57.009 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 40.96s
2025-07-24 19:14:57.015 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0016206253406613371, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.0244757588519606, 'kl': 0.11534863918326622, 'response_length': 47.97785213381745, 'total_length': 208.35744884402254, 'teacher_total_length': 220.3618611180505, 'return': -2.3096819309420277e-06, 'policy_update_steps': 1.0}
Episode [5/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [12:28<10:25, 104.23s/it]2025-07-24 19:14:57.060 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:15:27.232 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:15:27.399 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 19:15:27.400 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.34s
2025-07-24 19:15:29.042 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0165,avg_reflection_pattern_score: 0.0028,avg_pass_at_n: 1.0000,avg_num_tokens: 48.7987,std_num_tokens: 18.3877,avg_correct_num_tokens: 48.7884,std_correct_num_tokens: 18.3725,avg_incorrect_num_tokens: 49.3106,std_incorrect_num_tokens: 19.1257
2025-07-24 19:15:29.442 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.04s
2025-07-24 19:15:31.910 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.46s
2025-07-24 19:15:54.509 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 173
2025-07-24 19:15:54.509 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.60s
2025-07-24 19:15:56.022 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 19:15:56.022 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.7864590806595855e-05, avg_kl: 0.26246352002799855, avg_response_length: 48.826622362081714, avg_orm_score: 0.0, avg_custom_rewards: 3.7864590806595855e-05
2025-07-24 19:15:56.046 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter59_replay_buffer.jsonl
2025-07-24 19:15:57.124 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.08s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.0202, ret=-0.000274, glen=50.3, tlen=211, kl=0.0271, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=0.0202, ret=-0.000274, glen=50.3, tlen=211, kl=0.0271, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=-0.00204, ret=-0.000166, glen=48.1, tlen=209, kl=0.0332, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=-0.00204, ret=-0.000166, glen=48.1, tlen=209, kl=0.0332, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=-0.0162, ret=-6.35e-5, glen=47.3, tlen=208, kl=0.0245, act_lr=1e-6, ent=1.04]   Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.13it/s, pg=-0.0162, ret=-6.35e-5, glen=47.3, tlen=208, kl=0.0245, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.13it/s, pg=-0.0392, ret=0.000164, glen=47.6, tlen=208, kl=0.0219, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:34,  1.14it/s, pg=-0.0392, ret=0.000164, glen=47.6, tlen=208, kl=0.0219, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:34,  1.14it/s, pg=-0.0231, ret=-2.1e-5, glen=47.3, tlen=208, kl=0.108, act_lr=1e-6, ent=1.04]  Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:33,  1.16it/s, pg=-0.0231, ret=-2.1e-5, glen=47.3, tlen=208, kl=0.108, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:33,  1.16it/s, pg=0.00885, ret=-0.000123, glen=50.8, tlen=211, kl=0.262, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:32,  1.16it/s, pg=0.00885, ret=-0.000123, glen=50.8, tlen=211, kl=0.262, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:32,  1.16it/s, pg=-0.0571, ret=0.000124, glen=46.7, tlen=207, kl=0.0252, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:31,  1.17it/s, pg=-0.0571, ret=0.000124, glen=46.7, tlen=207, kl=0.0252, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:31,  1.17it/s, pg=-0.0564, ret=8.26e-5, glen=49.4, tlen=210, kl=0.417, act_lr=1e-6, ent=1.07]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=-0.0564, ret=8.26e-5, glen=49.4, tlen=210, kl=0.417, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=-0.00793, ret=0.000144, glen=50.2, tlen=211, kl=0.0862, act_lr=1e-6, ent=1.08]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=-0.00793, ret=0.000144, glen=50.2, tlen=211, kl=0.0862, act_lr=1e-6, ent=1.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.0272, ret=-5.96e-6, glen=47.7, tlen=208, kl=0.0262, act_lr=1e-6, ent=1.03]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=0.0272, ret=-5.96e-6, glen=47.7, tlen=208, kl=0.0262, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=-0.129, ret=0.000332, glen=45.6, tlen=206, kl=0.0272, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.129, ret=0.000332, glen=45.6, tlen=206, kl=0.0272, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=0.00232, ret=-0.000131, glen=51.7, tlen=212, kl=0.022, act_lr=1e-6, ent=1.1]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.17it/s, pg=0.00232, ret=-0.000131, glen=51.7, tlen=212, kl=0.022, act_lr=1e-6, ent=1.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.17it/s, pg=0.0546, ret=-0.000165, glen=48.2, tlen=209, kl=0.0273, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.0546, ret=-0.000165, glen=48.2, tlen=209, kl=0.0273, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=0.00757, ret=3.2e-5, glen=50.8, tlen=211, kl=0.0237, act_lr=1e-6, ent=1]      Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=0.00757, ret=3.2e-5, glen=50.8, tlen=211, kl=0.0237, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=0.0519, ret=-2.77e-5, glen=48.9, tlen=209, kl=0.0448, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:12<00:24,  1.18it/s, pg=0.0519, ret=-2.77e-5, glen=48.9, tlen=209, kl=0.0448, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.18it/s, pg=0.132, ret=-0.000328, glen=46.8, tlen=207, kl=0.0251, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:23,  1.18it/s, pg=0.132, ret=-0.000328, glen=46.8, tlen=207, kl=0.0251, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.18it/s, pg=-0.049, ret=0.000228, glen=49.7, tlen=210, kl=1.25, act_lr=1e-6, ent=1.03]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:22,  1.17it/s, pg=-0.049, ret=0.000228, glen=49.7, tlen=210, kl=1.25, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:22,  1.17it/s, pg=0.0327, ret=-6.17e-5, glen=46.8, tlen=207, kl=0.0319, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.18it/s, pg=0.0327, ret=-6.17e-5, glen=46.8, tlen=207, kl=0.0319, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.18it/s, pg=-0.000763, ret=6.45e-5, glen=46.8, tlen=208, kl=0.375, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.18it/s, pg=-0.000763, ret=6.45e-5, glen=46.8, tlen=208, kl=0.375, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.18it/s, pg=-0.0735, ret=0.000208, glen=50.9, tlen=211, kl=0.0568, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.18it/s, pg=-0.0735, ret=0.000208, glen=50.9, tlen=211, kl=0.0568, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.18it/s, pg=0.042, ret=5.72e-5, glen=49.9, tlen=210, kl=0.426, act_lr=1e-6, ent=1.05]    Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.18it/s, pg=0.042, ret=5.72e-5, glen=49.9, tlen=210, kl=0.426, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.18it/s, pg=-0.066, ret=0.00023, glen=48.9, tlen=209, kl=1, act_lr=1e-6, ent=0.992]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:18<00:18,  1.18it/s, pg=-0.066, ret=0.00023, glen=48.9, tlen=209, kl=1, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.18it/s, pg=0.0242, ret=0.000109, glen=50.5, tlen=211, kl=0.0311, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.18it/s, pg=0.0242, ret=0.000109, glen=50.5, tlen=211, kl=0.0311, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=0.0147, ret=-0.000152, glen=48.2, tlen=209, kl=0.271, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.15it/s, pg=0.0147, ret=-0.000152, glen=48.2, tlen=209, kl=0.271, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.15it/s, pg=0.283, ret=-0.00072, glen=49.7, tlen=210, kl=0.0325, act_lr=1e-6, ent=1.06] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.16it/s, pg=0.283, ret=-0.00072, glen=49.7, tlen=210, kl=0.0325, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.16it/s, pg=0.0735, ret=-5.65e-5, glen=47.1, tlen=208, kl=0.189, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.16it/s, pg=0.0735, ret=-5.65e-5, glen=47.1, tlen=208, kl=0.189, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.16it/s, pg=0.0563, ret=-0.000123, glen=48.5, tlen=209, kl=1.1, act_lr=1e-6, ent=1.03] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.0563, ret=-0.000123, glen=48.5, tlen=209, kl=1.1, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.0521, ret=2.47e-5, glen=47.4, tlen=208, kl=1.46, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.0521, ret=2.47e-5, glen=47.4, tlen=208, kl=1.46, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.104, ret=0.000339, glen=47.8, tlen=208, kl=0.0377, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=-0.104, ret=0.000339, glen=47.8, tlen=208, kl=0.0377, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=-0.0841, ret=0.00018, glen=51.4, tlen=212, kl=0.0297, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.0841, ret=0.00018, glen=51.4, tlen=212, kl=0.0297, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.0176, ret=-6.3e-5, glen=48.9, tlen=210, kl=1.02, act_lr=1e-6, ent=1.05]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:26<00:11,  1.12it/s, pg=-0.0176, ret=-6.3e-5, glen=48.9, tlen=210, kl=1.02, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.0405, ret=-6.44e-5, glen=50, tlen=211, kl=0.0243, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=0.0405, ret=-6.44e-5, glen=50, tlen=211, kl=0.0243, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.0216, ret=4.05e-6, glen=50.3, tlen=211, kl=0.0308, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.0216, ret=4.05e-6, glen=50.3, tlen=211, kl=0.0308, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.0746, ret=0.000106, glen=49, tlen=210, kl=0.0438, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=-0.0746, ret=0.000106, glen=49, tlen=210, kl=0.0438, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=-0.0838, ret=0.000209, glen=47.5, tlen=208, kl=0.468, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0838, ret=0.000209, glen=47.5, tlen=208, kl=0.468, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.116, ret=0.000279, glen=49.6, tlen=210, kl=0.858, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=-0.116, ret=0.000279, glen=49.6, tlen=210, kl=0.858, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=0.0349, ret=-6.37e-5, glen=47.1, tlen=208, kl=0.021, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=0.0349, ret=-6.37e-5, glen=47.1, tlen=208, kl=0.021, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0871, ret=0.000213, glen=48.3, tlen=209, kl=0.168, act_lr=1e-6, ent=1]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:32<00:05,  1.17it/s, pg=-0.0871, ret=0.000213, glen=48.3, tlen=209, kl=0.168, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.106, ret=0.000293, glen=48.7, tlen=209, kl=0.159, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.106, ret=0.000293, glen=48.7, tlen=209, kl=0.159, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0411, ret=-0.000142, glen=48.6, tlen=210, kl=0.0471, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.18it/s, pg=0.0411, ret=-0.000142, glen=48.6, tlen=210, kl=0.0471, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.18it/s, pg=0.0744, ret=-1.87e-5, glen=51, tlen=212, kl=0.0809, act_lr=1e-6, ent=1.04]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.18it/s, pg=0.0744, ret=-1.87e-5, glen=51, tlen=212, kl=0.0809, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.18it/s, pg=-0.0635, ret=0.000122, glen=49.3, tlen=210, kl=0.383, act_lr=1e-6, ent=1.07]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=-0.0635, ret=0.000122, glen=49.3, tlen=210, kl=0.383, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=0.168, ret=-0.000425, glen=48.9, tlen=210, kl=0.531, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=0.168, ret=-0.000425, glen=48.9, tlen=210, kl=0.531, act_lr=1e-6, ent=1.04]
2025-07-24 19:16:35.332 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.04s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=0.0728, ret=-0.000249, glen=49.8, tlen=211, kl=0.036, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.13it/s, pg=0.0728, ret=-0.000249, glen=49.8, tlen=211, kl=0.036, act_lr=1e-6, ent=1.05]
2025-07-24 19:16:36.142 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 19:16:38.732 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 19:16:39.085 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.91s
2025-07-24 19:16:39.090 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005533044988458806, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.0372295420278201, 'kl': 0.25841903330927546, 'response_length': 48.81986860795455, 'total_length': 209.47914921153676, 'teacher_total_length': 221.44048205288973, 'return': 2.2740275550280455e-06, 'policy_update_steps': 1.0}
Episode [5/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [14:10<08:37, 103.54s/it]2025-07-24 19:16:39.095 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:31,  1.86it/s, est. speed input: 340.47 toks/s, output: 33.49 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/172 [00:01<00:01, 89.73it/s, est. speed input: 9662.20 toks/s, output: 1520.27 toks/s]Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 77/172 [00:01<00:00, 97.66it/s, est. speed input: 10568.17 toks/s, output: 1770.70 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:01<00:00, 120.99it/s, est. speed input: 14933.22 toks/s, output: 3173.16 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:01<00:00, 130.98it/s, est. speed input: 15156.61 toks/s, output: 3246.75 toks/s][32m [repeated 45x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:07<00:00, 22.32it/s, est. speed input: 4046.43 toks/s, output: 1020.81 toks/s] [32m [repeated 8x across cluster][0m
2025-07-24 19:16:47.601 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 194.3348,strategyqa_test/accuracy: 0.5226,eval_accuracy: 0.5226
2025-07-24 19:16:47.911 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:17:19.157 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:17:19.341 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:17:19.341 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.43s
2025-07-24 19:17:21.150 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0168,avg_reflection_pattern_score: 0.0017,avg_pass_at_n: 1.0000,avg_num_tokens: 45.7183,std_num_tokens: 17.5458,avg_correct_num_tokens: 45.7517,std_correct_num_tokens: 17.1747,avg_incorrect_num_tokens: 44.5546,std_incorrect_num_tokens: 27.4691
2025-07-24 19:17:21.581 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.24s
2025-07-24 19:17:24.308 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.72s
2025-07-24 19:17:46.454 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:17:46.455 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.14s
2025-07-24 19:17:47.789 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 19:17:47.789 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00011291304955888978, avg_kl: 1.5899241727941176, avg_response_length: 45.736430134492764, avg_orm_score: 0.0, avg_custom_rewards: 0.00011291304955888978
2025-07-24 19:17:47.821 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter60_replay_buffer.jsonl
2025-07-24 19:17:48.889 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.07s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s, pg=-0.0758, ret=4.94e-5, glen=46.8, tlen=207, kl=0.0465, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:00<00:41,  1.00it/s, pg=-0.0758, ret=4.94e-5, glen=46.8, tlen=207, kl=0.0465, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:41,  1.00it/s, pg=0.0916, ret=-2.98e-5, glen=46.3, tlen=206, kl=0.033, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=0.0916, ret=-2.98e-5, glen=46.3, tlen=206, kl=0.033, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=-0.0107, ret=0.0001, glen=45, tlen=205, kl=5.24, act_lr=1e-6, ent=0.99]    Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.13it/s, pg=-0.0107, ret=0.0001, glen=45, tlen=205, kl=5.24, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.13it/s, pg=-0.12, ret=0.000349, glen=45.7, tlen=206, kl=0.0439, act_lr=1e-6, ent=1]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.12it/s, pg=-0.12, ret=0.000349, glen=45.7, tlen=206, kl=0.0439, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.12it/s, pg=-0.076, ret=0.000244, glen=46.5, tlen=207, kl=4.85, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:34,  1.11it/s, pg=-0.076, ret=0.000244, glen=46.5, tlen=207, kl=4.85, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:34,  1.11it/s, pg=-0.0879, ret=0.00029, glen=46, tlen=206, kl=0.0378, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:33,  1.10it/s, pg=-0.0879, ret=0.00029, glen=46, tlen=206, kl=0.0378, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:33,  1.10it/s, pg=-0.0202, ret=6.68e-5, glen=45.1, tlen=205, kl=2.88, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:32,  1.12it/s, pg=-0.0202, ret=6.68e-5, glen=45.1, tlen=205, kl=2.88, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:32,  1.12it/s, pg=0.00357, ret=-0.000101, glen=45.4, tlen=205, kl=0.035, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.14it/s, pg=0.00357, ret=-0.000101, glen=45.4, tlen=205, kl=0.035, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:08<00:30,  1.14it/s, pg=0.0548, ret=-0.000139, glen=46.5, tlen=206, kl=0.0541, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=0.0548, ret=-0.000139, glen=46.5, tlen=206, kl=0.0541, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=-0.0862, ret=0.000193, glen=44.1, tlen=204, kl=1.66, act_lr=1e-6, ent=0.992] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.16it/s, pg=-0.0862, ret=0.000193, glen=44.1, tlen=204, kl=1.66, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.16it/s, pg=-0.033, ret=3.84e-5, glen=46.4, tlen=206, kl=4.85, act_lr=1e-6, ent=1]      Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.16it/s, pg=-0.033, ret=3.84e-5, glen=46.4, tlen=206, kl=4.85, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.16it/s, pg=0.0828, ret=-0.00014, glen=47.6, tlen=207, kl=0.751, act_lr=1e-6, ent=0.993]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.16it/s, pg=0.0828, ret=-0.00014, glen=47.6, tlen=207, kl=0.751, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.16it/s, pg=-0.0333, ret=6.53e-5, glen=44.9, tlen=205, kl=0.0507, act_lr=1e-6, ent=1]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=-0.0333, ret=6.53e-5, glen=44.9, tlen=205, kl=0.0507, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=0.142, ret=-0.000294, glen=45.9, tlen=206, kl=0.169, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:25,  1.14it/s, pg=0.142, ret=-0.000294, glen=45.9, tlen=206, kl=0.169, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:25,  1.14it/s, pg=-0.0392, ret=-3.61e-5, glen=45.2, tlen=205, kl=0.0276, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.0392, ret=-3.61e-5, glen=45.2, tlen=205, kl=0.0276, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:14<00:24,  1.15it/s, pg=0.139, ret=-0.000543, glen=47.2, tlen=207, kl=0.0433, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=0.139, ret=-0.000543, glen=47.2, tlen=207, kl=0.0433, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=-0.0403, ret=0.000177, glen=46.4, tlen=206, kl=0.0569, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.16it/s, pg=-0.0403, ret=0.000177, glen=46.4, tlen=206, kl=0.0569, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.16it/s, pg=-0.134, ret=0.000364, glen=44.9, tlen=204, kl=0.564, act_lr=1e-6, ent=0.99]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=-0.134, ret=0.000364, glen=44.9, tlen=204, kl=0.564, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=-0.0251, ret=6.33e-5, glen=46.7, tlen=207, kl=0.048, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=-0.0251, ret=6.33e-5, glen=46.7, tlen=207, kl=0.048, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=-0.0317, ret=0.000237, glen=47.5, tlen=207, kl=0.319, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=-0.0317, ret=0.000237, glen=47.5, tlen=207, kl=0.319, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=-0.0308, ret=0.000203, glen=44.2, tlen=204, kl=0.0385, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=-0.0308, ret=0.000203, glen=44.2, tlen=204, kl=0.0385, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.17it/s, pg=-0.0381, ret=0.000163, glen=48.5, tlen=209, kl=0.0405, act_lr=1e-6, ent=1.06]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.0381, ret=0.000163, glen=48.5, tlen=209, kl=0.0405, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:20<00:17,  1.17it/s, pg=-0.0738, ret=0.000249, glen=44.1, tlen=204, kl=0.822, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.15it/s, pg=-0.0738, ret=0.000249, glen=44.1, tlen=204, kl=0.822, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.15it/s, pg=0.127, ret=-0.000373, glen=45, tlen=205, kl=0.043, act_lr=1e-6, ent=1.01]    Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.15it/s, pg=0.127, ret=-0.000373, glen=45, tlen=205, kl=0.043, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.15it/s, pg=0.202, ret=-0.000222, glen=47, tlen=207, kl=0.262, act_lr=1e-6, ent=1.13]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.16it/s, pg=0.202, ret=-0.000222, glen=47, tlen=207, kl=0.262, act_lr=1e-6, ent=1.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.16it/s, pg=0.0742, ret=-0.000309, glen=46.2, tlen=206, kl=0.0728, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.16it/s, pg=0.0742, ret=-0.000309, glen=46.2, tlen=206, kl=0.0728, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.16it/s, pg=0.0907, ret=-0.000413, glen=46.5, tlen=206, kl=0.0602, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.17it/s, pg=0.0907, ret=-0.000413, glen=46.5, tlen=206, kl=0.0602, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.17it/s, pg=-0.0764, ret=0.000207, glen=45.5, tlen=206, kl=2.29, act_lr=1e-6, ent=1.03]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=-0.0764, ret=0.000207, glen=45.5, tlen=206, kl=2.29, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=0.0835, ret=-0.000158, glen=46.3, tlen=206, kl=0.0458, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.06it/s, pg=0.0835, ret=-0.000158, glen=46.3, tlen=206, kl=0.0458, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.06it/s, pg=0.0414, ret=-0.000346, glen=45.9, tlen=206, kl=0.0779, act_lr=1e-6, ent=1.03] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.09it/s, pg=0.0414, ret=-0.000346, glen=45.9, tlen=206, kl=0.0779, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.09it/s, pg=-0.00987, ret=-1.63e-6, glen=43, tlen=203, kl=0.0322, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.00987, ret=-1.63e-6, glen=43, tlen=203, kl=0.0322, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:10,  1.12it/s, pg=-0.0444, ret=2.94e-5, glen=44.5, tlen=204, kl=14.6, act_lr=1e-6, ent=1]      Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.13it/s, pg=-0.0444, ret=2.94e-5, glen=44.5, tlen=204, kl=14.6, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.13it/s, pg=-0.129, ret=0.000704, glen=45.4, tlen=205, kl=0.0285, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=-0.129, ret=0.000704, glen=45.4, tlen=205, kl=0.0285, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=-0.0685, ret=0.000238, glen=45, tlen=205, kl=0.0373, act_lr=1e-6, ent=0.992] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.15it/s, pg=-0.0685, ret=0.000238, glen=45, tlen=205, kl=0.0373, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.15it/s, pg=-0.0743, ret=0.000141, glen=45.2, tlen=205, kl=0.0455, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=-0.0743, ret=0.000141, glen=45.2, tlen=205, kl=0.0455, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=0.04, ret=-0.000189, glen=45.7, tlen=206, kl=1.09, act_lr=1e-6, ent=0.982]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.16it/s, pg=0.04, ret=-0.000189, glen=45.7, tlen=206, kl=1.09, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.16it/s, pg=-0.00644, ret=6.07e-5, glen=45.6, tlen=205, kl=0.0406, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.00644, ret=6.07e-5, glen=45.6, tlen=205, kl=0.0406, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=0.0203, ret=-2.94e-5, glen=46.7, tlen=207, kl=0.0325, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.0203, ret=-2.94e-5, glen=46.7, tlen=207, kl=0.0325, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.135, ret=-0.000349, glen=44.6, tlen=205, kl=7.46, act_lr=1e-6, ent=1.04]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.17it/s, pg=0.135, ret=-0.000349, glen=44.6, tlen=205, kl=7.46, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.121, ret=0.000377, glen=45.5, tlen=206, kl=3.4, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=-0.121, ret=0.000377, glen=45.5, tlen=206, kl=3.4, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=0.0797, ret=-0.000367, glen=45.3, tlen=205, kl=2.44, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=0.0797, ret=-0.000367, glen=45.3, tlen=205, kl=2.44, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=-0.0323, ret=-3.39e-5, glen=45.2, tlen=205, kl=10.3, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.17it/s, pg=-0.0323, ret=-3.39e-5, glen=45.2, tlen=205, kl=10.3, act_lr=1e-6, ent=1.02]
2025-07-24 19:18:26.535 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.47s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.17it/s, pg=-0.0455, ret=-2.78e-5, glen=46.2, tlen=206, kl=2.62, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=-0.0455, ret=-2.78e-5, glen=46.2, tlen=206, kl=2.62, act_lr=1e-6, ent=0.992]
2025-07-24 19:18:27.206 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:18:29.572 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.37s
2025-07-24 19:18:29.922 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 40.98s
2025-07-24 19:18:29.928 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0036462739456531615, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.0065658009329508, 'kl': 1.5719476744186047, 'response_length': 45.74758307878361, 'total_length': 205.60816707167515, 'teacher_total_length': 217.66350892532702, 'return': 1.1805045201177817e-05, 'policy_update_steps': 1.0}
Episode [5/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [16:01<07:03, 105.82s/it]2025-07-24 19:18:29.974 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:18:59.149 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:18:59.328 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:18:59.328 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 29.35s
2025-07-24 19:19:01.082 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0169,avg_reflection_pattern_score: 0.0017,avg_pass_at_n: 1.0000,avg_num_tokens: 46.0577,std_num_tokens: 17.3503,avg_correct_num_tokens: 46.0370,std_correct_num_tokens: 17.2852,avg_incorrect_num_tokens: 46.7397,std_incorrect_num_tokens: 19.3552
2025-07-24 19:19:01.527 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.20s
2025-07-24 19:19:04.329 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.80s
2025-07-24 19:19:26.418 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:19:26.418 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.09s
2025-07-24 19:19:27.694 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 19:19:27.695 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.609738272450426e-05, avg_kl: 5.336269244025735, avg_response_length: 46.089972641888785, avg_orm_score: 0.0, avg_custom_rewards: 1.609738272450426e-05
2025-07-24 19:19:27.724 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter61_replay_buffer.jsonl
2025-07-24 19:19:28.765 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.04s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=0.0293, ret=-0.00025, glen=47.6, tlen=207, kl=0.114, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=0.0293, ret=-0.00025, glen=47.6, tlen=207, kl=0.114, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0576, ret=0.000212, glen=46.2, tlen=207, kl=0.0544, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=-0.0576, ret=0.000212, glen=46.2, tlen=207, kl=0.0544, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=0.0198, ret=-4.82e-5, glen=47.1, tlen=208, kl=0.0797, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:36,  1.09it/s, pg=0.0198, ret=-4.82e-5, glen=47.1, tlen=208, kl=0.0797, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:36,  1.09it/s, pg=-0.0819, ret=0.000187, glen=46.6, tlen=207, kl=0.0957, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.12it/s, pg=-0.0819, ret=0.000187, glen=46.6, tlen=207, kl=0.0957, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.12it/s, pg=-0.0397, ret=3.8e-5, glen=46.8, tlen=207, kl=0.124, act_lr=1e-6, ent=1.02]   Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.14it/s, pg=-0.0397, ret=3.8e-5, glen=46.8, tlen=207, kl=0.124, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.14it/s, pg=0.0869, ret=-0.000168, glen=46.3, tlen=206, kl=1.01, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.15it/s, pg=0.0869, ret=-0.000168, glen=46.3, tlen=206, kl=1.01, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.15it/s, pg=0.034, ret=-0.000174, glen=46.9, tlen=207, kl=40.6, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.14it/s, pg=0.034, ret=-0.000174, glen=46.9, tlen=207, kl=40.6, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.14it/s, pg=0.139, ret=-0.000365, glen=46.4, tlen=207, kl=1.47, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.14it/s, pg=0.139, ret=-0.000365, glen=46.4, tlen=207, kl=1.47, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.14it/s, pg=0.0609, ret=-0.000333, glen=47.7, tlen=208, kl=0.127, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.14it/s, pg=0.0609, ret=-0.000333, glen=47.7, tlen=208, kl=0.127, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.14it/s, pg=0.0659, ret=-0.000331, glen=45.9, tlen=206, kl=22.2, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.15it/s, pg=0.0659, ret=-0.000331, glen=45.9, tlen=206, kl=22.2, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.15it/s, pg=-0.0297, ret=7e-5, glen=46.3, tlen=206, kl=0.347, act_lr=1e-6, ent=1.05]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.16it/s, pg=-0.0297, ret=7e-5, glen=46.3, tlen=206, kl=0.347, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.16it/s, pg=-0.0616, ret=0.000203, glen=46.4, tlen=207, kl=0.168, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.16it/s, pg=-0.0616, ret=0.000203, glen=46.4, tlen=207, kl=0.168, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.16it/s, pg=0.134, ret=-0.000212, glen=47.4, tlen=208, kl=0.13, act_lr=1e-6, ent=0.995] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=0.134, ret=-0.000212, glen=47.4, tlen=208, kl=0.13, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=0.0186, ret=1.02e-5, glen=46.9, tlen=207, kl=24.7, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=0.0186, ret=1.02e-5, glen=46.9, tlen=207, kl=24.7, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:24,  1.17it/s, pg=-0.0135, ret=8.53e-6, glen=43.9, tlen=204, kl=0.0612, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.17it/s, pg=-0.0135, ret=8.53e-6, glen=43.9, tlen=204, kl=0.0612, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.17it/s, pg=-0.0743, ret=0.000283, glen=45.7, tlen=205, kl=0.901, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.17it/s, pg=-0.0743, ret=0.000283, glen=45.7, tlen=205, kl=0.901, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.17it/s, pg=-0.0922, ret=0.000261, glen=45.7, tlen=205, kl=4.28, act_lr=1e-6, ent=1.03] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.17it/s, pg=-0.0922, ret=0.000261, glen=45.7, tlen=205, kl=4.28, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.17it/s, pg=0.0438, ret=0.000129, glen=45.7, tlen=206, kl=20.2, act_lr=1e-6, ent=1.07] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=0.0438, ret=0.000129, glen=45.7, tlen=206, kl=20.2, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=-0.00757, ret=4.26e-5, glen=46.1, tlen=206, kl=0.152, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.15it/s, pg=-0.00757, ret=4.26e-5, glen=46.1, tlen=206, kl=0.152, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.15it/s, pg=-0.0638, ret=0.000102, glen=44.6, tlen=205, kl=0.0574, act_lr=1e-6, ent=1]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:20,  1.13it/s, pg=-0.0638, ret=0.000102, glen=44.6, tlen=205, kl=0.0574, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:20,  1.13it/s, pg=-0.0106, ret=5.59e-5, glen=47.6, tlen=208, kl=11.4, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:19,  1.14it/s, pg=-0.0106, ret=5.59e-5, glen=47.6, tlen=208, kl=11.4, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:19,  1.14it/s, pg=0.000214, ret=-2.03e-5, glen=45.8, tlen=206, kl=13.3, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:18,  1.15it/s, pg=0.000214, ret=-2.03e-5, glen=45.8, tlen=206, kl=13.3, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:20<00:18,  1.15it/s, pg=-0.0128, ret=0.00014, glen=45.4, tlen=205, kl=0.0427, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.16it/s, pg=-0.0128, ret=0.00014, glen=45.4, tlen=205, kl=0.0427, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.16it/s, pg=-0.014, ret=0.000127, glen=45.4, tlen=205, kl=0.462, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.16it/s, pg=-0.014, ret=0.000127, glen=45.4, tlen=205, kl=0.462, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.16it/s, pg=-0.0859, ret=0.000202, glen=43.6, tlen=204, kl=2.52, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.17it/s, pg=-0.0859, ret=0.000202, glen=43.6, tlen=204, kl=2.52, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.17it/s, pg=-0.00299, ret=-0.000152, glen=46.3, tlen=207, kl=0.074, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.17it/s, pg=-0.00299, ret=-0.000152, glen=46.3, tlen=207, kl=0.074, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.17it/s, pg=0.0201, ret=-5.26e-5, glen=47, tlen=207, kl=1.78, act_lr=1e-6, ent=1.06]      Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.17it/s, pg=0.0201, ret=-5.26e-5, glen=47, tlen=207, kl=1.78, act_lr=1e-6, ent=1.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.17it/s, pg=-0.0711, ret=0.000338, glen=43.6, tlen=204, kl=0.137, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=-0.0711, ret=0.000338, glen=43.6, tlen=204, kl=0.137, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=-0.0345, ret=0.000202, glen=46.9, tlen=207, kl=0.085, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.00it/s, pg=-0.0345, ret=0.000202, glen=46.9, tlen=207, kl=0.085, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.00it/s, pg=0.0258, ret=-1.48e-5, glen=46.8, tlen=207, kl=0.123, act_lr=1e-6, ent=1.03] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:12,  1.03it/s, pg=0.0258, ret=-1.48e-5, glen=46.8, tlen=207, kl=0.123, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:12,  1.03it/s, pg=0.00253, ret=8.75e-5, glen=45.6, tlen=206, kl=0.143, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:11,  1.07it/s, pg=0.00253, ret=8.75e-5, glen=45.6, tlen=206, kl=0.143, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:11,  1.07it/s, pg=0.0965, ret=-0.000211, glen=47.2, tlen=207, kl=0.775, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:10,  1.10it/s, pg=0.0965, ret=-0.000211, glen=47.2, tlen=207, kl=0.775, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:29<00:10,  1.10it/s, pg=-0.0592, ret=0.000184, glen=45.7, tlen=206, kl=38.6, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.12it/s, pg=-0.0592, ret=0.000184, glen=45.7, tlen=206, kl=38.6, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.12it/s, pg=0.0281, ret=-5.9e-5, glen=48, tlen=208, kl=5.74, act_lr=1e-6, ent=1.04]     Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.14it/s, pg=0.0281, ret=-5.9e-5, glen=48, tlen=208, kl=5.74, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.14it/s, pg=0.133, ret=-0.000505, glen=43.4, tlen=204, kl=0.979, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.14it/s, pg=0.133, ret=-0.000505, glen=43.4, tlen=204, kl=0.979, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.14it/s, pg=-0.0963, ret=0.00033, glen=44.1, tlen=205, kl=0.0779, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.15it/s, pg=-0.0963, ret=0.00033, glen=44.1, tlen=205, kl=0.0779, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.15it/s, pg=0.0944, ret=-0.000245, glen=43.4, tlen=203, kl=0.0546, act_lr=1e-6, ent=1]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.16it/s, pg=0.0944, ret=-0.000245, glen=43.4, tlen=203, kl=0.0546, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.16it/s, pg=0.0205, ret=-0.000127, glen=46.2, tlen=206, kl=0.48, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.16it/s, pg=0.0205, ret=-0.000127, glen=46.2, tlen=206, kl=0.48, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:34<00:04,  1.16it/s, pg=-0.0391, ret=5.92e-5, glen=46.2, tlen=206, kl=33, act_lr=1e-6, ent=1.01]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.0391, ret=5.92e-5, glen=46.2, tlen=206, kl=33, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:35<00:03,  1.17it/s, pg=0.0353, ret=-0.000113, glen=48, tlen=209, kl=0.0878, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=0.0353, ret=-0.000113, glen=48, tlen=209, kl=0.0878, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0602, ret=0.000227, glen=48.5, tlen=209, kl=0.0557, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.14it/s, pg=-0.0602, ret=0.000227, glen=48.5, tlen=209, kl=0.0557, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.14it/s, pg=-0.00403, ret=-0.000272, glen=45, tlen=205, kl=0.0828, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.15it/s, pg=-0.00403, ret=-0.000272, glen=45, tlen=205, kl=0.0828, act_lr=1e-6, ent=1.01]
2025-07-24 19:20:06.749 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.82s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.15it/s, pg=-0.106, ret=0.000307, glen=46.4, tlen=206, kl=0.101, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.11it/s, pg=-0.106, ret=0.000307, glen=46.4, tlen=206, kl=0.101, act_lr=1e-6, ent=1.02]
2025-07-24 19:20:07.398 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 19:20:09.573 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.18s
2025-07-24 19:20:09.917 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.11s
2025-07-24 19:20:09.923 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007093562636264535, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.0218692629836326, 'kl': 5.276510514492212, 'response_length': 46.10001053921012, 'total_length': 206.23532814203307, 'teacher_total_length': 218.33604431152344, 'return': 3.553960278771038e-06, 'policy_update_steps': 1.0}
Episode [5/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [17:41<05:12, 104.02s/it]2025-07-24 19:20:09.968 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:20:38.705 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:20:38.884 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:20:38.885 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 28.92s
2025-07-24 19:20:40.824 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0167,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 45.6738,std_num_tokens: 16.9120,avg_correct_num_tokens: 45.6590,std_correct_num_tokens: 16.8578,avg_incorrect_num_tokens: 46.2835,std_incorrect_num_tokens: 19.0018
2025-07-24 19:20:41.232 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.35s
2025-07-24 19:20:43.722 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.49s
2025-07-24 19:21:06.060 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:21:06.061 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.34s
2025-07-24 19:21:07.268 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.77s
2025-07-24 19:21:07.268 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -4.444392689038068e-05, avg_kl: 21.551999080882354, avg_response_length: 45.69253984339097, avg_orm_score: 0.0, avg_custom_rewards: -4.444392689038068e-05
2025-07-24 19:21:07.303 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter62_replay_buffer.jsonl
2025-07-24 19:21:08.352 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.05s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=-0.0285, ret=-4.46e-5, glen=47, tlen=207, kl=13.4, act_lr=1e-6, ent=1]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0285, ret=-4.46e-5, glen=47, tlen=207, kl=13.4, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=0.0331, ret=-6.64e-6, glen=44.5, tlen=205, kl=3.67, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.08it/s, pg=0.0331, ret=-6.64e-6, glen=44.5, tlen=205, kl=3.67, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.08it/s, pg=-0.134, ret=0.000383, glen=45.8, tlen=207, kl=49.4, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.12it/s, pg=-0.134, ret=0.000383, glen=45.8, tlen=207, kl=49.4, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.12it/s, pg=0.0913, ret=-0.000234, glen=46.6, tlen=207, kl=17.4, act_lr=1e-6, ent=1]   Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:35,  1.11it/s, pg=0.0913, ret=-0.000234, glen=46.6, tlen=207, kl=17.4, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:35,  1.11it/s, pg=0.134, ret=-0.000197, glen=45.7, tlen=206, kl=2.19, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.13it/s, pg=0.134, ret=-0.000197, glen=45.7, tlen=206, kl=2.19, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.13it/s, pg=-0.0397, ret=7.46e-5, glen=46.4, tlen=207, kl=5.56, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.15it/s, pg=-0.0397, ret=7.46e-5, glen=46.4, tlen=207, kl=5.56, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.15it/s, pg=0.0788, ret=-0.000178, glen=46.4, tlen=207, kl=1.18, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.16it/s, pg=0.0788, ret=-0.000178, glen=46.4, tlen=207, kl=1.18, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.16it/s, pg=0.218, ret=-0.000291, glen=46.6, tlen=207, kl=32.4, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=0.218, ret=-0.000291, glen=46.6, tlen=207, kl=32.4, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.08, ret=0.000162, glen=44.1, tlen=204, kl=4.96, act_lr=1e-6, ent=0.943] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.17it/s, pg=-0.08, ret=0.000162, glen=44.1, tlen=204, kl=4.96, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.17it/s, pg=-0.0901, ret=0.000309, glen=46, tlen=207, kl=3.5, act_lr=1e-6, ent=0.972] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.17it/s, pg=-0.0901, ret=0.000309, glen=46, tlen=207, kl=3.5, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.17it/s, pg=0.0851, ret=-0.000288, glen=45.5, tlen=205, kl=3.34, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.17it/s, pg=0.0851, ret=-0.000288, glen=45.5, tlen=205, kl=3.34, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.17it/s, pg=-0.0259, ret=6.8e-5, glen=45.9, tlen=206, kl=4.87, act_lr=1e-6, ent=0.987]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.17it/s, pg=-0.0259, ret=6.8e-5, glen=45.9, tlen=206, kl=4.87, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.17it/s, pg=-0.064, ret=0.000222, glen=45.8, tlen=206, kl=7.34, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=-0.064, ret=0.000222, glen=45.8, tlen=206, kl=7.34, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=-0.0225, ret=4.81e-5, glen=46.3, tlen=207, kl=2.08, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.18it/s, pg=-0.0225, ret=4.81e-5, glen=46.3, tlen=207, kl=2.08, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.18it/s, pg=0.00159, ret=-1.8e-5, glen=44.5, tlen=204, kl=3.54, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:12<00:23,  1.18it/s, pg=0.00159, ret=-1.8e-5, glen=44.5, tlen=204, kl=3.54, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.18it/s, pg=-0.0837, ret=0.000228, glen=44.2, tlen=205, kl=38.1, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:22,  1.18it/s, pg=-0.0837, ret=0.000228, glen=44.2, tlen=205, kl=38.1, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:22,  1.18it/s, pg=-0.058, ret=0.00015, glen=46.8, tlen=207, kl=2.57, act_lr=1e-6, ent=0.976]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.18it/s, pg=-0.058, ret=0.00015, glen=46.8, tlen=207, kl=2.57, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.18it/s, pg=0.095, ret=-0.000349, glen=44.5, tlen=205, kl=3.69, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.16it/s, pg=0.095, ret=-0.000349, glen=44.5, tlen=205, kl=3.69, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.16it/s, pg=0.0391, ret=-8.13e-5, glen=46.9, tlen=207, kl=10.7, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.16it/s, pg=0.0391, ret=-8.13e-5, glen=46.9, tlen=207, kl=10.7, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.16it/s, pg=-0.0253, ret=4.1e-5, glen=45.1, tlen=205, kl=12.3, act_lr=1e-6, ent=0.969] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=-0.0253, ret=4.1e-5, glen=45.1, tlen=205, kl=12.3, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=0.0175, ret=-8.4e-7, glen=46.3, tlen=207, kl=35, act_lr=1e-6, ent=0.963]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=0.0175, ret=-8.4e-7, glen=46.3, tlen=207, kl=35, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=-0.0557, ret=0.000153, glen=46.1, tlen=206, kl=32.8, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:18<00:17,  1.17it/s, pg=-0.0557, ret=0.000153, glen=46.1, tlen=206, kl=32.8, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=0.0479, ret=-0.000155, glen=46.6, tlen=207, kl=22.1, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.17it/s, pg=0.0479, ret=-0.000155, glen=46.6, tlen=207, kl=22.1, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=0.124, ret=-0.000164, glen=46, tlen=206, kl=4.5, act_lr=1e-6, ent=1.01]     Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.17it/s, pg=0.124, ret=-0.000164, glen=46, tlen=206, kl=4.5, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.17it/s, pg=0.0253, ret=-5.68e-5, glen=46.2, tlen=206, kl=78.3, act_lr=1e-6, ent=1]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.18it/s, pg=0.0253, ret=-5.68e-5, glen=46.2, tlen=206, kl=78.3, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.18it/s, pg=0.00232, ret=-8.71e-5, glen=44.8, tlen=204, kl=52.5, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.17it/s, pg=0.00232, ret=-8.71e-5, glen=44.8, tlen=204, kl=52.5, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.17it/s, pg=-0.0582, ret=0.000227, glen=45.1, tlen=206, kl=7.29, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.18it/s, pg=-0.0582, ret=0.000227, glen=45.1, tlen=206, kl=7.29, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.18it/s, pg=0.00342, ret=4.28e-5, glen=45.6, tlen=206, kl=100, act_lr=1e-6, ent=0.962]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=0.00342, ret=4.28e-5, glen=45.6, tlen=206, kl=100, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=-0.0345, ret=-0.000185, glen=45, tlen=205, kl=9.06, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.01it/s, pg=-0.0345, ret=-0.000185, glen=45, tlen=205, kl=9.06, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.01it/s, pg=0.0822, ret=-0.000236, glen=45, tlen=205, kl=153, act_lr=1e-6, ent=0.94]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:12,  1.06it/s, pg=0.0822, ret=-0.000236, glen=45, tlen=205, kl=153, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:12,  1.06it/s, pg=-0.0735, ret=0.000213, glen=46.5, tlen=207, kl=2.86, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:11,  1.09it/s, pg=-0.0735, ret=0.000213, glen=46.5, tlen=207, kl=2.86, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:11,  1.09it/s, pg=-0.154, ret=0.000488, glen=42.9, tlen=203, kl=3.27, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.11it/s, pg=-0.154, ret=0.000488, glen=42.9, tlen=203, kl=3.27, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.11it/s, pg=-0.0171, ret=1.63e-5, glen=44.3, tlen=204, kl=10.2, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.13it/s, pg=-0.0171, ret=1.63e-5, glen=44.3, tlen=204, kl=10.2, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.13it/s, pg=0.0255, ret=-0.000206, glen=45.2, tlen=205, kl=21.4, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.15it/s, pg=0.0255, ret=-0.000206, glen=45.2, tlen=205, kl=21.4, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.15it/s, pg=0.0481, ret=-0.000244, glen=46.5, tlen=207, kl=2.92, act_lr=1e-6, ent=0.99] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=0.0481, ret=-0.000244, glen=46.5, tlen=207, kl=2.92, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=-0.0363, ret=7.26e-5, glen=44.4, tlen=205, kl=6.53, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.16it/s, pg=-0.0363, ret=7.26e-5, glen=44.4, tlen=205, kl=6.53, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.16it/s, pg=-0.0907, ret=0.000246, glen=46.1, tlen=206, kl=75.5, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.0907, ret=0.000246, glen=46.1, tlen=206, kl=75.5, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=0.168, ret=-0.000497, glen=46.5, tlen=207, kl=4.79, act_lr=1e-6, ent=0.999] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.168, ret=-0.000497, glen=46.5, tlen=207, kl=4.79, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=-0.00891, ret=5.1e-5, glen=47.8, tlen=208, kl=39.8, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.17it/s, pg=-0.00891, ret=5.1e-5, glen=47.8, tlen=208, kl=39.8, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.118, ret=0.000343, glen=45.3, tlen=205, kl=21.3, act_lr=1e-6, ent=1.04] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=-0.118, ret=0.000343, glen=45.3, tlen=205, kl=21.3, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0413, ret=-1.23e-5, glen=44.5, tlen=205, kl=1.7, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=-0.0413, ret=-1.23e-5, glen=44.5, tlen=205, kl=1.7, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=0.0269, ret=-0.000124, glen=47.7, tlen=208, kl=5.95, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.18it/s, pg=0.0269, ret=-0.000124, glen=47.7, tlen=208, kl=5.95, act_lr=1e-6, ent=0.968]
2025-07-24 19:21:45.916 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.40s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.18it/s, pg=-0.019, ret=4.96e-5, glen=45.6, tlen=206, kl=3.12, act_lr=1e-6, ent=0.986]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=-0.019, ret=4.96e-5, glen=45.6, tlen=206, kl=3.12, act_lr=1e-6, ent=0.986]
2025-07-24 19:21:46.765 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 19:21:49.283 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 19:21:49.626 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.23s
2025-07-24 19:21:49.631 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00029555032419603926, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9789920684903167, 'kl': 21.32122093023256, 'response_length': 45.69054368484852, 'total_length': 205.9569478589435, 'teacher_total_length': 217.9683813050736, 'return': -1.569330548633399e-06, 'policy_update_steps': 1.0}
Episode [5/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [19:21<03:25, 102.70s/it]2025-07-24 19:21:49.674 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:22:19.428 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:22:19.614 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:22:19.615 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 29.94s
2025-07-24 19:22:21.258 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0166,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 45.0917,std_num_tokens: 15.7078,avg_correct_num_tokens: 45.0880,std_correct_num_tokens: 15.6221,avg_incorrect_num_tokens: 45.2785,std_incorrect_num_tokens: 19.5733
2025-07-24 19:22:21.559 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.94s
2025-07-24 19:22:24.176 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.61s
2025-07-24 19:22:46.160 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:22:46.161 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 21.98s
2025-07-24 19:22:47.353 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.77s
2025-07-24 19:22:47.353 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.6303479103097584e-05, avg_kl: 102.32352941176471, avg_response_length: 45.12496533113367, avg_orm_score: 0.0, avg_custom_rewards: 1.6303479103097584e-05
2025-07-24 19:22:47.382 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter63_replay_buffer.jsonl
2025-07-24 19:22:48.442 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=-0.0985, ret=0.000241, glen=46.1, tlen=206, kl=504, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0985, ret=0.000241, glen=46.1, tlen=206, kl=504, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=0.0243, ret=-0.000107, glen=46.3, tlen=207, kl=27.7, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.08it/s, pg=0.0243, ret=-0.000107, glen=46.3, tlen=207, kl=27.7, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.08it/s, pg=0.0185, ret=-0.000108, glen=44.8, tlen=206, kl=38.3, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:36,  1.10it/s, pg=0.0185, ret=-0.000108, glen=44.8, tlen=206, kl=38.3, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:36,  1.10it/s, pg=0.024, ret=-0.000122, glen=46.6, tlen=207, kl=141, act_lr=1e-6, ent=0.995]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.13it/s, pg=0.024, ret=-0.000122, glen=46.6, tlen=207, kl=141, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.13it/s, pg=-0.0599, ret=0.000123, glen=47.4, tlen=208, kl=503, act_lr=1e-6, ent=1.05]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.13it/s, pg=-0.0599, ret=0.000123, glen=47.4, tlen=208, kl=503, act_lr=1e-6, ent=1.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.13it/s, pg=0.117, ret=-0.000235, glen=47, tlen=208, kl=77.5, act_lr=1e-6, ent=0.955] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.14it/s, pg=0.117, ret=-0.000235, glen=47, tlen=208, kl=77.5, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.14it/s, pg=-0.0574, ret=0.000114, glen=46.5, tlen=207, kl=34.7, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.15it/s, pg=-0.0574, ret=0.000114, glen=46.5, tlen=207, kl=34.7, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.15it/s, pg=0.00528, ret=9.76e-6, glen=43.7, tlen=204, kl=33.4, act_lr=1e-6, ent=0.948] Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=0.00528, ret=9.76e-6, glen=43.7, tlen=204, kl=33.4, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.0447, ret=4.22e-5, glen=45.6, tlen=206, kl=68.6, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.17it/s, pg=-0.0447, ret=4.22e-5, glen=45.6, tlen=206, kl=68.6, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.17it/s, pg=-0.00772, ret=-6.17e-5, glen=46.3, tlen=207, kl=408, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.17it/s, pg=-0.00772, ret=-6.17e-5, glen=46.3, tlen=207, kl=408, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.17it/s, pg=-0.000854, ret=-3.26e-5, glen=45.3, tlen=206, kl=154, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.17it/s, pg=-0.000854, ret=-3.26e-5, glen=45.3, tlen=206, kl=154, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.17it/s, pg=0.0777, ret=-0.000246, glen=45.7, tlen=207, kl=64.9, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.17it/s, pg=0.0777, ret=-0.000246, glen=45.7, tlen=207, kl=64.9, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.17it/s, pg=0.00906, ret=-4.65e-5, glen=44.7, tlen=205, kl=85.8, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=0.00906, ret=-4.65e-5, glen=44.7, tlen=205, kl=85.8, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=0.143, ret=-0.000296, glen=45.1, tlen=205, kl=265, act_lr=1e-6, ent=0.98]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:25,  1.15it/s, pg=0.143, ret=-0.000296, glen=45.1, tlen=205, kl=265, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:25,  1.15it/s, pg=-0.0245, ret=0.000146, glen=43.7, tlen=204, kl=126, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.0245, ret=0.000146, glen=43.7, tlen=204, kl=126, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.0636, ret=0.000263, glen=45, tlen=206, kl=30.1, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.16it/s, pg=-0.0636, ret=0.000263, glen=45, tlen=206, kl=30.1, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=0.0314, ret=-3.84e-5, glen=44.3, tlen=205, kl=126, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.15it/s, pg=0.0314, ret=-3.84e-5, glen=44.3, tlen=205, kl=126, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.15it/s, pg=0.00324, ret=1.64e-5, glen=45.4, tlen=206, kl=67.7, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:22,  1.12it/s, pg=0.00324, ret=1.64e-5, glen=45.4, tlen=206, kl=67.7, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:22,  1.12it/s, pg=0.0314, ret=-0.000134, glen=45.6, tlen=206, kl=97.7, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:21,  1.14it/s, pg=0.0314, ret=-0.000134, glen=45.6, tlen=206, kl=97.7, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:21,  1.14it/s, pg=0.0294, ret=-8.08e-5, glen=45.1, tlen=206, kl=34, act_lr=1e-6, ent=0.94]    Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:20,  1.13it/s, pg=0.0294, ret=-8.08e-5, glen=45.1, tlen=206, kl=34, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:20,  1.13it/s, pg=0.105, ret=-0.000119, glen=45.5, tlen=206, kl=52.9, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:19,  1.14it/s, pg=0.105, ret=-0.000119, glen=45.5, tlen=206, kl=52.9, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:19,  1.14it/s, pg=0.0526, ret=-0.000124, glen=44.7, tlen=205, kl=41.8, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:18,  1.15it/s, pg=0.0526, ret=-0.000124, glen=44.7, tlen=205, kl=41.8, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:20<00:18,  1.15it/s, pg=0.0414, ret=-0.000232, glen=44.1, tlen=205, kl=34.9, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.16it/s, pg=0.0414, ret=-0.000232, glen=44.1, tlen=205, kl=34.9, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.16it/s, pg=-0.0757, ret=0.000175, glen=44.3, tlen=205, kl=27.3, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.17it/s, pg=-0.0757, ret=0.000175, glen=44.3, tlen=205, kl=27.3, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.17it/s, pg=-0.0821, ret=0.000188, glen=43.7, tlen=204, kl=126, act_lr=1e-6, ent=0.981] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.17it/s, pg=-0.0821, ret=0.000188, glen=43.7, tlen=204, kl=126, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.17it/s, pg=0.066, ret=-0.000115, glen=45.9, tlen=207, kl=42.5, act_lr=1e-6, ent=0.98] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.17it/s, pg=0.066, ret=-0.000115, glen=45.9, tlen=207, kl=42.5, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.17it/s, pg=-0.0172, ret=9.98e-5, glen=45.1, tlen=206, kl=19.2, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.18it/s, pg=-0.0172, ret=9.98e-5, glen=45.1, tlen=206, kl=19.2, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.18it/s, pg=-0.0468, ret=0.000165, glen=44.3, tlen=205, kl=101, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.18it/s, pg=-0.0468, ret=0.000165, glen=44.3, tlen=205, kl=101, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.18it/s, pg=0.0268, ret=-0.000107, glen=46.4, tlen=207, kl=59.8, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:14,  1.00s/it, pg=0.0268, ret=-0.000107, glen=46.4, tlen=207, kl=59.8, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:14,  1.00s/it, pg=-0.0208, ret=3.19e-5, glen=43.8, tlen=204, kl=18.5, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:12,  1.05it/s, pg=-0.0208, ret=3.19e-5, glen=43.8, tlen=204, kl=18.5, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:12,  1.05it/s, pg=-0.0253, ret=6.34e-5, glen=45, tlen=206, kl=158, act_lr=1e-6, ent=0.978]   Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:11,  1.08it/s, pg=-0.0253, ret=6.34e-5, glen=45, tlen=206, kl=158, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:11,  1.08it/s, pg=0.00311, ret=-0.000115, glen=44.2, tlen=205, kl=71.8, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.11it/s, pg=0.00311, ret=-0.000115, glen=44.2, tlen=205, kl=71.8, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:29<00:09,  1.11it/s, pg=-0.0854, ret=0.000154, glen=43.8, tlen=205, kl=42.1, act_lr=1e-6, ent=1.01]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.13it/s, pg=-0.0854, ret=0.000154, glen=43.8, tlen=205, kl=42.1, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.13it/s, pg=-0.103, ret=0.00027, glen=45.2, tlen=206, kl=14.7, act_lr=1e-6, ent=0.958] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.14it/s, pg=-0.103, ret=0.00027, glen=45.2, tlen=206, kl=14.7, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.14it/s, pg=-0.0122, ret=9.07e-5, glen=45.1, tlen=206, kl=188, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.15it/s, pg=-0.0122, ret=9.07e-5, glen=45.1, tlen=206, kl=188, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.15it/s, pg=-0.00513, ret=0.000114, glen=44.6, tlen=205, kl=53.4, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.16it/s, pg=-0.00513, ret=0.000114, glen=44.6, tlen=205, kl=53.4, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.16it/s, pg=0.0851, ret=-0.000111, glen=46.9, tlen=208, kl=119, act_lr=1e-6, ent=0.999]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=0.0851, ret=-0.000111, glen=46.9, tlen=208, kl=119, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=-0.106, ret=0.000311, glen=45.6, tlen=207, kl=40.2, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=-0.106, ret=0.000311, glen=45.6, tlen=207, kl=40.2, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:34<00:04,  1.17it/s, pg=0.022, ret=-0.00012, glen=44.8, tlen=206, kl=61.6, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=0.022, ret=-0.00012, glen=44.8, tlen=206, kl=61.6, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.0707, ret=8.6e-5, glen=43.9, tlen=205, kl=27.6, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=-0.0707, ret=8.6e-5, glen=43.9, tlen=205, kl=27.6, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=0.012, ret=6.09e-5, glen=44.2, tlen=205, kl=47.7, act_lr=1e-6, ent=0.991] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.18it/s, pg=0.012, ret=6.09e-5, glen=44.2, tlen=205, kl=47.7, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.18it/s, pg=-0.0363, ret=6.26e-5, glen=43.6, tlen=204, kl=64.8, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.18it/s, pg=-0.0363, ret=6.26e-5, glen=43.6, tlen=204, kl=64.8, act_lr=1e-6, ent=0.986]
2025-07-24 19:23:26.274 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.63s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.18it/s, pg=0.0169, ret=-5.43e-5, glen=45.5, tlen=206, kl=58.3, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=0.0169, ret=-5.43e-5, glen=45.5, tlen=206, kl=58.3, act_lr=1e-6, ent=0.992]
2025-07-24 19:23:27.115 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 19:23:29.715 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 19:23:30.048 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.52s
2025-07-24 19:23:30.054 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.002298532530318859, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9771226090054179, 'kl': 101.37572763132495, 'response_length': 45.11982185896053, 'total_length': 205.8122970226199, 'teacher_total_length': 217.83640519962754, 'return': 5.199267074796046e-06, 'policy_update_steps': 1.0}
Episode [5/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [21:01<01:42, 102.01s/it]2025-07-24 19:23:30.060 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:35,  1.79it/s, est. speed input: 321.02 toks/s, output: 34.07 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   3%|‚ñé         | 6/172 [00:00<00:15, 10.93it/s, est. speed input: 1569.61 toks/s, output: 180.04 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  10%|‚ñâ         | 17/172 [00:00<00:05, 29.87it/s, est. speed input: 3714.87 toks/s, output: 486.91 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:01<00:00, 116.85it/s, est. speed input: 15097.26 toks/s, output: 3233.11 toks/s]
2025-07-24 19:23:34.743 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 190.0975,strategyqa_test/accuracy: 0.5138,eval_accuracy: 0.5138
2025-07-24 19:23:35.000 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:23:55.640 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:23:55.813 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 19:23:55.813 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 20.81s
2025-07-24 19:23:56.853 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0167,avg_reflection_pattern_score: 0.0019,avg_pass_at_n: 1.0000,avg_num_tokens: 46.8053,std_num_tokens: 19.4363,avg_correct_num_tokens: 46.7525,std_correct_num_tokens: 19.3505,avg_incorrect_num_tokens: 50.0290,std_incorrect_num_tokens: 23.8886
2025-07-24 19:23:57.009 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.20s
2025-07-24 19:23:58.456 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.45s
2025-07-24 19:24:10.623 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 90
2025-07-24 19:24:10.623 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.17s
2025-07-24 19:24:11.634 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.56s
2025-07-24 19:24:11.634 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.9275824716832075e-05, avg_kl: 395.8875, avg_response_length: 46.870580969916446, avg_orm_score: 0.0, avg_custom_rewards: 2.9275824716832075e-05
2025-07-24 19:24:11.656 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter64_replay_buffer.jsonl
2025-07-24 19:24:12.203 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.55s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   0%|          | 0/171 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:01<00:00, 131.04it/s, est. speed input: 15757.83 toks/s, output: 3303.27 toks/s][32m [repeated 43x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 47.21it/s, est. speed input: 8568.45 toks/s, output: 2053.59 toks/s] [32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s, pg=0.016, ret=-4.13e-5, glen=49.5, tlen=211, kl=221, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:00<00:21,  1.02it/s, pg=0.016, ret=-4.13e-5, glen=49.5, tlen=211, kl=221, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:01<00:21,  1.02it/s, pg=0.169, ret=-0.000341, glen=47.4, tlen=209, kl=220, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:01<00:19,  1.10it/s, pg=0.169, ret=-0.000341, glen=47.4, tlen=209, kl=220, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:02<00:19,  1.10it/s, pg=0.0802, ret=-6.82e-5, glen=48.6, tlen=210, kl=1245.5, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:02<00:18,  1.10it/s, pg=0.0802, ret=-6.82e-5, glen=48.6, tlen=210, kl=1245.5, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:03<00:18,  1.10it/s, pg=-0.0771, ret=0.000171, glen=45.5, tlen=207, kl=192, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:03<00:16,  1.13it/s, pg=-0.0771, ret=0.000171, glen=45.5, tlen=207, kl=192, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:04<00:16,  1.13it/s, pg=0.0175, ret=-8.65e-5, glen=47.4, tlen=209, kl=384, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:04<00:15,  1.14it/s, pg=0.0175, ret=-8.65e-5, glen=47.4, tlen=209, kl=384, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:05<00:15,  1.14it/s, pg=0.014, ret=4.9e-6, glen=47.8, tlen=209, kl=370, act_lr=1e-6, ent=0.94]    Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:05<00:14,  1.14it/s, pg=0.014, ret=4.9e-6, glen=47.8, tlen=209, kl=370, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:06<00:14,  1.14it/s, pg=0.0347, ret=-0.000121, glen=46.8, tlen=208, kl=528, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:06<00:13,  1.15it/s, pg=0.0347, ret=-0.000121, glen=46.8, tlen=208, kl=528, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:07<00:13,  1.15it/s, pg=0.125, ret=-0.000194, glen=46.8, tlen=208, kl=194, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:12,  1.16it/s, pg=0.125, ret=-0.000194, glen=46.8, tlen=208, kl=194, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:12,  1.16it/s, pg=-0.0162, ret=2.35e-5, glen=45.7, tlen=207, kl=174, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:07<00:12,  1.16it/s, pg=-0.0162, ret=2.35e-5, glen=45.7, tlen=207, kl=174, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:08<00:12,  1.16it/s, pg=0.0307, ret=-0.000196, glen=45, tlen=206, kl=552, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:08<00:11,  1.17it/s, pg=0.0307, ret=-0.000196, glen=45, tlen=206, kl=552, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:09<00:11,  1.17it/s, pg=-0.0883, ret=0.000218, glen=48.3, tlen=209, kl=428, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:09<00:10,  1.17it/s, pg=-0.0883, ret=0.000218, glen=48.3, tlen=209, kl=428, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:10<00:10,  1.17it/s, pg=-0.0275, ret=6.33e-5, glen=46.1, tlen=207, kl=938, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:10<00:09,  1.17it/s, pg=-0.0275, ret=6.33e-5, glen=46.1, tlen=207, kl=938, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:11<00:09,  1.17it/s, pg=-0.00415, ret=-4.19e-7, glen=46.6, tlen=208, kl=485, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:11<00:08,  1.17it/s, pg=-0.00415, ret=-4.19e-7, glen=46.6, tlen=208, kl=485, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:12<00:08,  1.17it/s, pg=-0.0514, ret=7.55e-5, glen=47.2, tlen=208, kl=267, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:12<00:07,  1.16it/s, pg=-0.0514, ret=7.55e-5, glen=47.2, tlen=208, kl=267, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:13<00:07,  1.16it/s, pg=-0.00764, ret=-3.22e-6, glen=45, tlen=206, kl=391, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=-0.00764, ret=-3.22e-6, glen=45, tlen=206, kl=391, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=-0.0195, ret=0.000112, glen=47.4, tlen=209, kl=156, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:13<00:06,  1.13it/s, pg=-0.0195, ret=0.000112, glen=47.4, tlen=209, kl=156, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:14<00:06,  1.13it/s, pg=-0.0537, ret=8.92e-5, glen=46.6, tlen=208, kl=113, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:14<00:05,  1.14it/s, pg=-0.0537, ret=8.92e-5, glen=46.6, tlen=208, kl=113, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:15<00:05,  1.14it/s, pg=-0.00482, ret=-9.44e-5, glen=44.8, tlen=206, kl=368, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:15<00:04,  1.15it/s, pg=-0.00482, ret=-9.44e-5, glen=44.8, tlen=206, kl=368, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:16<00:04,  1.15it/s, pg=-0.0808, ret=0.00021, glen=48, tlen=209, kl=247, act_lr=1e-6, ent=0.944]   Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:16<00:03,  1.16it/s, pg=-0.0808, ret=0.00021, glen=48, tlen=209, kl=247, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:17<00:03,  1.16it/s, pg=-0.0614, ret=0.000111, glen=45.5, tlen=207, kl=162, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:17<00:02,  1.17it/s, pg=-0.0614, ret=0.000111, glen=45.5, tlen=207, kl=162, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:18<00:02,  1.17it/s, pg=0.106, ret=-0.000172, glen=47, tlen=208, kl=128, act_lr=1e-6, ent=0.923]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:18<00:01,  1.17it/s, pg=0.106, ret=-0.000172, glen=47, tlen=208, kl=128, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:19<00:01,  1.17it/s, pg=0.0478, ret=3.42e-5, glen=46.7, tlen=207, kl=719, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:19<00:00,  1.17it/s, pg=0.0478, ret=3.42e-5, glen=46.7, tlen=207, kl=719, act_lr=1e-6, ent=0.926]
2025-07-24 19:24:32.377 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.02s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:19<00:00,  1.17it/s, pg=-0.157, ret=0.000372, glen=47.7, tlen=209, kl=479, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:19<00:00,  1.10it/s, pg=-0.157, ret=0.000372, glen=47.7, tlen=209, kl=479, act_lr=1e-6, ent=0.95]
2025-07-24 19:24:33.084 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.65s
2025-07-24 19:24:35.591 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-24 19:24:35.936 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 23.70s
2025-07-24 19:24:35.939 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00037284519361413044, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9424190158429353, 'kl': 389.72690350076425, 'response_length': 46.83603004787279, 'total_length': 208.07047702955163, 'teacher_total_length': 219.9940716287364, 'return': 7.2574741510755345e-06, 'policy_update_steps': 1.0}
Episode [5/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [22:07<00:00, 91.07s/it] 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,351] [INFO] [utils.py:782:see_memory_usage] MA 3.68 GB         Max_MA 3.68 GB         CA 4.7 GB         Max_CA 5 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,351] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.56 GB, percent = 21.8%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7c3591400>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,352] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:02:28,353] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
2025-07-24 19:24:44.714 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:04:17,036] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 19:02:27,816] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:06:07,849] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:07:55,358] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:09:47,093] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:11:30,574] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:13:13,686] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:16:35,325] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:18:26,527] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:20:06,742] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:23:26,267] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:24:32,370] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:42,520] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:42,703] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 2034, num_elems = 10.66B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,198] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,198] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,205] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,206] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,489] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,490] [INFO] [utils.py:782:see_memory_usage] MA 4.41 GB         Max_MA 9.37 GB         CA 5.44 GB         Max_CA 44 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,490] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.37 GB, percent = 21.9%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [6/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [5/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [22:16<00:00, 102.79s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,708] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,709] [INFO] [utils.py:782:see_memory_usage] MA 4.41 GB         Max_MA 4.41 GB         CA 5.44 GB         Max_CA 5 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,709] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.37 GB, percent = 21.9%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7c0076540>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   global_rank .................. 0huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 19:24:45.030 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:25:13.518 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:25:13.698 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:25:13.698 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 28.67s
2025-07-24 19:25:15.385 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0164,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 45.2704,std_num_tokens: 15.5269,avg_correct_num_tokens: 45.2468,std_correct_num_tokens: 15.4851,avg_incorrect_num_tokens: 47.1800,std_incorrect_num_tokens: 18.4978
2025-07-24 19:25:15.795 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.10s
2025-07-24 19:25:18.187 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.39s
2025-07-24 19:25:40.840 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:25:40.841 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.65s
2025-07-24 19:25:42.217 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 19:25:42.217 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.9188338525812415e-05, avg_kl: 0.0, avg_response_length: 45.286771549898035, avg_orm_score: 0.0, avg_custom_rewards: 3.9188338525812415e-05
2025-07-24 19:25:42.243 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter65_replay_buffer.jsonl
2025-07-24 19:25:43.306 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.07s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=0.0839, ret=-0.000177, glen=45.3, tlen=206, kl=0, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=0.0839, ret=-0.000177, glen=45.3, tlen=206, kl=0, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0624, ret=0.000144, glen=45.1, tlen=206, kl=0, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.08it/s, pg=-0.0624, ret=0.000144, glen=45.1, tlen=206, kl=0, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.08it/s, pg=0.0164, ret=-9.73e-5, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.12it/s, pg=0.0164, ret=-9.73e-5, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.12it/s, pg=0.188, ret=-0.000298, glen=44.1, tlen=205, kl=0, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:35,  1.09it/s, pg=0.188, ret=-0.000298, glen=44.1, tlen=205, kl=0, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:35,  1.09it/s, pg=0.0346, ret=-0.000132, glen=44, tlen=204, kl=0, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:34,  1.10it/s, pg=0.0346, ret=-0.000132, glen=44, tlen=204, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:34,  1.10it/s, pg=0.12, ret=-0.000259, glen=42.9, tlen=203, kl=0, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:33,  1.10it/s, pg=0.12, ret=-0.000259, glen=42.9, tlen=203, kl=0, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:33,  1.10it/s, pg=0.0473, ret=-7.64e-6, glen=46.6, tlen=207, kl=0, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:32,  1.12it/s, pg=0.0473, ret=-7.64e-6, glen=46.6, tlen=207, kl=0, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:32,  1.12it/s, pg=-0.0991, ret=0.000213, glen=45.5, tlen=206, kl=0, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.14it/s, pg=-0.0991, ret=0.000213, glen=45.5, tlen=206, kl=0, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:08<00:30,  1.14it/s, pg=0.00653, ret=-6.88e-5, glen=43.5, tlen=204, kl=0, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=0.00653, ret=-6.88e-5, glen=43.5, tlen=204, kl=0, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=-0.114, ret=0.000253, glen=45.5, tlen=206, kl=0, act_lr=1e-6, ent=0.903] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.16it/s, pg=-0.114, ret=0.000253, glen=45.5, tlen=206, kl=0, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.16it/s, pg=-0.0468, ret=2.81e-6, glen=44.1, tlen=204, kl=0, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.16it/s, pg=-0.0468, ret=2.81e-6, glen=44.1, tlen=204, kl=0, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.16it/s, pg=-0.0812, ret=0.000177, glen=44.2, tlen=204, kl=0, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:27,  1.14it/s, pg=-0.0812, ret=0.000177, glen=44.2, tlen=204, kl=0, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:27,  1.14it/s, pg=-0.123, ret=0.000267, glen=44.6, tlen=205, kl=0, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:26,  1.15it/s, pg=-0.123, ret=0.000267, glen=44.6, tlen=205, kl=0, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:26,  1.15it/s, pg=0.107, ret=-0.000199, glen=45.8, tlen=206, kl=0, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:25,  1.16it/s, pg=0.107, ret=-0.000199, glen=45.8, tlen=206, kl=0, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:25,  1.16it/s, pg=-0.0309, ret=0.000102, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.16it/s, pg=-0.0309, ret=0.000102, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:14<00:24,  1.16it/s, pg=-0.0159, ret=9.57e-5, glen=45.1, tlen=206, kl=0, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.17it/s, pg=-0.0159, ret=9.57e-5, glen=45.1, tlen=206, kl=0, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:15<00:23,  1.17it/s, pg=0.0491, ret=-8.55e-5, glen=44.5, tlen=205, kl=0, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.13it/s, pg=0.0491, ret=-8.55e-5, glen=44.5, tlen=205, kl=0, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.13it/s, pg=0.0166, ret=6.61e-5, glen=44.9, tlen=205, kl=0, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.14it/s, pg=0.0166, ret=6.61e-5, glen=44.9, tlen=205, kl=0, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.14it/s, pg=-0.0537, ret=9.22e-5, glen=44.1, tlen=205, kl=0, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.15it/s, pg=-0.0537, ret=9.22e-5, glen=44.1, tlen=205, kl=0, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.15it/s, pg=-0.0643, ret=0.0001, glen=46, tlen=206, kl=0, act_lr=1e-6, ent=0.956]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.16it/s, pg=-0.0643, ret=0.0001, glen=46, tlen=206, kl=0, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.16it/s, pg=0.111, ret=-0.000282, glen=44.1, tlen=204, kl=0, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.16it/s, pg=0.111, ret=-0.000282, glen=44.1, tlen=204, kl=0, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.16it/s, pg=-0.0765, ret=0.000131, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.0765, ret=0.000131, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:20<00:17,  1.17it/s, pg=-0.0529, ret=0.000143, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=-0.0529, ret=0.000143, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=-0.0545, ret=4.56e-5, glen=46.5, tlen=207, kl=0, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.17it/s, pg=-0.0545, ret=4.56e-5, glen=46.5, tlen=207, kl=0, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.17it/s, pg=0.044, ret=-0.000224, glen=47.7, tlen=208, kl=0, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.17it/s, pg=0.044, ret=-0.000224, glen=47.7, tlen=208, kl=0, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.17it/s, pg=-0.043, ret=0.000117, glen=47.2, tlen=208, kl=0, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.18it/s, pg=-0.043, ret=0.000117, glen=47.2, tlen=208, kl=0, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.18it/s, pg=0.0754, ret=-0.000212, glen=45.2, tlen=205, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.18it/s, pg=0.0754, ret=-0.000212, glen=45.2, tlen=205, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.18it/s, pg=-0.0304, ret=0.00013, glen=47.3, tlen=208, kl=0, act_lr=1e-6, ent=0.979] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.18it/s, pg=-0.0304, ret=0.00013, glen=47.3, tlen=208, kl=0, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.18it/s, pg=0.0538, ret=-8.89e-5, glen=44.9, tlen=206, kl=0, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.08it/s, pg=0.0538, ret=-8.89e-5, glen=44.9, tlen=206, kl=0, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.08it/s, pg=-0.0938, ret=0.000198, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.10it/s, pg=-0.0938, ret=0.000198, glen=45.4, tlen=206, kl=0, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.10it/s, pg=0.0716, ret=-0.000152, glen=43.7, tlen=204, kl=0, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.13it/s, pg=0.0716, ret=-0.000152, glen=43.7, tlen=204, kl=0, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:10,  1.13it/s, pg=-0.0342, ret=0.000245, glen=44.2, tlen=204, kl=0, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=-0.0342, ret=0.000245, glen=44.2, tlen=204, kl=0, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=0.0307, ret=-7.92e-5, glen=46.3, tlen=207, kl=0, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=0.0307, ret=-7.92e-5, glen=46.3, tlen=207, kl=0, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=0.0688, ret=-0.000107, glen=46, tlen=206, kl=0, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.16it/s, pg=0.0688, ret=-0.000107, glen=46, tlen=206, kl=0, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.16it/s, pg=0.0349, ret=-4.07e-5, glen=45.9, tlen=207, kl=0, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=0.0349, ret=-4.07e-5, glen=45.9, tlen=207, kl=0, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=0.0251, ret=-0.000148, glen=45.6, tlen=206, kl=0, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:05,  1.17it/s, pg=0.0251, ret=-0.000148, glen=45.6, tlen=206, kl=0, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:05,  1.17it/s, pg=-0.0432, ret=0.000145, glen=45, tlen=205, kl=0, act_lr=1e-6, ent=0.973]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.0432, ret=0.000145, glen=45, tlen=205, kl=0, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=0.0263, ret=-0.000141, glen=46.5, tlen=207, kl=0, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.16it/s, pg=0.0263, ret=-0.000141, glen=46.5, tlen=207, kl=0, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.16it/s, pg=-0.0132, ret=1.3e-5, glen=46.5, tlen=207, kl=0, act_lr=1e-6, ent=0.948]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.17it/s, pg=-0.0132, ret=1.3e-5, glen=46.5, tlen=207, kl=0, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=0.118, ret=-0.000245, glen=43, tlen=203, kl=0, act_lr=1e-6, ent=0.904] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=0.118, ret=-0.000245, glen=43, tlen=203, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0543, ret=5.87e-5, glen=46.1, tlen=207, kl=0, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=-0.0543, ret=5.87e-5, glen=46.1, tlen=207, kl=0, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=-0.121, ret=0.000276, glen=44.1, tlen=205, kl=0, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.17it/s, pg=-0.121, ret=0.000276, glen=44.1, tlen=205, kl=0, act_lr=1e-6, ent=0.947]
2025-07-24 19:26:20.982 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.51s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.17it/s, pg=-0.122, ret=0.000276, glen=48.6, tlen=209, kl=0, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=-0.122, ret=0.000276, glen=48.6, tlen=209, kl=0, act_lr=1e-6, ent=0.967]
2025-07-24 19:26:21.639 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.59s
2025-07-24 19:26:23.977 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.34s
2025-07-24 19:26:24.375 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.02s
2025-07-24 19:26:24.396 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0023639590241188225, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9490521647209345, 'kl': 0.0, 'response_length': 45.28624272900959, 'total_length': 205.73775038608284, 'teacher_total_length': 217.77645838537882, 'return': 5.793551344838708e-06, 'policy_update_steps': 1.0}

Episode [6/20]:   8%|‚ñä         | 1/13 [01:39<19:56, 99.68s/it][A2025-07-24 19:26:24.445 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:26:52.881 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:26:53.062 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:26:53.063 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 28.62s
2025-07-24 19:26:54.635 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0172,avg_reflection_pattern_score: 0.0023,avg_pass_at_n: 1.0000,avg_num_tokens: 44.9952,std_num_tokens: 14.9084,avg_correct_num_tokens: 44.9430,std_correct_num_tokens: 14.8172,avg_incorrect_num_tokens: 48.4508,std_incorrect_num_tokens: 19.7348
2025-07-24 19:26:55.049 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.99s
2025-07-24 19:26:57.442 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.39s
2025-07-24 19:27:19.599 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:27:19.599 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.15s
2025-07-24 19:27:20.918 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 19:27:20.918 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.583617200253202e-05, avg_kl: 0.028066119025735293, avg_response_length: 45.01002996108111, avg_orm_score: 0.0, avg_custom_rewards: 3.583617200253202e-05
2025-07-24 19:27:20.947 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter66_replay_buffer.jsonl
2025-07-24 19:27:21.979 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.03s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=-0.0905, ret=0.000191, glen=43.8, tlen=204, kl=0.0271, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0905, ret=0.000191, glen=43.8, tlen=204, kl=0.0271, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0863, ret=0.000183, glen=45.7, tlen=207, kl=0.0284, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=-0.0863, ret=0.000183, glen=45.7, tlen=207, kl=0.0284, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=0.0282, ret=-6.81e-5, glen=43.5, tlen=204, kl=0.0278, act_lr=1e-6, ent=0.944] Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.12it/s, pg=0.0282, ret=-6.81e-5, glen=43.5, tlen=204, kl=0.0278, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.12it/s, pg=0.0797, ret=-0.000141, glen=45.1, tlen=206, kl=0.0284, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.12it/s, pg=0.0797, ret=-0.000141, glen=45.1, tlen=206, kl=0.0284, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.12it/s, pg=0.00873, ret=0.000103, glen=44, tlen=205, kl=0.0258, act_lr=1e-6, ent=0.908]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.12it/s, pg=0.00873, ret=0.000103, glen=44, tlen=205, kl=0.0258, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.12it/s, pg=-0.0826, ret=0.000255, glen=43.3, tlen=205, kl=0.0277, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.14it/s, pg=-0.0826, ret=0.000255, glen=43.3, tlen=205, kl=0.0277, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.14it/s, pg=0.0959, ret=-0.000241, glen=44.7, tlen=205, kl=0.0268, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.15it/s, pg=0.0959, ret=-0.000241, glen=44.7, tlen=205, kl=0.0268, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.15it/s, pg=0.119, ret=-0.00029, glen=45.2, tlen=206, kl=0.0277, act_lr=1e-6, ent=0.903]  Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.14it/s, pg=0.119, ret=-0.00029, glen=45.2, tlen=206, kl=0.0277, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.14it/s, pg=-0.045, ret=0.000121, glen=46.2, tlen=207, kl=0.0255, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.15it/s, pg=-0.045, ret=0.000121, glen=46.2, tlen=207, kl=0.0255, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=-0.00467, ret=5.99e-5, glen=45.5, tlen=206, kl=0.0298, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.16it/s, pg=-0.00467, ret=5.99e-5, glen=45.5, tlen=206, kl=0.0298, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.16it/s, pg=0.0193, ret=-4.16e-6, glen=45.1, tlen=206, kl=0.0256, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.16it/s, pg=0.0193, ret=-4.16e-6, glen=45.1, tlen=206, kl=0.0256, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.16it/s, pg=-0.0502, ret=0.000177, glen=45.7, tlen=207, kl=0.0284, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.17it/s, pg=-0.0502, ret=0.000177, glen=45.7, tlen=207, kl=0.0284, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.17it/s, pg=-0.0622, ret=9.1e-5, glen=45.2, tlen=206, kl=0.0291, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=-0.0622, ret=9.1e-5, glen=45.2, tlen=206, kl=0.0291, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=0.0662, ret=-0.000264, glen=43.8, tlen=205, kl=0.026, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=0.0662, ret=-0.000264, glen=43.8, tlen=205, kl=0.026, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:24,  1.17it/s, pg=-0.0199, ret=5.32e-5, glen=44.3, tlen=205, kl=0.0282, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.0199, ret=5.32e-5, glen=44.3, tlen=205, kl=0.0282, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.0638, ret=0.000168, glen=44.3, tlen=205, kl=0.0298, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.15it/s, pg=-0.0638, ret=0.000168, glen=44.3, tlen=205, kl=0.0298, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.15it/s, pg=-0.128, ret=0.000304, glen=46.3, tlen=207, kl=0.03, act_lr=1e-6, ent=0.938]   Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.16it/s, pg=-0.128, ret=0.000304, glen=46.3, tlen=207, kl=0.03, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.16it/s, pg=0.0474, ret=-0.000168, glen=45.1, tlen=206, kl=0.0306, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=0.0474, ret=-0.000168, glen=45.1, tlen=206, kl=0.0306, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=-0.126, ret=0.000314, glen=46, tlen=207, kl=0.0286, act_lr=1e-6, ent=0.95]    Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=-0.126, ret=0.000314, glen=46, tlen=207, kl=0.0286, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=0.00208, ret=5.56e-5, glen=44.8, tlen=206, kl=0.0312, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=0.00208, ret=5.56e-5, glen=44.8, tlen=206, kl=0.0312, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=-0.0781, ret=0.000178, glen=45.4, tlen=206, kl=0.0316, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=-0.0781, ret=0.000178, glen=45.4, tlen=206, kl=0.0316, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.17it/s, pg=-0.115, ret=0.000236, glen=45.3, tlen=206, kl=0.0293, act_lr=1e-6, ent=0.895] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.18it/s, pg=-0.115, ret=0.000236, glen=45.3, tlen=206, kl=0.0293, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.18it/s, pg=0.104, ret=-0.000344, glen=45.5, tlen=206, kl=0.0283, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.18it/s, pg=0.104, ret=-0.000344, glen=45.5, tlen=206, kl=0.0283, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.18it/s, pg=-0.101, ret=0.000238, glen=43.8, tlen=204, kl=0.0256, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.18it/s, pg=-0.101, ret=0.000238, glen=43.8, tlen=204, kl=0.0256, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.18it/s, pg=-0.0049, ret=-0.00012, glen=47.8, tlen=208, kl=0.0255, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.15it/s, pg=-0.0049, ret=-0.00012, glen=47.8, tlen=208, kl=0.0255, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.15it/s, pg=0.114, ret=-0.000146, glen=47.3, tlen=208, kl=0.0267, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.16it/s, pg=0.114, ret=-0.000146, glen=47.3, tlen=208, kl=0.0267, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.16it/s, pg=0.043, ret=-1.74e-5, glen=44.7, tlen=205, kl=0.0275, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.16it/s, pg=0.043, ret=-1.74e-5, glen=44.7, tlen=205, kl=0.0275, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.16it/s, pg=-0.0839, ret=0.000197, glen=45.2, tlen=207, kl=0.0274, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=-0.0839, ret=0.000197, glen=45.2, tlen=207, kl=0.0274, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=-0.052, ret=0.000132, glen=45.1, tlen=206, kl=0.028, act_lr=1e-6, ent=0.889]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.07it/s, pg=-0.052, ret=0.000132, glen=45.1, tlen=206, kl=0.028, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.07it/s, pg=0.202, ret=-0.000293, glen=45.5, tlen=206, kl=0.0282, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.10it/s, pg=0.202, ret=-0.000293, glen=45.5, tlen=206, kl=0.0282, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.10it/s, pg=0.102, ret=-0.000298, glen=45, tlen=206, kl=0.0287, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=0.102, ret=-0.000298, glen=45, tlen=206, kl=0.0287, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.0444, ret=5.86e-5, glen=45, tlen=205, kl=0.0272, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.14it/s, pg=-0.0444, ret=5.86e-5, glen=45, tlen=205, kl=0.0272, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=0.17, ret=-0.000357, glen=44.3, tlen=205, kl=0.031, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=0.17, ret=-0.000357, glen=44.3, tlen=205, kl=0.031, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=-0.0127, ret=5.11e-5, glen=44.3, tlen=205, kl=0.0282, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.16it/s, pg=-0.0127, ret=5.11e-5, glen=44.3, tlen=205, kl=0.0282, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.16it/s, pg=0.0228, ret=-1.53e-5, glen=45.6, tlen=206, kl=0.0341, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=0.0228, ret=-1.53e-5, glen=45.6, tlen=206, kl=0.0341, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=-0.0756, ret=0.000203, glen=44.6, tlen=205, kl=0.029, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:05,  1.17it/s, pg=-0.0756, ret=0.000203, glen=44.6, tlen=205, kl=0.029, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:05,  1.17it/s, pg=-0.0832, ret=0.000119, glen=45.1, tlen=206, kl=0.0231, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.0832, ret=0.000119, glen=45.1, tlen=206, kl=0.0231, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=0.0939, ret=-0.000235, glen=45.3, tlen=206, kl=0.027, act_lr=1e-6, ent=0.9]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:32<00:04,  1.17it/s, pg=0.0939, ret=-0.000235, glen=45.3, tlen=206, kl=0.027, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.0399, ret=-0.000134, glen=45.2, tlen=206, kl=0.028, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.18it/s, pg=0.0399, ret=-0.000134, glen=45.2, tlen=206, kl=0.028, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.18it/s, pg=-0.0968, ret=0.000173, glen=44.1, tlen=205, kl=0.0259, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.18it/s, pg=-0.0968, ret=0.000173, glen=44.1, tlen=205, kl=0.0259, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.18it/s, pg=-0.0551, ret=3.86e-5, glen=44.8, tlen=205, kl=0.0274, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.18it/s, pg=-0.0551, ret=3.86e-5, glen=44.8, tlen=205, kl=0.0274, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.18it/s, pg=0.115, ret=-0.000311, glen=43.9, tlen=204, kl=0.03, act_lr=1e-6, ent=0.895]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.18it/s, pg=0.115, ret=-0.000311, glen=43.9, tlen=204, kl=0.03, act_lr=1e-6, ent=0.895]
2025-07-24 19:27:59.481 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.33s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.18it/s, pg=0.0558, ret=-8.25e-5, glen=45, tlen=205, kl=0.0259, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.13it/s, pg=0.0558, ret=-8.25e-5, glen=45, tlen=205, kl=0.0259, act_lr=1e-6, ent=0.919]
2025-07-24 19:28:00.162 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 19:28:02.664 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.50s
2025-07-24 19:28:03.015 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 40.99s
2025-07-24 19:28:03.020 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007467935251635174, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9157889707143917, 'kl': 0.028047783430232558, 'response_length': 45.012209515238915, 'total_length': 205.6596917440725, 'teacher_total_length': 217.63018443972567, 'return': 3.945738277506343e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  15%|‚ñà‚ñå        | 2/13 [03:18<18:09, 99.06s/it][A2025-07-24 19:28:03.066 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:28:33.001 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:28:33.185 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:28:33.185 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.12s
2025-07-24 19:28:34.833 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 46.4227,std_num_tokens: 15.5146,avg_correct_num_tokens: 46.3439,std_correct_num_tokens: 15.4109,avg_incorrect_num_tokens: 50.2096,std_incorrect_num_tokens: 19.4985
2025-07-24 19:28:35.277 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.09s
2025-07-24 19:28:37.880 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.60s
2025-07-24 19:28:59.943 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 171
2025-07-24 19:28:59.944 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.06s
2025-07-24 19:29:01.203 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 19:29:01.203 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -3.350858286578666e-06, avg_kl: 0.03597719069809942, avg_response_length: 46.44899504365977, avg_orm_score: 0.0, avg_custom_rewards: -3.350858286578666e-06
2025-07-24 19:29:01.229 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter67_replay_buffer.jsonl
2025-07-24 19:29:02.290 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=-0.0476, ret=9.51e-5, glen=47, tlen=208, kl=0.0347, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:43,  1.03s/it, pg=-0.0476, ret=9.51e-5, glen=47, tlen=208, kl=0.0347, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:43,  1.03s/it, pg=-0.00167, ret=-5.84e-5, glen=46.7, tlen=208, kl=0.0355, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:38,  1.06it/s, pg=-0.00167, ret=-5.84e-5, glen=46.7, tlen=208, kl=0.0355, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:38,  1.06it/s, pg=0.0322, ret=-0.000225, glen=46.6, tlen=208, kl=0.0367, act_lr=1e-6, ent=0.952] Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:37,  1.08it/s, pg=0.0322, ret=-0.000225, glen=46.6, tlen=208, kl=0.0367, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:37,  1.08it/s, pg=0.00153, ret=-0.000147, glen=46.7, tlen=208, kl=0.0346, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:35,  1.10it/s, pg=0.00153, ret=-0.000147, glen=46.7, tlen=208, kl=0.0346, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:35,  1.10it/s, pg=-0.0792, ret=0.000193, glen=44.8, tlen=206, kl=0.0384, act_lr=1e-6, ent=0.948] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.12it/s, pg=-0.0792, ret=0.000193, glen=44.8, tlen=206, kl=0.0384, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.12it/s, pg=-0.0506, ret=8.98e-5, glen=46.8, tlen=208, kl=0.0365, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.14it/s, pg=-0.0506, ret=8.98e-5, glen=46.8, tlen=208, kl=0.0365, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.14it/s, pg=-0.159, ret=0.000443, glen=45.5, tlen=206, kl=0.037, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.15it/s, pg=-0.159, ret=0.000443, glen=45.5, tlen=206, kl=0.037, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.15it/s, pg=0.187, ret=-0.00057, glen=46.5, tlen=208, kl=0.0381, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.13it/s, pg=0.187, ret=-0.00057, glen=46.5, tlen=208, kl=0.0381, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:08<00:30,  1.13it/s, pg=-0.0396, ret=0.000126, glen=46.1, tlen=207, kl=0.0309, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:30,  1.13it/s, pg=-0.0396, ret=0.000126, glen=46.1, tlen=207, kl=0.0309, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:30,  1.13it/s, pg=-0.0696, ret=0.000231, glen=47.1, tlen=208, kl=0.0357, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.14it/s, pg=-0.0696, ret=0.000231, glen=47.1, tlen=208, kl=0.0357, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.14it/s, pg=-0.0014, ret=-5.43e-5, glen=45.8, tlen=207, kl=0.0322, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.15it/s, pg=-0.0014, ret=-5.43e-5, glen=45.8, tlen=207, kl=0.0322, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.15it/s, pg=0.0193, ret=5.14e-6, glen=47.5, tlen=209, kl=0.0354, act_lr=1e-6, ent=0.924]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.16it/s, pg=0.0193, ret=5.14e-6, glen=47.5, tlen=209, kl=0.0354, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.16it/s, pg=-0.0823, ret=0.000202, glen=46, tlen=207, kl=0.041, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.16it/s, pg=-0.0823, ret=0.000202, glen=46, tlen=207, kl=0.041, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.16it/s, pg=0.0186, ret=-4.29e-5, glen=46.6, tlen=208, kl=0.0326, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=0.0186, ret=-4.29e-5, glen=46.6, tlen=208, kl=0.0326, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:24,  1.17it/s, pg=0.206, ret=-0.000375, glen=47, tlen=208, kl=0.0449, act_lr=1e-6, ent=0.937]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.17it/s, pg=0.206, ret=-0.000375, glen=47, tlen=208, kl=0.0449, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:14<00:23,  1.17it/s, pg=-0.00909, ret=0.000121, glen=45.2, tlen=206, kl=0.0365, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.17it/s, pg=-0.00909, ret=0.000121, glen=45.2, tlen=206, kl=0.0365, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.17it/s, pg=0.0362, ret=-0.000233, glen=46.8, tlen=207, kl=0.0333, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.17it/s, pg=0.0362, ret=-0.000233, glen=46.8, tlen=207, kl=0.0333, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.17it/s, pg=-0.0291, ret=0.0002, glen=45.5, tlen=206, kl=0.0391, act_lr=1e-6, ent=0.967]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=-0.0291, ret=0.0002, glen=45.5, tlen=206, kl=0.0391, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=-0.0122, ret=-9.04e-5, glen=45.7, tlen=207, kl=0.0342, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=-0.0122, ret=-9.04e-5, glen=45.7, tlen=207, kl=0.0342, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=0.107, ret=-0.000234, glen=45.1, tlen=206, kl=0.0391, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=0.107, ret=-0.000234, glen=45.1, tlen=206, kl=0.0391, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=-0.00717, ret=-7.46e-5, glen=47.2, tlen=208, kl=0.0347, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=-0.00717, ret=-7.46e-5, glen=47.2, tlen=208, kl=0.0347, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.17it/s, pg=-0.0226, ret=-7.07e-6, glen=44.7, tlen=206, kl=0.0362, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.0226, ret=-7.07e-6, glen=44.7, tlen=206, kl=0.0362, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.00742, ret=2.22e-6, glen=47.8, tlen=209, kl=0.0408, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.17it/s, pg=-0.00742, ret=2.22e-6, glen=47.8, tlen=209, kl=0.0408, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=-0.0678, ret=0.000156, glen=45.4, tlen=206, kl=0.0361, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.18it/s, pg=-0.0678, ret=0.000156, glen=45.4, tlen=206, kl=0.0361, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.18it/s, pg=0.0642, ret=-0.000153, glen=48.5, tlen=210, kl=0.0353, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.18it/s, pg=0.0642, ret=-0.000153, glen=48.5, tlen=210, kl=0.0353, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.18it/s, pg=0.105, ret=-0.000248, glen=47.1, tlen=208, kl=0.038, act_lr=1e-6, ent=0.973]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.14it/s, pg=0.105, ret=-0.000248, glen=47.1, tlen=208, kl=0.038, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.14it/s, pg=-0.0454, ret=9.09e-5, glen=45.1, tlen=206, kl=0.0312, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.15it/s, pg=-0.0454, ret=9.09e-5, glen=45.1, tlen=206, kl=0.0312, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.15it/s, pg=-0.114, ret=0.000322, glen=48.4, tlen=210, kl=0.0352, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.16it/s, pg=-0.114, ret=0.000322, glen=48.4, tlen=210, kl=0.0352, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.16it/s, pg=-0.167, ret=0.000442, glen=46.8, tlen=208, kl=0.0347, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.06it/s, pg=-0.167, ret=0.000442, glen=46.8, tlen=208, kl=0.0347, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.06it/s, pg=-0.00217, ret=-7.52e-5, glen=47.2, tlen=208, kl=0.0334, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.09it/s, pg=-0.00217, ret=-7.52e-5, glen=47.2, tlen=208, kl=0.0334, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.09it/s, pg=0.0244, ret=-0.000156, glen=46.9, tlen=208, kl=0.0316, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.11it/s, pg=0.0244, ret=-0.000156, glen=46.9, tlen=208, kl=0.0316, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:10,  1.11it/s, pg=-0.0524, ret=6.96e-5, glen=45.5, tlen=207, kl=0.0366, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.13it/s, pg=-0.0524, ret=6.96e-5, glen=45.5, tlen=207, kl=0.0366, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.13it/s, pg=-0.0785, ret=0.000191, glen=47.3, tlen=208, kl=0.0371, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.12it/s, pg=-0.0785, ret=0.000191, glen=47.3, tlen=208, kl=0.0371, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.12it/s, pg=-0.0164, ret=1.13e-5, glen=45.4, tlen=206, kl=0.0364, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.13it/s, pg=-0.0164, ret=1.13e-5, glen=45.4, tlen=206, kl=0.0364, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.13it/s, pg=-0.107, ret=0.000321, glen=46, tlen=207, kl=0.0389, act_lr=1e-6, ent=0.947]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.15it/s, pg=-0.107, ret=0.000321, glen=46, tlen=207, kl=0.0389, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.15it/s, pg=0.036, ret=-5.13e-7, glen=44.8, tlen=206, kl=0.0386, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.16it/s, pg=0.036, ret=-5.13e-7, glen=44.8, tlen=206, kl=0.0386, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.16it/s, pg=0.0652, ret=-0.000103, glen=44.7, tlen=206, kl=0.0361, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.16it/s, pg=0.0652, ret=-0.000103, glen=44.7, tlen=206, kl=0.0361, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.16it/s, pg=0.0439, ret=-6.93e-5, glen=47.3, tlen=208, kl=0.0341, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.0439, ret=-6.93e-5, glen=47.3, tlen=208, kl=0.0341, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:34<00:04,  1.17it/s, pg=-0.0147, ret=3.73e-5, glen=45.4, tlen=206, kl=0.0312, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.0147, ret=3.73e-5, glen=45.4, tlen=206, kl=0.0312, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=0.0287, ret=-2.32e-5, glen=46.7, tlen=208, kl=0.0408, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=0.0287, ret=-2.32e-5, glen=46.7, tlen=208, kl=0.0408, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=0.277, ret=-0.000513, glen=49.2, tlen=210, kl=0.035, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.18it/s, pg=0.277, ret=-0.000513, glen=49.2, tlen=210, kl=0.035, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.18it/s, pg=-0.0236, ret=8.87e-5, glen=48.9, tlen=210, kl=0.0353, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.18it/s, pg=-0.0236, ret=8.87e-5, glen=48.9, tlen=210, kl=0.0353, act_lr=1e-6, ent=0.928]
2025-07-24 19:29:40.254 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.52s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.18it/s, pg=-0.014, ret=8.99e-5, glen=45.7, tlen=207, kl=0.0358, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=-0.014, ret=8.99e-5, glen=45.7, tlen=207, kl=0.0358, act_lr=1e-6, ent=0.96]
2025-07-24 19:29:40.937 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:29:43.302 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 19:29:43.632 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.02s
2025-07-24 19:29:43.638 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0015764458234920058, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9428219393242238, 'kl': 0.03603487236555233, 'response_length': 46.43819764603016, 'total_length': 207.3896438243777, 'teacher_total_length': 219.36280113042787, 'return': 1.7960575654120307e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [04:58<16:37, 99.77s/it][A2025-07-24 19:29:43.682 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:30:12.308 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:30:12.484 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:30:12.485 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 28.80s
2025-07-24 19:30:14.196 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0017,avg_pass_at_n: 1.0000,avg_num_tokens: 45.5315,std_num_tokens: 14.2093,avg_correct_num_tokens: 45.5170,std_correct_num_tokens: 14.2118,avg_incorrect_num_tokens: 46.7172,std_incorrect_num_tokens: 13.9603
2025-07-24 19:30:14.621 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.14s
2025-07-24 19:30:17.162 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.54s
2025-07-24 19:30:39.215 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:30:39.215 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.05s
2025-07-24 19:30:40.485 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 19:30:40.485 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.6792444777401056e-05, avg_kl: 0.055550608915441176, avg_response_length: 45.53729730493882, avg_orm_score: 0.0, avg_custom_rewards: 3.6792444777401056e-05
2025-07-24 19:30:40.514 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter68_replay_buffer.jsonl
2025-07-24 19:30:41.552 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.04s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=0.129, ret=-0.0003, glen=45.8, tlen=206, kl=0.054, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.00s/it, pg=0.129, ret=-0.0003, glen=45.8, tlen=206, kl=0.054, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.00s/it, pg=0.0309, ret=-9.34e-5, glen=44.8, tlen=205, kl=0.0593, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=0.0309, ret=-9.34e-5, glen=44.8, tlen=205, kl=0.0593, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=-0.0135, ret=4.89e-5, glen=44.9, tlen=206, kl=0.0581, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.12it/s, pg=-0.0135, ret=4.89e-5, glen=44.9, tlen=206, kl=0.0581, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.12it/s, pg=0.0518, ret=-3.78e-5, glen=46.9, tlen=207, kl=0.0558, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:35,  1.10it/s, pg=0.0518, ret=-3.78e-5, glen=46.9, tlen=207, kl=0.0558, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:35,  1.10it/s, pg=-0.108, ret=0.000246, glen=46.9, tlen=207, kl=0.0551, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.13it/s, pg=-0.108, ret=0.000246, glen=46.9, tlen=207, kl=0.0551, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.13it/s, pg=0.0391, ret=-9.77e-5, glen=46.4, tlen=207, kl=0.0594, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.14it/s, pg=0.0391, ret=-9.77e-5, glen=46.4, tlen=207, kl=0.0594, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.14it/s, pg=-0.0608, ret=8.76e-5, glen=44, tlen=204, kl=0.0674, act_lr=1e-6, ent=0.933]  Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.15it/s, pg=-0.0608, ret=8.76e-5, glen=44, tlen=204, kl=0.0674, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.15it/s, pg=-0.0731, ret=0.000153, glen=47.2, tlen=207, kl=0.0491, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.0731, ret=0.000153, glen=47.2, tlen=207, kl=0.0491, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=0.109, ret=-0.000268, glen=44, tlen=204, kl=0.053, act_lr=1e-6, ent=0.948]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.16it/s, pg=0.109, ret=-0.000268, glen=44, tlen=204, kl=0.053, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.16it/s, pg=0.000977, ret=1.5e-5, glen=44.6, tlen=205, kl=0.0546, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.17it/s, pg=0.000977, ret=1.5e-5, glen=44.6, tlen=205, kl=0.0546, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.17it/s, pg=0.113, ret=-0.000282, glen=47.2, tlen=207, kl=0.0534, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.17it/s, pg=0.113, ret=-0.000282, glen=47.2, tlen=207, kl=0.0534, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.17it/s, pg=-0.0632, ret=0.000188, glen=45, tlen=205, kl=0.0493, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.17it/s, pg=-0.0632, ret=0.000188, glen=45, tlen=205, kl=0.0493, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.17it/s, pg=0.121, ret=-0.000176, glen=46.9, tlen=207, kl=0.0528, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=0.121, ret=-0.000176, glen=46.9, tlen=207, kl=0.0528, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=-0.0434, ret=4.41e-5, glen=45.7, tlen=206, kl=0.0461, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.18it/s, pg=-0.0434, ret=4.41e-5, glen=45.7, tlen=206, kl=0.0461, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:24,  1.18it/s, pg=0.0163, ret=-4.32e-5, glen=47.1, tlen=208, kl=0.0684, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=0.0163, ret=-4.32e-5, glen=47.1, tlen=208, kl=0.0684, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=0.152, ret=-0.000275, glen=45.5, tlen=206, kl=0.0483, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.16it/s, pg=0.152, ret=-0.000275, glen=45.5, tlen=206, kl=0.0483, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=0.0194, ret=-5.46e-5, glen=46.2, tlen=207, kl=0.0514, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.16it/s, pg=0.0194, ret=-5.46e-5, glen=46.2, tlen=207, kl=0.0514, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.16it/s, pg=-0.0437, ret=0.000112, glen=44.4, tlen=205, kl=0.0501, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.16it/s, pg=-0.0437, ret=0.000112, glen=44.4, tlen=205, kl=0.0501, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.16it/s, pg=0.0043, ret=-0.000153, glen=44.7, tlen=205, kl=0.0601, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=0.0043, ret=-0.000153, glen=44.7, tlen=205, kl=0.0601, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=0.0323, ret=-3.37e-5, glen=45.4, tlen=206, kl=0.0468, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=0.0323, ret=-3.37e-5, glen=45.4, tlen=206, kl=0.0468, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=-0.0628, ret=0.000111, glen=45.1, tlen=205, kl=0.0538, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:19,  1.15it/s, pg=-0.0628, ret=0.000111, glen=45.1, tlen=205, kl=0.0538, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:19,  1.15it/s, pg=0.0247, ret=-3.89e-5, glen=45.4, tlen=206, kl=0.0545, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:18,  1.16it/s, pg=0.0247, ret=-3.89e-5, glen=45.4, tlen=206, kl=0.0545, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:18,  1.16it/s, pg=-0.05, ret=0.000124, glen=44.9, tlen=205, kl=0.0521, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.16it/s, pg=-0.05, ret=0.000124, glen=44.9, tlen=205, kl=0.0521, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.16it/s, pg=-0.0286, ret=8.35e-5, glen=45.2, tlen=205, kl=0.0543, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.17it/s, pg=-0.0286, ret=8.35e-5, glen=45.2, tlen=205, kl=0.0543, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.17it/s, pg=-0.0909, ret=0.0002, glen=48.3, tlen=208, kl=0.0724, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.17it/s, pg=-0.0909, ret=0.0002, glen=48.3, tlen=208, kl=0.0724, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.17it/s, pg=-0.0183, ret=2.13e-5, glen=43.8, tlen=204, kl=0.0558, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.16it/s, pg=-0.0183, ret=2.13e-5, glen=43.8, tlen=204, kl=0.0558, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.16it/s, pg=-0.0667, ret=0.00012, glen=44.5, tlen=205, kl=0.0524, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.16it/s, pg=-0.0667, ret=0.00012, glen=44.5, tlen=205, kl=0.0524, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.16it/s, pg=-0.0161, ret=3.28e-5, glen=44.6, tlen=205, kl=0.0484, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:13,  1.15it/s, pg=-0.0161, ret=3.28e-5, glen=44.6, tlen=205, kl=0.0484, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:13,  1.15it/s, pg=0.0237, ret=-6.36e-5, glen=45.7, tlen=206, kl=0.0488, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.06it/s, pg=0.0237, ret=-6.36e-5, glen=45.7, tlen=206, kl=0.0488, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.06it/s, pg=-0.0392, ret=4.08e-5, glen=45.3, tlen=205, kl=0.0482, act_lr=1e-6, ent=0.87] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.09it/s, pg=-0.0392, ret=4.08e-5, glen=45.3, tlen=205, kl=0.0482, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.09it/s, pg=0.0982, ret=-0.000263, glen=46.1, tlen=207, kl=0.0646, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=0.0982, ret=-0.000263, glen=46.1, tlen=207, kl=0.0646, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.0848, ret=0.000202, glen=46.2, tlen=207, kl=0.0464, act_lr=1e-6, ent=0.88] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.14it/s, pg=-0.0848, ret=0.000202, glen=46.2, tlen=207, kl=0.0464, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=-0.0578, ret=6.1e-5, glen=46.5, tlen=207, kl=0.0559, act_lr=1e-6, ent=0.865] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=-0.0578, ret=6.1e-5, glen=46.5, tlen=207, kl=0.0559, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=-0.0815, ret=0.00017, glen=46.2, tlen=206, kl=0.0488, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.16it/s, pg=-0.0815, ret=0.00017, glen=46.2, tlen=206, kl=0.0488, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.16it/s, pg=0.0123, ret=3.82e-5, glen=44.9, tlen=205, kl=0.0511, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=0.0123, ret=3.82e-5, glen=44.9, tlen=205, kl=0.0511, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=-0.0592, ret=8.33e-5, glen=45.7, tlen=206, kl=0.0532, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:05,  1.17it/s, pg=-0.0592, ret=8.33e-5, glen=45.7, tlen=206, kl=0.0532, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:05,  1.17it/s, pg=-0.00513, ret=8.69e-5, glen=45.6, tlen=206, kl=0.0521, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.00513, ret=8.69e-5, glen=45.6, tlen=206, kl=0.0521, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=0.018, ret=-6.61e-5, glen=44.5, tlen=205, kl=0.0506, act_lr=1e-6, ent=0.879]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.018, ret=-6.61e-5, glen=44.5, tlen=205, kl=0.0506, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.0145, ret=-3.1e-6, glen=44.7, tlen=205, kl=0.0527, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.18it/s, pg=0.0145, ret=-3.1e-6, glen=44.7, tlen=205, kl=0.0527, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.18it/s, pg=-0.12, ret=0.000257, glen=45.1, tlen=205, kl=0.0787, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.18it/s, pg=-0.12, ret=0.000257, glen=45.1, tlen=205, kl=0.0787, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.18it/s, pg=0.204, ret=-0.000331, glen=46.5, tlen=207, kl=0.0974, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.18it/s, pg=0.204, ret=-0.000331, glen=46.5, tlen=207, kl=0.0974, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.18it/s, pg=-0.0331, ret=5.75e-5, glen=44.1, tlen=205, kl=0.0504, act_lr=1e-6, ent=0.869]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.18it/s, pg=-0.0331, ret=5.75e-5, glen=44.1, tlen=205, kl=0.0504, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.18it/s, pg=0.0859, ret=-0.000134, glen=45.6, tlen=206, kl=0.0508, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.13it/s, pg=0.0859, ret=-0.000134, glen=45.6, tlen=206, kl=0.0508, act_lr=1e-6, ent=0.938]2025-07-24 19:31:19.279 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.56s

2025-07-24 19:31:19.939 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 19:31:22.275 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.34s
2025-07-24 19:31:22.605 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.01s
2025-07-24 19:31:22.634 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.001872306646302689, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9129955685415934, 'kl': 0.055485215297965115, 'response_length': 45.54764361714208, 'total_length': 205.85954000783522, 'teacher_total_length': 217.91118320198947, 'return': -3.048794518013723e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [06:37<14:55, 99.47s/it][A2025-07-24 19:31:22.684 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:31:51.988 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:31:52.169 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:31:52.170 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 29.49s
2025-07-24 19:31:53.896 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0163,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 45.6521,std_num_tokens: 14.3879,avg_correct_num_tokens: 45.6540,std_correct_num_tokens: 14.3465,avg_incorrect_num_tokens: 45.5435,std_incorrect_num_tokens: 16.6273
2025-07-24 19:31:54.469 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.30s
2025-07-24 19:31:56.913 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.44s
2025-07-24 19:32:19.099 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:32:19.099 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.19s
2025-07-24 19:32:20.354 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 19:32:20.354 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.105716194327482e-05, avg_kl: 0.06715159696691177, avg_response_length: 45.67457410026999, avg_orm_score: 0.0, avg_custom_rewards: 3.105716194327482e-05
2025-07-24 19:32:20.391 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter69_replay_buffer.jsonl
2025-07-24 19:32:21.444 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=0.0382, ret=-0.000105, glen=46.5, tlen=207, kl=0.0825, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:43,  1.02s/it, pg=0.0382, ret=-0.000105, glen=46.5, tlen=207, kl=0.0825, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:43,  1.02s/it, pg=-0.0444, ret=7.98e-5, glen=46.4, tlen=206, kl=0.0521, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:38,  1.07it/s, pg=-0.0444, ret=7.98e-5, glen=46.4, tlen=206, kl=0.0521, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:38,  1.07it/s, pg=-0.0112, ret=0.000148, glen=45.2, tlen=205, kl=0.0536, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:36,  1.08it/s, pg=-0.0112, ret=0.000148, glen=45.2, tlen=205, kl=0.0536, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:36,  1.08it/s, pg=0.076, ret=-4.94e-5, glen=44.4, tlen=205, kl=0.0453, act_lr=1e-6, ent=0.905]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:36,  1.07it/s, pg=0.076, ret=-4.94e-5, glen=44.4, tlen=205, kl=0.0453, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:36,  1.07it/s, pg=-0.0583, ret=0.000193, glen=46.2, tlen=207, kl=0.0583, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:34,  1.10it/s, pg=-0.0583, ret=0.000193, glen=46.2, tlen=207, kl=0.0583, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:34,  1.10it/s, pg=-0.105, ret=0.000245, glen=44.9, tlen=205, kl=0.0687, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.13it/s, pg=-0.105, ret=0.000245, glen=44.9, tlen=205, kl=0.0687, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.13it/s, pg=0.0211, ret=3.9e-5, glen=46.4, tlen=207, kl=0.104, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.13it/s, pg=0.0211, ret=3.9e-5, glen=46.4, tlen=207, kl=0.104, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.13it/s, pg=0.0562, ret=-4.54e-5, glen=44.5, tlen=205, kl=0.055, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:31,  1.12it/s, pg=0.0562, ret=-4.54e-5, glen=44.5, tlen=205, kl=0.055, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:08<00:31,  1.12it/s, pg=0.0274, ret=-2.81e-5, glen=45, tlen=206, kl=0.0502, act_lr=1e-6, ent=0.883] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:30,  1.13it/s, pg=0.0274, ret=-2.81e-5, glen=45, tlen=206, kl=0.0502, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:30,  1.13it/s, pg=0.0464, ret=-0.000148, glen=46.3, tlen=207, kl=0.0529, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.14it/s, pg=0.0464, ret=-0.000148, glen=46.3, tlen=207, kl=0.0529, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.14it/s, pg=0.0609, ret=-0.000129, glen=46.7, tlen=207, kl=0.0634, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.15it/s, pg=0.0609, ret=-0.000129, glen=46.7, tlen=207, kl=0.0634, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.15it/s, pg=0.137, ret=-0.000329, glen=46.2, tlen=207, kl=0.0483, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.15it/s, pg=0.137, ret=-0.000329, glen=46.2, tlen=207, kl=0.0483, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.15it/s, pg=0.0516, ret=4.56e-5, glen=45.9, tlen=206, kl=0.054, act_lr=1e-6, ent=0.909]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:26,  1.14it/s, pg=0.0516, ret=4.56e-5, glen=45.9, tlen=206, kl=0.054, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:26,  1.14it/s, pg=0.113, ret=-9.85e-5, glen=45.5, tlen=206, kl=0.11, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:25,  1.15it/s, pg=0.113, ret=-9.85e-5, glen=45.5, tlen=206, kl=0.11, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:25,  1.15it/s, pg=0.0382, ret=-8.83e-5, glen=45.9, tlen=206, kl=0.0562, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.16it/s, pg=0.0382, ret=-8.83e-5, glen=45.9, tlen=206, kl=0.0562, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:14<00:24,  1.16it/s, pg=0.0956, ret=-0.000372, glen=45.3, tlen=205, kl=0.0527, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=0.0956, ret=-0.000372, glen=45.3, tlen=205, kl=0.0527, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=-0.0553, ret=0.000173, glen=44, tlen=204, kl=0.0872, act_lr=1e-6, ent=0.884]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.17it/s, pg=-0.0553, ret=0.000173, glen=44, tlen=204, kl=0.0872, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.17it/s, pg=-0.0455, ret=0.000152, glen=46.5, tlen=207, kl=0.053, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=-0.0455, ret=0.000152, glen=46.5, tlen=207, kl=0.053, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=0.0607, ret=-0.000209, glen=46.1, tlen=207, kl=0.049, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=0.0607, ret=-0.000209, glen=46.1, tlen=207, kl=0.049, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=-0.00232, ret=3.9e-6, glen=44.5, tlen=205, kl=0.0543, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.18it/s, pg=-0.00232, ret=3.9e-6, glen=44.5, tlen=205, kl=0.0543, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.18it/s, pg=-0.0221, ret=-1.2e-6, glen=44.1, tlen=204, kl=0.0637, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.18it/s, pg=-0.0221, ret=-1.2e-6, glen=44.1, tlen=204, kl=0.0637, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.18it/s, pg=-0.103, ret=0.000273, glen=47.2, tlen=208, kl=0.252, act_lr=1e-6, ent=1.07]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.103, ret=0.000273, glen=47.2, tlen=208, kl=0.252, act_lr=1e-6, ent=1.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:20<00:17,  1.17it/s, pg=0.139, ret=-0.000398, glen=45.2, tlen=205, kl=0.0539, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.15it/s, pg=0.139, ret=-0.000398, glen=45.2, tlen=205, kl=0.0539, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:21<00:17,  1.15it/s, pg=-0.0438, ret=2.47e-5, glen=47.4, tlen=208, kl=0.0493, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.16it/s, pg=-0.0438, ret=2.47e-5, glen=47.4, tlen=208, kl=0.0493, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.16it/s, pg=-0.0521, ret=7.96e-5, glen=46.2, tlen=206, kl=0.0553, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.16it/s, pg=-0.0521, ret=7.96e-5, glen=46.2, tlen=206, kl=0.0553, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.16it/s, pg=-0.0427, ret=1.77e-5, glen=45.3, tlen=205, kl=0.058, act_lr=1e-6, ent=0.885] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.17it/s, pg=-0.0427, ret=1.77e-5, glen=45.3, tlen=205, kl=0.058, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.17it/s, pg=-0.0142, ret=1.17e-5, glen=45.2, tlen=206, kl=0.053, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.17it/s, pg=-0.0142, ret=1.17e-5, glen=45.2, tlen=206, kl=0.053, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.17it/s, pg=0.0153, ret=-3.8e-5, glen=46.9, tlen=207, kl=0.0721, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=0.0153, ret=-3.8e-5, glen=46.9, tlen=207, kl=0.0721, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=0.028, ret=-0.000163, glen=45.2, tlen=205, kl=0.0588, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:14,  1.00s/it, pg=0.028, ret=-0.000163, glen=45.2, tlen=205, kl=0.0588, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:14,  1.00s/it, pg=0.0225, ret=-0.000102, glen=46.3, tlen=206, kl=0.0497, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:12,  1.05it/s, pg=0.0225, ret=-0.000102, glen=46.3, tlen=206, kl=0.0497, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:12,  1.05it/s, pg=0.101, ret=-0.000203, glen=44.5, tlen=205, kl=0.0649, act_lr=1e-6, ent=0.908] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:11,  1.08it/s, pg=0.101, ret=-0.000203, glen=44.5, tlen=205, kl=0.0649, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:11,  1.08it/s, pg=-0.0137, ret=-4.64e-6, glen=46.7, tlen=207, kl=0.0599, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.11it/s, pg=-0.0137, ret=-4.64e-6, glen=46.7, tlen=207, kl=0.0599, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:29<00:09,  1.11it/s, pg=0.0188, ret=-0.000173, glen=45.9, tlen=207, kl=0.0829, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.13it/s, pg=0.0188, ret=-0.000173, glen=45.9, tlen=207, kl=0.0829, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.13it/s, pg=-0.0291, ret=-3.83e-6, glen=45.8, tlen=206, kl=0.0551, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.14it/s, pg=-0.0291, ret=-3.83e-6, glen=45.8, tlen=206, kl=0.0551, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.14it/s, pg=-0.0025, ret=-7.49e-5, glen=45.6, tlen=206, kl=0.049, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.15it/s, pg=-0.0025, ret=-7.49e-5, glen=45.6, tlen=206, kl=0.049, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.15it/s, pg=-0.118, ret=0.000296, glen=45, tlen=205, kl=0.0515, act_lr=1e-6, ent=0.917]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.16it/s, pg=-0.118, ret=0.000296, glen=45, tlen=205, kl=0.0515, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.16it/s, pg=-0.0137, ret=0.000113, glen=44, tlen=204, kl=0.0457, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.0137, ret=0.000113, glen=44, tlen=204, kl=0.0457, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=0.0533, ret=-5.45e-5, glen=43.8, tlen=204, kl=0.161, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.0533, ret=-5.45e-5, glen=43.8, tlen=204, kl=0.161, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:34<00:04,  1.17it/s, pg=-0.113, ret=0.00028, glen=45.4, tlen=206, kl=0.0512, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.113, ret=0.00028, glen=45.4, tlen=206, kl=0.0512, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:35<00:03,  1.17it/s, pg=-0.0751, ret=0.000233, glen=46.6, tlen=207, kl=0.0781, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0751, ret=0.000233, glen=46.6, tlen=207, kl=0.0781, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0746, ret=0.000171, glen=45.3, tlen=206, kl=0.0596, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=-0.0746, ret=0.000171, glen=45.3, tlen=206, kl=0.0596, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=-0.0412, ret=0.000128, glen=45.9, tlen=206, kl=0.0533, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.17it/s, pg=-0.0412, ret=0.000128, glen=45.9, tlen=206, kl=0.0533, act_lr=1e-6, ent=0.887]
2025-07-24 19:32:59.391 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.76s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.17it/s, pg=-0.104, ret=0.000246, glen=47.6, tlen=208, kl=0.0544, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.11it/s, pg=-0.104, ret=0.000246, glen=47.6, tlen=208, kl=0.0544, act_lr=1e-6, ent=0.922]
2025-07-24 19:33:00.082 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 19:33:02.679 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 19:33:03.011 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.50s
2025-07-24 19:33:03.017 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0003534139588821766, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9187469870545143, 'kl': 0.06702937636264535, 'response_length': 45.66697533186092, 'total_length': 206.03120138478835, 'teacher_total_length': 218.01452317348748, 'return': 3.133727532681511e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [08:18<13:18, 99.80s/it][A2025-07-24 19:33:03.024 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:35,  1.79it/s, est. speed input: 324.74 toks/s, output: 34.09 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  24%|‚ñà‚ñà‚ñç       | 41/172 [00:01<00:01, 66.25it/s, est. speed input: 6580.15 toks/s, output: 1060.01 toks/s]Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 51/172 [00:01<00:01, 74.25it/s, est. speed input: 7487.66 toks/s, output: 1269.05 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/172 [00:01<00:00, 124.84it/s, est. speed input: 15338.39 toks/s, output: 3405.47 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:02<00:00, 51.34it/s, est. speed input: 12059.50 toks/s, output: 2841.15 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:02<00:00, 66.52it/s, est. speed input: 12059.50 toks/s, output: 2841.15 toks/s]
2025-07-24 19:33:08.028 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 192.8413,strategyqa_test/accuracy: 0.4949,eval_accuracy: 0.4949
2025-07-24 19:33:08.320 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:33:37.238 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:33:37.417 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:33:37.418 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 29.10s
2025-07-24 19:33:39.095 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0164,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 45.6913,std_num_tokens: 14.1919,avg_correct_num_tokens: 45.6755,std_correct_num_tokens: 14.1168,avg_incorrect_num_tokens: 47.1744,std_incorrect_num_tokens: 19.9885
2025-07-24 19:33:39.456 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.04s
2025-07-24 19:33:41.882 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.42s
2025-07-24 19:34:04.033 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 170
2025-07-24 19:34:04.033 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.15s
2025-07-24 19:34:05.253 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.77s
2025-07-24 19:34:05.254 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -3.564121502944652e-05, avg_kl: 0.1554400275735294, avg_response_length: 45.72071679059197, avg_orm_score: 0.0, avg_custom_rewards: -3.564121502944652e-05
2025-07-24 19:34:05.280 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter70_replay_buffer.jsonl
2025-07-24 19:34:06.357 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.08s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 144/172 [00:01<00:00, 129.84it/s, est. speed input: 14310.71 toks/s, output: 3170.13 toks/s][32m [repeated 42x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:04<00:00, 41.56it/s, est. speed input: 7528.62 toks/s, output: 1840.77 toks/s]  [32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s, pg=-0.00623, ret=0.000114, glen=46.7, tlen=207, kl=0.0521, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:00<00:41,  1.00it/s, pg=-0.00623, ret=0.000114, glen=46.7, tlen=207, kl=0.0521, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:41,  1.00it/s, pg=-0.0471, ret=3.59e-5, glen=44.3, tlen=204, kl=0.0687, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=-0.0471, ret=3.59e-5, glen=44.3, tlen=204, kl=0.0687, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=-0.0152, ret=-8.57e-7, glen=45.1, tlen=205, kl=0.0591, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.13it/s, pg=-0.0152, ret=-8.57e-7, glen=45.1, tlen=205, kl=0.0591, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.13it/s, pg=-0.0772, ret=0.000171, glen=44.6, tlen=205, kl=0.0811, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.15it/s, pg=-0.0772, ret=0.000171, glen=44.6, tlen=205, kl=0.0811, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.15it/s, pg=0.16, ret=-0.000288, glen=47, tlen=208, kl=0.0734, act_lr=1e-6, ent=0.887]    Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.14it/s, pg=0.16, ret=-0.000288, glen=47, tlen=208, kl=0.0734, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.14it/s, pg=-0.032, ret=2.39e-5, glen=45, tlen=205, kl=0.0649, act_lr=1e-6, ent=0.858]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.15it/s, pg=-0.032, ret=2.39e-5, glen=45, tlen=205, kl=0.0649, act_lr=1e-6, ent=0.858]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.15it/s, pg=-0.0621, ret=2.01e-7, glen=48.1, tlen=209, kl=0.0811, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.16it/s, pg=-0.0621, ret=2.01e-7, glen=48.1, tlen=209, kl=0.0811, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.16it/s, pg=-0.0668, ret=8.91e-5, glen=47.1, tlen=208, kl=0.214, act_lr=1e-6, ent=0.859] Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.15it/s, pg=-0.0668, ret=8.91e-5, glen=47.1, tlen=208, kl=0.214, act_lr=1e-6, ent=0.859]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.15it/s, pg=0.0849, ret=-0.000229, glen=44.7, tlen=205, kl=0.0575, act_lr=1e-6, ent=0.858]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.15it/s, pg=0.0849, ret=-0.000229, glen=44.7, tlen=205, kl=0.0575, act_lr=1e-6, ent=0.858]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=-0.0969, ret=0.000209, glen=48.9, tlen=209, kl=0.079, act_lr=1e-6, ent=0.867] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.14it/s, pg=-0.0969, ret=0.000209, glen=48.9, tlen=209, kl=0.079, act_lr=1e-6, ent=0.867]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.14it/s, pg=-0.0749, ret=0.000137, glen=44.3, tlen=205, kl=0.0577, act_lr=1e-6, ent=0.863]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.15it/s, pg=-0.0749, ret=0.000137, glen=44.3, tlen=205, kl=0.0577, act_lr=1e-6, ent=0.863]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.15it/s, pg=-0.0592, ret=0.000109, glen=47.2, tlen=208, kl=0.0604, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.16it/s, pg=-0.0592, ret=0.000109, glen=47.2, tlen=208, kl=0.0604, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.16it/s, pg=0.113, ret=-0.000258, glen=45.7, tlen=206, kl=1.06, act_lr=1e-6, ent=0.901]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:26,  1.14it/s, pg=0.113, ret=-0.000258, glen=45.7, tlen=206, kl=1.06, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:26,  1.14it/s, pg=0.00806, ret=-8.97e-6, glen=45.3, tlen=205, kl=0.0534, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:25,  1.15it/s, pg=0.00806, ret=-8.97e-6, glen=45.3, tlen=205, kl=0.0534, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:25,  1.15it/s, pg=0.108, ret=-0.000104, glen=45.1, tlen=205, kl=0.0574, act_lr=1e-6, ent=0.873] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.16it/s, pg=0.108, ret=-0.000104, glen=45.1, tlen=205, kl=0.0574, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.16it/s, pg=0.125, ret=-0.000144, glen=46.8, tlen=208, kl=0.472, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.16it/s, pg=0.125, ret=-0.000144, glen=46.8, tlen=208, kl=0.472, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=-0.0146, ret=0.000112, glen=46.8, tlen=207, kl=0.241, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.16it/s, pg=-0.0146, ret=0.000112, glen=46.8, tlen=207, kl=0.241, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.16it/s, pg=0.163, ret=-0.00021, glen=45.5, tlen=206, kl=0.0643, act_lr=1e-6, ent=0.881] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=0.163, ret=-0.00021, glen=45.5, tlen=206, kl=0.0643, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=0.022, ret=1.17e-5, glen=46.8, tlen=207, kl=0.0919, act_lr=1e-6, ent=0.853] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=0.022, ret=1.17e-5, glen=46.8, tlen=207, kl=0.0919, act_lr=1e-6, ent=0.853]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=0.0864, ret=-0.000311, glen=45.5, tlen=206, kl=0.0553, act_lr=1e-6, ent=0.866]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=0.0864, ret=-0.000311, glen=45.5, tlen=206, kl=0.0553, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=0.0857, ret=-0.000125, glen=45.6, tlen=206, kl=0.0529, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.18it/s, pg=0.0857, ret=-0.000125, glen=45.6, tlen=206, kl=0.0529, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.18it/s, pg=0.0625, ret=-0.000184, glen=46, tlen=206, kl=0.0538, act_lr=1e-6, ent=0.871]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=0.0625, ret=-0.000184, glen=46, tlen=206, kl=0.0538, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=0.144, ret=-0.000171, glen=43.6, tlen=204, kl=0.286, act_lr=1e-6, ent=0.872]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.17it/s, pg=0.144, ret=-0.000171, glen=43.6, tlen=204, kl=0.286, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=-0.024, ret=5.88e-5, glen=48.7, tlen=210, kl=0.248, act_lr=1e-6, ent=0.873] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.14it/s, pg=-0.024, ret=5.88e-5, glen=48.7, tlen=210, kl=0.248, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.14it/s, pg=-0.0317, ret=1.14e-5, glen=46.3, tlen=207, kl=0.053, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.15it/s, pg=-0.0317, ret=1.14e-5, glen=46.3, tlen=207, kl=0.053, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.15it/s, pg=-0.107, ret=0.000216, glen=47, tlen=208, kl=0.146, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.16it/s, pg=-0.107, ret=0.000216, glen=47, tlen=208, kl=0.146, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.16it/s, pg=0.0354, ret=-9.16e-5, glen=45.6, tlen=206, kl=0.0938, act_lr=1e-6, ent=0.879]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.17it/s, pg=0.0354, ret=-9.16e-5, glen=45.6, tlen=206, kl=0.0938, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.17it/s, pg=-0.0365, ret=0.000123, glen=44.3, tlen=205, kl=0.151, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=-0.0365, ret=0.000123, glen=44.3, tlen=205, kl=0.151, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=0.0234, ret=-2.76e-5, glen=46.1, tlen=206, kl=0.0569, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.00it/s, pg=0.0234, ret=-2.76e-5, glen=46.1, tlen=206, kl=0.0569, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.00it/s, pg=-0.11, ret=0.000222, glen=45, tlen=205, kl=0.0535, act_lr=1e-6, ent=0.863]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:12,  1.04it/s, pg=-0.11, ret=0.000222, glen=45, tlen=205, kl=0.0535, act_lr=1e-6, ent=0.863]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:12,  1.04it/s, pg=0.0682, ret=-0.000188, glen=45.5, tlen=206, kl=0.127, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:11,  1.04it/s, pg=0.0682, ret=-0.000188, glen=45.5, tlen=206, kl=0.127, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:28<00:11,  1.04it/s, pg=-0.0532, ret=0.000112, glen=44.6, tlen=205, kl=0.0681, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:10,  1.07it/s, pg=-0.0532, ret=0.000112, glen=44.6, tlen=205, kl=0.0681, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:29<00:10,  1.07it/s, pg=-0.0292, ret=5.26e-5, glen=46.6, tlen=206, kl=0.0549, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:09,  1.10it/s, pg=-0.0292, ret=5.26e-5, glen=46.6, tlen=206, kl=0.0549, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:09,  1.10it/s, pg=-0.0535, ret=7.51e-5, glen=44.3, tlen=204, kl=0.0548, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:08,  1.12it/s, pg=-0.0535, ret=7.51e-5, glen=44.3, tlen=204, kl=0.0548, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:08,  1.12it/s, pg=0.0705, ret=-0.00015, glen=46.9, tlen=208, kl=0.0759, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:07,  1.14it/s, pg=0.0705, ret=-0.00015, glen=46.9, tlen=208, kl=0.0759, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:07,  1.14it/s, pg=0.0377, ret=-9.7e-5, glen=45.3, tlen=206, kl=0.0686, act_lr=1e-6, ent=0.873] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.15it/s, pg=0.0377, ret=-9.7e-5, glen=45.3, tlen=206, kl=0.0686, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.15it/s, pg=-0.0667, ret=0.00012, glen=44.8, tlen=205, kl=0.0585, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.16it/s, pg=-0.0667, ret=0.00012, glen=44.8, tlen=205, kl=0.0585, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.16it/s, pg=-0.0566, ret=9.64e-5, glen=44.9, tlen=205, kl=0.0657, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=-0.0566, ret=9.64e-5, glen=44.9, tlen=205, kl=0.0657, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:34<00:04,  1.17it/s, pg=0.0433, ret=-0.000108, glen=46.2, tlen=207, kl=0.401, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=0.0433, ret=-0.000108, glen=46.2, tlen=207, kl=0.401, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:35<00:03,  1.17it/s, pg=-0.0552, ret=7.32e-5, glen=44, tlen=204, kl=0.0587, act_lr=1e-6, ent=0.89]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0552, ret=7.32e-5, glen=44, tlen=204, kl=0.0587, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.028, ret=-5.97e-6, glen=44.3, tlen=204, kl=0.952, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=-0.028, ret=-5.97e-6, glen=44.3, tlen=204, kl=0.952, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=-0.0927, ret=0.000147, glen=45.4, tlen=206, kl=0.348, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.18it/s, pg=-0.0927, ret=0.000147, glen=45.4, tlen=206, kl=0.348, act_lr=1e-6, ent=0.89]
2025-07-24 19:34:44.278 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.74s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.18it/s, pg=-0.0952, ret=0.000212, glen=46.6, tlen=207, kl=0.087, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=-0.0952, ret=0.000212, glen=46.6, tlen=207, kl=0.087, act_lr=1e-6, ent=0.919]
2025-07-24 19:34:45.118 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 19:34:47.750 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.63s
2025-07-24 19:34:48.098 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.68s
2025-07-24 19:34:48.111 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.001145490380220635, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8841877607412116, 'kl': 0.15477805913880813, 'response_length': 45.76401280247888, 'total_length': 206.0845524987509, 'teacher_total_length': 218.04197763842205, 'return': -4.015809471913895e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [10:03<11:51, 101.60s/it][A2025-07-24 19:34:48.161 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:35:16.739 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:35:16.925 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:35:16.926 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 28.77s
2025-07-24 19:35:18.713 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0164,avg_reflection_pattern_score: 0.0018,avg_pass_at_n: 1.0000,avg_num_tokens: 45.9812,std_num_tokens: 13.9419,avg_correct_num_tokens: 45.9590,std_correct_num_tokens: 13.8777,avg_incorrect_num_tokens: 47.7451,std_incorrect_num_tokens: 18.2481
2025-07-24 19:35:19.129 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.20s
2025-07-24 19:35:21.707 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.58s
2025-07-24 19:35:43.873 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 171
2025-07-24 19:35:43.874 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.16s
2025-07-24 19:35:45.205 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 19:35:45.205 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -2.1092039675925645e-05, avg_kl: 0.10923793859649122, avg_response_length: 46.00307939763655, avg_orm_score: 0.0, avg_custom_rewards: -2.1092039675925645e-05
2025-07-24 19:35:45.232 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter71_replay_buffer.jsonl
2025-07-24 19:35:46.298 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.07s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:01<?, ?it/s, pg=-0.0321, ret=3.02e-5, glen=44.4, tlen=205, kl=0.169, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0321, ret=3.02e-5, glen=44.4, tlen=205, kl=0.169, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:42,  1.01s/it, pg=-0.0459, ret=0.000149, glen=46.8, tlen=207, kl=0.0616, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=-0.0459, ret=0.000149, glen=46.8, tlen=207, kl=0.0616, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=-0.101, ret=0.000217, glen=46.1, tlen=206, kl=0.109, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.12it/s, pg=-0.101, ret=0.000217, glen=46.1, tlen=206, kl=0.109, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.12it/s, pg=0.0816, ret=-0.000175, glen=45.9, tlen=206, kl=0.0562, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:35,  1.11it/s, pg=0.0816, ret=-0.000175, glen=45.9, tlen=206, kl=0.0562, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:35,  1.11it/s, pg=-0.135, ret=0.000261, glen=45.3, tlen=206, kl=0.123, act_lr=1e-6, ent=0.9]    Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.12it/s, pg=-0.135, ret=0.000261, glen=45.3, tlen=206, kl=0.123, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.12it/s, pg=0.00262, ret=9.61e-5, glen=45.4, tlen=205, kl=0.0554, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.14it/s, pg=0.00262, ret=9.61e-5, glen=45.4, tlen=205, kl=0.0554, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.14it/s, pg=-0.093, ret=0.000224, glen=47, tlen=207, kl=0.127, act_lr=1e-6, ent=0.934]   Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.15it/s, pg=-0.093, ret=0.000224, glen=47, tlen=207, kl=0.127, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.15it/s, pg=0.176, ret=-0.0004, glen=47.5, tlen=208, kl=0.0565, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=0.176, ret=-0.0004, glen=47.5, tlen=208, kl=0.0565, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=0.0297, ret=-0.000131, glen=45, tlen=205, kl=0.0572, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.15it/s, pg=0.0297, ret=-0.000131, glen=45, tlen=205, kl=0.0572, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.15it/s, pg=0.107, ret=-0.000204, glen=45.1, tlen=205, kl=0.0616, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.16it/s, pg=0.107, ret=-0.000204, glen=45.1, tlen=205, kl=0.0616, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.16it/s, pg=-0.0566, ret=0.000126, glen=46.2, tlen=206, kl=0.0606, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.16it/s, pg=-0.0566, ret=0.000126, glen=46.2, tlen=206, kl=0.0606, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.16it/s, pg=0.0181, ret=-1.11e-6, glen=44.8, tlen=205, kl=0.0813, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.16it/s, pg=0.0181, ret=-1.11e-6, glen=44.8, tlen=205, kl=0.0813, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.16it/s, pg=0.133, ret=-0.000144, glen=47.5, tlen=207, kl=0.0825, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=0.133, ret=-0.000144, glen=47.5, tlen=207, kl=0.0825, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=-0.0281, ret=2.68e-5, glen=46.7, tlen=207, kl=0.0624, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=-0.0281, ret=2.68e-5, glen=46.7, tlen=207, kl=0.0624, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:24,  1.17it/s, pg=0.0431, ret=-0.000105, glen=46.1, tlen=206, kl=0.151, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=0.0431, ret=-0.000105, glen=46.1, tlen=206, kl=0.151, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:24,  1.15it/s, pg=-0.0214, ret=9.32e-5, glen=45.8, tlen=206, kl=0.0739, act_lr=1e-6, ent=0.862]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.16it/s, pg=-0.0214, ret=9.32e-5, glen=45.8, tlen=206, kl=0.0739, act_lr=1e-6, ent=0.862]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.16it/s, pg=0.056, ret=3.39e-5, glen=46.6, tlen=207, kl=0.301, act_lr=1e-6, ent=0.886]   Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.16it/s, pg=0.056, ret=3.39e-5, glen=46.6, tlen=207, kl=0.301, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.16it/s, pg=-0.0142, ret=3.97e-5, glen=45.3, tlen=206, kl=0.0554, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=-0.0142, ret=3.97e-5, glen=45.3, tlen=206, kl=0.0554, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=-0.000854, ret=2.03e-5, glen=46.7, tlen=207, kl=0.117, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=-0.000854, ret=2.03e-5, glen=46.7, tlen=207, kl=0.117, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=0.197, ret=-0.000507, glen=47.6, tlen=208, kl=0.178, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=0.197, ret=-0.000507, glen=47.6, tlen=208, kl=0.178, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=0.00415, ret=-1.86e-5, glen=45.3, tlen=206, kl=0.0568, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.17it/s, pg=0.00415, ret=-1.86e-5, glen=45.3, tlen=206, kl=0.0568, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.17it/s, pg=-0.0463, ret=9.83e-5, glen=46.5, tlen=207, kl=0.0569, act_lr=1e-6, ent=0.882] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.0463, ret=9.83e-5, glen=46.5, tlen=207, kl=0.0569, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.17it/s, pg=-0.0414, ret=0.000123, glen=45.6, tlen=206, kl=0.0541, act_lr=1e-6, ent=0.855]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.17it/s, pg=-0.0414, ret=0.000123, glen=45.6, tlen=206, kl=0.0541, act_lr=1e-6, ent=0.855]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.17it/s, pg=-0.107, ret=0.000188, glen=45.2, tlen=205, kl=0.0778, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.16it/s, pg=-0.107, ret=0.000188, glen=45.2, tlen=205, kl=0.0778, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.16it/s, pg=-0.0134, ret=2.13e-5, glen=47.7, tlen=209, kl=0.0957, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.16it/s, pg=-0.0134, ret=2.13e-5, glen=47.7, tlen=209, kl=0.0957, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.16it/s, pg=0.104, ret=-0.000276, glen=46.9, tlen=207, kl=0.145, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.17it/s, pg=0.104, ret=-0.000276, glen=46.9, tlen=207, kl=0.145, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.17it/s, pg=-0.0881, ret=0.000153, glen=44.9, tlen=205, kl=0.0608, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.17it/s, pg=-0.0881, ret=0.000153, glen=44.9, tlen=205, kl=0.0608, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.17it/s, pg=-0.113, ret=0.000233, glen=44.6, tlen=205, kl=0.0556, act_lr=1e-6, ent=0.895] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=-0.113, ret=0.000233, glen=44.6, tlen=205, kl=0.0556, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=0.107, ret=-0.000283, glen=47.1, tlen=207, kl=0.75, act_lr=1e-6, ent=0.877]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.07it/s, pg=0.107, ret=-0.000283, glen=47.1, tlen=207, kl=0.75, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.07it/s, pg=0.101, ret=-0.000172, glen=47.1, tlen=207, kl=0.0544, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.10it/s, pg=0.101, ret=-0.000172, glen=47.1, tlen=207, kl=0.0544, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.10it/s, pg=0.0796, ret=-5.73e-5, glen=46, tlen=206, kl=0.0615, act_lr=1e-6, ent=0.931]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=0.0796, ret=-5.73e-5, glen=46, tlen=206, kl=0.0615, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.00879, ret=-6.82e-5, glen=47.6, tlen=208, kl=0.0964, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.14it/s, pg=-0.00879, ret=-6.82e-5, glen=47.6, tlen=208, kl=0.0964, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=-0.00875, ret=-7.77e-5, glen=46.9, tlen=207, kl=0.127, act_lr=1e-6, ent=0.887] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=-0.00875, ret=-7.77e-5, glen=46.9, tlen=207, kl=0.127, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=0.00462, ret=-6.3e-5, glen=45.5, tlen=206, kl=0.0568, act_lr=1e-6, ent=0.884] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.15it/s, pg=0.00462, ret=-6.3e-5, glen=45.5, tlen=206, kl=0.0568, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.15it/s, pg=-0.0571, ret=9.53e-5, glen=46.2, tlen=207, kl=0.105, act_lr=1e-6, ent=0.872] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.16it/s, pg=-0.0571, ret=9.53e-5, glen=46.2, tlen=207, kl=0.105, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.16it/s, pg=-0.111, ret=0.000261, glen=46.4, tlen=206, kl=0.0571, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.17it/s, pg=-0.111, ret=0.000261, glen=46.4, tlen=206, kl=0.0571, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.17it/s, pg=-0.0546, ret=2.41e-6, glen=45.9, tlen=206, kl=0.0773, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.17it/s, pg=-0.0546, ret=2.41e-6, glen=45.9, tlen=206, kl=0.0773, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.17it/s, pg=-0.13, ret=0.000269, glen=43.4, tlen=204, kl=0.0773, act_lr=1e-6, ent=0.874] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=-0.13, ret=0.000269, glen=43.4, tlen=204, kl=0.0773, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.118, ret=-0.000175, glen=44.4, tlen=205, kl=0.0593, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.17it/s, pg=0.118, ret=-0.000175, glen=44.4, tlen=205, kl=0.0593, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.0975, ret=0.000212, glen=46, tlen=207, kl=0.059, act_lr=1e-6, ent=0.898]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=-0.0975, ret=0.000212, glen=46, tlen=207, kl=0.059, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0146, ret=-8.89e-5, glen=45.7, tlen=206, kl=0.0834, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=-0.0146, ret=-8.89e-5, glen=45.7, tlen=206, kl=0.0834, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=0.0649, ret=-0.000234, glen=45.8, tlen=206, kl=0.344, act_lr=1e-6, ent=0.878] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.17it/s, pg=0.0649, ret=-0.000234, glen=45.8, tlen=206, kl=0.344, act_lr=1e-6, ent=0.878]
2025-07-24 19:36:23.826 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.17it/s, pg=-0.0546, ret=0.000162, glen=45.6, tlen=206, kl=0.0536, act_lr=1e-6, ent=0.868]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.13it/s, pg=-0.0546, ret=0.000162, glen=45.6, tlen=206, kl=0.0536, act_lr=1e-6, ent=0.868]
2025-07-24 19:36:24.506 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:36:26.773 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-24 19:36:27.121 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 40.78s
2025-07-24 19:36:27.126 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0010600589042486146, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8942753922107608, 'kl': 0.10942042151162791, 'response_length': 46.00185030560161, 'total_length': 206.38743307424147, 'teacher_total_length': 218.37974016056504, 'return': -1.03067194749899e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [11:42<10:04, 100.75s/it][A2025-07-24 19:36:27.172 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:36:56.015 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:36:56.208 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:36:56.209 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 29.04s
2025-07-24 19:36:57.806 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0161,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 46.8430,std_num_tokens: 14.1746,avg_correct_num_tokens: 46.8789,std_correct_num_tokens: 14.1601,avg_incorrect_num_tokens: 44.1038,std_incorrect_num_tokens: 14.9836
2025-07-24 19:36:58.140 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.93s
2025-07-24 19:37:00.518 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.37s
2025-07-24 19:37:22.775 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 171
2025-07-24 19:37:22.775 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.25s
2025-07-24 19:37:23.957 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 19:37:23.958 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.4609420105204944e-05, avg_kl: 1.3007298519736843, avg_response_length: 46.864666832817925, avg_orm_score: 0.0, avg_custom_rewards: 3.4609420105204944e-05
2025-07-24 19:37:23.982 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter72_replay_buffer.jsonl
2025-07-24 19:37:25.038 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s, pg=-0.0749, ret=8.07e-5, glen=46.2, tlen=206, kl=0.305, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:00<00:41,  1.00it/s, pg=-0.0749, ret=8.07e-5, glen=46.2, tlen=206, kl=0.305, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:41,  1.00it/s, pg=-0.0457, ret=0.000103, glen=47.7, tlen=207, kl=0.066, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.09it/s, pg=-0.0457, ret=0.000103, glen=47.7, tlen=207, kl=0.066, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.09it/s, pg=0.0346, ret=-0.000107, glen=46.2, tlen=206, kl=0.286, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:36,  1.10it/s, pg=0.0346, ret=-0.000107, glen=46.2, tlen=206, kl=0.286, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:36,  1.10it/s, pg=-0.00671, ret=-4.19e-6, glen=47.4, tlen=207, kl=0.0662, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.13it/s, pg=-0.00671, ret=-4.19e-6, glen=47.4, tlen=207, kl=0.0662, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.13it/s, pg=0.0699, ret=-0.000154, glen=46.1, tlen=206, kl=0.164, act_lr=1e-6, ent=0.912]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.14it/s, pg=0.0699, ret=-0.000154, glen=46.1, tlen=206, kl=0.164, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.14it/s, pg=-0.104, ret=0.000205, glen=46.9, tlen=207, kl=0.101, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:33,  1.12it/s, pg=-0.104, ret=0.000205, glen=46.9, tlen=207, kl=0.101, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:33,  1.12it/s, pg=-0.0369, ret=6.65e-5, glen=46.9, tlen=207, kl=0.0698, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.13it/s, pg=-0.0369, ret=6.65e-5, glen=46.9, tlen=207, kl=0.0698, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.13it/s, pg=0.0336, ret=-7.97e-5, glen=46.3, tlen=207, kl=0.0703, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.15it/s, pg=0.0336, ret=-7.97e-5, glen=46.3, tlen=207, kl=0.0703, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.15it/s, pg=0.0841, ret=-0.000243, glen=48.4, tlen=208, kl=0.0652, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.16it/s, pg=0.0841, ret=-0.000243, glen=48.4, tlen=208, kl=0.0652, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.16it/s, pg=-0.000763, ret=-2.51e-5, glen=47.4, tlen=207, kl=0.743, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.16it/s, pg=-0.000763, ret=-2.51e-5, glen=47.4, tlen=207, kl=0.743, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.16it/s, pg=-0.0258, ret=-3.57e-5, glen=46.1, tlen=206, kl=0.0708, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.16it/s, pg=-0.0258, ret=-3.57e-5, glen=46.1, tlen=206, kl=0.0708, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.16it/s, pg=0.0422, ret=-4.71e-5, glen=47.8, tlen=207, kl=0.0741, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.16it/s, pg=0.0422, ret=-4.71e-5, glen=47.8, tlen=207, kl=0.0741, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.16it/s, pg=0.0316, ret=8.28e-5, glen=48, tlen=208, kl=0.0725, act_lr=1e-6, ent=0.911]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=0.0316, ret=8.28e-5, glen=48, tlen=208, kl=0.0725, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=0.149, ret=-0.000355, glen=45.8, tlen=205, kl=0.071, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=0.149, ret=-0.000355, glen=45.8, tlen=205, kl=0.071, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:13<00:24,  1.17it/s, pg=-0.0712, ret=0.000165, glen=46.9, tlen=207, kl=0.0708, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.17it/s, pg=-0.0712, ret=0.000165, glen=46.9, tlen=207, kl=0.0708, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.17it/s, pg=-0.0263, ret=6.78e-5, glen=46.7, tlen=207, kl=0.36, act_lr=1e-6, ent=0.917]   Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.17it/s, pg=-0.0263, ret=6.78e-5, glen=46.7, tlen=207, kl=0.36, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.17it/s, pg=-0.0643, ret=0.000149, glen=47, tlen=207, kl=0.076, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.17it/s, pg=-0.0643, ret=0.000149, glen=47, tlen=207, kl=0.076, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.17it/s, pg=-0.035, ret=8.45e-5, glen=47.4, tlen=207, kl=0.198, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=-0.035, ret=8.45e-5, glen=47.4, tlen=207, kl=0.198, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=0.144, ret=-7.18e-5, glen=46.2, tlen=206, kl=0.315, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.18it/s, pg=0.144, ret=-7.18e-5, glen=46.2, tlen=206, kl=0.315, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.18it/s, pg=0.0159, ret=-4.32e-5, glen=46.4, tlen=206, kl=27.5, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.18it/s, pg=0.0159, ret=-4.32e-5, glen=46.4, tlen=206, kl=27.5, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.18it/s, pg=0.127, ret=-0.000402, glen=46.8, tlen=207, kl=0.118, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:18,  1.18it/s, pg=0.127, ret=-0.000402, glen=46.8, tlen=207, kl=0.118, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:18,  1.18it/s, pg=-0.0977, ret=0.000235, glen=45.4, tlen=205, kl=0.0696, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.18it/s, pg=-0.0977, ret=0.000235, glen=45.4, tlen=205, kl=0.0696, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:17,  1.18it/s, pg=0.0386, ret=-8.88e-5, glen=48.4, tlen=209, kl=4.17, act_lr=1e-6, ent=0.905]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:16,  1.18it/s, pg=0.0386, ret=-8.88e-5, glen=48.4, tlen=209, kl=4.17, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:16,  1.18it/s, pg=0.000793, ret=-4.18e-6, glen=47.7, tlen=208, kl=0.0684, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.18it/s, pg=0.000793, ret=-4.18e-6, glen=47.7, tlen=208, kl=0.0684, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.18it/s, pg=-0.0394, ret=7.29e-5, glen=46.9, tlen=207, kl=18.4, act_lr=1e-6, ent=0.888]    Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.18it/s, pg=-0.0394, ret=7.29e-5, glen=46.9, tlen=207, kl=18.4, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.18it/s, pg=-0.073, ret=0.000184, glen=47.8, tlen=208, kl=0.0769, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.18it/s, pg=-0.073, ret=0.000184, glen=47.8, tlen=208, kl=0.0769, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.18it/s, pg=0.176, ret=-0.000287, glen=48.3, tlen=208, kl=0.0725, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.18it/s, pg=0.176, ret=-0.000287, glen=48.3, tlen=208, kl=0.0725, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.18it/s, pg=0.113, ret=-0.000259, glen=46.8, tlen=207, kl=0.0765, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.18it/s, pg=0.113, ret=-0.000259, glen=46.8, tlen=207, kl=0.0765, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.18it/s, pg=-0.0687, ret=0.000136, glen=45.8, tlen=206, kl=0.11, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.07it/s, pg=-0.0687, ret=0.000136, glen=45.8, tlen=206, kl=0.11, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.07it/s, pg=-0.137, ret=0.000317, glen=47.8, tlen=208, kl=0.155, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.10it/s, pg=-0.137, ret=0.000317, glen=47.8, tlen=208, kl=0.155, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.10it/s, pg=-0.085, ret=0.0002, glen=45.3, tlen=205, kl=0.0699, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:26<00:10,  1.12it/s, pg=-0.085, ret=0.0002, glen=45.3, tlen=205, kl=0.0699, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.0333, ret=0.000117, glen=47.9, tlen=208, kl=0.2, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.14it/s, pg=-0.0333, ret=0.000117, glen=47.9, tlen=208, kl=0.2, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.14it/s, pg=0.0593, ret=-0.000148, glen=47.3, tlen=207, kl=0.0624, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.15it/s, pg=0.0593, ret=-0.000148, glen=47.3, tlen=207, kl=0.0624, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.15it/s, pg=0.0322, ret=-2.53e-6, glen=45.9, tlen=206, kl=0.0673, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:07,  1.16it/s, pg=0.0322, ret=-2.53e-6, glen=45.9, tlen=206, kl=0.0673, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:07,  1.16it/s, pg=-0.0575, ret=0.000121, glen=46.6, tlen=206, kl=0.0718, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:06,  1.15it/s, pg=-0.0575, ret=0.000121, glen=46.6, tlen=206, kl=0.0718, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:06,  1.15it/s, pg=-0.0833, ret=0.000155, glen=45.3, tlen=205, kl=0.0679, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.15it/s, pg=-0.0833, ret=0.000155, glen=45.3, tlen=205, kl=0.0679, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.15it/s, pg=0.1, ret=-0.000309, glen=47.5, tlen=207, kl=0.846, act_lr=1e-6, ent=0.909]    Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.16it/s, pg=0.1, ret=-0.000309, glen=47.5, tlen=207, kl=0.846, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.16it/s, pg=0.0431, ret=-5.43e-5, glen=46.8, tlen=207, kl=0.0724, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:32<00:04,  1.17it/s, pg=0.0431, ret=-5.43e-5, glen=46.8, tlen=207, kl=0.0724, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.17it/s, pg=0.054, ret=-0.000166, glen=47.5, tlen=208, kl=0.0708, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.15it/s, pg=0.054, ret=-0.000166, glen=47.5, tlen=208, kl=0.0708, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.15it/s, pg=-0.0626, ret=0.000127, glen=46.7, tlen=207, kl=0.289, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.15it/s, pg=-0.0626, ret=0.000127, glen=46.7, tlen=207, kl=0.289, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.15it/s, pg=0.0969, ret=-0.000201, glen=45.1, tlen=205, kl=0.0703, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.16it/s, pg=0.0969, ret=-0.000201, glen=45.1, tlen=205, kl=0.0703, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.16it/s, pg=-0.0798, ret=0.00018, glen=46.3, tlen=206, kl=0.0701, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.16it/s, pg=-0.0798, ret=0.00018, glen=46.3, tlen=206, kl=0.0701, act_lr=1e-6, ent=0.939]
2025-07-24 19:38:02.617 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.16it/s, pg=-0.111, ret=0.000262, glen=47.5, tlen=208, kl=0.0646, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.13it/s, pg=-0.111, ret=0.000262, glen=47.5, tlen=208, kl=0.0646, act_lr=1e-6, ent=0.907]
2025-07-24 19:38:03.285 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 19:38:05.544 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.26s
2025-07-24 19:38:05.891 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 40.75s
2025-07-24 19:38:05.898 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0006132347639216933, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9210066601287487, 'kl': 1.3042957394622092, 'response_length': 46.87247857382131, 'total_length': 206.79356277820676, 'teacher_total_length': 218.76551322049872, 'return': 5.784071575806931e-07, 'policy_update_steps': 1.0}

Episode [6/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [13:21<08:20, 100.12s/it][A2025-07-24 19:38:05.940 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:38:35.975 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:38:36.161 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:38:36.162 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.22s
2025-07-24 19:38:37.980 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0012,avg_pass_at_n: 1.0000,avg_num_tokens: 48.7437,std_num_tokens: 14.6241,avg_correct_num_tokens: 48.7414,std_correct_num_tokens: 14.6226,avg_incorrect_num_tokens: 49.0508,std_incorrect_num_tokens: 14.8272
2025-07-24 19:38:38.384 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.22s
2025-07-24 19:38:40.777 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.39s
2025-07-24 19:39:03.379 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 173
2025-07-24 19:39:03.379 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.60s
2025-07-24 19:39:04.523 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.68s
2025-07-24 19:39:04.523 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 4.7308815417089904e-05, avg_kl: 5.317720093479046, avg_response_length: 48.77794856694393, avg_orm_score: 0.0, avg_custom_rewards: 4.7308815417089904e-05
2025-07-24 19:39:04.548 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter73_replay_buffer.jsonl
2025-07-24 19:39:05.662 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.12s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=-0.0471, ret=3.73e-5, glen=48.7, tlen=209, kl=0.0712, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.01it/s, pg=-0.0471, ret=3.73e-5, glen=48.7, tlen=209, kl=0.0712, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.01it/s, pg=-0.0298, ret=5.84e-5, glen=51.3, tlen=212, kl=0.0684, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=-0.0298, ret=5.84e-5, glen=51.3, tlen=212, kl=0.0684, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=-0.0869, ret=0.000149, glen=47.3, tlen=208, kl=0.0836, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.11it/s, pg=-0.0869, ret=0.000149, glen=47.3, tlen=208, kl=0.0836, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.11it/s, pg=-0.0448, ret=4.63e-5, glen=49.4, tlen=210, kl=0.0751, act_lr=1e-6, ent=0.937] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:36,  1.11it/s, pg=-0.0448, ret=4.63e-5, glen=49.4, tlen=210, kl=0.0751, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:36,  1.11it/s, pg=0.0289, ret=6.72e-6, glen=50.5, tlen=212, kl=0.0775, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.12it/s, pg=0.0289, ret=6.72e-6, glen=50.5, tlen=212, kl=0.0775, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.12it/s, pg=0.126, ret=-0.000156, glen=48.3, tlen=209, kl=0.0787, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=0.126, ret=-0.000156, glen=48.3, tlen=209, kl=0.0787, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=-0.0759, ret=0.000141, glen=50.6, tlen=211, kl=0.0736, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.15it/s, pg=-0.0759, ret=0.000141, glen=50.6, tlen=211, kl=0.0736, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.15it/s, pg=0.074, ret=-8.3e-5, glen=49.8, tlen=210, kl=0.0724, act_lr=1e-6, ent=0.888]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.16it/s, pg=0.074, ret=-8.3e-5, glen=49.8, tlen=210, kl=0.0724, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:31,  1.16it/s, pg=0.153, ret=-0.000296, glen=46.5, tlen=207, kl=0.0769, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:31,  1.13it/s, pg=0.153, ret=-0.000296, glen=46.5, tlen=207, kl=0.0769, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:31,  1.13it/s, pg=-0.0946, ret=0.000154, glen=48, tlen=209, kl=0.0768, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.14it/s, pg=-0.0946, ret=0.000154, glen=48, tlen=209, kl=0.0768, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.14it/s, pg=-0.0857, ret=0.000151, glen=47, tlen=207, kl=17.4, act_lr=1e-6, ent=0.925]  Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:29,  1.13it/s, pg=-0.0857, ret=0.000151, glen=47, tlen=207, kl=17.4, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:29,  1.13it/s, pg=0.0983, ret=-5.6e-5, glen=48.9, tlen=209, kl=130, act_lr=1e-6, ent=0.908] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.14it/s, pg=0.0983, ret=-5.6e-5, glen=48.9, tlen=209, kl=130, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.14it/s, pg=-0.0603, ret=4.95e-5, glen=49.2, tlen=210, kl=0.0768, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.15it/s, pg=-0.0603, ret=4.95e-5, glen=49.2, tlen=210, kl=0.0768, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.15it/s, pg=-0.00653, ret=2.34e-5, glen=49.6, tlen=210, kl=0.0786, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.16it/s, pg=-0.00653, ret=2.34e-5, glen=49.6, tlen=210, kl=0.0786, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.16it/s, pg=0.022, ret=-1.3e-5, glen=46.7, tlen=207, kl=0.0732, act_lr=1e-6, ent=0.888]   Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.16it/s, pg=0.022, ret=-1.3e-5, glen=46.7, tlen=207, kl=0.0732, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:24,  1.16it/s, pg=0.123, ret=-0.000211, glen=45.6, tlen=206, kl=0.08, act_lr=1e-6, ent=0.872]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=0.123, ret=-0.000211, glen=45.6, tlen=206, kl=0.08, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=0.035, ret=-4.87e-5, glen=49, tlen=210, kl=0.0682, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.035, ret=-4.87e-5, glen=49, tlen=210, kl=0.0682, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=0.0193, ret=4.09e-5, glen=49.3, tlen=210, kl=0.0719, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=0.0193, ret=4.09e-5, glen=49.3, tlen=210, kl=0.0719, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=0.0341, ret=-7.56e-5, glen=48.8, tlen=209, kl=0.0828, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=0.0341, ret=-7.56e-5, glen=48.8, tlen=209, kl=0.0828, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0818, ret=0.000147, glen=48.9, tlen=210, kl=0.077, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0818, ret=0.000147, glen=48.9, tlen=210, kl=0.077, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0143, ret=9.36e-6, glen=48, tlen=209, kl=11.5, act_lr=1e-6, ent=0.881]    Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0143, ret=9.36e-6, glen=48, tlen=209, kl=11.5, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.0648, ret=-6.54e-5, glen=50.5, tlen=211, kl=0.141, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:19,  1.15it/s, pg=0.0648, ret=-6.54e-5, glen=50.5, tlen=211, kl=0.141, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:19,  1.15it/s, pg=-0.042, ret=4.97e-5, glen=49.7, tlen=210, kl=0.0801, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=-0.042, ret=4.97e-5, glen=49.7, tlen=210, kl=0.0801, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=0.0616, ret=-9.47e-5, glen=50.3, tlen=211, kl=0.0702, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.16it/s, pg=0.0616, ret=-9.47e-5, glen=50.3, tlen=211, kl=0.0702, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.16it/s, pg=-0.107, ret=0.000178, glen=49.3, tlen=210, kl=0.07, act_lr=1e-6, ent=0.924]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.14it/s, pg=-0.107, ret=0.000178, glen=49.3, tlen=210, kl=0.07, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.14it/s, pg=0.0453, ret=-9.22e-5, glen=49.3, tlen=210, kl=0.0713, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.15it/s, pg=0.0453, ret=-9.22e-5, glen=49.3, tlen=210, kl=0.0713, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.15it/s, pg=-0.0972, ret=0.000167, glen=48, tlen=209, kl=0.0769, act_lr=1e-6, ent=0.893] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.16it/s, pg=-0.0972, ret=0.000167, glen=48, tlen=209, kl=0.0769, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.16it/s, pg=0.0695, ret=-0.000104, glen=48.5, tlen=209, kl=0.231, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.16it/s, pg=0.0695, ret=-0.000104, glen=48.5, tlen=209, kl=0.231, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.16it/s, pg=0.0973, ret=-9.29e-5, glen=47.7, tlen=208, kl=0.0862, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.06it/s, pg=0.0973, ret=-9.29e-5, glen=47.7, tlen=208, kl=0.0862, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.06it/s, pg=-0.071, ret=8.01e-5, glen=49, tlen=209, kl=65.6, act_lr=1e-6, ent=0.883]     Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.09it/s, pg=-0.071, ret=8.01e-5, glen=49, tlen=209, kl=65.6, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.09it/s, pg=0.219, ret=-0.000374, glen=47, tlen=208, kl=0.973, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:12,  1.08it/s, pg=0.219, ret=-0.000374, glen=47, tlen=208, kl=0.973, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:12,  1.08it/s, pg=0.111, ret=-7.76e-5, glen=49.3, tlen=209, kl=0.0731, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.11it/s, pg=0.111, ret=-7.76e-5, glen=49.3, tlen=209, kl=0.0731, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.11it/s, pg=0.0607, ret=-0.000169, glen=48.9, tlen=209, kl=0.0676, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.13it/s, pg=0.0607, ret=-0.000169, glen=48.9, tlen=209, kl=0.0676, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.13it/s, pg=-0.0311, ret=7.25e-5, glen=48.6, tlen=209, kl=0.373, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.14it/s, pg=-0.0311, ret=7.25e-5, glen=48.6, tlen=209, kl=0.373, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.14it/s, pg=-0.0582, ret=0.000106, glen=48.5, tlen=209, kl=0.0709, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.15it/s, pg=-0.0582, ret=0.000106, glen=48.5, tlen=209, kl=0.0709, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.15it/s, pg=0.0176, ret=-8e-5, glen=50, tlen=210, kl=0.776, act_lr=1e-6, ent=0.933]       Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=0.0176, ret=-8e-5, glen=50, tlen=210, kl=0.776, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=0.0353, ret=-9.59e-5, glen=48.9, tlen=209, kl=0.0784, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.16it/s, pg=0.0353, ret=-9.59e-5, glen=48.9, tlen=209, kl=0.0784, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.16it/s, pg=-0.0803, ret=0.000138, glen=49.8, tlen=210, kl=0.0768, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0803, ret=0.000138, glen=49.8, tlen=210, kl=0.0768, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=-0.0366, ret=6.38e-5, glen=49.8, tlen=211, kl=0.0798, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0366, ret=6.38e-5, glen=49.8, tlen=211, kl=0.0798, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.027, ret=5.48e-5, glen=50.4, tlen=211, kl=0.42, act_lr=1e-6, ent=0.903]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.027, ret=5.48e-5, glen=50.4, tlen=211, kl=0.42, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0669, ret=0.000128, glen=49.8, tlen=210, kl=0.0757, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0669, ret=0.000128, glen=49.8, tlen=210, kl=0.0757, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.0574, ret=7.94e-5, glen=48.2, tlen=209, kl=0.0839, act_lr=1e-6, ent=0.866] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.0574, ret=7.94e-5, glen=48.2, tlen=209, kl=0.0839, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.0907, ret=0.000152, glen=46.8, tlen=207, kl=0.0847, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.0907, ret=0.000152, glen=46.8, tlen=207, kl=0.0847, act_lr=1e-6, ent=0.909]
2025-07-24 19:39:44.305 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.44s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0203, ret=2.71e-5, glen=48.5, tlen=209, kl=0.0831, act_lr=1e-6, ent=0.867] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0203, ret=2.71e-5, glen=48.5, tlen=209, kl=0.0831, act_lr=1e-6, ent=0.867]
2025-07-24 19:39:45.131 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 19:39:47.722 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 19:39:48.053 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.31s
2025-07-24 19:39:48.058 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0018586245450106535, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9080252660946413, 'kl': 5.228428234430877, 'response_length': 48.817345879294656, 'total_length': 209.32486724853516, 'teacher_total_length': 221.29618870128286, 'return': 2.866572313822954e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [15:03<06:43, 100.76s/it][A2025-07-24 19:39:48.103 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:40:19.013 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:40:19.191 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:40:19.192 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.09s
2025-07-24 19:40:20.918 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0022,avg_pass_at_n: 1.0000,avg_num_tokens: 50.3376,std_num_tokens: 15.1986,avg_correct_num_tokens: 50.2902,std_correct_num_tokens: 15.1716,avg_incorrect_num_tokens: 55.0864,std_incorrect_num_tokens: 17.0455
2025-07-24 19:40:21.319 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.13s
2025-07-24 19:40:23.968 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.65s
2025-07-24 19:40:46.370 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 19:40:46.370 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.40s
2025-07-24 19:40:47.634 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 19:40:47.634 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.9523579407825896e-05, avg_kl: 3.0775188577586206, avg_response_length: 50.3528284314035, avg_orm_score: 0.0, avg_custom_rewards: 1.9523579407825896e-05
2025-07-24 19:40:47.662 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter74_replay_buffer.jsonl
2025-07-24 19:40:48.764 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.10s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.0498, ret=-7.92e-5, glen=49.4, tlen=209, kl=0.0839, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=0.0498, ret=-7.92e-5, glen=49.4, tlen=209, kl=0.0839, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=0.0822, ret=-0.000256, glen=50.9, tlen=211, kl=0.0974, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.08it/s, pg=0.0822, ret=-0.000256, glen=50.9, tlen=211, kl=0.0974, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.08it/s, pg=0.0132, ret=-8.91e-5, glen=49.9, tlen=210, kl=0.0865, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=0.0132, ret=-8.91e-5, glen=49.9, tlen=210, kl=0.0865, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=0.11, ret=-0.00013, glen=48.7, tlen=209, kl=0.0795, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:36,  1.11it/s, pg=0.11, ret=-0.00013, glen=48.7, tlen=209, kl=0.0795, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:36,  1.11it/s, pg=-0.0194, ret=2.99e-6, glen=49.8, tlen=210, kl=0.0922, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.13it/s, pg=-0.0194, ret=2.99e-6, glen=49.8, tlen=210, kl=0.0922, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.13it/s, pg=-0.0428, ret=0.000103, glen=50.3, tlen=211, kl=0.252, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.0428, ret=0.000103, glen=50.3, tlen=211, kl=0.252, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=-0.0153, ret=8.69e-5, glen=49, tlen=210, kl=0.0991, act_lr=1e-6, ent=0.926]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.15it/s, pg=-0.0153, ret=8.69e-5, glen=49, tlen=210, kl=0.0991, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.15it/s, pg=-0.0891, ret=0.000184, glen=49.5, tlen=210, kl=0.229, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=-0.0891, ret=0.000184, glen=49.5, tlen=210, kl=0.229, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.069, ret=-0.000175, glen=51.1, tlen=212, kl=0.0856, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=0.069, ret=-0.000175, glen=51.1, tlen=212, kl=0.0856, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.00195, ret=-1.65e-5, glen=51.8, tlen=212, kl=0.0936, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=0.00195, ret=-1.65e-5, glen=51.8, tlen=212, kl=0.0936, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=-0.0214, ret=3.18e-5, glen=50.3, tlen=210, kl=0.0918, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.0214, ret=3.18e-5, glen=50.3, tlen=210, kl=0.0918, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=0.0673, ret=-3.54e-5, glen=50.5, tlen=211, kl=0.129, act_lr=1e-6, ent=0.907] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.17it/s, pg=0.0673, ret=-3.54e-5, glen=50.5, tlen=211, kl=0.129, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.17it/s, pg=0.0638, ret=-3.04e-5, glen=50.5, tlen=210, kl=0.416, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.0638, ret=-3.04e-5, glen=50.5, tlen=210, kl=0.416, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=-0.0339, ret=1.79e-5, glen=49.7, tlen=210, kl=0.104, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=-0.0339, ret=1.79e-5, glen=49.7, tlen=210, kl=0.104, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=-0.0153, ret=1.05e-6, glen=52.8, tlen=214, kl=0.0818, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=-0.0153, ret=1.05e-6, glen=52.8, tlen=214, kl=0.0818, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=0.197, ret=-0.000427, glen=50.1, tlen=210, kl=0.0856, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:24,  1.16it/s, pg=0.197, ret=-0.000427, glen=50.1, tlen=210, kl=0.0856, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.15, ret=-0.000203, glen=49.8, tlen=210, kl=0.082, act_lr=1e-6, ent=0.927]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.15, ret=-0.000203, glen=49.8, tlen=210, kl=0.082, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.0455, ret=4.77e-5, glen=52, tlen=212, kl=0.0815, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.0455, ret=4.77e-5, glen=52, tlen=212, kl=0.0815, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.0137, ret=4.24e-5, glen=50.2, tlen=211, kl=0.0901, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.0137, ret=4.24e-5, glen=50.2, tlen=211, kl=0.0901, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0432, ret=9.58e-5, glen=49.9, tlen=210, kl=0.41, act_lr=1e-6, ent=0.926]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.15it/s, pg=-0.0432, ret=9.58e-5, glen=49.9, tlen=210, kl=0.41, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.15it/s, pg=-0.0562, ret=9.37e-5, glen=51.1, tlen=212, kl=0.0925, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.16it/s, pg=-0.0562, ret=9.37e-5, glen=51.1, tlen=212, kl=0.0925, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.16it/s, pg=-0.0356, ret=1.93e-6, glen=50.2, tlen=211, kl=0.0804, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.16it/s, pg=-0.0356, ret=1.93e-6, glen=50.2, tlen=211, kl=0.0804, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.16it/s, pg=-0.0767, ret=0.000149, glen=49, tlen=209, kl=0.0864, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.17it/s, pg=-0.0767, ret=0.000149, glen=49, tlen=209, kl=0.0864, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=0.0489, ret=-8.44e-5, glen=52.5, tlen=213, kl=0.111, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.0489, ret=-8.44e-5, glen=52.5, tlen=213, kl=0.111, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=0.0311, ret=-7.84e-6, glen=48.7, tlen=209, kl=0.0903, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.15it/s, pg=0.0311, ret=-7.84e-6, glen=48.7, tlen=209, kl=0.0903, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.15it/s, pg=0.155, ret=-0.00023, glen=52.7, tlen=213, kl=0.117, act_lr=1e-6, ent=0.949]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.15it/s, pg=0.155, ret=-0.00023, glen=52.7, tlen=213, kl=0.117, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.15it/s, pg=0.082, ret=-0.00014, glen=49.7, tlen=210, kl=0.0854, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.16it/s, pg=0.082, ret=-0.00014, glen=49.7, tlen=210, kl=0.0854, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.16it/s, pg=-0.0455, ret=0.000102, glen=49.9, tlen=210, kl=0.087, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.16it/s, pg=-0.0455, ret=0.000102, glen=49.9, tlen=210, kl=0.087, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.16it/s, pg=-0.031, ret=0.000137, glen=50.5, tlen=211, kl=0.0837, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.06it/s, pg=-0.031, ret=0.000137, glen=50.5, tlen=211, kl=0.0837, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.06it/s, pg=0.101, ret=-0.000128, glen=50.1, tlen=210, kl=0.0835, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.09it/s, pg=0.101, ret=-0.000128, glen=50.1, tlen=210, kl=0.0835, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.09it/s, pg=-0.0209, ret=8.7e-6, glen=49, tlen=209, kl=0.0878, act_lr=1e-6, ent=0.921]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0209, ret=8.7e-6, glen=49, tlen=209, kl=0.0878, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0431, ret=8.13e-5, glen=51.4, tlen=212, kl=0.0873, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.13it/s, pg=-0.0431, ret=8.13e-5, glen=51.4, tlen=212, kl=0.0873, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.0388, ret=-0.000157, glen=50.9, tlen=211, kl=0.0916, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.0388, ret=-0.000157, glen=50.9, tlen=211, kl=0.0916, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.11, ret=0.000217, glen=49, tlen=209, kl=129, act_lr=1e-6, ent=0.931]       Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=-0.11, ret=0.000217, glen=49, tlen=209, kl=129, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=-0.0802, ret=0.00012, glen=49.8, tlen=210, kl=0.0869, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0802, ret=0.00012, glen=49.8, tlen=210, kl=0.0869, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.124, ret=0.000216, glen=50.6, tlen=210, kl=0.0898, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=-0.124, ret=0.000216, glen=50.6, tlen=210, kl=0.0898, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.0898, ret=0.000187, glen=50.8, tlen=211, kl=0.107, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0898, ret=0.000187, glen=50.8, tlen=211, kl=0.107, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=0.0118, ret=-0.000105, glen=52.2, tlen=213, kl=0.0854, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0118, ret=-0.000105, glen=52.2, tlen=213, kl=0.0854, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0323, ret=1.56e-5, glen=51.2, tlen=211, kl=0.0911, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.0323, ret=1.56e-5, glen=51.2, tlen=211, kl=0.0911, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.102, ret=0.00021, glen=48.9, tlen=209, kl=0.0856, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.102, ret=0.00021, glen=48.9, tlen=209, kl=0.0856, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.204, ret=-0.000428, glen=50.8, tlen=211, kl=0.0913, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.18it/s, pg=0.204, ret=-0.000428, glen=50.8, tlen=211, kl=0.0913, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.18it/s, pg=-0.115, ret=0.000213, glen=50.1, tlen=210, kl=0.088, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=-0.115, ret=0.000213, glen=50.1, tlen=210, kl=0.088, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=-0.0453, ret=0.000159, glen=50.7, tlen=211, kl=0.0824, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=-0.0453, ret=0.000159, glen=50.7, tlen=211, kl=0.0824, act_lr=1e-6, ent=0.935]
2025-07-24 19:41:27.184 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.25s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=-0.0415, ret=0.000129, glen=49.1, tlen=209, kl=0.094, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=-0.0415, ret=0.000129, glen=49.1, tlen=209, kl=0.094, act_lr=1e-6, ent=0.924]
2025-07-24 19:41:27.869 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.63s
2025-07-24 19:41:30.192 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-24 19:41:30.540 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.73s
2025-07-24 19:41:30.546 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0019669966264204545, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.941788225011392, 'kl': 3.0443170720880683, 'response_length': 50.35667688196356, 'total_length': 210.60434133356267, 'teacher_total_length': 222.59318473122337, 'return': -1.5994411755441846e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [16:45<05:03, 101.29s/it][A2025-07-24 19:41:30.597 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:42:01.495 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:42:01.679 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:42:01.679 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.08s
2025-07-24 19:42:03.540 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0024,avg_pass_at_n: 1.0000,avg_num_tokens: 50.8660,std_num_tokens: 14.7097,avg_correct_num_tokens: 50.8520,std_correct_num_tokens: 14.6935,avg_incorrect_num_tokens: 52.6936,std_incorrect_num_tokens: 16.5992
2025-07-24 19:42:03.829 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.15s
2025-07-24 19:42:06.285 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.45s
2025-07-24 19:42:28.939 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 19:42:28.939 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.65s
2025-07-24 19:42:30.180 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.80s
2025-07-24 19:42:30.181 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.5008082655923706e-05, avg_kl: 71.25785435267858, avg_response_length: 50.89450402396066, avg_orm_score: 0.0, avg_custom_rewards: 2.5008082655923706e-05
2025-07-24 19:42:30.210 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter75_replay_buffer.jsonl
2025-07-24 19:42:31.367 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=-0.123, ret=0.000213, glen=51.4, tlen=212, kl=0.0883, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.01it/s, pg=-0.123, ret=0.000213, glen=51.4, tlen=212, kl=0.0883, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.01it/s, pg=0.118, ret=-0.000192, glen=50.6, tlen=212, kl=0.0857, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.10it/s, pg=0.118, ret=-0.000192, glen=50.6, tlen=212, kl=0.0857, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.10it/s, pg=-0.0291, ret=2.93e-5, glen=50.4, tlen=211, kl=0.0828, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:37,  1.09it/s, pg=-0.0291, ret=2.93e-5, glen=50.4, tlen=211, kl=0.0828, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:37,  1.09it/s, pg=0.0305, ret=-0.000111, glen=51.6, tlen=212, kl=0.0815, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.12it/s, pg=0.0305, ret=-0.000111, glen=51.6, tlen=212, kl=0.0815, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.12it/s, pg=-0.035, ret=2.65e-5, glen=50.6, tlen=211, kl=0.0851, act_lr=1e-6, ent=0.937]  Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.14it/s, pg=-0.035, ret=2.65e-5, glen=50.6, tlen=211, kl=0.0851, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.14it/s, pg=-0.0189, ret=3.19e-5, glen=51.6, tlen=212, kl=3.1e+3, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.15it/s, pg=-0.0189, ret=3.19e-5, glen=51.6, tlen=212, kl=3.1e+3, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.15it/s, pg=0.0691, ret=-5.31e-5, glen=51, tlen=212, kl=0.0857, act_lr=1e-6, ent=0.958]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.16it/s, pg=0.0691, ret=-5.31e-5, glen=51, tlen=212, kl=0.0857, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.16it/s, pg=0.113, ret=-0.000204, glen=49.3, tlen=210, kl=0.0778, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:30,  1.16it/s, pg=0.113, ret=-0.000204, glen=49.3, tlen=210, kl=0.0778, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:30,  1.16it/s, pg=-0.0415, ret=6.01e-5, glen=49.5, tlen=210, kl=0.0858, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.17it/s, pg=-0.0415, ret=6.01e-5, glen=49.5, tlen=210, kl=0.0858, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.17it/s, pg=0.0115, ret=-6.54e-5, glen=52.4, tlen=213, kl=0.0754, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=0.0115, ret=-6.54e-5, glen=52.4, tlen=213, kl=0.0754, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=-0.0215, ret=3.45e-5, glen=51.5, tlen=212, kl=0.0775, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.0215, ret=3.45e-5, glen=51.5, tlen=212, kl=0.0775, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=-0.0842, ret=0.000156, glen=51.1, tlen=212, kl=0.0868, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=-0.0842, ret=0.000156, glen=51.1, tlen=212, kl=0.0868, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.0233, ret=2.21e-5, glen=50.5, tlen=211, kl=2.44, act_lr=1e-6, ent=0.898]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:27,  1.14it/s, pg=-0.0233, ret=2.21e-5, glen=50.5, tlen=211, kl=2.44, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:27,  1.14it/s, pg=-0.0132, ret=5.55e-5, glen=49.4, tlen=210, kl=0.0844, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=-0.0132, ret=5.55e-5, glen=49.4, tlen=210, kl=0.0844, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=-0.0414, ret=6.09e-5, glen=50.9, tlen=212, kl=0.11, act_lr=1e-6, ent=0.963]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.16it/s, pg=-0.0414, ret=6.09e-5, glen=50.9, tlen=212, kl=0.11, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.16it/s, pg=-0.0466, ret=4.95e-5, glen=50.6, tlen=211, kl=0.0842, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:24,  1.17it/s, pg=-0.0466, ret=4.95e-5, glen=50.6, tlen=211, kl=0.0842, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.17it/s, pg=-0.0911, ret=0.000154, glen=49.4, tlen=210, kl=0.0938, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=-0.0911, ret=0.000154, glen=49.4, tlen=210, kl=0.0938, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.052, ret=7.69e-5, glen=49.7, tlen=210, kl=0.0905, act_lr=1e-6, ent=0.939]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.052, ret=7.69e-5, glen=49.7, tlen=210, kl=0.0905, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.00208, ret=2.83e-5, glen=50.9, tlen=211, kl=0.0901, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.00208, ret=2.83e-5, glen=50.9, tlen=211, kl=0.0901, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0826, ret=0.000145, glen=49.9, tlen=211, kl=0.0831, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0826, ret=0.000145, glen=49.9, tlen=211, kl=0.0831, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=0.0072, ret=0.00014, glen=52.3, tlen=213, kl=0.895, act_lr=1e-6, ent=0.944]   Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.18it/s, pg=0.0072, ret=0.00014, glen=52.3, tlen=213, kl=0.895, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.18it/s, pg=0.0989, ret=-4.46e-5, glen=52.8, tlen=213, kl=0.0872, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.18it/s, pg=0.0989, ret=-4.46e-5, glen=52.8, tlen=213, kl=0.0872, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.18it/s, pg=-0.0885, ret=0.000159, glen=50, tlen=211, kl=0.0977, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.18it/s, pg=-0.0885, ret=0.000159, glen=50, tlen=211, kl=0.0977, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=0.129, ret=-0.00016, glen=48.9, tlen=209, kl=0.0839, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:16,  1.18it/s, pg=0.129, ret=-0.00016, glen=48.9, tlen=209, kl=0.0839, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:16,  1.18it/s, pg=0.0386, ret=-5.4e-5, glen=52, tlen=213, kl=0.0781, act_lr=1e-6, ent=0.919]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.18it/s, pg=0.0386, ret=-5.4e-5, glen=52, tlen=213, kl=0.0781, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.18it/s, pg=-0.00659, ret=3.14e-5, glen=49.3, tlen=210, kl=0.138, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.18it/s, pg=-0.00659, ret=3.14e-5, glen=49.3, tlen=210, kl=0.138, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.18it/s, pg=-0.0829, ret=0.000149, glen=51.2, tlen=212, kl=0.436, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.18it/s, pg=-0.0829, ret=0.000149, glen=51.2, tlen=212, kl=0.436, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.18it/s, pg=-0.0524, ret=5.84e-5, glen=51.7, tlen=212, kl=0.0846, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.18it/s, pg=-0.0524, ret=5.84e-5, glen=51.7, tlen=212, kl=0.0846, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.18it/s, pg=0.0798, ret=-8.38e-5, glen=51.8, tlen=212, kl=0.0775, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=0.0798, ret=-8.38e-5, glen=51.8, tlen=212, kl=0.0775, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.0723, ret=-6.13e-5, glen=50.2, tlen=211, kl=0.0857, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.0723, ret=-6.13e-5, glen=50.2, tlen=211, kl=0.0857, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.0634, ret=0.000117, glen=50.1, tlen=211, kl=0.0808, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:26<00:11,  1.12it/s, pg=-0.0634, ret=0.000117, glen=50.1, tlen=211, kl=0.0808, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0491, ret=5.15e-5, glen=52.6, tlen=213, kl=0.0782, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=-0.0491, ret=5.15e-5, glen=52.6, tlen=213, kl=0.0782, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.0809, ret=-0.000177, glen=51.9, tlen=213, kl=0.087, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.0809, ret=-0.000177, glen=51.9, tlen=213, kl=0.087, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=0.163, ret=-0.000237, glen=51.4, tlen=212, kl=6.38, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=0.163, ret=-0.000237, glen=51.4, tlen=212, kl=6.38, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=0.0942, ret=-0.000288, glen=52.9, tlen=214, kl=0.0836, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.0942, ret=-0.000288, glen=52.9, tlen=214, kl=0.0836, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.084, ret=0.000166, glen=51.5, tlen=212, kl=0.0828, act_lr=1e-6, ent=0.989] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.084, ret=0.000166, glen=51.5, tlen=212, kl=0.0828, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=0.0732, ret=-0.000168, glen=50, tlen=211, kl=0.0873, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.17it/s, pg=0.0732, ret=-0.000168, glen=50, tlen=211, kl=0.0873, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.17it/s, pg=0.00537, ret=-5.85e-5, glen=52.7, tlen=214, kl=0.0819, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:32<00:05,  1.17it/s, pg=0.00537, ret=-5.85e-5, glen=52.7, tlen=214, kl=0.0819, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.00439, ret=-3.89e-5, glen=50.2, tlen=211, kl=0.0767, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.00439, ret=-3.89e-5, glen=50.2, tlen=211, kl=0.0767, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.116, ret=-0.000197, glen=49.7, tlen=210, kl=0.0802, act_lr=1e-6, ent=0.937]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.116, ret=-0.000197, glen=49.7, tlen=210, kl=0.0802, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0774, ret=0.000133, glen=49.7, tlen=210, kl=0.0829, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0774, ret=0.000133, glen=49.7, tlen=210, kl=0.0829, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.016, ret=3.12e-5, glen=50.5, tlen=211, kl=0.0818, act_lr=1e-6, ent=0.945]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.016, ret=3.12e-5, glen=50.5, tlen=211, kl=0.0818, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.0995, ret=0.000172, glen=52.9, tlen=214, kl=0.0759, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.0995, ret=0.000172, glen=52.9, tlen=214, kl=0.0759, act_lr=1e-6, ent=0.955]
2025-07-24 19:43:09.720 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.15s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.01, ret=-1.59e-5, glen=50.6, tlen=211, kl=0.0886, act_lr=1e-6, ent=0.938]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=-0.01, ret=-1.59e-5, glen=50.6, tlen=211, kl=0.0886, act_lr=1e-6, ent=0.938]
2025-07-24 19:43:10.583 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 19:43:13.102 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 19:43:13.431 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.99s
2025-07-24 19:43:13.437 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0008891712535511364, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9468247158960863, 'kl': 70.85351286218925, 'response_length': 50.88531121340665, 'total_length': 211.52352211692116, 'teacher_total_length': 223.49848487160423, 'return': 3.2222656872446267e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [18:28<03:23, 101.78s/it][A2025-07-24 19:43:13.483 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:43:44.653 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:43:44.844 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:43:44.844 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.36s
2025-07-24 19:43:46.528 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0022,avg_pass_at_n: 1.0000,avg_num_tokens: 51.0844,std_num_tokens: 15.0860,avg_correct_num_tokens: 51.0577,std_correct_num_tokens: 15.0860,avg_incorrect_num_tokens: 54.4688,std_incorrect_num_tokens: 14.7054
2025-07-24 19:43:46.953 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.11s
2025-07-24 19:43:49.579 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.62s
2025-07-24 19:44:12.368 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 19:44:12.369 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.79s
2025-07-24 19:44:13.632 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 19:44:13.633 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4438678377440998e-05, avg_kl: 0.19234095982142857, avg_response_length: 51.11669871738979, avg_orm_score: 0.0, avg_custom_rewards: 1.4438678377440998e-05
2025-07-24 19:44:13.658 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter76_replay_buffer.jsonl
2025-07-24 19:44:14.771 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.11s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.0249, ret=-0.000109, glen=49.9, tlen=210, kl=0.912, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=0.0249, ret=-0.000109, glen=49.9, tlen=210, kl=0.912, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=-0.00806, ret=1.68e-5, glen=51.8, tlen=212, kl=0.101, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=-0.00806, ret=1.68e-5, glen=51.8, tlen=212, kl=0.101, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=0.00165, ret=-7.26e-5, glen=51.3, tlen=212, kl=0.0876, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=0.00165, ret=-7.26e-5, glen=51.3, tlen=212, kl=0.0876, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=0.186, ret=-0.000277, glen=51.5, tlen=212, kl=0.0902, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:34,  1.14it/s, pg=0.186, ret=-0.000277, glen=51.5, tlen=212, kl=0.0902, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:34,  1.14it/s, pg=0.00476, ret=4.98e-5, glen=52.7, tlen=213, kl=0.101, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:33,  1.15it/s, pg=0.00476, ret=4.98e-5, glen=52.7, tlen=213, kl=0.101, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:33,  1.15it/s, pg=-0.00781, ret=-1.59e-5, glen=51.4, tlen=211, kl=0.0889, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.13it/s, pg=-0.00781, ret=-1.59e-5, glen=51.4, tlen=211, kl=0.0889, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.13it/s, pg=0.26, ret=-0.000344, glen=49.7, tlen=210, kl=0.0909, act_lr=1e-6, ent=0.914]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.14it/s, pg=0.26, ret=-0.000344, glen=49.7, tlen=210, kl=0.0909, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.14it/s, pg=-0.0658, ret=7.4e-5, glen=50.4, tlen=211, kl=0.0978, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=-0.0658, ret=7.4e-5, glen=50.4, tlen=211, kl=0.0978, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=-0.0197, ret=3.15e-5, glen=51.5, tlen=212, kl=0.0916, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.13it/s, pg=-0.0197, ret=3.15e-5, glen=51.5, tlen=212, kl=0.0916, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=0.0253, ret=-6.37e-5, glen=50.6, tlen=211, kl=0.0884, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.14it/s, pg=0.0253, ret=-6.37e-5, glen=50.6, tlen=211, kl=0.0884, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.14it/s, pg=0.0747, ret=-6.41e-5, glen=50.8, tlen=211, kl=0.101, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.15it/s, pg=0.0747, ret=-6.41e-5, glen=50.8, tlen=211, kl=0.101, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.15it/s, pg=0.012, ret=-4.42e-5, glen=51.6, tlen=212, kl=0.121, act_lr=1e-6, ent=0.903] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=0.012, ret=-4.42e-5, glen=51.6, tlen=212, kl=0.121, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.0785, ret=0.000138, glen=52.3, tlen=213, kl=0.085, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.16it/s, pg=-0.0785, ret=0.000138, glen=52.3, tlen=213, kl=0.085, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.16it/s, pg=0.0958, ret=-0.000162, glen=50.4, tlen=210, kl=0.0927, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.14it/s, pg=0.0958, ret=-0.000162, glen=50.4, tlen=210, kl=0.0927, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.14it/s, pg=-0.07, ret=0.000133, glen=51.1, tlen=211, kl=0.107, act_lr=1e-6, ent=0.952]   Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.15it/s, pg=-0.07, ret=0.000133, glen=51.1, tlen=211, kl=0.107, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.15it/s, pg=-0.0807, ret=0.000148, glen=54, tlen=214, kl=0.0914, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.0807, ret=0.000148, glen=54, tlen=214, kl=0.0914, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.00391, ret=-4.01e-5, glen=52.3, tlen=212, kl=0.0924, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.14it/s, pg=0.00391, ret=-4.01e-5, glen=52.3, tlen=212, kl=0.0924, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.14it/s, pg=-0.0536, ret=5.48e-5, glen=50.6, tlen=211, kl=0.0948, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.15it/s, pg=-0.0536, ret=5.48e-5, glen=50.6, tlen=211, kl=0.0948, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.15it/s, pg=0.046, ret=-9.21e-5, glen=52, tlen=212, kl=0.117, act_lr=1e-6, ent=0.932]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=0.046, ret=-9.21e-5, glen=52, tlen=212, kl=0.117, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=-0.0869, ret=0.000156, glen=50, tlen=210, kl=0.0934, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.16it/s, pg=-0.0869, ret=0.000156, glen=50, tlen=210, kl=0.0934, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.16it/s, pg=0.0215, ret=-6.9e-5, glen=51.6, tlen=212, kl=1.49, act_lr=1e-6, ent=0.946]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=0.0215, ret=-6.9e-5, glen=51.6, tlen=212, kl=1.49, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=-0.0854, ret=0.00015, glen=51.3, tlen=211, kl=0.0927, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.0854, ret=0.00015, glen=51.3, tlen=211, kl=0.0927, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.17it/s, pg=-0.0802, ret=0.000138, glen=52.8, tlen=213, kl=0.0906, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.0802, ret=0.000138, glen=52.8, tlen=213, kl=0.0906, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=0.0718, ret=-0.000101, glen=50.6, tlen=211, kl=0.0913, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.0718, ret=-0.000101, glen=50.6, tlen=211, kl=0.0913, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=0.187, ret=-0.000258, glen=51, tlen=211, kl=0.089, act_lr=1e-6, ent=0.944]    Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=0.187, ret=-0.000258, glen=51, tlen=211, kl=0.089, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.0619, ret=-7.9e-5, glen=50.6, tlen=211, kl=0.102, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.0619, ret=-7.9e-5, glen=50.6, tlen=211, kl=0.102, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.0138, ret=-3.11e-5, glen=50.5, tlen=211, kl=0.142, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.0138, ret=-3.11e-5, glen=50.5, tlen=211, kl=0.142, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.0663, ret=0.000117, glen=51.6, tlen=211, kl=0.0922, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.0663, ret=0.000117, glen=51.6, tlen=211, kl=0.0922, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.0825, ret=0.000144, glen=52.2, tlen=212, kl=0.113, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=-0.0825, ret=0.000144, glen=52.2, tlen=212, kl=0.113, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.144, ret=-0.00025, glen=49.7, tlen=210, kl=0.107, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.144, ret=-0.00025, glen=49.7, tlen=210, kl=0.107, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=0.00714, ret=2.07e-5, glen=50.7, tlen=211, kl=1.5, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.00714, ret=2.07e-5, glen=50.7, tlen=211, kl=1.5, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.12it/s, pg=-0.0654, ret=0.000119, glen=49.3, tlen=209, kl=0.095, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=-0.0654, ret=0.000119, glen=49.3, tlen=209, kl=0.095, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.0874, ret=-0.000161, glen=49.3, tlen=209, kl=0.108, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.0874, ret=-0.000161, glen=49.3, tlen=209, kl=0.108, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.0284, ret=4.95e-5, glen=48.8, tlen=209, kl=0.0978, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.0284, ret=4.95e-5, glen=48.8, tlen=209, kl=0.0978, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=-0.0833, ret=0.000144, glen=50.1, tlen=210, kl=0.118, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0833, ret=0.000144, glen=50.1, tlen=210, kl=0.118, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0996, ret=0.000177, glen=52.3, tlen=212, kl=0.611, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=-0.0996, ret=0.000177, glen=52.3, tlen=212, kl=0.611, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.104, ret=0.000174, glen=51.4, tlen=212, kl=0.0917, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.104, ret=0.000174, glen=51.4, tlen=212, kl=0.0917, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=-0.105, ret=0.000192, glen=49.9, tlen=210, kl=0.1, act_lr=1e-6, ent=0.952]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.105, ret=0.000192, glen=49.9, tlen=210, kl=0.1, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0187, ret=-1.41e-5, glen=52.7, tlen=213, kl=0.087, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.18it/s, pg=-0.0187, ret=-1.41e-5, glen=52.7, tlen=213, kl=0.087, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.18it/s, pg=0.0557, ret=-0.000154, glen=51.8, tlen=212, kl=0.1, act_lr=1e-6, ent=0.947]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0557, ret=-0.000154, glen=51.8, tlen=212, kl=0.1, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0164, ret=3.4e-5, glen=50.6, tlen=211, kl=0.0948, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0164, ret=3.4e-5, glen=50.6, tlen=211, kl=0.0948, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.058, ret=-9.05e-5, glen=50.1, tlen=210, kl=0.1, act_lr=1e-6, ent=0.938]   Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=0.058, ret=-9.05e-5, glen=50.1, tlen=210, kl=0.1, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=-0.0205, ret=6.35e-5, glen=51.7, tlen=212, kl=0.087, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.0205, ret=6.35e-5, glen=51.7, tlen=212, kl=0.087, act_lr=1e-6, ent=0.929]
2025-07-24 19:44:53.266 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.32s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0818, ret=0.000119, glen=52.3, tlen=212, kl=0.098, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0818, ret=0.000119, glen=52.3, tlen=212, kl=0.098, act_lr=1e-6, ent=0.969]
2025-07-24 19:44:54.153 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-24 19:44:56.655 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.50s
2025-07-24 19:44:56.989 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.17s
2025-07-24 19:44:56.995 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0008014332164417614, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9394291828979145, 'kl': 0.19183904338966717, 'response_length': 51.107694365761496, 'total_length': 211.2680875604803, 'teacher_total_length': 223.3126244978471, 'return': -1.0741350293260026e-06, 'policy_update_steps': 1.0}

Episode [6/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [20:12<01:42, 102.32s/it][A2025-07-24 19:44:57.001 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:42,  1.66it/s, est. speed input: 305.79 toks/s, output: 34.90 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/172 [00:02<00:00, 98.65it/s, est. speed input: 13535.49 toks/s, output: 3413.36 toks/s] 
2025-07-24 19:45:00.624 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 228.6157,strategyqa_test/accuracy: 0.5473,eval_accuracy: 0.5473
2025-07-24 19:45:00.911 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:45:17.504 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:45:17.683 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:45:17.684 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 16.77s
2025-07-24 19:45:18.799 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0019,avg_pass_at_n: 1.0000,avg_num_tokens: 51.7148,std_num_tokens: 14.5321,avg_correct_num_tokens: 51.7553,std_correct_num_tokens: 14.5347,avg_incorrect_num_tokens: 43.8636,std_incorrect_num_tokens: 11.6044
2025-07-24 19:45:19.067 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.38s
2025-07-24 19:45:20.488 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.42s
2025-07-24 19:45:32.593 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 92
2025-07-24 19:45:32.593 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.10s
2025-07-24 19:45:33.523 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.49s
2025-07-24 19:45:33.523 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.1600447919057764e-05, avg_kl: 0.15175462805706522, avg_response_length: 51.79310491810674, avg_orm_score: 0.0, avg_custom_rewards: 2.1600447919057764e-05
2025-07-24 19:45:33.545 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter77_replay_buffer.jsonl
2025-07-24 19:45:34.126 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.58s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:02<00:00, 104.78it/s, est. speed input: 12680.68 toks/s, output: 3274.58 toks/s][32m [repeated 51x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:02<00:00, 60.89it/s, est. speed input: 11064.06 toks/s, output: 3013.63 toks/s][32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s, pg=-0.0323, ret=6.77e-5, glen=54, tlen=215, kl=0.106, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:00<00:21,  1.03it/s, pg=-0.0323, ret=6.77e-5, glen=54, tlen=215, kl=0.106, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:01<00:21,  1.03it/s, pg=0.166, ret=-0.000218, glen=52.1, tlen=212, kl=0.139, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:01<00:19,  1.10it/s, pg=0.166, ret=-0.000218, glen=52.1, tlen=212, kl=0.139, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:02<00:19,  1.10it/s, pg=0.0881, ret=-0.000117, glen=52.4, tlen=213, kl=0.114, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:02<00:18,  1.09it/s, pg=0.0881, ret=-0.000117, glen=52.4, tlen=213, kl=0.114, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:03<00:18,  1.09it/s, pg=-0.0768, ret=0.00012, glen=53.1, tlen=213, kl=0.117, act_lr=1e-6, ent=0.973] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:03<00:16,  1.12it/s, pg=-0.0768, ret=0.00012, glen=53.1, tlen=213, kl=0.117, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:04<00:16,  1.12it/s, pg=-0.0631, ret=0.000113, glen=52.3, tlen=213, kl=0.118, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:04<00:15,  1.14it/s, pg=-0.0631, ret=0.000113, glen=52.3, tlen=213, kl=0.118, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:05<00:15,  1.14it/s, pg=-0.0341, ret=2.11e-5, glen=49.8, tlen=210, kl=0.127, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:05<00:14,  1.15it/s, pg=-0.0341, ret=2.11e-5, glen=49.8, tlen=210, kl=0.127, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:06<00:14,  1.15it/s, pg=0.0348, ret=-3.16e-5, glen=50.9, tlen=211, kl=0.109, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:06<00:13,  1.15it/s, pg=0.0348, ret=-3.16e-5, glen=50.9, tlen=211, kl=0.109, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:07<00:13,  1.15it/s, pg=-0.0423, ret=6.48e-5, glen=54, tlen=214, kl=0.125, act_lr=1e-6, ent=0.951]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:12,  1.16it/s, pg=-0.0423, ret=6.48e-5, glen=54, tlen=214, kl=0.125, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:12,  1.16it/s, pg=0.0287, ret=-5.1e-6, glen=50.2, tlen=210, kl=0.104, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:07<00:12,  1.16it/s, pg=0.0287, ret=-5.1e-6, glen=50.2, tlen=210, kl=0.104, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:08<00:12,  1.16it/s, pg=0.134, ret=-0.000245, glen=52.4, tlen=212, kl=0.12, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:08<00:11,  1.14it/s, pg=0.134, ret=-0.000245, glen=52.4, tlen=212, kl=0.12, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:09<00:11,  1.14it/s, pg=-0.0613, ret=9.29e-5, glen=51.4, tlen=211, kl=0.105, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:09<00:10,  1.13it/s, pg=-0.0613, ret=9.29e-5, glen=51.4, tlen=211, kl=0.105, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:10<00:10,  1.13it/s, pg=-0.00061, ret=6.14e-6, glen=50.2, tlen=210, kl=0.13, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:10<00:09,  1.14it/s, pg=-0.00061, ret=6.14e-6, glen=50.2, tlen=210, kl=0.13, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:11<00:09,  1.14it/s, pg=0.0295, ret=-7.25e-5, glen=51.6, tlen=212, kl=0.123, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:11<00:08,  1.15it/s, pg=0.0295, ret=-7.25e-5, glen=51.6, tlen=212, kl=0.123, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:12<00:08,  1.15it/s, pg=0.0383, ret=3.11e-7, glen=52.3, tlen=212, kl=0.128, act_lr=1e-6, ent=0.993] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:12<00:07,  1.16it/s, pg=0.0383, ret=3.11e-7, glen=52.3, tlen=212, kl=0.128, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:13<00:07,  1.16it/s, pg=-0.0453, ret=8.37e-5, glen=51.3, tlen=211, kl=0.121, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=-0.0453, ret=8.37e-5, glen=51.3, tlen=211, kl=0.121, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=-0.0802, ret=0.000128, glen=52.9, tlen=213, kl=0.156, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:13<00:06,  1.16it/s, pg=-0.0802, ret=0.000128, glen=52.9, tlen=213, kl=0.156, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:14<00:06,  1.16it/s, pg=-0.0224, ret=8.84e-6, glen=51.3, tlen=211, kl=0.116, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:14<00:05,  1.17it/s, pg=-0.0224, ret=8.84e-6, glen=51.3, tlen=211, kl=0.116, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:15<00:05,  1.17it/s, pg=-0.0336, ret=1.65e-5, glen=51.3, tlen=211, kl=0.118, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:15<00:04,  1.17it/s, pg=-0.0336, ret=1.65e-5, glen=51.3, tlen=211, kl=0.118, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:16<00:04,  1.17it/s, pg=0.0377, ret=-6.45e-5, glen=50.2, tlen=210, kl=0.124, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:16<00:03,  1.17it/s, pg=0.0377, ret=-6.45e-5, glen=50.2, tlen=210, kl=0.124, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:17<00:03,  1.17it/s, pg=0.00061, ret=-9.13e-6, glen=53.3, tlen=213, kl=0.111, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:17<00:02,  1.17it/s, pg=0.00061, ret=-9.13e-6, glen=53.3, tlen=213, kl=0.111, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:18<00:02,  1.17it/s, pg=-0.043, ret=6.42e-5, glen=51.8, tlen=212, kl=0.132, act_lr=1e-6, ent=0.956]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:18<00:01,  1.17it/s, pg=-0.043, ret=6.42e-5, glen=51.8, tlen=212, kl=0.132, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:19<00:01,  1.17it/s, pg=-0.0776, ret=0.000125, glen=51.3, tlen=211, kl=0.11, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:19<00:00,  1.16it/s, pg=-0.0776, ret=0.000125, glen=51.3, tlen=211, kl=0.11, act_lr=1e-6, ent=0.934]
2025-07-24 19:45:54.333 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:20<00:00,  1.16it/s, pg=0.074, ret=-0.000118, glen=51, tlen=211, kl=0.838, act_lr=1e-6, ent=0.91]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:20<00:00,  1.10it/s, pg=0.074, ret=-0.000118, glen=51, tlen=211, kl=0.838, act_lr=1e-6, ent=0.91]
2025-07-24 19:45:55.005 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:45:57.398 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.39s
2025-07-24 19:45:57.748 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 23.59s
2025-07-24 19:45:57.751 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0007994278617527174, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9575821446335834, 'kl': 0.15175462805706522, 'response_length': 51.79310508396315, 'total_length': 211.7740306024966, 'teacher_total_length': 223.64458100692084, 'return': 1.3483742567705517e-06, 'policy_update_steps': 1.0}

Episode [6/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [21:13<00:00, 89.73s/it] [A[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7c0076540>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,710] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,711] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:24:44,709] [INFO] [utils.py:782:see_memory_usage] MA 4.41 GB         Max_MA 4.41 GB         CA 5.44 GB         Max_CA 5 GB 2025-07-24 19:46:06.714 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:26:20,973] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 19:24:44,150] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:27:59,473] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:29:40,246] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:31:19,271] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:32:59,383] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:34:44,271] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:36:23,818] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:38:02,609] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:39:44,298] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:41:27,177] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:44:53,259] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:45:54,326] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:04,480] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:04,667] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 2373, num_elems = 12.44B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,188] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,189] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,196] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,197] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,469] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,470] [INFO] [utils.py:782:see_memory_usage] MA 5.14 GB         Max_MA 10.1 GB         CA 6.17 GB         Max_CA 48 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,470] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.9 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [7/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [6/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [21:22<00:00, 98.62s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,708] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,709] [INFO] [utils.py:782:see_memory_usage] MA 5.14 GB         Max_MA 5.14 GB         CA 6.17 GB         Max_CA 6 GB 
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,709] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.9 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:999:print] DeepSpeedEngine configuration:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7b40dfb30>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,711] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,712] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 19:46:07.041 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:46:39.808 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:46:39.986 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:46:39.987 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.95s
2025-07-24 19:46:41.692 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0029,avg_pass_at_n: 1.0000,avg_num_tokens: 53.8165,std_num_tokens: 15.9768,avg_correct_num_tokens: 53.7906,std_correct_num_tokens: 15.9602,avg_incorrect_num_tokens: 58.8571,std_incorrect_num_tokens: 18.2477
2025-07-24 19:46:42.115 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.13s
2025-07-24 19:46:44.847 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.73s
2025-07-24 19:47:08.435 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 177
2025-07-24 19:47:08.435 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.59s
2025-07-24 19:47:10.334 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 19:47:10.334 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4165003315708731e-05, avg_kl: 0.0, avg_response_length: 53.84345898385775, avg_orm_score: 0.0, avg_custom_rewards: 1.4165003315708731e-05
2025-07-24 19:47:10.363 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter78_replay_buffer.jsonl
2025-07-24 19:47:11.519 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.00262, ret=-4.45e-6, glen=52.7, tlen=213, kl=0, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:46,  1.06s/it, pg=-0.00262, ret=-4.45e-6, glen=52.7, tlen=213, kl=0, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:46,  1.06s/it, pg=-0.0167, ret=-6.99e-7, glen=53.6, tlen=214, kl=0, act_lr=1e-6, ent=0.972] Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:40,  1.06it/s, pg=-0.0167, ret=-6.99e-7, glen=53.6, tlen=214, kl=0, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:40,  1.06it/s, pg=-0.0925, ret=0.000138, glen=53.7, tlen=215, kl=0, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.11it/s, pg=-0.0925, ret=0.000138, glen=53.7, tlen=215, kl=0, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.11it/s, pg=-0.0737, ret=0.000113, glen=52.8, tlen=214, kl=0, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.13it/s, pg=-0.0737, ret=0.000113, glen=52.8, tlen=214, kl=0, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.13it/s, pg=-0.00757, ret=5.19e-6, glen=53.3, tlen=214, kl=0, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:34,  1.15it/s, pg=-0.00757, ret=5.19e-6, glen=53.3, tlen=214, kl=0, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:34,  1.15it/s, pg=0.129, ret=-0.000144, glen=53.6, tlen=214, kl=0, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=0.129, ret=-0.000144, glen=53.6, tlen=214, kl=0, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=0.0239, ret=-2.25e-5, glen=52.5, tlen=213, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=0.0239, ret=-2.25e-5, glen=52.5, tlen=213, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=-0.0225, ret=2.75e-5, glen=55.2, tlen=216, kl=0, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=-0.0225, ret=2.75e-5, glen=55.2, tlen=216, kl=0, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=-0.00171, ret=2.75e-6, glen=55.3, tlen=216, kl=0, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.15it/s, pg=-0.00171, ret=2.75e-6, glen=55.3, tlen=216, kl=0, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=-0.0704, ret=0.000101, glen=54.8, tlen=215, kl=0, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=-0.0704, ret=0.000101, glen=54.8, tlen=215, kl=0, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=0.0622, ret=-0.000123, glen=54.9, tlen=216, kl=0, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=0.0622, ret=-0.000123, glen=54.9, tlen=216, kl=0, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.0577, ret=9.03e-5, glen=55, tlen=216, kl=0, act_lr=1e-6, ent=0.951]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.0577, ret=9.03e-5, glen=55, tlen=216, kl=0, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=0.026, ret=9.88e-6, glen=53.9, tlen=215, kl=0, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=0.026, ret=9.88e-6, glen=53.9, tlen=215, kl=0, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0547, ret=-1.95e-5, glen=54.2, tlen=215, kl=0, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0547, ret=-1.95e-5, glen=54.2, tlen=215, kl=0, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.0253, ret=1.63e-5, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0253, ret=1.63e-5, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.24, ret=-0.000347, glen=51.9, tlen=212, kl=0, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=0.24, ret=-0.000347, glen=51.9, tlen=212, kl=0, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.0831, ret=-0.000216, glen=53.1, tlen=214, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=0.0831, ret=-0.000216, glen=53.1, tlen=214, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0599, ret=8.85e-5, glen=52.9, tlen=213, kl=0, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=-0.0599, ret=8.85e-5, glen=52.9, tlen=213, kl=0, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=-0.0593, ret=8.49e-5, glen=51.9, tlen=213, kl=0, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.0593, ret=8.49e-5, glen=51.9, tlen=213, kl=0, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0686, ret=9.85e-5, glen=54.8, tlen=216, kl=0, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0686, ret=9.85e-5, glen=54.8, tlen=216, kl=0, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=0.0148, ret=-9.63e-7, glen=54.2, tlen=215, kl=0, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=0.0148, ret=-9.63e-7, glen=54.2, tlen=215, kl=0, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=0.114, ret=-0.000216, glen=54.8, tlen=216, kl=0, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.114, ret=-0.000216, glen=54.8, tlen=216, kl=0, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.0634, ret=-1.82e-5, glen=53.2, tlen=213, kl=0, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=0.0634, ret=-1.82e-5, glen=53.2, tlen=213, kl=0, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0765, ret=0.000106, glen=56, tlen=217, kl=0, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0765, ret=0.000106, glen=56, tlen=217, kl=0, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.00378, ret=-1.89e-5, glen=53, tlen=214, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.00378, ret=-1.89e-5, glen=53, tlen=214, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0292, ret=3.29e-5, glen=53.4, tlen=214, kl=0, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0292, ret=3.29e-5, glen=53.4, tlen=214, kl=0, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.0209, ret=-3.23e-5, glen=54.9, tlen=215, kl=0, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=0.0209, ret=-3.23e-5, glen=54.9, tlen=215, kl=0, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0215, ret=-1.17e-5, glen=56.7, tlen=217, kl=0, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.16it/s, pg=-0.0215, ret=-1.17e-5, glen=56.7, tlen=217, kl=0, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.16it/s, pg=-0.095, ret=0.00014, glen=55.8, tlen=217, kl=0, act_lr=1e-6, ent=0.905]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.06it/s, pg=-0.095, ret=0.00014, glen=55.8, tlen=217, kl=0, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.06it/s, pg=0.0718, ret=-0.000146, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=0.0718, ret=-0.000146, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=0.0139, ret=-2.08e-5, glen=53, tlen=214, kl=0, act_lr=1e-6, ent=0.963]   Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=0.0139, ret=-2.08e-5, glen=53, tlen=214, kl=0, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=0.096, ret=-0.000127, glen=55.1, tlen=215, kl=0, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.13it/s, pg=0.096, ret=-0.000127, glen=55.1, tlen=215, kl=0, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.0688, ret=0.000101, glen=54.2, tlen=215, kl=0, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.14it/s, pg=-0.0688, ret=0.000101, glen=54.2, tlen=215, kl=0, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=0.022, ret=3.69e-6, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.954]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=0.022, ret=3.69e-6, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=-0.0745, ret=0.000113, glen=54.3, tlen=215, kl=0, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0745, ret=0.000113, glen=54.3, tlen=215, kl=0, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.194, ret=-0.000142, glen=54.9, tlen=216, kl=0, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=0.194, ret=-0.000142, glen=54.9, tlen=216, kl=0, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0867, ret=0.000116, glen=55.8, tlen=217, kl=0, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0867, ret=0.000116, glen=55.8, tlen=217, kl=0, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0219, ret=1.76e-5, glen=53.2, tlen=214, kl=0, act_lr=1e-6, ent=0.913]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=0.0219, ret=1.76e-5, glen=53.2, tlen=214, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0927, ret=0.000139, glen=54.5, tlen=215, kl=0, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0927, ret=0.000139, glen=54.5, tlen=215, kl=0, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.00848, ret=-2.88e-5, glen=51.9, tlen=213, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.00848, ret=-2.88e-5, glen=51.9, tlen=213, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0552, ret=8.3e-5, glen=53.7, tlen=215, kl=0, act_lr=1e-6, ent=0.938]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0552, ret=8.3e-5, glen=53.7, tlen=215, kl=0, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0225, ret=2.07e-5, glen=52.5, tlen=213, kl=0, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0225, ret=2.07e-5, glen=52.5, tlen=213, kl=0, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=0.00464, ret=-1.32e-5, glen=53.2, tlen=214, kl=0, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.00464, ret=-1.32e-5, glen=53.2, tlen=214, kl=0, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0308, ret=1.68e-5, glen=54.8, tlen=216, kl=0, act_lr=1e-6, ent=0.966] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0308, ret=1.68e-5, glen=54.8, tlen=216, kl=0, act_lr=1e-6, ent=0.966]
2025-07-24 19:47:50.813 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.08s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0719, ret=9.86e-5, glen=53.3, tlen=214, kl=0, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.0719, ret=9.86e-5, glen=53.3, tlen=214, kl=0, act_lr=1e-6, ent=0.921]
2025-07-24 19:47:51.502 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 19:47:53.824 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-24 19:47:54.162 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.55s
2025-07-24 19:47:54.167 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0008714463975694445, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9443010846773784, 'kl': 0.0, 'response_length': 53.89007822672526, 'total_length': 214.6049814860026, 'teacher_total_length': 226.58916354709203, 'return': 2.4386400077168624e-06, 'policy_update_steps': 1.0}
Episode [7/20]:   8%|‚ñä         | 1/13 [01:47<21:29, 107.45s/it]2025-07-24 19:47:54.210 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:48:27.221 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:48:27.401 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:48:27.401 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.19s
2025-07-24 19:48:29.284 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0034,avg_pass_at_n: 1.0000,avg_num_tokens: 55.0422,std_num_tokens: 15.7713,avg_correct_num_tokens: 55.0204,std_correct_num_tokens: 15.7679,avg_incorrect_num_tokens: 58.2143,std_incorrect_num_tokens: 15.9539
2025-07-24 19:48:29.694 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.29s
2025-07-24 19:48:32.106 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.41s
2025-07-24 19:48:55.188 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 19:48:55.188 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.08s
2025-07-24 19:48:56.414 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 19:48:56.415 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.599764496674029e-07, avg_kl: 0.0012327901433023174, avg_response_length: 55.101541433441504, avg_orm_score: 0.0, avg_custom_rewards: 3.599764496674029e-07
2025-07-24 19:48:56.439 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter79_replay_buffer.jsonl
2025-07-24 19:48:57.625 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0326, ret=3.72e-5, glen=57.1, tlen=217, kl=0.00118, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0326, ret=3.72e-5, glen=57.1, tlen=217, kl=0.00118, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.0225, ret=1.99e-5, glen=54.6, tlen=214, kl=0.00114, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=0.0225, ret=1.99e-5, glen=54.6, tlen=214, kl=0.00114, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=0.156, ret=-0.000333, glen=55.3, tlen=215, kl=0.00127, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.12it/s, pg=0.156, ret=-0.000333, glen=55.3, tlen=215, kl=0.00127, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.12it/s, pg=-0.032, ret=2.95e-5, glen=56, tlen=216, kl=0.00131, act_lr=1e-6, ent=0.971]   Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:35,  1.14it/s, pg=-0.032, ret=2.95e-5, glen=56, tlen=216, kl=0.00131, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:35,  1.14it/s, pg=0.0138, ret=-5.59e-5, glen=55.9, tlen=216, kl=0.00124, act_lr=1e-6, ent=1] Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.08it/s, pg=0.0138, ret=-5.59e-5, glen=55.9, tlen=216, kl=0.00124, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.08it/s, pg=-0.0918, ret=0.000146, glen=54.2, tlen=214, kl=0.00116, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:35,  1.09it/s, pg=-0.0918, ret=0.000146, glen=54.2, tlen=214, kl=0.00116, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:35,  1.09it/s, pg=0.0338, ret=-8.84e-5, glen=52.6, tlen=212, kl=0.00124, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:34,  1.12it/s, pg=0.0338, ret=-8.84e-5, glen=52.6, tlen=212, kl=0.00124, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:34,  1.12it/s, pg=-0.0302, ret=4.59e-5, glen=53.3, tlen=213, kl=0.00125, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=-0.0302, ret=4.59e-5, glen=53.3, tlen=213, kl=0.00125, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.13it/s, pg=0.147, ret=-0.000223, glen=57.6, tlen=217, kl=0.00121, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=0.147, ret=-0.000223, glen=57.6, tlen=217, kl=0.00121, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=-0.0609, ret=0.000106, glen=55.4, tlen=215, kl=0.00135, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=-0.0609, ret=0.000106, glen=55.4, tlen=215, kl=0.00135, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=-0.0253, ret=1.06e-5, glen=52.7, tlen=213, kl=0.00115, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=-0.0253, ret=1.06e-5, glen=52.7, tlen=213, kl=0.00115, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=0.144, ret=-0.000205, glen=54.4, tlen=214, kl=0.00136, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=0.144, ret=-0.000205, glen=54.4, tlen=214, kl=0.00136, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=-0.0429, ret=3.89e-5, glen=53.9, tlen=213, kl=0.00117, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.0429, ret=3.89e-5, glen=53.9, tlen=213, kl=0.00117, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0348, ret=-5.31e-5, glen=54.7, tlen=215, kl=0.00125, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0348, ret=-5.31e-5, glen=54.7, tlen=215, kl=0.00125, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.00769, ret=9.23e-6, glen=55.5, tlen=215, kl=0.00135, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.00769, ret=9.23e-6, glen=55.5, tlen=215, kl=0.00135, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.17it/s, pg=0.0944, ret=-0.000191, glen=55.6, tlen=215, kl=0.00131, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.0944, ret=-0.000191, glen=55.6, tlen=215, kl=0.00131, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.005, ret=4.53e-5, glen=53.5, tlen=213, kl=0.00118, act_lr=1e-6, ent=0.947]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=0.005, ret=4.53e-5, glen=53.5, tlen=213, kl=0.00118, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0964, ret=0.000172, glen=54.5, tlen=214, kl=0.00121, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.14it/s, pg=-0.0964, ret=0.000172, glen=54.5, tlen=214, kl=0.00121, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.14it/s, pg=-0.0769, ret=0.000137, glen=55.7, tlen=215, kl=0.00125, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.15it/s, pg=-0.0769, ret=0.000137, glen=55.7, tlen=215, kl=0.00125, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.15it/s, pg=0.153, ret=-0.000212, glen=55.1, tlen=215, kl=0.00123, act_lr=1e-6, ent=0.955] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=0.153, ret=-0.000212, glen=55.1, tlen=215, kl=0.00123, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=-0.0319, ret=5.79e-6, glen=54, tlen=214, kl=0.00123, act_lr=1e-6, ent=0.935]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:21,  1.14it/s, pg=-0.0319, ret=5.79e-6, glen=54, tlen=214, kl=0.00123, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:21,  1.14it/s, pg=0.0441, ret=-3.53e-7, glen=56.6, tlen=216, kl=0.00121, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.15it/s, pg=0.0441, ret=-3.53e-7, glen=56.6, tlen=216, kl=0.00121, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.15it/s, pg=0.0894, ret=-0.000203, glen=54.9, tlen=214, kl=0.00121, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.16it/s, pg=0.0894, ret=-0.000203, glen=54.9, tlen=214, kl=0.00121, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.16it/s, pg=-0.06, ret=0.0001, glen=53.4, tlen=213, kl=0.00123, act_lr=1e-6, ent=0.956]    Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.16it/s, pg=-0.06, ret=0.0001, glen=53.4, tlen=213, kl=0.00123, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.16it/s, pg=0.0645, ret=-0.00019, glen=55.1, tlen=215, kl=0.00123, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.0645, ret=-0.00019, glen=55.1, tlen=215, kl=0.00123, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0675, ret=0.000113, glen=55.5, tlen=215, kl=0.00119, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0675, ret=0.000113, glen=55.5, tlen=215, kl=0.00119, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.0848, ret=0.000137, glen=56.3, tlen=216, kl=0.00128, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0848, ret=0.000137, glen=56.3, tlen=216, kl=0.00128, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.00757, ret=2.04e-5, glen=54.8, tlen=214, kl=0.00126, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.00757, ret=2.04e-5, glen=54.8, tlen=214, kl=0.00126, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.0438, ret=-5.35e-5, glen=56.8, tlen=216, kl=0.00115, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=0.0438, ret=-5.35e-5, glen=56.8, tlen=216, kl=0.00115, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=0.0506, ret=-9.36e-5, glen=54.7, tlen=215, kl=0.00126, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=0.0506, ret=-9.36e-5, glen=54.7, tlen=215, kl=0.00126, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=0.158, ret=-0.000203, glen=54.7, tlen=215, kl=0.00125, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.158, ret=-0.000203, glen=54.7, tlen=215, kl=0.00125, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=0.00439, ret=1.37e-5, glen=55.5, tlen=215, kl=0.00114, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.00439, ret=1.37e-5, glen=55.5, tlen=215, kl=0.00114, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0798, ret=0.000141, glen=55.4, tlen=215, kl=0.00119, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0798, ret=0.000141, glen=55.4, tlen=215, kl=0.00119, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0827, ret=0.000131, glen=56.5, tlen=216, kl=0.00128, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=-0.0827, ret=0.000131, glen=56.5, tlen=216, kl=0.00128, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=0.0413, ret=-6.76e-5, glen=55.6, tlen=216, kl=0.00117, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.0413, ret=-6.76e-5, glen=55.6, tlen=216, kl=0.00117, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.102, ret=-0.000164, glen=54.5, tlen=213, kl=0.00126, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=0.102, ret=-0.000164, glen=54.5, tlen=213, kl=0.00126, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.00653, ret=2.5e-5, glen=54.2, tlen=214, kl=0.00127, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.00653, ret=2.5e-5, glen=54.2, tlen=214, kl=0.00127, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.103, ret=0.000169, glen=54.8, tlen=215, kl=0.00117, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.103, ret=0.000169, glen=54.8, tlen=215, kl=0.00117, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:05,  1.17it/s, pg=-0.0934, ret=0.000163, glen=54.1, tlen=214, kl=0.00122, act_lr=1e-6, ent=0.99]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0934, ret=0.000163, glen=54.1, tlen=214, kl=0.00122, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0364, ret=4.22e-5, glen=56, tlen=215, kl=0.00118, act_lr=1e-6, ent=0.959]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0364, ret=4.22e-5, glen=56, tlen=215, kl=0.00118, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0865, ret=0.00015, glen=54.3, tlen=214, kl=0.00126, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0865, ret=0.00015, glen=54.3, tlen=214, kl=0.00126, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=0.0662, ret=-0.00015, glen=54.5, tlen=214, kl=0.0013, act_lr=1e-6, ent=0.977] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=0.0662, ret=-0.00015, glen=54.5, tlen=214, kl=0.0013, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0854, ret=0.000149, glen=56.7, tlen=216, kl=0.00124, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0854, ret=0.000149, glen=56.7, tlen=216, kl=0.00124, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=0.0325, ret=-2.55e-5, glen=56.5, tlen=218, kl=0.00137, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=0.0325, ret=-2.55e-5, glen=56.5, tlen=218, kl=0.00137, act_lr=1e-6, ent=0.912]
2025-07-24 19:49:37.124 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.32s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.109, ret=0.00019, glen=56.3, tlen=216, kl=0.00113, act_lr=1e-6, ent=0.97]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.109, ret=0.00019, glen=56.3, tlen=216, kl=0.00113, act_lr=1e-6, ent=0.97]
2025-07-24 19:49:37.922 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.74s
2025-07-24 19:49:40.483 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 19:49:40.812 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.13s
2025-07-24 19:49:40.818 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0015825695461697048, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.958471143245697, 'kl': 0.001232698228624132, 'response_length': 55.09739354451497, 'total_length': 214.81707899305556, 'teacher_total_length': 226.73749186197918, 'return': -3.6769138710547445e-06, 'policy_update_steps': 1.0}
Episode [7/20]:  15%|‚ñà‚ñå        | 2/13 [03:34<19:36, 106.98s/it]2025-07-24 19:49:40.824 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:41,  1.68it/s, est. speed input: 295.23 toks/s, output: 35.23 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/172 [00:02<00:00, 107.18it/s, est. speed input: 12471.34 toks/s, output: 3518.98 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:02<00:00, 49.32it/s, est. speed input: 10666.34 toks/s, output: 3119.62 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:02<00:00, 58.70it/s, est. speed input: 10666.34 toks/s, output: 3119.62 toks/s]
2025-07-24 19:49:45.008 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 248.6638,strategyqa_test/accuracy: 0.5633,eval_accuracy: 0.5633
2025-07-24 19:49:45.289 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:50:18.501 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:50:18.682 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:50:18.682 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.39s
2025-07-24 19:50:20.598 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 55.1726,std_num_tokens: 17.0726,avg_correct_num_tokens: 55.1885,std_correct_num_tokens: 17.0805,avg_incorrect_num_tokens: 52.8182,std_incorrect_num_tokens: 15.6961
2025-07-24 19:50:21.022 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.34s
2025-07-24 19:50:23.526 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.50s
2025-07-24 19:50:46.708 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 19:50:46.708 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.18s
2025-07-24 19:50:48.005 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.80s
2025-07-24 19:50:48.006 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 6.545651599429967e-06, avg_kl: 0.002680874942393785, avg_response_length: 55.20192307032896, avg_orm_score: 0.0, avg_custom_rewards: 6.545651599429967e-06
2025-07-24 19:50:48.031 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter80_replay_buffer.jsonl
2025-07-24 19:50:49.208 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:02<00:00, 98.08it/s, est. speed input: 11969.71 toks/s, output: 3410.71 toks/s] [32m [repeated 55x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 50.46it/s, est. speed input: 9157.91 toks/s, output: 2538.96 toks/s] [32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0912, ret=0.00017, glen=56.6, tlen=217, kl=0.00273, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.02s/it, pg=-0.0912, ret=0.00017, glen=56.6, tlen=217, kl=0.00273, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.02s/it, pg=0.118, ret=-0.000304, glen=54.2, tlen=214, kl=0.00272, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=0.118, ret=-0.000304, glen=54.2, tlen=214, kl=0.00272, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0158, ret=1.28e-5, glen=54.9, tlen=215, kl=0.00277, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.12it/s, pg=-0.0158, ret=1.28e-5, glen=54.9, tlen=215, kl=0.00277, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.12it/s, pg=-0.0342, ret=3.99e-5, glen=54.5, tlen=214, kl=0.00247, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.14it/s, pg=-0.0342, ret=3.99e-5, glen=54.5, tlen=214, kl=0.00247, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.14it/s, pg=0.165, ret=-0.000231, glen=56.6, tlen=217, kl=0.0027, act_lr=1e-6, ent=1.01]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.13it/s, pg=0.165, ret=-0.000231, glen=56.6, tlen=217, kl=0.0027, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.13it/s, pg=0.0996, ret=-0.000117, glen=53.6, tlen=214, kl=0.00249, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.14it/s, pg=0.0996, ret=-0.000117, glen=53.6, tlen=214, kl=0.00249, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.14it/s, pg=-0.0151, ret=2.81e-5, glen=54.1, tlen=214, kl=0.00269, act_lr=1e-6, ent=0.985] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:32,  1.15it/s, pg=-0.0151, ret=2.81e-5, glen=54.1, tlen=214, kl=0.00269, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:32,  1.15it/s, pg=-0.0251, ret=6.83e-5, glen=55.1, tlen=215, kl=0.00253, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=-0.0251, ret=6.83e-5, glen=55.1, tlen=215, kl=0.00253, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=0.145, ret=-0.000227, glen=54.2, tlen=214, kl=0.00241, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.14it/s, pg=0.145, ret=-0.000227, glen=54.2, tlen=214, kl=0.00241, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=0.0265, ret=-5.28e-5, glen=55.1, tlen=215, kl=0.00272, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=0.0265, ret=-5.28e-5, glen=55.1, tlen=215, kl=0.00272, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=0.0808, ret=-2.86e-5, glen=56.1, tlen=216, kl=0.0029, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=0.0808, ret=-2.86e-5, glen=56.1, tlen=216, kl=0.0029, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=0.0411, ret=-2.25e-5, glen=57.4, tlen=218, kl=0.00274, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=0.0411, ret=-2.25e-5, glen=57.4, tlen=218, kl=0.00274, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=-0.0179, ret=2.87e-5, glen=54.9, tlen=215, kl=0.00252, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=-0.0179, ret=2.87e-5, glen=54.9, tlen=215, kl=0.00252, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=-0.0682, ret=0.000125, glen=53.1, tlen=213, kl=0.00276, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.0682, ret=0.000125, glen=53.1, tlen=213, kl=0.00276, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.084, ret=0.000147, glen=56.1, tlen=216, kl=0.00261, act_lr=1e-6, ent=0.999] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.084, ret=0.000147, glen=56.1, tlen=216, kl=0.00261, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.0901, ret=-9.91e-5, glen=53.5, tlen=213, kl=0.0028, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=0.0901, ret=-9.91e-5, glen=53.5, tlen=213, kl=0.0028, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.00598, ret=4.08e-5, glen=52.7, tlen=213, kl=0.00298, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=0.00598, ret=4.08e-5, glen=52.7, tlen=213, kl=0.00298, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=0.0896, ret=-0.000207, glen=55.9, tlen=216, kl=0.00248, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=0.0896, ret=-0.000207, glen=55.9, tlen=216, kl=0.00248, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=0.0577, ret=-9.72e-5, glen=54.4, tlen=214, kl=0.00263, act_lr=1e-6, ent=0.97]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=0.0577, ret=-9.72e-5, glen=54.4, tlen=214, kl=0.00263, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0911, ret=0.000161, glen=54.7, tlen=215, kl=0.00291, act_lr=1e-6, ent=0.99]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0911, ret=0.000161, glen=54.7, tlen=215, kl=0.00291, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=0.0225, ret=-8.15e-5, glen=54.8, tlen=215, kl=0.00275, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=0.0225, ret=-8.15e-5, glen=54.8, tlen=215, kl=0.00275, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=0.0231, ret=-1.02e-5, glen=54.1, tlen=214, kl=0.00282, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.0231, ret=-1.02e-5, glen=54.1, tlen=214, kl=0.00282, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.0312, ret=-0.000215, glen=57.7, tlen=217, kl=0.00274, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=0.0312, ret=-0.000215, glen=57.7, tlen=217, kl=0.00274, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.000977, ret=3.19e-5, glen=55, tlen=215, kl=0.0026, act_lr=1e-6, ent=0.972]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.000977, ret=3.19e-5, glen=55, tlen=215, kl=0.0026, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0667, ret=0.000115, glen=56.3, tlen=217, kl=0.00266, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0667, ret=0.000115, glen=56.3, tlen=217, kl=0.00266, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0952, ret=0.000162, glen=54.8, tlen=215, kl=0.00278, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0952, ret=0.000162, glen=54.8, tlen=215, kl=0.00278, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.00708, ret=5.86e-5, glen=55.8, tlen=216, kl=0.00297, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.00708, ret=5.86e-5, glen=55.8, tlen=216, kl=0.00297, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0598, ret=0.000104, glen=53.7, tlen=214, kl=0.00257, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0598, ret=0.000104, glen=53.7, tlen=214, kl=0.00257, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0251, ret=3.04e-5, glen=55.6, tlen=216, kl=0.00272, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0251, ret=3.04e-5, glen=55.6, tlen=216, kl=0.00272, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=0.005, ret=-5.92e-5, glen=55.7, tlen=216, kl=0.00294, act_lr=1e-6, ent=0.994] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=0.005, ret=-5.92e-5, glen=55.7, tlen=216, kl=0.00294, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0422, ret=7.24e-5, glen=55.9, tlen=216, kl=0.00261, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0422, ret=7.24e-5, glen=55.9, tlen=216, kl=0.00261, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.0859, ret=-0.000184, glen=56.7, tlen=216, kl=0.00239, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=0.0859, ret=-0.000184, glen=56.7, tlen=216, kl=0.00239, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0703, ret=0.000126, glen=55, tlen=215, kl=0.00286, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0703, ret=0.000126, glen=55, tlen=215, kl=0.00286, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0201, ret=5.86e-5, glen=54.4, tlen=215, kl=0.00303, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0201, ret=5.86e-5, glen=54.4, tlen=215, kl=0.00303, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.0237, ret=-4.01e-6, glen=55.8, tlen=216, kl=0.00208, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.0237, ret=-4.01e-6, glen=55.8, tlen=216, kl=0.00208, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0483, ret=4.98e-5, glen=54.9, tlen=215, kl=0.00229, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.0483, ret=4.98e-5, glen=54.9, tlen=215, kl=0.00229, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=0.027, ret=-8.33e-5, glen=55.2, tlen=215, kl=0.00224, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.027, ret=-8.33e-5, glen=55.2, tlen=215, kl=0.00224, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0118, ret=-9.87e-6, glen=58.7, tlen=219, kl=0.0026, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=-0.0118, ret=-9.87e-6, glen=58.7, tlen=219, kl=0.0026, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0722, ret=0.000128, glen=53.7, tlen=214, kl=0.00304, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0722, ret=0.000128, glen=53.7, tlen=214, kl=0.00304, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.00806, ret=1.85e-5, glen=55.1, tlen=215, kl=0.00267, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.00806, ret=1.85e-5, glen=55.1, tlen=215, kl=0.00267, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.0623, ret=-7.24e-5, glen=54.7, tlen=214, kl=0.00344, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=0.0623, ret=-7.24e-5, glen=54.7, tlen=214, kl=0.00344, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0792, ret=0.00014, glen=55.5, tlen=216, kl=0.00254, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0792, ret=0.00014, glen=55.5, tlen=216, kl=0.00254, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=0.0033, ret=-5.93e-5, glen=56.9, tlen=217, kl=0.00249, act_lr=1e-6, ent=1.03] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.0033, ret=-5.93e-5, glen=56.9, tlen=217, kl=0.00249, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0646, ret=0.000123, glen=55.2, tlen=215, kl=0.00231, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0646, ret=0.000123, glen=55.2, tlen=215, kl=0.00231, act_lr=1e-6, ent=0.954]
2025-07-24 19:51:28.422 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.04s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0861, ret=0.000151, glen=54.2, tlen=214, kl=0.00293, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.0861, ret=0.000151, glen=54.2, tlen=214, kl=0.00293, act_lr=1e-6, ent=0.957]
2025-07-24 19:51:29.109 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 19:51:31.120 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.01s
2025-07-24 19:51:31.452 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.19s
2025-07-24 19:51:31.458 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 8.002387152777778e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9817325843705071, 'kl': 0.0026811811659071183, 'response_length': 55.17686606513129, 'total_length': 215.30895114474828, 'teacher_total_length': 227.32792392306857, 'return': 6.111789035559114e-07, 'policy_update_steps': 1.0}
Episode [7/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:24<18:06, 108.65s/it]2025-07-24 19:51:31.516 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:52:05.594 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:52:05.785 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 19:52:05.786 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.27s
2025-07-24 19:52:07.786 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0023,avg_pass_at_n: 1.0000,avg_num_tokens: 55.9293,std_num_tokens: 16.7259,avg_correct_num_tokens: 55.8825,std_correct_num_tokens: 16.6886,avg_incorrect_num_tokens: 62.9815,std_incorrect_num_tokens: 20.4310
2025-07-24 19:52:08.219 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.43s
2025-07-24 19:52:10.744 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.52s
2025-07-24 19:52:33.820 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 19:52:33.820 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.07s
2025-07-24 19:52:35.133 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 19:52:35.133 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 8.032532986357559e-06, avg_kl: 0.003740534436103352, avg_response_length: 55.990765896589394, avg_orm_score: 0.0, avg_custom_rewards: 8.032532986357559e-06
2025-07-24 19:52:35.164 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter81_replay_buffer.jsonl
2025-07-24 19:52:36.358 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.20s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0398, ret=3.59e-5, glen=55.6, tlen=216, kl=0.00303, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0398, ret=3.59e-5, glen=55.6, tlen=216, kl=0.00303, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0742, ret=0.000119, glen=55.5, tlen=216, kl=0.00373, act_lr=1e-6, ent=1]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.0742, ret=0.000119, glen=55.5, tlen=216, kl=0.00373, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0115, ret=4.43e-5, glen=58.5, tlen=219, kl=0.00359, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.12it/s, pg=-0.0115, ret=4.43e-5, glen=58.5, tlen=219, kl=0.00359, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.12it/s, pg=0.0981, ret=-0.000114, glen=55.1, tlen=215, kl=0.00414, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=0.0981, ret=-0.000114, glen=55.1, tlen=215, kl=0.00414, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=0.0722, ret=-0.000141, glen=57.4, tlen=218, kl=0.00359, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.14it/s, pg=0.0722, ret=-0.000141, glen=57.4, tlen=218, kl=0.00359, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.14it/s, pg=-0.0836, ret=0.000124, glen=55.4, tlen=216, kl=0.00393, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=-0.0836, ret=0.000124, glen=55.4, tlen=216, kl=0.00393, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=-0.0366, ret=5.54e-6, glen=55.1, tlen=216, kl=0.00407, act_lr=1e-6, ent=0.986] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.13it/s, pg=-0.0366, ret=5.54e-6, glen=55.1, tlen=216, kl=0.00407, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.13it/s, pg=0.0415, ret=-7.21e-5, glen=57.8, tlen=218, kl=0.00377, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=0.0415, ret=-7.21e-5, glen=57.8, tlen=218, kl=0.00377, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=-0.0912, ret=0.000146, glen=56, tlen=216, kl=0.0037, act_lr=1e-6, ent=0.964]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.15it/s, pg=-0.0912, ret=0.000146, glen=56, tlen=216, kl=0.0037, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=0.0518, ret=-0.000111, glen=57.6, tlen=218, kl=0.00292, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=0.0518, ret=-0.000111, glen=57.6, tlen=218, kl=0.00292, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=0.108, ret=-0.000163, glen=57.1, tlen=217, kl=0.00333, act_lr=1e-6, ent=0.955] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=0.108, ret=-0.000163, glen=57.1, tlen=217, kl=0.00333, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.0083, ret=3.38e-5, glen=57.7, tlen=218, kl=0.00368, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.0083, ret=3.38e-5, glen=57.7, tlen=218, kl=0.00368, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=-0.0258, ret=3.81e-5, glen=55.6, tlen=216, kl=0.00349, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.0258, ret=3.81e-5, glen=55.6, tlen=216, kl=0.00349, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0398, ret=-0.000107, glen=55.5, tlen=216, kl=0.00388, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.15it/s, pg=0.0398, ret=-0.000107, glen=55.5, tlen=216, kl=0.00388, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.15it/s, pg=0.124, ret=-0.000132, glen=55.4, tlen=216, kl=0.00477, act_lr=1e-6, ent=0.984] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.16it/s, pg=0.124, ret=-0.000132, glen=55.4, tlen=216, kl=0.00477, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.16it/s, pg=-0.0671, ret=0.000103, glen=57, tlen=217, kl=0.00346, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.16it/s, pg=-0.0671, ret=0.000103, glen=57, tlen=217, kl=0.00346, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.16it/s, pg=0.0176, ret=-9.63e-5, glen=56.6, tlen=217, kl=0.0042, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=0.0176, ret=-9.63e-5, glen=56.6, tlen=217, kl=0.0042, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0399, ret=2.58e-5, glen=56.5, tlen=217, kl=0.00319, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=-0.0399, ret=2.58e-5, glen=56.5, tlen=217, kl=0.00319, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=-0.054, ret=4.98e-5, glen=54.6, tlen=215, kl=0.00364, act_lr=1e-6, ent=0.982] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.054, ret=4.98e-5, glen=54.6, tlen=215, kl=0.00364, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=0.0897, ret=-0.000222, glen=59.3, tlen=220, kl=0.0036, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=0.0897, ret=-0.000222, glen=59.3, tlen=220, kl=0.0036, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.0653, ret=0.000107, glen=52.9, tlen=213, kl=0.00409, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.0653, ret=0.000107, glen=52.9, tlen=213, kl=0.00409, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.0928, ret=0.000157, glen=53.8, tlen=214, kl=0.00439, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0928, ret=0.000157, glen=53.8, tlen=214, kl=0.00439, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.107, ret=0.000164, glen=60.4, tlen=221, kl=0.00318, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=-0.107, ret=0.000164, glen=60.4, tlen=221, kl=0.00318, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0401, ret=3.54e-5, glen=57.2, tlen=218, kl=0.00369, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.18it/s, pg=-0.0401, ret=3.54e-5, glen=57.2, tlen=218, kl=0.00369, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.18it/s, pg=0.109, ret=-0.000132, glen=54.4, tlen=215, kl=0.00334, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:16,  1.18it/s, pg=0.109, ret=-0.000132, glen=54.4, tlen=215, kl=0.00334, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:16,  1.18it/s, pg=0.00415, ret=2.34e-5, glen=55.5, tlen=216, kl=0.0036, act_lr=1e-6, ent=0.98]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.18it/s, pg=0.00415, ret=2.34e-5, glen=55.5, tlen=216, kl=0.0036, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.18it/s, pg=0.0796, ret=-0.000103, glen=54.4, tlen=215, kl=0.00382, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.18it/s, pg=0.0796, ret=-0.000103, glen=54.4, tlen=215, kl=0.00382, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.18it/s, pg=0.00415, ret=1.8e-5, glen=56.3, tlen=217, kl=0.00374, act_lr=1e-6, ent=0.993]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.18it/s, pg=0.00415, ret=1.8e-5, glen=56.3, tlen=217, kl=0.00374, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.18it/s, pg=0.00122, ret=-1.66e-5, glen=55.7, tlen=216, kl=0.00415, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=0.00122, ret=-1.66e-5, glen=55.7, tlen=216, kl=0.00415, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0756, ret=0.000125, glen=56.7, tlen=217, kl=0.00348, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0756, ret=0.000125, glen=56.7, tlen=217, kl=0.00348, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0499, ret=3.69e-5, glen=55.8, tlen=217, kl=0.00362, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:26<00:12,  1.12it/s, pg=-0.0499, ret=3.69e-5, glen=55.8, tlen=217, kl=0.00362, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.127, ret=-6.27e-5, glen=55.9, tlen=217, kl=0.00361, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=0.127, ret=-6.27e-5, glen=55.9, tlen=217, kl=0.00361, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.00616, ret=-9.81e-5, glen=55.8, tlen=216, kl=0.00377, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=0.00616, ret=-9.81e-5, glen=55.8, tlen=216, kl=0.00377, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0928, ret=0.000156, glen=56.8, tlen=217, kl=0.00451, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0928, ret=0.000156, glen=56.8, tlen=217, kl=0.00451, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.166, ret=-0.000198, glen=55.5, tlen=216, kl=0.00375, act_lr=1e-6, ent=0.978] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.17it/s, pg=0.166, ret=-0.000198, glen=55.5, tlen=216, kl=0.00375, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.17it/s, pg=0.00208, ret=3.29e-5, glen=56.1, tlen=217, kl=0.00401, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=0.00208, ret=3.29e-5, glen=56.1, tlen=217, kl=0.00401, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=0.0596, ret=-7.44e-5, glen=53.8, tlen=214, kl=0.0041, act_lr=1e-6, ent=0.974] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0596, ret=-7.44e-5, glen=53.8, tlen=214, kl=0.0041, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0475, ret=-0.000115, glen=56.4, tlen=217, kl=0.00318, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=0.0475, ret=-0.000115, glen=56.4, tlen=217, kl=0.00318, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0653, ret=0.000103, glen=53.2, tlen=213, kl=0.00356, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0653, ret=0.000103, glen=53.2, tlen=213, kl=0.00356, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0813, ret=-9.37e-5, glen=55.6, tlen=216, kl=0.00409, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=0.0813, ret=-9.37e-5, glen=55.6, tlen=216, kl=0.00409, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.00732, ret=-2.78e-5, glen=54.2, tlen=214, kl=0.00396, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=0.00732, ret=-2.78e-5, glen=54.2, tlen=214, kl=0.00396, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0964, ret=0.000154, glen=58.5, tlen=219, kl=0.00394, act_lr=1e-6, ent=0.99] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.0964, ret=0.000154, glen=58.5, tlen=219, kl=0.00394, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.0543, ret=8.66e-5, glen=55, tlen=215, kl=0.00402, act_lr=1e-6, ent=0.966]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0543, ret=8.66e-5, glen=55, tlen=215, kl=0.00402, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=-0.0978, ret=0.000168, glen=55.4, tlen=216, kl=0.00345, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.0978, ret=0.000168, glen=55.4, tlen=216, kl=0.00345, act_lr=1e-6, ent=0.976]
2025-07-24 19:53:15.524 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.99s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.00195, ret=4.47e-5, glen=55, tlen=215, kl=0.00351, act_lr=1e-6, ent=0.939]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.00195, ret=4.47e-5, glen=55, tlen=215, kl=0.00351, act_lr=1e-6, ent=0.939]
2025-07-24 19:53:16.385 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 19:53:18.956 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 19:53:19.287 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.88s
2025-07-24 19:53:19.292 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007453070746527777, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9770848711331686, 'kl': 0.003739251030815972, 'response_length': 55.991783650716144, 'total_length': 216.4491434733073, 'teacher_total_length': 228.39699978298611, 'return': 1.2866951389393458e-06, 'policy_update_steps': 1.0}
Episode [7/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [07:12<16:14, 108.33s/it]2025-07-24 19:53:19.337 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:53:53.122 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:53:53.303 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:53:53.304 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.97s
2025-07-24 19:53:55.036 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0020,avg_pass_at_n: 1.0000,avg_num_tokens: 56.0490,std_num_tokens: 16.8206,avg_correct_num_tokens: 56.0279,std_correct_num_tokens: 16.7858,avg_incorrect_num_tokens: 61.2424,std_incorrect_num_tokens: 23.3498
2025-07-24 19:53:55.448 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.14s
2025-07-24 19:53:57.945 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.50s
2025-07-24 19:54:21.293 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 19:54:21.293 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.35s
2025-07-24 19:54:22.589 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 19:54:22.589 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 6.422770532191466e-06, avg_kl: 0.004964988324895251, avg_response_length: 56.354732279005, avg_orm_score: 0.0, avg_custom_rewards: 6.422770532191466e-06
2025-07-24 19:54:22.616 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter82_replay_buffer.jsonl
2025-07-24 19:54:23.799 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=0.111, ret=-0.000163, glen=57.5, tlen=218, kl=0.00457, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.111, ret=-0.000163, glen=57.5, tlen=218, kl=0.00457, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0728, ret=9.1e-5, glen=55.3, tlen=215, kl=0.00574, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.0728, ret=9.1e-5, glen=55.3, tlen=215, kl=0.00574, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0685, ret=9.23e-5, glen=55.9, tlen=216, kl=0.00421, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.13it/s, pg=-0.0685, ret=9.23e-5, glen=55.9, tlen=216, kl=0.00421, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.13it/s, pg=0.135, ret=-0.000161, glen=55.6, tlen=216, kl=0.00598, act_lr=1e-6, ent=0.998]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=0.135, ret=-0.000161, glen=55.6, tlen=216, kl=0.00598, act_lr=1e-6, ent=0.998]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=0.0726, ret=-4.95e-5, glen=54.7, tlen=215, kl=0.00533, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.13it/s, pg=0.0726, ret=-4.95e-5, glen=54.7, tlen=215, kl=0.00533, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.13it/s, pg=-0.0758, ret=0.0001, glen=56.8, tlen=217, kl=0.00507, act_lr=1e-6, ent=0.989] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.11it/s, pg=-0.0758, ret=0.0001, glen=56.8, tlen=217, kl=0.00507, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.11it/s, pg=0.123, ret=-5.82e-5, glen=56.2, tlen=216, kl=0.00449, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.13it/s, pg=0.123, ret=-5.82e-5, glen=56.2, tlen=216, kl=0.00449, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.13it/s, pg=0.0671, ret=-0.000123, glen=56.2, tlen=216, kl=0.00536, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=0.0671, ret=-0.000123, glen=56.2, tlen=216, kl=0.00536, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=-0.0698, ret=9.44e-5, glen=54.5, tlen=215, kl=0.00542, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.15it/s, pg=-0.0698, ret=9.44e-5, glen=54.5, tlen=215, kl=0.00542, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=0.0397, ret=-4.26e-5, glen=54.3, tlen=215, kl=0.00507, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=0.0397, ret=-4.26e-5, glen=54.3, tlen=215, kl=0.00507, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=0.11, ret=-0.00015, glen=56.5, tlen=217, kl=0.00468, act_lr=1e-6, ent=1.01]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=0.11, ret=-0.00015, glen=56.5, tlen=217, kl=0.00468, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=0.0825, ret=-0.000111, glen=57.6, tlen=218, kl=0.00524, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=0.0825, ret=-0.000111, glen=57.6, tlen=218, kl=0.00524, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=0.0219, ret=-4.43e-5, glen=58.6, tlen=219, kl=0.00439, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=0.0219, ret=-4.43e-5, glen=58.6, tlen=219, kl=0.00439, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=0.101, ret=-2.4e-5, glen=56.2, tlen=216, kl=0.00481, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.15it/s, pg=0.101, ret=-2.4e-5, glen=56.2, tlen=216, kl=0.00481, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.15it/s, pg=-0.00684, ret=-8.51e-6, glen=55.1, tlen=215, kl=0.00534, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.14it/s, pg=-0.00684, ret=-8.51e-6, glen=55.1, tlen=215, kl=0.00534, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:26,  1.14it/s, pg=0.0336, ret=-4.25e-5, glen=55.9, tlen=216, kl=0.00626, act_lr=1e-6, ent=1.02]   Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=0.0336, ret=-4.25e-5, glen=55.9, tlen=216, kl=0.00626, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=0.0277, ret=-0.000133, glen=57.3, tlen=217, kl=0.0048, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.15it/s, pg=0.0277, ret=-0.000133, glen=57.3, tlen=217, kl=0.0048, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.15it/s, pg=-0.00464, ret=-2.89e-5, glen=57.6, tlen=218, kl=0.00493, act_lr=1e-6, ent=1]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.16it/s, pg=-0.00464, ret=-2.89e-5, glen=57.6, tlen=218, kl=0.00493, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.16it/s, pg=0.177, ret=-0.000142, glen=54.9, tlen=215, kl=0.00452, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=0.177, ret=-0.000142, glen=54.9, tlen=215, kl=0.00452, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0638, ret=8.39e-5, glen=57.5, tlen=218, kl=0.00457, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0638, ret=8.39e-5, glen=57.5, tlen=218, kl=0.00457, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.0419, ret=9.96e-7, glen=57.2, tlen=218, kl=0.00481, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.0419, ret=9.96e-7, glen=57.2, tlen=218, kl=0.00481, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.0624, ret=7.8e-5, glen=56.3, tlen=216, kl=0.00548, act_lr=1e-6, ent=0.981] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0624, ret=7.8e-5, glen=56.3, tlen=216, kl=0.00548, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.17it/s, pg=0.0259, ret=-5.54e-5, glen=56.5, tlen=216, kl=0.00532, act_lr=1e-6, ent=0.993]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=0.0259, ret=-5.54e-5, glen=56.5, tlen=216, kl=0.00532, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0769, ret=0.000103, glen=56.2, tlen=216, kl=0.00425, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0769, ret=0.000103, glen=56.2, tlen=216, kl=0.00425, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0585, ret=6.7e-5, glen=56.2, tlen=216, kl=0.00528, act_lr=1e-6, ent=1.01]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.18it/s, pg=-0.0585, ret=6.7e-5, glen=56.2, tlen=216, kl=0.00528, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.18it/s, pg=-0.069, ret=9.58e-5, glen=56.3, tlen=217, kl=0.00443, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.18it/s, pg=-0.069, ret=9.58e-5, glen=56.3, tlen=217, kl=0.00443, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.18it/s, pg=-0.037, ret=4.36e-5, glen=68.8, tlen=230, kl=0.00471, act_lr=1e-6, ent=0.835]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.14it/s, pg=-0.037, ret=4.36e-5, glen=68.8, tlen=230, kl=0.00471, act_lr=1e-6, ent=0.835]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.14it/s, pg=0.0455, ret=-2.94e-5, glen=54.4, tlen=214, kl=0.00457, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.15it/s, pg=0.0455, ret=-2.94e-5, glen=54.4, tlen=214, kl=0.00457, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.15it/s, pg=0.05, ret=-3.22e-5, glen=56.9, tlen=217, kl=0.00484, act_lr=1e-6, ent=0.977]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.06it/s, pg=0.05, ret=-3.22e-5, glen=56.9, tlen=217, kl=0.00484, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.06it/s, pg=-0.0732, ret=0.000101, glen=57.1, tlen=217, kl=0.00465, act_lr=1e-6, ent=0.996]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=-0.0732, ret=0.000101, glen=57.1, tlen=217, kl=0.00465, act_lr=1e-6, ent=0.996]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=0.00128, ret=-3.14e-5, glen=55, tlen=215, kl=0.00474, act_lr=1e-6, ent=0.954]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=0.00128, ret=-3.14e-5, glen=55, tlen=215, kl=0.00474, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.11it/s, pg=-0.0552, ret=7.04e-5, glen=55.1, tlen=215, kl=0.0045, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.0552, ret=7.04e-5, glen=55.1, tlen=215, kl=0.0045, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.00598, ret=3.87e-6, glen=54.9, tlen=215, kl=0.00573, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.14it/s, pg=-0.00598, ret=3.87e-6, glen=54.9, tlen=215, kl=0.00573, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=-0.0475, ret=6.09e-5, glen=55.3, tlen=215, kl=0.00484, act_lr=1e-6, ent=0.967] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=-0.0475, ret=6.09e-5, glen=55.3, tlen=215, kl=0.00484, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=-0.057, ret=6.97e-5, glen=55.6, tlen=216, kl=0.00533, act_lr=1e-6, ent=0.977] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.057, ret=6.97e-5, glen=55.6, tlen=216, kl=0.00533, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.0497, ret=-0.000146, glen=55.6, tlen=215, kl=0.00407, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=0.0497, ret=-0.000146, glen=55.6, tlen=215, kl=0.00407, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=-0.0729, ret=0.000101, glen=55.4, tlen=216, kl=0.00497, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0729, ret=0.000101, glen=55.4, tlen=216, kl=0.00497, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0484, ret=5.72e-5, glen=57.1, tlen=217, kl=0.00606, act_lr=1e-6, ent=0.944] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0484, ret=5.72e-5, glen=57.1, tlen=217, kl=0.00606, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0286, ret=-5.91e-5, glen=57, tlen=217, kl=0.0047, act_lr=1e-6, ent=0.97]    Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=0.0286, ret=-5.91e-5, glen=57, tlen=217, kl=0.0047, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0452, ret=-1.61e-5, glen=54.5, tlen=215, kl=0.00517, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.14it/s, pg=0.0452, ret=-1.61e-5, glen=54.5, tlen=215, kl=0.00517, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.14it/s, pg=-0.0676, ret=8.8e-5, glen=54.2, tlen=214, kl=0.00533, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.15it/s, pg=-0.0676, ret=8.8e-5, glen=54.2, tlen=214, kl=0.00533, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.15it/s, pg=-0.07, ret=8.82e-5, glen=54.2, tlen=214, kl=0.00475, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.16it/s, pg=-0.07, ret=8.82e-5, glen=54.2, tlen=214, kl=0.00475, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.16it/s, pg=-0.0154, ret=-8.71e-6, glen=55.7, tlen=216, kl=0.00441, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.16it/s, pg=-0.0154, ret=-8.71e-6, glen=55.7, tlen=216, kl=0.00441, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.16it/s, pg=-0.0742, ret=9.91e-5, glen=56.4, tlen=217, kl=0.00523, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0742, ret=9.91e-5, glen=56.4, tlen=217, kl=0.00523, act_lr=1e-6, ent=0.95]
2025-07-24 19:55:03.242 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.27s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=-0.0762, ret=0.0001, glen=58.9, tlen=219, kl=0.00467, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0762, ret=0.0001, glen=58.9, tlen=219, kl=0.00467, act_lr=1e-6, ent=0.988]
2025-07-24 19:55:03.919 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 19:55:06.199 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.28s
2025-07-24 19:55:06.536 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.68s
2025-07-24 19:55:06.543 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005016750759548611, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9801262458165486, 'kl': 0.004968092176649306, 'response_length': 56.3393559773763, 'total_length': 216.50875990125868, 'teacher_total_length': 228.2541765001085, 'return': 6.458401306493518e-07, 'policy_update_steps': 1.0}
Episode [7/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [08:59<14:23, 107.94s/it]2025-07-24 19:55:06.591 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:55:40.174 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:55:40.356 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:55:40.356 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.77s
2025-07-24 19:55:42.008 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0023,avg_pass_at_n: 1.0000,avg_num_tokens: 56.1658,std_num_tokens: 16.3067,avg_correct_num_tokens: 56.1676,std_correct_num_tokens: 16.2687,avg_incorrect_num_tokens: 55.8140,std_incorrect_num_tokens: 22.3682
2025-07-24 19:55:42.399 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.04s
2025-07-24 19:55:44.967 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.53s
2025-07-24 19:56:08.072 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 19:56:08.072 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.10s
2025-07-24 19:56:09.213 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.74s
2025-07-24 19:56:09.214 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.6579751170047836e-05, avg_kl: 0.00576364527867493, avg_response_length: 56.19735474826237, avg_orm_score: 0.0, avg_custom_rewards: 2.6579751170047836e-05
2025-07-24 19:56:09.240 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter83_replay_buffer.jsonl
2025-07-24 19:56:10.451 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=0.0161, ret=-7.09e-6, glen=54.4, tlen=215, kl=0.00552, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.0161, ret=-7.09e-6, glen=54.4, tlen=215, kl=0.00552, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0531, ret=7.57e-5, glen=54.9, tlen=215, kl=0.00499, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.0531, ret=7.57e-5, glen=54.9, tlen=215, kl=0.00499, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0684, ret=0.000117, glen=54.6, tlen=215, kl=0.00574, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.09it/s, pg=-0.0684, ret=0.000117, glen=54.6, tlen=215, kl=0.00574, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.09it/s, pg=0.0442, ret=-0.000102, glen=57.2, tlen=218, kl=0.00569, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=0.0442, ret=-0.000102, glen=57.2, tlen=218, kl=0.00569, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=-0.0548, ret=8.38e-5, glen=55.2, tlen=216, kl=0.00604, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.10it/s, pg=-0.0548, ret=8.38e-5, glen=55.2, tlen=216, kl=0.00604, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.10it/s, pg=0.0768, ret=-4.36e-5, glen=58.4, tlen=219, kl=0.00575, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.12it/s, pg=0.0768, ret=-4.36e-5, glen=58.4, tlen=219, kl=0.00575, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.12it/s, pg=-0.0563, ret=9.23e-5, glen=55.6, tlen=216, kl=0.00516, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=-0.0563, ret=9.23e-5, glen=55.6, tlen=216, kl=0.00516, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=0.0491, ret=-0.000129, glen=56.2, tlen=217, kl=0.00642, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=0.0491, ret=-0.000129, glen=56.2, tlen=217, kl=0.00642, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.15it/s, pg=-0.0688, ret=0.000115, glen=56, tlen=217, kl=0.00544, act_lr=1e-6, ent=0.986]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=-0.0688, ret=0.000115, glen=56, tlen=217, kl=0.00544, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=0.0906, ret=-0.000152, glen=56.6, tlen=217, kl=0.00527, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=0.0906, ret=-0.000152, glen=56.6, tlen=217, kl=0.00527, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=0.286, ret=-0.00031, glen=58.4, tlen=219, kl=0.00587, act_lr=1e-6, ent=0.976]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=0.286, ret=-0.00031, glen=58.4, tlen=219, kl=0.00587, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.0837, ret=0.000128, glen=55.6, tlen=217, kl=0.00579, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=-0.0837, ret=0.000128, glen=55.6, tlen=217, kl=0.00579, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=-0.0635, ret=0.000108, glen=54.5, tlen=215, kl=0.00468, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.0635, ret=0.000108, glen=54.5, tlen=215, kl=0.00468, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0939, ret=-0.00016, glen=54.9, tlen=215, kl=0.00584, act_lr=1e-6, ent=0.97]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0939, ret=-0.00016, glen=54.9, tlen=215, kl=0.00584, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.0813, ret=0.000132, glen=55.1, tlen=216, kl=0.00755, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0813, ret=0.000132, glen=55.1, tlen=216, kl=0.00755, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.00366, ret=5.69e-5, glen=56.8, tlen=217, kl=0.00469, act_lr=1e-6, ent=0.979] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.18it/s, pg=0.00366, ret=5.69e-5, glen=56.8, tlen=217, kl=0.00469, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.18it/s, pg=0.0482, ret=1.66e-5, glen=54, tlen=215, kl=0.00592, act_lr=1e-6, ent=0.969]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.18it/s, pg=0.0482, ret=1.66e-5, glen=54, tlen=215, kl=0.00592, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.18it/s, pg=-0.0701, ret=0.000101, glen=56.9, tlen=217, kl=0.00571, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:22,  1.18it/s, pg=-0.0701, ret=0.000101, glen=56.9, tlen=217, kl=0.00571, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:22,  1.18it/s, pg=0.0298, ret=-4.3e-5, glen=56.1, tlen=217, kl=0.00593, act_lr=1e-6, ent=0.953]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.15it/s, pg=0.0298, ret=-4.3e-5, glen=56.1, tlen=217, kl=0.00593, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.15it/s, pg=0.0525, ret=-9.97e-5, glen=56.1, tlen=217, kl=0.00674, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=0.0525, ret=-9.97e-5, glen=56.1, tlen=217, kl=0.00674, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=-0.0519, ret=3.81e-5, glen=53.8, tlen=214, kl=0.00581, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.16it/s, pg=-0.0519, ret=3.81e-5, glen=53.8, tlen=214, kl=0.00581, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.16it/s, pg=0.0914, ret=-0.000233, glen=55.6, tlen=217, kl=0.00639, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.0914, ret=-0.000233, glen=55.6, tlen=217, kl=0.00639, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.0379, ret=-0.000111, glen=56.5, tlen=217, kl=0.00539, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=0.0379, ret=-0.000111, glen=56.5, tlen=217, kl=0.00539, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0867, ret=0.000135, glen=56.3, tlen=217, kl=0.00588, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0867, ret=0.000135, glen=56.3, tlen=217, kl=0.00588, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0708, ret=0.000106, glen=57.8, tlen=219, kl=0.00633, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0708, ret=0.000106, glen=57.8, tlen=219, kl=0.00633, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=0.0148, ret=-1.27e-5, glen=58.7, tlen=220, kl=0.00616, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=0.0148, ret=-1.27e-5, glen=58.7, tlen=220, kl=0.00616, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.084, ret=-0.00022, glen=54.1, tlen=215, kl=0.00602, act_lr=1e-6, ent=0.989] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=0.084, ret=-0.00022, glen=54.1, tlen=215, kl=0.00602, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.064, ret=9.34e-5, glen=59, tlen=220, kl=0.00502, act_lr=1e-6, ent=1.01]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.18it/s, pg=-0.064, ret=9.34e-5, glen=59, tlen=220, kl=0.00502, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.18it/s, pg=-0.0922, ret=0.000148, glen=56.2, tlen=217, kl=0.00523, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0922, ret=0.000148, glen=56.2, tlen=217, kl=0.00523, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=0.0299, ret=-1.08e-5, glen=57.6, tlen=218, kl=0.00615, act_lr=1e-6, ent=0.999] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=0.0299, ret=-1.08e-5, glen=57.6, tlen=218, kl=0.00615, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=0.101, ret=9.23e-6, glen=54.7, tlen=216, kl=0.0069, act_lr=1e-6, ent=0.978]   Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.101, ret=9.23e-6, glen=54.7, tlen=216, kl=0.0069, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0699, ret=0.000112, glen=57, tlen=218, kl=0.00651, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.0699, ret=0.000112, glen=57, tlen=218, kl=0.00651, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0314, ret=-5.54e-6, glen=56.3, tlen=217, kl=0.00622, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0314, ret=-5.54e-6, glen=56.3, tlen=217, kl=0.00622, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.011, ret=1.11e-5, glen=55.5, tlen=216, kl=0.00595, act_lr=1e-6, ent=0.977]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.011, ret=1.11e-5, glen=55.5, tlen=216, kl=0.00595, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.00531, ret=-2.51e-5, glen=58.4, tlen=219, kl=0.00488, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.00531, ret=-2.51e-5, glen=58.4, tlen=219, kl=0.00488, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0215, ret=-2.6e-5, glen=55.9, tlen=216, kl=0.00642, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0215, ret=-2.6e-5, glen=55.9, tlen=216, kl=0.00642, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=0.024, ret=7.72e-6, glen=59.3, tlen=220, kl=0.00554, act_lr=1e-6, ent=0.951] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.024, ret=7.72e-6, glen=59.3, tlen=220, kl=0.00554, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0717, ret=0.000112, glen=54.9, tlen=215, kl=0.00523, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0717, ret=0.000112, glen=54.9, tlen=215, kl=0.00523, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0563, ret=-0.00013, glen=55.9, tlen=216, kl=0.00584, act_lr=1e-6, ent=1.01]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=0.0563, ret=-0.00013, glen=55.9, tlen=216, kl=0.00584, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0815, ret=0.000136, glen=57.2, tlen=218, kl=0.00534, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0815, ret=0.000136, glen=57.2, tlen=218, kl=0.00534, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.0311, ret=-0.000109, glen=55.5, tlen=216, kl=0.00537, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=0.0311, ret=-0.000109, glen=55.5, tlen=216, kl=0.00537, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=0.0143, ret=1.07e-5, glen=58.1, tlen=219, kl=0.00525, act_lr=1e-6, ent=0.955]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=0.0143, ret=1.07e-5, glen=58.1, tlen=219, kl=0.00525, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.0828, ret=0.000129, glen=54.1, tlen=215, kl=0.00622, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0828, ret=0.000129, glen=54.1, tlen=215, kl=0.00622, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=0.0195, ret=-1.35e-5, glen=56.5, tlen=217, kl=0.00565, act_lr=1e-6, ent=0.955] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=0.0195, ret=-1.35e-5, glen=56.5, tlen=217, kl=0.00565, act_lr=1e-6, ent=0.955]
2025-07-24 19:56:49.695 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.00198, ret=-3.97e-5, glen=56, tlen=216, kl=0.00478, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.00198, ret=-3.97e-5, glen=56, tlen=216, kl=0.00478, act_lr=1e-6, ent=0.982]
2025-07-24 19:56:50.520 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 19:56:53.099 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 19:56:53.446 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.92s
2025-07-24 19:56:53.452 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00032145182291666666, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9719460434383816, 'kl': 0.005760277642144097, 'response_length': 56.189011637369795, 'total_length': 216.77061428493923, 'teacher_total_length': 228.78878682454427, 'return': 1.9942849677237163e-06, 'policy_update_steps': 1.0}
Episode [7/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [10:46<12:33, 107.59s/it]2025-07-24 19:56:53.499 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:57:27.398 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:57:27.575 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:57:27.576 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.08s
2025-07-24 19:57:29.425 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0038,avg_pass_at_n: 1.0000,avg_num_tokens: 56.6367,std_num_tokens: 16.9880,avg_correct_num_tokens: 56.5652,std_correct_num_tokens: 16.8891,avg_incorrect_num_tokens: 66.8421,std_incorrect_num_tokens: 25.7244
2025-07-24 19:57:29.839 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.26s
2025-07-24 19:57:32.340 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.50s
2025-07-24 19:57:55.456 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 19:57:55.457 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.11s
2025-07-24 19:57:56.782 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 19:57:56.783 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.3433560708215117e-06, avg_kl: 0.00955660516323324, avg_response_length: 56.64522810355245, avg_orm_score: 0.0, avg_custom_rewards: 1.3433560708215117e-06
2025-07-24 19:57:56.810 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter84_replay_buffer.jsonl
2025-07-24 19:57:57.984 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0353, ret=6.04e-5, glen=56.5, tlen=216, kl=0.00845, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0353, ret=6.04e-5, glen=56.5, tlen=216, kl=0.00845, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.0348, ret=-0.000126, glen=58.4, tlen=218, kl=0.00951, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=0.0348, ret=-0.000126, glen=58.4, tlen=218, kl=0.00951, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.0393, ret=4.86e-5, glen=55.6, tlen=215, kl=0.0106, act_lr=1e-6, ent=0.978] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.10it/s, pg=-0.0393, ret=4.86e-5, glen=55.6, tlen=215, kl=0.0106, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.10it/s, pg=-0.0366, ret=7.2e-5, glen=56.2, tlen=216, kl=0.00932, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:37,  1.08it/s, pg=-0.0366, ret=7.2e-5, glen=56.2, tlen=216, kl=0.00932, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:37,  1.08it/s, pg=0.253, ret=-0.000306, glen=57.3, tlen=217, kl=0.0105, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.11it/s, pg=0.253, ret=-0.000306, glen=57.3, tlen=217, kl=0.0105, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.11it/s, pg=-0.0278, ret=4.42e-5, glen=55.8, tlen=216, kl=0.0093, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.13it/s, pg=-0.0278, ret=4.42e-5, glen=55.8, tlen=216, kl=0.0093, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.13it/s, pg=-0.0835, ret=0.000128, glen=57.3, tlen=217, kl=0.00966, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=-0.0835, ret=0.000128, glen=57.3, tlen=217, kl=0.00966, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=-0.0743, ret=0.00013, glen=56.6, tlen=216, kl=0.00795, act_lr=1e-6, ent=0.979] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:33,  1.12it/s, pg=-0.0743, ret=0.00013, glen=56.6, tlen=216, kl=0.00795, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:33,  1.12it/s, pg=-0.0408, ret=7.06e-5, glen=56.1, tlen=216, kl=0.0105, act_lr=1e-6, ent=0.966] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=-0.0408, ret=7.06e-5, glen=56.1, tlen=216, kl=0.0105, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=0.133, ret=-0.000209, glen=54.5, tlen=214, kl=0.00813, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:31,  1.12it/s, pg=0.133, ret=-0.000209, glen=54.5, tlen=214, kl=0.00813, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:31,  1.12it/s, pg=0.167, ret=-0.000312, glen=56.7, tlen=217, kl=0.0102, act_lr=1e-6, ent=0.99]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:30,  1.13it/s, pg=0.167, ret=-0.000312, glen=56.7, tlen=217, kl=0.0102, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:30,  1.13it/s, pg=-0.113, ret=0.000183, glen=56.2, tlen=216, kl=0.0113, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.14it/s, pg=-0.113, ret=0.000183, glen=56.2, tlen=216, kl=0.0113, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.14it/s, pg=-0.106, ret=0.00017, glen=55.5, tlen=215, kl=0.0101, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=-0.106, ret=0.00017, glen=55.5, tlen=215, kl=0.0101, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=0.15, ret=-0.000173, glen=58.9, tlen=218, kl=0.00703, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=0.15, ret=-0.000173, glen=58.9, tlen=218, kl=0.00703, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=-0.0499, ret=5.91e-5, glen=56.3, tlen=216, kl=0.00986, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0499, ret=5.91e-5, glen=56.3, tlen=216, kl=0.00986, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.17it/s, pg=0.107, ret=-7.32e-5, glen=54.5, tlen=214, kl=0.0111, act_lr=1e-6, ent=0.962]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.107, ret=-7.32e-5, glen=54.5, tlen=214, kl=0.0111, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.101, ret=0.000175, glen=56, tlen=216, kl=0.0116, act_lr=1e-6, ent=0.985] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.101, ret=0.000175, glen=56, tlen=216, kl=0.0116, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.101, ret=0.000163, glen=57.5, tlen=217, kl=0.00912, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.15it/s, pg=-0.101, ret=0.000163, glen=57.5, tlen=217, kl=0.00912, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.15it/s, pg=0.0663, ret=-0.000126, glen=60.1, tlen=220, kl=0.00714, act_lr=1e-6, ent=1]   Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.13it/s, pg=0.0663, ret=-0.000126, glen=60.1, tlen=220, kl=0.00714, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.13it/s, pg=-0.0651, ret=0.00011, glen=57.2, tlen=217, kl=0.01, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.15it/s, pg=-0.0651, ret=0.00011, glen=57.2, tlen=217, kl=0.01, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.15it/s, pg=-0.0544, ret=7.35e-5, glen=55.9, tlen=216, kl=0.0107, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.15it/s, pg=-0.0544, ret=7.35e-5, glen=55.9, tlen=216, kl=0.0107, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.15it/s, pg=0.0414, ret=-7.02e-5, glen=58.1, tlen=218, kl=0.00729, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.16it/s, pg=0.0414, ret=-7.02e-5, glen=58.1, tlen=218, kl=0.00729, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.16it/s, pg=-0.0506, ret=3.95e-5, glen=53.2, tlen=213, kl=0.0117, act_lr=1e-6, ent=0.967] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0506, ret=3.95e-5, glen=53.2, tlen=213, kl=0.0117, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:21<00:18,  1.17it/s, pg=-0.0101, ret=3.69e-6, glen=56.5, tlen=216, kl=0.00935, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0101, ret=3.69e-6, glen=56.5, tlen=216, kl=0.00935, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.0234, ret=5.85e-5, glen=58.8, tlen=218, kl=0.00757, act_lr=1e-6, ent=1.03]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.13it/s, pg=0.0234, ret=5.85e-5, glen=58.8, tlen=218, kl=0.00757, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.13it/s, pg=-0.0577, ret=5.48e-5, glen=56.4, tlen=216, kl=0.0126, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.14it/s, pg=-0.0577, ret=5.48e-5, glen=56.4, tlen=216, kl=0.0126, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.14it/s, pg=-0.0161, ret=-8.63e-6, glen=55.9, tlen=216, kl=0.00858, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.15it/s, pg=-0.0161, ret=-8.63e-6, glen=55.9, tlen=216, kl=0.00858, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.15it/s, pg=0.000122, ret=-9.48e-5, glen=55.6, tlen=215, kl=0.0102, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.16it/s, pg=0.000122, ret=-9.48e-5, glen=55.6, tlen=215, kl=0.0102, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.16it/s, pg=0.033, ret=-7.99e-5, glen=58.5, tlen=218, kl=0.0089, act_lr=1e-6, ent=0.992]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.06it/s, pg=0.033, ret=-7.99e-5, glen=58.5, tlen=218, kl=0.0089, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.06it/s, pg=0.0159, ret=1.85e-5, glen=55.8, tlen=215, kl=0.00774, act_lr=1e-6, ent=0.996]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=0.0159, ret=1.85e-5, glen=55.8, tlen=215, kl=0.00774, act_lr=1e-6, ent=0.996]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=0.23, ret=-0.000307, glen=58.7, tlen=219, kl=0.00832, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.23, ret=-0.000307, glen=58.7, tlen=219, kl=0.00832, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=0.0546, ret=-0.000112, glen=60, tlen=220, kl=0.00867, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.0546, ret=-0.000112, glen=60, tlen=220, kl=0.00867, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:29<00:11,  1.14it/s, pg=0.0288, ret=-2.12e-5, glen=55.9, tlen=216, kl=0.0115, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.0288, ret=-2.12e-5, glen=55.9, tlen=216, kl=0.0115, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0826, ret=0.000129, glen=56.3, tlen=216, kl=0.0106, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0826, ret=0.000129, glen=56.3, tlen=216, kl=0.0106, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.0576, ret=-7.2e-5, glen=54.2, tlen=213, kl=0.00957, act_lr=1e-6, ent=0.971] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.0576, ret=-7.2e-5, glen=54.2, tlen=213, kl=0.00957, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0425, ret=2.2e-5, glen=57.8, tlen=217, kl=0.0107, act_lr=1e-6, ent=0.993] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0425, ret=2.2e-5, glen=57.8, tlen=217, kl=0.0107, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=0.0618, ret=-0.000104, glen=57.5, tlen=218, kl=0.0086, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0618, ret=-0.000104, glen=57.5, tlen=218, kl=0.0086, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.103, ret=0.000174, glen=55.9, tlen=216, kl=0.011, act_lr=1e-6, ent=0.954]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.103, ret=0.000174, glen=55.9, tlen=216, kl=0.011, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:05,  1.17it/s, pg=-0.108, ret=0.000179, glen=56.9, tlen=217, kl=0.0101, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.108, ret=0.000179, glen=56.9, tlen=217, kl=0.0101, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0676, ret=-0.000122, glen=57, tlen=217, kl=0.0092, act_lr=1e-6, ent=0.966] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.18it/s, pg=0.0676, ret=-0.000122, glen=57, tlen=217, kl=0.0092, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.18it/s, pg=0.0315, ret=-8.56e-5, glen=56.3, tlen=217, kl=0.00974, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.18it/s, pg=0.0315, ret=-8.56e-5, glen=56.3, tlen=217, kl=0.00974, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.18it/s, pg=-0.0244, ret=1.75e-5, glen=55.3, tlen=215, kl=0.0122, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.0244, ret=1.75e-5, glen=55.3, tlen=215, kl=0.0122, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.0685, ret=9.41e-5, glen=56.2, tlen=216, kl=0.00778, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0685, ret=9.41e-5, glen=56.2, tlen=216, kl=0.00778, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=0.0327, ret=-7.38e-5, glen=56, tlen=216, kl=0.00764, act_lr=1e-6, ent=0.997]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=0.0327, ret=-7.38e-5, glen=56, tlen=216, kl=0.00764, act_lr=1e-6, ent=0.997]
2025-07-24 19:58:37.469 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.31s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.18it/s, pg=-0.0803, ret=0.000126, glen=57.4, tlen=217, kl=0.0088, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0803, ret=0.000126, glen=57.4, tlen=217, kl=0.0088, act_lr=1e-6, ent=0.963]
2025-07-24 19:58:38.318 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 19:58:40.906 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 19:58:41.253 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.22s
2025-07-24 19:58:41.259 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0004001193576388889, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9701883302794563, 'kl': 0.009567938910590278, 'response_length': 56.65118408203125, 'total_length': 216.42910529242621, 'teacher_total_length': 228.46607259114583, 'return': -1.6035331807668425e-06, 'policy_update_steps': 1.0}
Episode [7/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [12:34<10:45, 107.66s/it]2025-07-24 19:58:41.305 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 19:59:14.732 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 19:59:14.913 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 19:59:14.914 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.61s
2025-07-24 19:59:16.545 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 55.2639,std_num_tokens: 15.6775,avg_correct_num_tokens: 55.2403,std_correct_num_tokens: 15.6655,avg_incorrect_num_tokens: 61.0909,std_incorrect_num_tokens: 17.4484
2025-07-24 19:59:16.955 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.04s
2025-07-24 19:59:19.632 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.67s
2025-07-24 19:59:42.728 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 19:59:42.729 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.10s
2025-07-24 19:59:43.985 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 19:59:43.986 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4491370104755578e-05, avg_kl: 0.008141506923718398, avg_response_length: 55.28767221429375, avg_orm_score: 0.0, avg_custom_rewards: 1.4491370104755578e-05
2025-07-24 19:59:44.012 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter85_replay_buffer.jsonl
2025-07-24 19:59:45.173 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=-0.0917, ret=0.000145, glen=54.8, tlen=215, kl=0.00955, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.01it/s, pg=-0.0917, ret=0.000145, glen=54.8, tlen=215, kl=0.00955, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.01it/s, pg=-0.06, ret=8.14e-5, glen=54.2, tlen=214, kl=0.0084, act_lr=1e-6, ent=0.951]    Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.10it/s, pg=-0.06, ret=8.14e-5, glen=54.2, tlen=214, kl=0.0084, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.10it/s, pg=0.172, ret=-0.000268, glen=55.1, tlen=215, kl=0.00811, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.13it/s, pg=0.172, ret=-0.000268, glen=55.1, tlen=215, kl=0.00811, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.13it/s, pg=-0.0474, ret=5.91e-5, glen=56.1, tlen=217, kl=0.00851, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.0474, ret=5.91e-5, glen=56.1, tlen=217, kl=0.00851, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=0.0563, ret=-5.54e-5, glen=55.4, tlen=216, kl=0.00866, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.13it/s, pg=0.0563, ret=-5.54e-5, glen=55.4, tlen=216, kl=0.00866, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.13it/s, pg=0.174, ret=-0.000136, glen=57.5, tlen=218, kl=0.00875, act_lr=1e-6, ent=1]   Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.14it/s, pg=0.174, ret=-0.000136, glen=57.5, tlen=218, kl=0.00875, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.14it/s, pg=-0.0801, ret=0.000114, glen=55.3, tlen=215, kl=0.00851, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:32,  1.16it/s, pg=-0.0801, ret=0.000114, glen=55.3, tlen=215, kl=0.00851, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:32,  1.16it/s, pg=-0.0757, ret=9.9e-5, glen=55.1, tlen=215, kl=0.00807, act_lr=1e-6, ent=0.982]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0757, ret=9.9e-5, glen=55.1, tlen=215, kl=0.00807, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0072, ret=-1.36e-5, glen=55.6, tlen=216, kl=0.00805, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.13it/s, pg=-0.0072, ret=-1.36e-5, glen=55.6, tlen=216, kl=0.00805, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=-0.0876, ret=0.000111, glen=55.8, tlen=216, kl=0.00906, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.14it/s, pg=-0.0876, ret=0.000111, glen=55.8, tlen=216, kl=0.00906, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.14it/s, pg=0.0944, ret=-0.000158, glen=55.2, tlen=215, kl=0.0089, act_lr=1e-6, ent=0.985] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=0.0944, ret=-0.000158, glen=55.2, tlen=215, kl=0.0089, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=0.356, ret=-0.000405, glen=55.8, tlen=216, kl=0.00819, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=0.356, ret=-0.000405, glen=55.8, tlen=216, kl=0.00819, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=0.0131, ret=-3.04e-5, glen=53.2, tlen=213, kl=0.008, act_lr=1e-6, ent=0.956]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=0.0131, ret=-3.04e-5, glen=53.2, tlen=213, kl=0.008, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=-0.0397, ret=4.91e-5, glen=53.7, tlen=214, kl=0.00777, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.0397, ret=4.91e-5, glen=53.7, tlen=214, kl=0.00777, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=0.184, ret=-0.000269, glen=53.7, tlen=214, kl=0.00775, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.184, ret=-0.000269, glen=53.7, tlen=214, kl=0.00775, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0721, ret=9.06e-5, glen=54.7, tlen=215, kl=0.00758, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=-0.0721, ret=9.06e-5, glen=54.7, tlen=215, kl=0.00758, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0453, ret=2.14e-5, glen=55.3, tlen=215, kl=0.00861, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.0453, ret=2.14e-5, glen=55.3, tlen=215, kl=0.00861, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0596, ret=7.41e-5, glen=53.7, tlen=213, kl=0.00717, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:22,  1.17it/s, pg=-0.0596, ret=7.41e-5, glen=53.7, tlen=213, kl=0.00717, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:22,  1.17it/s, pg=-0.0923, ret=0.000123, glen=56.6, tlen=217, kl=0.00711, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.18it/s, pg=-0.0923, ret=0.000123, glen=56.6, tlen=217, kl=0.00711, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.18it/s, pg=-0.0552, ret=7.38e-5, glen=55.5, tlen=215, kl=0.00747, act_lr=1e-6, ent=0.979] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.18it/s, pg=-0.0552, ret=7.38e-5, glen=55.5, tlen=215, kl=0.00747, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.18it/s, pg=-0.0773, ret=0.000101, glen=55.6, tlen=216, kl=0.00797, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.18it/s, pg=-0.0773, ret=0.000101, glen=55.6, tlen=216, kl=0.00797, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.18it/s, pg=0.0155, ret=-4.19e-5, glen=55.5, tlen=216, kl=0.00859, act_lr=1e-6, ent=0.941] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.18it/s, pg=0.0155, ret=-4.19e-5, glen=55.5, tlen=216, kl=0.00859, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.18it/s, pg=-0.0684, ret=8.7e-5, glen=57.4, tlen=218, kl=0.00687, act_lr=1e-6, ent=0.969] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.18it/s, pg=-0.0684, ret=8.7e-5, glen=57.4, tlen=218, kl=0.00687, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.18it/s, pg=0.0567, ret=-2.95e-5, glen=56.2, tlen=216, kl=0.00811, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.18it/s, pg=0.0567, ret=-2.95e-5, glen=56.2, tlen=216, kl=0.00811, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.18it/s, pg=-0.0668, ret=9.05e-5, glen=55.5, tlen=216, kl=0.00796, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:16,  1.18it/s, pg=-0.0668, ret=9.05e-5, glen=55.5, tlen=216, kl=0.00796, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:16,  1.18it/s, pg=-0.051, ret=6.09e-5, glen=55.4, tlen=216, kl=0.00831, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.18it/s, pg=-0.051, ret=6.09e-5, glen=55.4, tlen=216, kl=0.00831, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.18it/s, pg=0.0129, ret=-1.84e-5, glen=55.6, tlen=216, kl=0.00701, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.18it/s, pg=0.0129, ret=-1.84e-5, glen=55.6, tlen=216, kl=0.00701, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.18it/s, pg=-0.0528, ret=6.65e-5, glen=55.9, tlen=216, kl=0.00931, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.18it/s, pg=-0.0528, ret=6.65e-5, glen=55.9, tlen=216, kl=0.00931, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.18it/s, pg=-0.062, ret=7.87e-5, glen=54.5, tlen=214, kl=0.00805, act_lr=1e-6, ent=0.921] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.062, ret=7.87e-5, glen=54.5, tlen=214, kl=0.00805, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0729, ret=8.59e-5, glen=56.3, tlen=217, kl=0.00794, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0729, ret=8.59e-5, glen=56.3, tlen=217, kl=0.00794, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=0.281, ret=-0.000274, glen=54.3, tlen=215, kl=0.00818, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:26<00:12,  1.12it/s, pg=0.281, ret=-0.000274, glen=54.3, tlen=215, kl=0.00818, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0595, ret=7.64e-5, glen=53.9, tlen=214, kl=0.0102, act_lr=1e-6, ent=0.968] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.0595, ret=7.64e-5, glen=53.9, tlen=214, kl=0.0102, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.0656, ret=-0.000152, glen=54.7, tlen=215, kl=0.00824, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=0.0656, ret=-0.000152, glen=54.7, tlen=215, kl=0.00824, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.0994, ret=-0.000146, glen=55.4, tlen=215, kl=0.00772, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.0994, ret=-0.000146, glen=55.4, tlen=215, kl=0.00772, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0693, ret=9.43e-5, glen=55.1, tlen=215, kl=0.008, act_lr=1e-6, ent=0.954]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0693, ret=9.43e-5, glen=55.1, tlen=215, kl=0.008, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0549, ret=7.14e-5, glen=55.7, tlen=216, kl=0.00816, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0549, ret=7.14e-5, glen=55.7, tlen=216, kl=0.00816, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=0.00159, ret=8.36e-7, glen=54.6, tlen=215, kl=0.0069, act_lr=1e-6, ent=0.991] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.00159, ret=8.36e-7, glen=54.6, tlen=215, kl=0.0069, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.033, ret=4e-5, glen=54.4, tlen=214, kl=0.008, act_lr=1e-6, ent=0.962]     Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=-0.033, ret=4e-5, glen=54.4, tlen=214, kl=0.008, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0728, ret=-0.00012, glen=55.7, tlen=216, kl=0.00683, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=0.0728, ret=-0.00012, glen=55.7, tlen=216, kl=0.00683, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0366, ret=-3.09e-5, glen=55.7, tlen=216, kl=0.00958, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=0.0366, ret=-3.09e-5, glen=55.7, tlen=216, kl=0.00958, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0721, ret=9.5e-5, glen=55.4, tlen=216, kl=0.0079, act_lr=1e-6, ent=0.965] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.18it/s, pg=-0.0721, ret=9.5e-5, glen=55.4, tlen=216, kl=0.0079, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.18it/s, pg=-0.0605, ret=8.02e-5, glen=56.6, tlen=217, kl=0.00802, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.0605, ret=8.02e-5, glen=56.6, tlen=217, kl=0.00802, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.057, ret=7.16e-5, glen=55.7, tlen=216, kl=0.00748, act_lr=1e-6, ent=0.966] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.057, ret=7.16e-5, glen=55.7, tlen=216, kl=0.00748, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=0.0875, ret=-0.000149, glen=53.4, tlen=214, kl=0.00822, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:37<00:00,  1.17it/s, pg=0.0875, ret=-0.000149, glen=53.4, tlen=214, kl=0.00822, act_lr=1e-6, ent=0.979]
2025-07-24 20:00:24.287 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.94s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0978, ret=0.000133, glen=56.7, tlen=217, kl=0.00833, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.0978, ret=0.000133, glen=56.7, tlen=217, kl=0.00833, act_lr=1e-6, ent=0.992]
2025-07-24 20:00:24.980 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 20:00:27.171 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.19s
2025-07-24 20:00:27.509 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.28s
2025-07-24 20:00:27.528 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00023600260416666667, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9664971854951646, 'kl': 0.008135138617621527, 'response_length': 55.28304409450955, 'total_length': 215.4347174750434, 'teacher_total_length': 227.46375189887152, 'return': -4.925494269830071e-07, 'policy_update_steps': 1.0}
Episode [7/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [14:20<08:56, 107.22s/it]2025-07-24 20:00:27.575 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:01:00.949 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:01:01.127 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:01:01.128 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.55s
2025-07-24 20:01:03.000 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0018,avg_pass_at_n: 1.0000,avg_num_tokens: 55.8751,std_num_tokens: 15.9429,avg_correct_num_tokens: 55.8089,std_correct_num_tokens: 15.8571,avg_incorrect_num_tokens: 66.6600,std_incorrect_num_tokens: 24.1161
2025-07-24 20:01:03.406 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.28s
2025-07-24 20:01:05.909 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.50s
2025-07-24 20:01:29.011 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 20:01:29.012 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.10s
2025-07-24 20:01:30.306 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 20:01:30.307 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -4.515179313261416e-06, avg_kl: 0.006795702033868715, avg_response_length: 55.90076540302298, avg_orm_score: 0.0, avg_custom_rewards: -4.515179313261416e-06
2025-07-24 20:01:30.336 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter86_replay_buffer.jsonl
2025-07-24 20:01:31.499 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=0.0122, ret=1.76e-6, glen=58.1, tlen=218, kl=0.00758, act_lr=1e-6, ent=1]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.02it/s, pg=0.0122, ret=1.76e-6, glen=58.1, tlen=218, kl=0.00758, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.02it/s, pg=0.119, ret=-0.000244, glen=56.7, tlen=217, kl=0.00803, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.10it/s, pg=0.119, ret=-0.000244, glen=56.7, tlen=217, kl=0.00803, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.10it/s, pg=-0.0779, ret=0.000124, glen=54.7, tlen=215, kl=0.00651, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.13it/s, pg=-0.0779, ret=0.000124, glen=54.7, tlen=215, kl=0.00651, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.13it/s, pg=-0.087, ret=0.000133, glen=57.3, tlen=218, kl=0.00682, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.087, ret=0.000133, glen=57.3, tlen=218, kl=0.00682, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=0.0235, ret=-3.69e-6, glen=54.8, tlen=216, kl=0.0065, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.14it/s, pg=0.0235, ret=-3.69e-6, glen=54.8, tlen=216, kl=0.0065, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.14it/s, pg=-0.0144, ret=4.04e-6, glen=54.5, tlen=215, kl=0.00693, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=-0.0144, ret=4.04e-6, glen=54.5, tlen=215, kl=0.00693, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=-0.0851, ret=0.000142, glen=55.4, tlen=217, kl=0.00714, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=-0.0851, ret=0.000142, glen=55.4, tlen=217, kl=0.00714, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=-0.0812, ret=0.00012, glen=54, tlen=215, kl=0.00655, act_lr=1e-6, ent=0.998]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.0812, ret=0.00012, glen=54, tlen=215, kl=0.00655, act_lr=1e-6, ent=0.998]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=0.0496, ret=-9.2e-5, glen=56, tlen=217, kl=0.00681, act_lr=1e-6, ent=0.958] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.16it/s, pg=0.0496, ret=-9.2e-5, glen=56, tlen=217, kl=0.00681, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.16it/s, pg=-0.0626, ret=9.52e-5, glen=56.3, tlen=217, kl=0.00581, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.17it/s, pg=-0.0626, ret=9.52e-5, glen=56.3, tlen=217, kl=0.00581, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.17it/s, pg=0.157, ret=-0.000108, glen=57.1, tlen=218, kl=0.00643, act_lr=1e-6, ent=1.02] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=0.157, ret=-0.000108, glen=57.1, tlen=218, kl=0.00643, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=0.0897, ret=-0.000194, glen=55.5, tlen=217, kl=0.00665, act_lr=1e-6, ent=0.993]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=0.0897, ret=-0.000194, glen=55.5, tlen=217, kl=0.00665, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=0.0748, ret=-0.000105, glen=55.6, tlen=216, kl=0.00755, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=0.0748, ret=-0.000105, glen=55.6, tlen=216, kl=0.00755, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=0.0458, ret=-5.25e-5, glen=54.9, tlen=215, kl=0.00655, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:27,  1.14it/s, pg=0.0458, ret=-5.25e-5, glen=54.9, tlen=215, kl=0.00655, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:27,  1.14it/s, pg=0.0703, ret=-0.000123, glen=57.1, tlen=218, kl=0.00703, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.15it/s, pg=0.0703, ret=-0.000123, glen=57.1, tlen=218, kl=0.00703, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.15it/s, pg=-0.0083, ret=1.12e-5, glen=56.3, tlen=217, kl=0.00744, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:25,  1.16it/s, pg=-0.0083, ret=1.12e-5, glen=56.3, tlen=217, kl=0.00744, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.16it/s, pg=0.00125, ret=-9.8e-5, glen=57, tlen=218, kl=0.00623, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.16it/s, pg=0.00125, ret=-9.8e-5, glen=57, tlen=218, kl=0.00623, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.16it/s, pg=0.0959, ret=-0.000103, glen=56.8, tlen=217, kl=0.00623, act_lr=1e-6, ent=1.03]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=0.0959, ret=-0.000103, glen=56.8, tlen=217, kl=0.00623, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=-0.0869, ret=0.000128, glen=55, tlen=215, kl=0.00699, act_lr=1e-6, ent=0.999] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.0869, ret=0.000128, glen=55, tlen=215, kl=0.00699, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=0.0623, ret=-9.77e-5, glen=56, tlen=217, kl=0.00655, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=0.0623, ret=-9.77e-5, glen=56, tlen=217, kl=0.00655, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=0.135, ret=-0.000121, glen=54.9, tlen=215, kl=0.00739, act_lr=1e-6, ent=0.996]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=0.135, ret=-0.000121, glen=54.9, tlen=215, kl=0.00739, act_lr=1e-6, ent=0.996]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.0403, ret=2.48e-5, glen=54.8, tlen=215, kl=0.00663, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0403, ret=2.48e-5, glen=54.8, tlen=215, kl=0.00663, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0704, ret=0.000103, glen=55.7, tlen=216, kl=0.00721, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=-0.0704, ret=0.000103, glen=55.7, tlen=216, kl=0.00721, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=0.0203, ret=-1.08e-5, glen=55.4, tlen=216, kl=0.00673, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=0.0203, ret=-1.08e-5, glen=55.4, tlen=216, kl=0.00673, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.0379, ret=-1.73e-5, glen=55.1, tlen=216, kl=0.00751, act_lr=1e-6, ent=0.99] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.0379, ret=-1.73e-5, glen=55.1, tlen=216, kl=0.00751, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0698, ret=0.000109, glen=57.3, tlen=218, kl=0.00567, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0698, ret=0.000109, glen=57.3, tlen=218, kl=0.00567, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.0298, ret=-6.3e-5, glen=57.5, tlen=218, kl=0.00723, act_lr=1e-6, ent=1.03]   Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=0.0298, ret=-6.3e-5, glen=57.5, tlen=218, kl=0.00723, act_lr=1e-6, ent=1.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0458, ret=2.75e-5, glen=54.3, tlen=215, kl=0.00591, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0458, ret=2.75e-5, glen=54.3, tlen=215, kl=0.00591, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0452, ret=3.49e-5, glen=55, tlen=216, kl=0.0059, act_lr=1e-6, ent=0.974]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.05it/s, pg=-0.0452, ret=3.49e-5, glen=55, tlen=216, kl=0.0059, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.05it/s, pg=-0.087, ret=0.000138, glen=55.8, tlen=217, kl=0.00736, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=-0.087, ret=0.000138, glen=55.8, tlen=217, kl=0.00736, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=0.0307, ret=-9.72e-5, glen=56.6, tlen=217, kl=0.0073, act_lr=1e-6, ent=1.02]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=0.0307, ret=-9.72e-5, glen=56.6, tlen=217, kl=0.0073, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=-0.087, ret=0.000127, glen=55.4, tlen=216, kl=0.00749, act_lr=1e-6, ent=0.996]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.13it/s, pg=-0.087, ret=0.000127, glen=55.4, tlen=216, kl=0.00749, act_lr=1e-6, ent=0.996]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=0.00598, ret=9.23e-6, glen=55.5, tlen=216, kl=0.00698, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.14it/s, pg=0.00598, ret=9.23e-6, glen=55.5, tlen=216, kl=0.00698, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=-0.0353, ret=3.13e-5, glen=56.8, tlen=218, kl=0.00667, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=-0.0353, ret=3.13e-5, glen=56.8, tlen=218, kl=0.00667, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=-0.0798, ret=0.000146, glen=57.2, tlen=218, kl=0.00658, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0798, ret=0.000146, glen=57.2, tlen=218, kl=0.00658, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.0725, ret=-9.58e-5, glen=54.3, tlen=215, kl=0.00646, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.15it/s, pg=0.0725, ret=-9.58e-5, glen=54.3, tlen=215, kl=0.00646, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.15it/s, pg=0.0133, ret=1.19e-5, glen=56.1, tlen=217, kl=0.00683, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.15it/s, pg=0.0133, ret=1.19e-5, glen=56.1, tlen=217, kl=0.00683, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.15it/s, pg=-0.0822, ret=0.000126, glen=55.3, tlen=216, kl=0.00707, act_lr=1e-6, ent=1] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:06,  1.15it/s, pg=-0.0822, ret=0.000126, glen=55.3, tlen=216, kl=0.00707, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:06,  1.15it/s, pg=-0.103, ret=0.000158, glen=56.1, tlen=217, kl=0.00701, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.16it/s, pg=-0.103, ret=0.000158, glen=56.1, tlen=217, kl=0.00701, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.16it/s, pg=-0.00317, ret=-9.38e-5, glen=56.9, tlen=218, kl=0.00688, act_lr=1e-6, ent=1.04]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.16it/s, pg=-0.00317, ret=-9.38e-5, glen=56.9, tlen=218, kl=0.00688, act_lr=1e-6, ent=1.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.16it/s, pg=-0.0176, ret=0, glen=55.8, tlen=217, kl=0.00609, act_lr=1e-6, ent=0.964]       Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0176, ret=0, glen=55.8, tlen=217, kl=0.00609, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=0.121, ret=-8.91e-5, glen=56.2, tlen=217, kl=0.00639, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=0.121, ret=-8.91e-5, glen=56.2, tlen=217, kl=0.00639, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0104, ret=3.32e-5, glen=57.6, tlen=219, kl=0.00655, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0104, ret=3.32e-5, glen=57.6, tlen=219, kl=0.00655, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=0.0859, ret=-0.00022, glen=56.7, tlen=218, kl=0.00639, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=0.0859, ret=-0.00022, glen=56.7, tlen=218, kl=0.00639, act_lr=1e-6, ent=0.985]
2025-07-24 20:02:10.886 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.14s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=-0.0588, ret=9.14e-5, glen=54.3, tlen=215, kl=0.00692, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.0588, ret=9.14e-5, glen=54.3, tlen=215, kl=0.00692, act_lr=1e-6, ent=0.964]
2025-07-24 20:02:11.563 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:02:13.834 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-24 20:02:14.184 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.57s
2025-07-24 20:02:14.191 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.000286865234375, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9973521153132121, 'kl': 0.006788296169704861, 'response_length': 55.90095401340061, 'total_length': 216.64409823947483, 'teacher_total_length': 228.7216284857856, 'return': -2.176866150370592e-06, 'policy_update_steps': 1.0}
Episode [7/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [16:07<07:08, 107.04s/it]2025-07-24 20:02:14.238 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:02:47.431 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:02:47.615 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:02:47.616 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.38s
2025-07-24 20:02:49.245 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0033,avg_pass_at_n: 1.0000,avg_num_tokens: 55.2987,std_num_tokens: 15.4280,avg_correct_num_tokens: 55.2805,std_correct_num_tokens: 15.4239,avg_incorrect_num_tokens: 58.0943,std_incorrect_num_tokens: 15.7962
2025-07-24 20:02:49.666 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.05s
2025-07-24 20:02:52.303 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.63s
2025-07-24 20:03:15.382 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 20:03:15.383 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.08s
2025-07-24 20:03:16.615 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.80s
2025-07-24 20:03:16.615 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.3583729969722599e-06, avg_kl: 0.006482513257245112, avg_response_length: 55.30855489975913, avg_orm_score: 0.0, avg_custom_rewards: 1.3583729969722599e-06
2025-07-24 20:03:16.643 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter87_replay_buffer.jsonl
2025-07-24 20:03:17.805 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0633, ret=0.000101, glen=55.3, tlen=217, kl=0.00617, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0633, ret=0.000101, glen=55.3, tlen=217, kl=0.00617, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0715, ret=0.000114, glen=54.6, tlen=216, kl=0.00637, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=-0.0715, ret=0.000114, glen=54.6, tlen=216, kl=0.00637, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.0947, ret=0.000157, glen=54, tlen=216, kl=0.00585, act_lr=1e-6, ent=0.953]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:39,  1.08it/s, pg=-0.0947, ret=0.000157, glen=54, tlen=216, kl=0.00585, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:39,  1.08it/s, pg=-0.111, ret=0.000188, glen=57.2, tlen=218, kl=0.00614, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:37,  1.09it/s, pg=-0.111, ret=0.000188, glen=57.2, tlen=218, kl=0.00614, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:37,  1.09it/s, pg=-0.102, ret=0.000173, glen=55.9, tlen=217, kl=0.00729, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.12it/s, pg=-0.102, ret=0.000173, glen=55.9, tlen=217, kl=0.00729, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.12it/s, pg=0.11, ret=-0.000217, glen=55.5, tlen=217, kl=0.00648, act_lr=1e-6, ent=0.948] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.14it/s, pg=0.11, ret=-0.000217, glen=55.5, tlen=217, kl=0.00648, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.14it/s, pg=0.0602, ret=-6.83e-5, glen=54.5, tlen=215, kl=0.00665, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.15it/s, pg=0.0602, ret=-6.83e-5, glen=54.5, tlen=215, kl=0.00665, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.15it/s, pg=0.126, ret=-0.000195, glen=54.4, tlen=216, kl=0.00677, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=0.126, ret=-0.000195, glen=54.4, tlen=216, kl=0.00677, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.14it/s, pg=0.102, ret=-0.000113, glen=56.1, tlen=217, kl=0.00614, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=0.102, ret=-0.000113, glen=56.1, tlen=217, kl=0.00614, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=-0.00879, ret=4.6e-5, glen=54.6, tlen=216, kl=0.00756, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=-0.00879, ret=4.6e-5, glen=54.6, tlen=216, kl=0.00756, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=-0.0977, ret=0.000161, glen=55.6, tlen=217, kl=0.00895, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=-0.0977, ret=0.000161, glen=55.6, tlen=217, kl=0.00895, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.104, ret=0.000177, glen=56, tlen=217, kl=0.00589, act_lr=1e-6, ent=0.931]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.104, ret=0.000177, glen=56, tlen=217, kl=0.00589, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=-0.00873, ret=1.28e-5, glen=56.3, tlen=218, kl=0.00599, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.00873, ret=1.28e-5, glen=56.3, tlen=218, kl=0.00599, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0719, ret=-0.000114, glen=54.1, tlen=215, kl=0.00649, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0719, ret=-0.000114, glen=54.1, tlen=215, kl=0.00649, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=0.0624, ret=-9.8e-5, glen=56.6, tlen=218, kl=0.006, act_lr=1e-6, ent=0.97]     Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.0624, ret=-9.8e-5, glen=56.6, tlen=218, kl=0.006, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.124, ret=-0.000229, glen=55.6, tlen=217, kl=0.00623, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=0.124, ret=-0.000229, glen=55.6, tlen=217, kl=0.00623, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.00293, ret=3.69e-6, glen=55.1, tlen=216, kl=0.00596, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.00293, ret=3.69e-6, glen=55.1, tlen=216, kl=0.00596, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0217, ret=2.6e-5, glen=54.9, tlen=216, kl=0.00636, act_lr=1e-6, ent=0.928]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.15it/s, pg=-0.0217, ret=2.6e-5, glen=54.9, tlen=216, kl=0.00636, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.15it/s, pg=-0.0308, ret=1.79e-5, glen=56.4, tlen=218, kl=0.0063, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:23,  1.13it/s, pg=-0.0308, ret=1.79e-5, glen=56.4, tlen=218, kl=0.0063, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:23,  1.13it/s, pg=0.0142, ret=7.43e-6, glen=56, tlen=218, kl=0.00594, act_lr=1e-6, ent=0.916]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.14it/s, pg=0.0142, ret=7.43e-6, glen=56, tlen=218, kl=0.00594, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.14it/s, pg=0.11, ret=-0.000225, glen=54.7, tlen=216, kl=0.0067, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.15it/s, pg=0.11, ret=-0.000225, glen=54.7, tlen=216, kl=0.0067, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.15it/s, pg=0.0597, ret=-9.49e-5, glen=57.3, tlen=219, kl=0.00795, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:20,  1.13it/s, pg=0.0597, ret=-9.49e-5, glen=57.3, tlen=219, kl=0.00795, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:20,  1.13it/s, pg=0.0959, ret=-0.000103, glen=54.9, tlen=216, kl=0.00589, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.15it/s, pg=0.0959, ret=-0.000103, glen=54.9, tlen=216, kl=0.00589, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.15it/s, pg=-0.072, ret=0.000123, glen=56.9, tlen=218, kl=0.00632, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.15it/s, pg=-0.072, ret=0.000123, glen=56.9, tlen=218, kl=0.00632, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.15it/s, pg=-0.00592, ret=-8.51e-7, glen=56.2, tlen=218, kl=0.0061, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.16it/s, pg=-0.00592, ret=-8.51e-7, glen=56.2, tlen=218, kl=0.0061, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.16it/s, pg=-0.067, ret=0.000112, glen=56.4, tlen=217, kl=0.00626, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.16it/s, pg=-0.067, ret=0.000112, glen=56.4, tlen=217, kl=0.00626, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.16it/s, pg=-0.0405, ret=5.04e-5, glen=53.1, tlen=214, kl=0.00638, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0405, ret=5.04e-5, glen=53.1, tlen=214, kl=0.00638, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0835, ret=0.000142, glen=54, tlen=215, kl=0.00655, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0835, ret=0.000142, glen=54, tlen=215, kl=0.00655, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.074, ret=0.000125, glen=55.2, tlen=216, kl=0.00571, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.05it/s, pg=-0.074, ret=0.000125, glen=55.2, tlen=216, kl=0.00571, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.05it/s, pg=0.0327, ret=-8.81e-5, glen=55.8, tlen=217, kl=0.00594, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=0.0327, ret=-8.81e-5, glen=55.8, tlen=217, kl=0.00594, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=-0.0428, ret=8.1e-5, glen=54.4, tlen=216, kl=0.00607, act_lr=1e-6, ent=0.897] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=-0.0428, ret=8.1e-5, glen=54.4, tlen=216, kl=0.00607, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.11it/s, pg=0.0309, ret=-0.000102, glen=55.1, tlen=216, kl=0.00609, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=0.0309, ret=-0.000102, glen=55.1, tlen=216, kl=0.00609, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=0.0856, ret=-8.62e-5, glen=55.7, tlen=217, kl=0.00597, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.14it/s, pg=0.0856, ret=-8.62e-5, glen=55.7, tlen=217, kl=0.00597, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=-0.025, ret=1.58e-5, glen=54.2, tlen=216, kl=0.00703, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=-0.025, ret=1.58e-5, glen=54.2, tlen=216, kl=0.00703, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=0.133, ret=-0.000221, glen=56.8, tlen=218, kl=0.00668, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.133, ret=-0.000221, glen=56.8, tlen=218, kl=0.00668, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.046, ret=-7.83e-5, glen=53.7, tlen=215, kl=0.00826, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=0.046, ret=-7.83e-5, glen=53.7, tlen=215, kl=0.00826, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0631, ret=0.000112, glen=56.1, tlen=218, kl=0.00642, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0631, ret=0.000112, glen=56.1, tlen=218, kl=0.00642, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=0.0268, ret=-1.04e-5, glen=53.6, tlen=215, kl=0.00622, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0268, ret=-1.04e-5, glen=53.6, tlen=215, kl=0.00622, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:05,  1.17it/s, pg=-0.0895, ret=0.000154, glen=52.9, tlen=215, kl=0.00646, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0895, ret=0.000154, glen=52.9, tlen=215, kl=0.00646, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0771, ret=0.000128, glen=53.4, tlen=214, kl=0.00652, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0771, ret=0.000128, glen=53.4, tlen=214, kl=0.00652, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0435, ret=4.61e-5, glen=54.6, tlen=216, kl=0.00717, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0435, ret=4.61e-5, glen=54.6, tlen=216, kl=0.00717, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=0.145, ret=-0.000266, glen=57.3, tlen=218, kl=0.00617, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=0.145, ret=-0.000266, glen=57.3, tlen=218, kl=0.00617, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0281, ret=2.01e-5, glen=56, tlen=217, kl=0.00624, act_lr=1e-6, ent=0.965]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0281, ret=2.01e-5, glen=56, tlen=217, kl=0.00624, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0339, ret=4.21e-5, glen=56.3, tlen=218, kl=0.00642, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0339, ret=4.21e-5, glen=56.3, tlen=218, kl=0.00642, act_lr=1e-6, ent=0.949]
2025-07-24 20:03:57.266 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.29s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=-0.0255, ret=3.75e-5, glen=55.7, tlen=217, kl=0.00658, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0255, ret=3.75e-5, glen=55.7, tlen=217, kl=0.00658, act_lr=1e-6, ent=0.948]
2025-07-24 20:03:58.098 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:04:00.662 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 20:04:01.012 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.16s
2025-07-24 20:04:01.061 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0011508517795138888, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9429710586865743, 'kl': 0.006481085883246528, 'response_length': 55.31517435709635, 'total_length': 216.5389383951823, 'teacher_total_length': 228.48691033257379, 'return': 1.4114905045264297e-06, 'policy_update_steps': 1.0}
Episode [7/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [17:54<05:20, 106.99s/it]2025-07-24 20:04:01.107 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:04:33.819 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:04:34.001 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:04:34.002 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.90s
2025-07-24 20:04:35.927 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 53.9792,std_num_tokens: 14.7181,avg_correct_num_tokens: 53.9601,std_correct_num_tokens: 14.7084,avg_incorrect_num_tokens: 57.4444,std_incorrect_num_tokens: 16.0063
2025-07-24 20:04:36.369 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.37s
2025-07-24 20:04:38.984 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.61s
2025-07-24 20:05:02.064 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 177
2025-07-24 20:05:02.064 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.08s
2025-07-24 20:05:04.822 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 20:05:04.822 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4018607431086106e-05, avg_kl: 0.007523703709834039, avg_response_length: 54.0015721294166, avg_orm_score: 0.0, avg_custom_rewards: 1.4018607431086106e-05
2025-07-24 20:05:04.853 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter88_replay_buffer.jsonl
2025-07-24 20:05:06.007 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.111, ret=0.000174, glen=55.2, tlen=216, kl=0.00751, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.02s/it, pg=-0.111, ret=0.000174, glen=55.2, tlen=216, kl=0.00751, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.02s/it, pg=-0.065, ret=0.000101, glen=53.7, tlen=214, kl=0.00761, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.065, ret=0.000101, glen=53.7, tlen=214, kl=0.00761, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0628, ret=9.7e-5, glen=54.8, tlen=215, kl=0.00694, act_lr=1e-6, ent=0.984] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.10it/s, pg=-0.0628, ret=9.7e-5, glen=54.8, tlen=215, kl=0.00694, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.10it/s, pg=-0.0828, ret=0.000133, glen=52.8, tlen=213, kl=0.00726, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.0828, ret=0.000133, glen=52.8, tlen=213, kl=0.00726, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=0.0152, ret=2.16e-5, glen=52.5, tlen=213, kl=0.00754, act_lr=1e-6, ent=0.991]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.09it/s, pg=0.0152, ret=2.16e-5, glen=52.5, tlen=213, kl=0.00754, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.09it/s, pg=0.0308, ret=-2.89e-5, glen=56.4, tlen=216, kl=0.00815, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.12it/s, pg=0.0308, ret=-2.89e-5, glen=56.4, tlen=216, kl=0.00815, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.12it/s, pg=-0.0896, ret=0.000126, glen=55.7, tlen=216, kl=0.00708, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.13it/s, pg=-0.0896, ret=0.000126, glen=55.7, tlen=216, kl=0.00708, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.13it/s, pg=-0.0311, ret=3.63e-5, glen=53.6, tlen=213, kl=0.00815, act_lr=1e-6, ent=0.984] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=-0.0311, ret=3.63e-5, glen=53.6, tlen=213, kl=0.00815, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.13it/s, pg=0.116, ret=-0.000237, glen=55, tlen=215, kl=0.00796, act_lr=1e-6, ent=0.972]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=0.116, ret=-0.000237, glen=55, tlen=215, kl=0.00796, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=0.0393, ret=-7.41e-5, glen=52.8, tlen=213, kl=0.00797, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=0.0393, ret=-7.41e-5, glen=52.8, tlen=213, kl=0.00797, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=-0.0613, ret=9.49e-5, glen=54.5, tlen=215, kl=0.00733, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=-0.0613, ret=9.49e-5, glen=54.5, tlen=215, kl=0.00733, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=0.00403, ret=-7.58e-6, glen=54, tlen=214, kl=0.00772, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=0.00403, ret=-7.58e-6, glen=54, tlen=214, kl=0.00772, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=-0.0851, ret=0.000106, glen=54.1, tlen=215, kl=0.007, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=-0.0851, ret=0.000106, glen=54.1, tlen=215, kl=0.007, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=-0.0559, ret=8.76e-5, glen=56.3, tlen=217, kl=0.00773, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.0559, ret=8.76e-5, glen=56.3, tlen=217, kl=0.00773, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.0339, ret=5e-5, glen=54.4, tlen=215, kl=0.00728, act_lr=1e-6, ent=0.943]   Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0339, ret=5e-5, glen=54.4, tlen=215, kl=0.00728, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.17it/s, pg=-0.072, ret=9.9e-5, glen=55.1, tlen=215, kl=0.00722, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.072, ret=9.9e-5, glen=55.1, tlen=215, kl=0.00722, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.00061, ret=-1.47e-5, glen=51.8, tlen=213, kl=0.00701, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=0.00061, ret=-1.47e-5, glen=51.8, tlen=213, kl=0.00701, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=0.0929, ret=-0.000105, glen=55.5, tlen=216, kl=0.00734, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=0.0929, ret=-0.000105, glen=55.5, tlen=216, kl=0.00734, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=0.0788, ret=-0.000155, glen=55.4, tlen=216, kl=0.0071, act_lr=1e-6, ent=0.996]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=0.0788, ret=-0.000155, glen=55.4, tlen=216, kl=0.0071, act_lr=1e-6, ent=0.996]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=0.0616, ret=-9.85e-5, glen=52.9, tlen=213, kl=0.00774, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.14it/s, pg=0.0616, ret=-9.85e-5, glen=52.9, tlen=213, kl=0.00774, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.14it/s, pg=0.028, ret=-1.61e-5, glen=52.7, tlen=213, kl=0.00806, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.15it/s, pg=0.028, ret=-1.61e-5, glen=52.7, tlen=213, kl=0.00806, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.15it/s, pg=-0.0973, ret=0.000149, glen=52.9, tlen=213, kl=0.00731, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.16it/s, pg=-0.0973, ret=0.000149, glen=52.9, tlen=213, kl=0.00731, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.16it/s, pg=0.0218, ret=-3.94e-5, glen=54.4, tlen=215, kl=0.00774, act_lr=1e-6, ent=0.974] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=0.0218, ret=-3.94e-5, glen=54.4, tlen=215, kl=0.00774, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=-0.0605, ret=9.5e-5, glen=53.8, tlen=214, kl=0.00843, act_lr=1e-6, ent=0.977] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0605, ret=9.5e-5, glen=53.8, tlen=214, kl=0.00843, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.129, ret=-0.000128, glen=54, tlen=214, kl=0.00644, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.129, ret=-0.000128, glen=54, tlen=214, kl=0.00644, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=0.26, ret=-0.000377, glen=53.9, tlen=214, kl=0.00748, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=0.26, ret=-0.000377, glen=53.9, tlen=214, kl=0.00748, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.0565, ret=8.42e-5, glen=53.3, tlen=214, kl=0.00733, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0565, ret=8.42e-5, glen=53.3, tlen=214, kl=0.00733, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=0.0134, ret=-3.48e-5, glen=53.3, tlen=214, kl=0.00733, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=0.0134, ret=-3.48e-5, glen=53.3, tlen=214, kl=0.00733, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.0343, ret=-2.62e-5, glen=53.3, tlen=214, kl=0.00824, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.06it/s, pg=0.0343, ret=-2.62e-5, glen=53.3, tlen=214, kl=0.00824, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.06it/s, pg=-0.00482, ret=-3.51e-5, glen=54.1, tlen=214, kl=0.00877, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=-0.00482, ret=-3.51e-5, glen=54.1, tlen=214, kl=0.00877, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=-0.0123, ret=-6.67e-5, glen=55.2, tlen=216, kl=0.00734, act_lr=1e-6, ent=0.982] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0123, ret=-6.67e-5, glen=55.2, tlen=216, kl=0.00734, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=-0.00598, ret=3.54e-6, glen=51.7, tlen=212, kl=0.00758, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.00598, ret=3.54e-6, glen=51.7, tlen=212, kl=0.00758, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=0.0808, ret=-0.000114, glen=52.8, tlen=213, kl=0.00729, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=0.0808, ret=-0.000114, glen=52.8, tlen=213, kl=0.00729, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0849, ret=0.000122, glen=53.8, tlen=214, kl=0.00783, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0849, ret=0.000122, glen=53.8, tlen=214, kl=0.00783, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0618, ret=9.04e-5, glen=52.7, tlen=213, kl=0.00795, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0618, ret=9.04e-5, glen=52.7, tlen=213, kl=0.00795, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.116, ret=-0.000131, glen=54, tlen=214, kl=0.00807, act_lr=1e-6, ent=0.976]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=0.116, ret=-0.000131, glen=54, tlen=214, kl=0.00807, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=0.0233, ret=-4.98e-5, glen=52.9, tlen=213, kl=0.00707, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0233, ret=-4.98e-5, glen=52.9, tlen=213, kl=0.00707, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0742, ret=0.000122, glen=56.6, tlen=217, kl=0.00697, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0742, ret=0.000122, glen=56.6, tlen=217, kl=0.00697, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.07, ret=9.81e-5, glen=54.7, tlen=215, kl=0.00747, act_lr=1e-6, ent=0.974]   Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.07, ret=9.81e-5, glen=54.7, tlen=215, kl=0.00747, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.019, ret=-4.57e-5, glen=56, tlen=216, kl=0.00758, act_lr=1e-6, ent=0.974] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=0.019, ret=-4.57e-5, glen=56, tlen=216, kl=0.00758, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0545, ret=7.83e-5, glen=54.1, tlen=215, kl=0.0079, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0545, ret=7.83e-5, glen=54.1, tlen=215, kl=0.0079, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0222, ret=8.72e-6, glen=55.6, tlen=216, kl=0.00739, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0222, ret=8.72e-6, glen=55.6, tlen=216, kl=0.00739, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=0.176, ret=-0.000162, glen=52.7, tlen=214, kl=0.00775, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.176, ret=-0.000162, glen=52.7, tlen=214, kl=0.00775, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.123, ret=0.000179, glen=53.7, tlen=214, kl=0.00703, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.123, ret=0.000179, glen=53.7, tlen=214, kl=0.00703, act_lr=1e-6, ent=0.964]
2025-07-24 20:05:45.382 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.20s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=0.171, ret=-0.000217, glen=53.1, tlen=214, kl=0.00717, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=0.171, ret=-0.000217, glen=53.1, tlen=214, kl=0.00717, act_lr=1e-6, ent=0.966]
2025-07-24 20:05:46.175 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.73s
2025-07-24 20:05:48.753 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 20:05:49.099 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.04s
2025-07-24 20:05:49.106 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0007242838541666667, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9788004345364041, 'kl': 0.007536994086371528, 'response_length': 54.028961690266925, 'total_length': 214.46932542588976, 'teacher_total_length': 226.51952853732638, 'return': -1.6107977798027504e-07, 'policy_update_steps': 1.0}
Episode [7/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [19:42<03:34, 107.31s/it]2025-07-24 20:05:49.153 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:06:21.191 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:06:21.363 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:06:21.363 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.21s
2025-07-24 20:06:23.161 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 52.9958,std_num_tokens: 14.7670,avg_correct_num_tokens: 52.9752,std_correct_num_tokens: 14.7647,avg_incorrect_num_tokens: 56.7333,std_incorrect_num_tokens: 14.7127
2025-07-24 20:06:23.729 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.37s
2025-07-24 20:06:26.179 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.45s
2025-07-24 20:06:48.891 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 176
2025-07-24 20:06:48.891 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.71s
2025-07-24 20:06:50.208 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 20:06:50.208 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -6.699656131042337e-06, avg_kl: 0.00965482538396662, avg_response_length: 53.01971643621271, avg_orm_score: 0.0, avg_custom_rewards: -6.699656131042337e-06
2025-07-24 20:06:50.242 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter89_replay_buffer.jsonl
2025-07-24 20:06:51.390 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.15s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0691, ret=0.000113, glen=51.9, tlen=212, kl=0.0108, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=-0.0691, ret=0.000113, glen=51.9, tlen=212, kl=0.0108, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=0.0107, ret=-2.65e-5, glen=52.5, tlen=213, kl=0.0103, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=0.0107, ret=-2.65e-5, glen=52.5, tlen=213, kl=0.0103, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=0.0204, ret=-5.09e-5, glen=53.8, tlen=214, kl=0.0101, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:37,  1.09it/s, pg=0.0204, ret=-5.09e-5, glen=53.8, tlen=214, kl=0.0101, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:37,  1.09it/s, pg=-0.0558, ret=8.25e-5, glen=55.4, tlen=216, kl=0.0112, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.12it/s, pg=-0.0558, ret=8.25e-5, glen=55.4, tlen=216, kl=0.0112, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.12it/s, pg=0.137, ret=-0.00019, glen=51.2, tlen=212, kl=0.00929, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.14it/s, pg=0.137, ret=-0.00019, glen=51.2, tlen=212, kl=0.00929, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.14it/s, pg=-0.0524, ret=8.19e-5, glen=54.2, tlen=215, kl=0.00987, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.15it/s, pg=-0.0524, ret=8.19e-5, glen=54.2, tlen=215, kl=0.00987, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.15it/s, pg=-0.0868, ret=0.000133, glen=55.3, tlen=216, kl=0.00935, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:31,  1.16it/s, pg=-0.0868, ret=0.000133, glen=55.3, tlen=216, kl=0.00935, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:31,  1.16it/s, pg=0.202, ret=-0.000255, glen=54.2, tlen=215, kl=0.00928, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.202, ret=-0.000255, glen=54.2, tlen=215, kl=0.00928, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=-0.0992, ret=0.000155, glen=53.9, tlen=214, kl=0.0112, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.14it/s, pg=-0.0992, ret=0.000155, glen=53.9, tlen=214, kl=0.0112, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=0.02, ret=-1.35e-5, glen=53.8, tlen=214, kl=0.00972, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=0.02, ret=-1.35e-5, glen=53.8, tlen=214, kl=0.00972, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=-0.0567, ret=8.75e-5, glen=53.3, tlen=214, kl=0.00966, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.0567, ret=8.75e-5, glen=53.3, tlen=214, kl=0.00966, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=0.00323, ret=-2.4e-5, glen=51.8, tlen=212, kl=0.0102, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=0.00323, ret=-2.4e-5, glen=51.8, tlen=212, kl=0.0102, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.0846, ret=0.000146, glen=55, tlen=216, kl=0.0101, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=-0.0846, ret=0.000146, glen=55, tlen=216, kl=0.0101, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=-0.0819, ret=0.000119, glen=52.5, tlen=213, kl=0.01, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=-0.0819, ret=0.000119, glen=52.5, tlen=213, kl=0.01, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=0.156, ret=-0.000212, glen=52.9, tlen=213, kl=0.00969, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=0.156, ret=-0.000212, glen=52.9, tlen=213, kl=0.00969, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=0.000122, ret=-2.22e-5, glen=54.4, tlen=215, kl=0.00996, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:23,  1.17it/s, pg=0.000122, ret=-2.22e-5, glen=54.4, tlen=215, kl=0.00996, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=0.0394, ret=-9.57e-5, glen=54.6, tlen=215, kl=0.00935, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.0394, ret=-9.57e-5, glen=54.6, tlen=215, kl=0.00935, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.1, ret=0.000154, glen=53.4, tlen=214, kl=0.00983, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=-0.1, ret=0.000154, glen=53.4, tlen=214, kl=0.00983, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=-0.0974, ret=0.000152, glen=51.7, tlen=212, kl=0.00955, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=-0.0974, ret=0.000152, glen=51.7, tlen=212, kl=0.00955, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=0.0901, ret=-0.000122, glen=53.4, tlen=214, kl=0.00997, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.0901, ret=-0.000122, glen=53.4, tlen=214, kl=0.00997, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=0.0568, ret=-0.000115, glen=52.2, tlen=213, kl=0.00896, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=0.0568, ret=-0.000115, glen=52.2, tlen=213, kl=0.00896, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=-0.00269, ret=-1.41e-5, glen=51.4, tlen=212, kl=0.00935, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.00269, ret=-1.41e-5, glen=51.4, tlen=212, kl=0.00935, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0094, ret=-9.04e-6, glen=51.3, tlen=212, kl=0.00894, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.17it/s, pg=0.0094, ret=-9.04e-6, glen=51.3, tlen=212, kl=0.00894, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=0.0764, ret=-8.88e-5, glen=52.9, tlen=214, kl=0.00951, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.16it/s, pg=0.0764, ret=-8.88e-5, glen=52.9, tlen=214, kl=0.00951, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.16it/s, pg=0.0205, ret=-8.77e-5, glen=53.5, tlen=214, kl=0.00876, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.16it/s, pg=0.0205, ret=-8.77e-5, glen=53.5, tlen=214, kl=0.00876, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.16it/s, pg=-0.0693, ret=0.000102, glen=51.8, tlen=213, kl=0.00923, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=-0.0693, ret=0.000102, glen=51.8, tlen=213, kl=0.00923, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=-0.0555, ret=3.21e-5, glen=52.3, tlen=212, kl=0.00912, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=-0.0555, ret=3.21e-5, glen=52.3, tlen=212, kl=0.00912, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.000793, ret=-1.29e-5, glen=53.7, tlen=214, kl=0.00859, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.000793, ret=-1.29e-5, glen=53.7, tlen=214, kl=0.00859, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.0618, ret=9.02e-5, glen=51.6, tlen=212, kl=0.00955, act_lr=1e-6, ent=0.925]  Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=-0.0618, ret=9.02e-5, glen=51.6, tlen=212, kl=0.00955, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.0961, ret=-0.000103, glen=51.6, tlen=212, kl=0.0098, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.0961, ret=-0.000103, glen=51.6, tlen=212, kl=0.0098, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=0.142, ret=-0.000214, glen=52, tlen=213, kl=0.00854, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.142, ret=-0.000214, glen=52, tlen=213, kl=0.00854, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.059, ret=-5.14e-5, glen=52.7, tlen=214, kl=0.00937, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.13it/s, pg=0.059, ret=-5.14e-5, glen=52.7, tlen=214, kl=0.00937, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.0127, ret=3.35e-5, glen=51.3, tlen=212, kl=0.00983, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.14it/s, pg=0.0127, ret=3.35e-5, glen=51.3, tlen=212, kl=0.00983, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.14it/s, pg=-0.0615, ret=0.000102, glen=52.5, tlen=213, kl=0.0111, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.0615, ret=0.000102, glen=52.5, tlen=213, kl=0.0111, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=0.0334, ret=-5.22e-5, glen=53.3, tlen=214, kl=0.00926, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.0334, ret=-5.22e-5, glen=53.3, tlen=214, kl=0.00926, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.138, ret=-0.000192, glen=53, tlen=213, kl=0.00986, act_lr=1e-6, ent=0.939]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=0.138, ret=-0.000192, glen=53, tlen=213, kl=0.00986, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.0479, ret=4.5e-5, glen=54.6, tlen=215, kl=0.00851, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0479, ret=4.5e-5, glen=54.6, tlen=215, kl=0.00851, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=-0.0596, ret=7.95e-5, glen=52.4, tlen=212, kl=0.00999, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0596, ret=7.95e-5, glen=52.4, tlen=212, kl=0.00999, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.092, ret=0.000137, glen=53.3, tlen=214, kl=0.0089, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.092, ret=0.000137, glen=53.3, tlen=214, kl=0.0089, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.107, ret=0.000154, glen=54.2, tlen=214, kl=0.0101, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.107, ret=0.000154, glen=54.2, tlen=214, kl=0.0101, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.149, ret=-0.00025, glen=53.3, tlen=214, kl=0.0097, act_lr=1e-6, ent=0.951] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.18it/s, pg=0.149, ret=-0.00025, glen=53.3, tlen=214, kl=0.0097, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.18it/s, pg=-0.0597, ret=9.14e-5, glen=52.3, tlen=213, kl=0.00989, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=-0.0597, ret=9.14e-5, glen=52.3, tlen=213, kl=0.00989, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=-0.0693, ret=9.5e-5, glen=52.9, tlen=214, kl=0.00916, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=-0.0693, ret=9.5e-5, glen=52.9, tlen=214, kl=0.00916, act_lr=1e-6, ent=0.922]
2025-07-24 20:07:29.780 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=-0.00385, ret=4.94e-6, glen=53.3, tlen=214, kl=0.00955, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=-0.00385, ret=4.94e-6, glen=53.3, tlen=214, kl=0.00955, act_lr=1e-6, ent=0.916]
2025-07-24 20:07:30.442 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 20:07:32.759 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-24 20:07:33.090 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.64s
2025-07-24 20:07:33.096 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -1.3871626420454545e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9388404718854211, 'kl': 0.00965482538396662, 'response_length': 53.01971643621271, 'total_length': 213.54500371759588, 'teacher_total_length': 225.51210888949308, 'return': -3.1214829670551064e-07, 'policy_update_steps': 1.0}
Episode [7/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [21:26<01:46, 106.30s/it]2025-07-24 20:07:33.101 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   1%|          | 1/172 [00:00<01:42,  1.66it/s, est. speed input: 301.00 toks/s, output: 34.92 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   2%|‚ñè         | 3/172 [00:00<00:37,  4.47it/s, est. speed input: 683.21 toks/s, output: 96.70 toks/s]Processed prompts:   4%|‚ñç         | 7/172 [00:00<00:14, 11.08it/s, est. speed input: 1395.62 toks/s, output: 222.10 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 161/172 [00:02<00:00, 115.57it/s, est. speed input: 13749.94 toks/s, output: 3528.83 toks/s]
2025-07-24 20:07:36.917 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 233.0073,strategyqa_test/accuracy: 0.5313,eval_accuracy: 0.5313
2025-07-24 20:07:37.180 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:07:53.865 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:07:54.046 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:07:54.047 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 16.87s
2025-07-24 20:07:55.130 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0028,avg_pass_at_n: 1.0000,avg_num_tokens: 51.8444,std_num_tokens: 14.1928,avg_correct_num_tokens: 51.8055,std_correct_num_tokens: 14.1677,avg_incorrect_num_tokens: 54.2985,std_incorrect_num_tokens: 15.4939
2025-07-24 20:07:55.410 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.36s
2025-07-24 20:07:56.799 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.39s
2025-07-24 20:08:08.922 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 92
2025-07-24 20:08:08.922 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.12s
2025-07-24 20:08:09.847 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.51s
2025-07-24 20:08:09.847 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 5.8083270825749345e-05, avg_kl: 0.011301455290421196, avg_response_length: 51.88795172649881, avg_orm_score: 0.0, avg_custom_rewards: 5.8083270825749345e-05
2025-07-24 20:08:09.902 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter90_replay_buffer.jsonl
2025-07-24 20:08:10.535 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.64s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:02<00:00, 118.26it/s, est. speed input: 13228.19 toks/s, output: 3521.86 toks/s][32m [repeated 49x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 57.01it/s, est. speed input: 10328.27 toks/s, output: 2777.86 toks/s] [32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s, pg=0.00409, ret=3.89e-5, glen=51.1, tlen=212, kl=0.0112, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:00<00:21,  1.03it/s, pg=0.00409, ret=3.89e-5, glen=51.1, tlen=212, kl=0.0112, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:01<00:21,  1.03it/s, pg=0.103, ret=-0.000227, glen=52, tlen=213, kl=0.0119, act_lr=1e-6, ent=0.969]  Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:01<00:19,  1.10it/s, pg=0.103, ret=-0.000227, glen=52, tlen=213, kl=0.0119, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:02<00:19,  1.10it/s, pg=0.0618, ret=-0.000146, glen=52.3, tlen=213, kl=0.0107, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:02<00:18,  1.09it/s, pg=0.0618, ret=-0.000146, glen=52.3, tlen=213, kl=0.0107, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:03<00:18,  1.09it/s, pg=0.202, ret=-0.000375, glen=53.4, tlen=214, kl=0.0118, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:03<00:17,  1.12it/s, pg=0.202, ret=-0.000375, glen=53.4, tlen=214, kl=0.0118, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:04<00:17,  1.12it/s, pg=-0.0489, ret=0.000149, glen=54.4, tlen=215, kl=0.0113, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:04<00:15,  1.13it/s, pg=-0.0489, ret=0.000149, glen=54.4, tlen=215, kl=0.0113, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:05<00:15,  1.13it/s, pg=0.089, ret=-0.000281, glen=49.1, tlen=210, kl=0.0111, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:05<00:15,  1.12it/s, pg=0.089, ret=-0.000281, glen=49.1, tlen=210, kl=0.0111, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:06<00:15,  1.12it/s, pg=-0.0502, ret=7.91e-5, glen=51.7, tlen=212, kl=0.0114, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:06<00:14,  1.13it/s, pg=-0.0502, ret=7.91e-5, glen=51.7, tlen=212, kl=0.0114, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:07<00:14,  1.13it/s, pg=0.102, ret=-0.000254, glen=53.8, tlen=215, kl=0.0125, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:13,  1.14it/s, pg=0.102, ret=-0.000254, glen=53.8, tlen=215, kl=0.0125, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:13,  1.14it/s, pg=-0.000946, ret=-2.97e-5, glen=52.9, tlen=214, kl=0.013, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:07<00:12,  1.15it/s, pg=-0.000946, ret=-2.97e-5, glen=52.9, tlen=214, kl=0.013, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:08<00:12,  1.15it/s, pg=-0.106, ret=0.000243, glen=51.9, tlen=212, kl=0.011, act_lr=1e-6, ent=0.958]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:08<00:11,  1.16it/s, pg=-0.106, ret=0.000243, glen=51.9, tlen=212, kl=0.011, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:09<00:11,  1.16it/s, pg=-0.0377, ret=0.000175, glen=51.2, tlen=212, kl=0.0127, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:09<00:10,  1.16it/s, pg=-0.0377, ret=0.000175, glen=51.2, tlen=212, kl=0.0127, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:10<00:10,  1.16it/s, pg=-0.0531, ret=0.000197, glen=51.5, tlen=212, kl=0.0114, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:10<00:09,  1.14it/s, pg=-0.0531, ret=0.000197, glen=51.5, tlen=212, kl=0.0114, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:11<00:09,  1.14it/s, pg=0.00623, ret=2.65e-6, glen=51.1, tlen=211, kl=0.0103, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:11<00:08,  1.15it/s, pg=0.00623, ret=2.65e-6, glen=51.1, tlen=211, kl=0.0103, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:12<00:08,  1.15it/s, pg=-0.102, ret=0.000208, glen=52.9, tlen=214, kl=0.0104, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:12<00:07,  1.16it/s, pg=-0.102, ret=0.000208, glen=52.9, tlen=214, kl=0.0104, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:13<00:07,  1.16it/s, pg=-0.0087, ret=2.15e-5, glen=50.7, tlen=211, kl=0.0112, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=-0.0087, ret=2.15e-5, glen=50.7, tlen=211, kl=0.0112, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=0.0569, ret=-0.000148, glen=52.6, tlen=213, kl=0.0116, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:13<00:06,  1.16it/s, pg=0.0569, ret=-0.000148, glen=52.6, tlen=213, kl=0.0116, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:14<00:06,  1.16it/s, pg=-0.0614, ret=0.000141, glen=52.3, tlen=213, kl=0.0109, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:14<00:05,  1.17it/s, pg=-0.0614, ret=0.000141, glen=52.3, tlen=213, kl=0.0109, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:15<00:05,  1.17it/s, pg=-0.0172, ret=1.33e-5, glen=51.4, tlen=212, kl=0.00992, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:15<00:04,  1.14it/s, pg=-0.0172, ret=1.33e-5, glen=51.4, tlen=212, kl=0.00992, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:16<00:04,  1.14it/s, pg=-0.0452, ret=3.46e-5, glen=53.4, tlen=214, kl=0.0105, act_lr=1e-6, ent=0.992] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:16<00:03,  1.15it/s, pg=-0.0452, ret=3.46e-5, glen=53.4, tlen=214, kl=0.0105, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:17<00:03,  1.15it/s, pg=-0.0782, ret=0.000149, glen=50.7, tlen=212, kl=0.011, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:17<00:02,  1.15it/s, pg=-0.0782, ret=0.000149, glen=50.7, tlen=212, kl=0.011, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:18<00:02,  1.15it/s, pg=-0.0435, ret=8.99e-5, glen=51.3, tlen=212, kl=0.0125, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:18<00:01,  1.16it/s, pg=-0.0435, ret=8.99e-5, glen=51.3, tlen=212, kl=0.0125, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:19<00:01,  1.16it/s, pg=0.0365, ret=-7.54e-5, glen=50, tlen=210, kl=0.0102, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:19<00:00,  1.16it/s, pg=0.0365, ret=-7.54e-5, glen=50, tlen=210, kl=0.0102, act_lr=1e-6, ent=0.963]
2025-07-24 20:08:30.816 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.12s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:20<00:00,  1.16it/s, pg=-0.0197, ret=6.88e-5, glen=51.7, tlen=213, kl=0.0115, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:20<00:00,  1.10it/s, pg=-0.0197, ret=6.88e-5, glen=51.7, tlen=213, kl=0.0115, act_lr=1e-6, ent=0.956]
2025-07-24 20:08:31.645 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:08:34.167 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 20:08:34.550 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 23.98s
2025-07-24 20:08:34.553 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005173061204993206, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.955056729524032, 'kl': 0.011301455290421196, 'response_length': 51.88795255578083, 'total_length': 212.50494318423063, 'teacher_total_length': 224.38744984502378, 'return': 3.1707060533454237e-06, 'policy_update_steps': 1.0}
Episode [7/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [22:27<00:00, 92.72s/it] [36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,709] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.9 GB, percent = 22.0%

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,710] [INFO] [config.py:1003:print]   activation_checkpointing_config  {[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,709] [INFO] [utils.py:782:see_memory_usage] MA 5.14 GB         Max_MA 5.14 GB         CA 6.17 GB         Max_CA 6 GB 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 2025-07-24 20:08:43.595 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 19:46:06,189] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:49:37,116] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:51:28,415] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:53:15,517] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:55:03,234] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:56:49,688] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 19:58:37,462] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:00:24,280] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:02:10,879] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:03:57,256] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:05:45,375] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:07:29,773] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:08:30,808] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:41,332] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:41,522] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 2712, num_elems = 14.22B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,046] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,046] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,053] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,054] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,343] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,344] [INFO] [utils.py:782:see_memory_usage] MA 5.87 GB         Max_MA 10.83 GB         CA 6.9 GB         Max_CA 46 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,344] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 107.98 GB, percent = 21.5%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [8/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [7/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [22:36<00:00, 104.38s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,589] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,590] [INFO] [utils.py:782:see_memory_usage] MA 5.87 GB         Max_MA 5.87 GB         CA 6.9 GB         Max_CA 7 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,590] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 107.97 GB, percent = 21.4%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 20:08:43.915 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:09:14.766 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:09:14.942 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:09:14.943 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.03s
2025-07-24 20:09:16.663 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0031,avg_pass_at_n: 1.0000,avg_num_tokens: 50.8269,std_num_tokens: 13.5432,avg_correct_num_tokens: 50.8443,std_correct_num_tokens: 13.5490,avg_incorrect_num_tokens: 48.3036,std_incorrect_num_tokens: 12.4167
2025-07-24 20:09:17.078 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.14s
2025-07-24 20:09:19.768 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.69s
2025-07-24 20:09:42.898 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 20:09:42.899 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.13s
2025-07-24 20:09:44.164 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 20:09:44.165 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.3213582736040864e-05, avg_kl: 0.0, avg_response_length: 50.80979952130999, avg_orm_score: 0.0, avg_custom_rewards: -1.3213582736040864e-05
2025-07-24 20:09:44.197 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter91_replay_buffer.jsonl
2025-07-24 20:09:45.342 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.15s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=-0.00775, ret=3.71e-5, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.02it/s, pg=-0.00775, ret=3.71e-5, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.02it/s, pg=-0.011, ret=9.81e-6, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.10it/s, pg=-0.011, ret=9.81e-6, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.10it/s, pg=0.0994, ret=-0.000164, glen=50.9, tlen=211, kl=0, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.13it/s, pg=0.0994, ret=-0.000164, glen=50.9, tlen=211, kl=0, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.13it/s, pg=-0.0158, ret=5.1e-6, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.904]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.12it/s, pg=-0.0158, ret=5.1e-6, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.12it/s, pg=-0.0138, ret=2.83e-6, glen=49.3, tlen=209, kl=0, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:35,  1.11it/s, pg=-0.0138, ret=2.83e-6, glen=49.3, tlen=209, kl=0, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:35,  1.11it/s, pg=-0.037, ret=2.65e-5, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.13it/s, pg=-0.037, ret=2.65e-5, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.13it/s, pg=0.157, ret=-0.000175, glen=48.2, tlen=208, kl=0, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.14it/s, pg=0.157, ret=-0.000175, glen=48.2, tlen=208, kl=0, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.14it/s, pg=0.0194, ret=-5.28e-6, glen=49.1, tlen=209, kl=0, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=0.0194, ret=-5.28e-6, glen=49.1, tlen=209, kl=0, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=-0.082, ret=0.000149, glen=51.4, tlen=212, kl=0, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.16it/s, pg=-0.082, ret=0.000149, glen=51.4, tlen=212, kl=0, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.16it/s, pg=-0.0726, ret=0.000141, glen=50, tlen=210, kl=0, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=-0.0726, ret=0.000141, glen=50, tlen=210, kl=0, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=-0.0666, ret=0.000113, glen=49.4, tlen=210, kl=0, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.17it/s, pg=-0.0666, ret=0.000113, glen=49.4, tlen=210, kl=0, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.17it/s, pg=0.0756, ret=-0.000101, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.17it/s, pg=0.0756, ret=-0.000101, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.17it/s, pg=-0.0819, ret=0.000145, glen=51.1, tlen=211, kl=0, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=-0.0819, ret=0.000145, glen=51.1, tlen=211, kl=0, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=0.0679, ret=-0.00012, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=0.0679, ret=-0.00012, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=0.0747, ret=-0.000122, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=0.0747, ret=-0.000122, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0607, ret=0.000106, glen=51.3, tlen=212, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:23,  1.17it/s, pg=-0.0607, ret=0.000106, glen=51.3, tlen=212, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.0556, ret=9.73e-5, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=-0.0556, ret=9.73e-5, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=0.189, ret=-0.000411, glen=49.1, tlen=210, kl=0, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.18it/s, pg=0.189, ret=-0.000411, glen=49.1, tlen=210, kl=0, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.18it/s, pg=0.106, ret=-0.000286, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.18it/s, pg=0.106, ret=-0.000286, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.18it/s, pg=0.0196, ret=1.92e-5, glen=50.9, tlen=211, kl=0, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.18it/s, pg=0.0196, ret=1.92e-5, glen=50.9, tlen=211, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.18it/s, pg=-0.0243, ret=8.68e-5, glen=50.8, tlen=212, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.18it/s, pg=-0.0243, ret=8.68e-5, glen=50.8, tlen=212, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.18it/s, pg=-0.0928, ret=0.000172, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:18<00:18,  1.18it/s, pg=-0.0928, ret=0.000172, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.18it/s, pg=0.0322, ret=-8.31e-5, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.18it/s, pg=0.0322, ret=-8.31e-5, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=0.155, ret=-0.000266, glen=52, tlen=212, kl=0, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.18it/s, pg=0.155, ret=-0.000266, glen=52, tlen=212, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.18it/s, pg=0.0169, ret=-8.49e-5, glen=49.9, tlen=210, kl=0, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.18it/s, pg=0.0169, ret=-8.49e-5, glen=49.9, tlen=210, kl=0, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.18it/s, pg=-0.108, ret=0.000177, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.18it/s, pg=-0.108, ret=0.000177, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.18it/s, pg=-0.0161, ret=5.51e-5, glen=50.9, tlen=211, kl=0, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.18it/s, pg=-0.0161, ret=5.51e-5, glen=50.9, tlen=211, kl=0, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.18it/s, pg=-0.0161, ret=4.77e-5, glen=50.3, tlen=210, kl=0, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.18it/s, pg=-0.0161, ret=4.77e-5, glen=50.3, tlen=210, kl=0, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.18it/s, pg=-0.0911, ret=0.000163, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=-0.0911, ret=0.000163, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.119, ret=-0.00024, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.923]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.119, ret=-0.00024, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.072, ret=0.000134, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:26<00:11,  1.12it/s, pg=-0.072, ret=0.000134, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0569, ret=0.000101, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=-0.0569, ret=0.000101, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.112, ret=-0.000104, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.112, ret=-0.000104, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.108, ret=0.000172, glen=50.3, tlen=211, kl=0, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=-0.108, ret=0.000172, glen=50.3, tlen=211, kl=0, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=0.0317, ret=-6.56e-5, glen=51.8, tlen=212, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.0317, ret=-6.56e-5, glen=51.8, tlen=212, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.00436, ret=-1.15e-5, glen=49.2, tlen=209, kl=0, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=-0.00436, ret=-1.15e-5, glen=49.2, tlen=209, kl=0, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.0658, ret=0.000114, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.955] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0658, ret=0.000114, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=0.0114, ret=5.31e-6, glen=50.4, tlen=211, kl=0, act_lr=1e-6, ent=0.933]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:32<00:05,  1.17it/s, pg=0.0114, ret=5.31e-6, glen=50.4, tlen=211, kl=0, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0173, ret=1.91e-5, glen=52.1, tlen=212, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.0173, ret=1.91e-5, glen=52.1, tlen=212, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.023, ret=-9.02e-5, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.023, ret=-9.02e-5, glen=50.6, tlen=211, kl=0, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0367, ret=8.66e-5, glen=50.9, tlen=212, kl=0, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0367, ret=8.66e-5, glen=50.9, tlen=212, kl=0, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.0578, ret=0.000104, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.0578, ret=0.000104, glen=51.6, tlen=212, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.0264, ret=4.24e-5, glen=52, tlen=213, kl=0, act_lr=1e-6, ent=0.965]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=-0.0264, ret=4.24e-5, glen=52, tlen=213, kl=0, act_lr=1e-6, ent=0.965]
2025-07-24 20:10:23.633 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.12s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=0.069, ret=-0.000198, glen=50.5, tlen=211, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=0.069, ret=-0.000198, glen=50.5, tlen=211, kl=0, act_lr=1e-6, ent=0.918]
2025-07-24 20:10:24.481 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 20:10:27.049 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 20:10:27.380 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.99s
2025-07-24 20:10:27.387 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0018073862249200995, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9315619346770373, 'kl': 0.0, 'response_length': 50.81100099736994, 'total_length': 211.17626398259944, 'teacher_total_length': 223.19000140103427, 'return': -4.494277143087609e-06, 'policy_update_steps': 1.0}

Episode [8/20]:   8%|‚ñä         | 1/13 [01:43<20:45, 103.79s/it][A2025-07-24 20:10:27.434 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:10:58.268 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:10:58.460 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 20:10:58.461 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.03s
2025-07-24 20:11:00.379 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 50.6971,std_num_tokens: 14.3660,avg_correct_num_tokens: 50.6643,std_correct_num_tokens: 14.3583,avg_incorrect_num_tokens: 56.1633,std_incorrect_num_tokens: 14.5985
2025-07-24 20:11:00.783 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.32s
2025-07-24 20:11:03.188 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.40s
2025-07-24 20:11:25.857 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 20:11:25.858 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.67s
2025-07-24 20:11:26.990 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.74s
2025-07-24 20:11:26.990 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.772371356257762e-05, avg_kl: 0.001199590748754041, avg_response_length: 50.715714904083605, avg_orm_score: 0.0, avg_custom_rewards: 1.772371356257762e-05
2025-07-24 20:11:27.017 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter92_replay_buffer.jsonl
2025-07-24 20:11:28.147 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0837, ret=0.000135, glen=50.8, tlen=211, kl=0.00123, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=-0.0837, ret=0.000135, glen=50.8, tlen=211, kl=0.00123, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=0.0369, ret=-0.000112, glen=52.1, tlen=212, kl=0.00129, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.08it/s, pg=0.0369, ret=-0.000112, glen=52.1, tlen=212, kl=0.00129, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.08it/s, pg=-0.0768, ret=0.000119, glen=52.6, tlen=213, kl=0.00133, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=-0.0768, ret=0.000119, glen=52.6, tlen=213, kl=0.00133, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=-0.0347, ret=7.35e-6, glen=49.5, tlen=210, kl=0.00114, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.12it/s, pg=-0.0347, ret=7.35e-6, glen=49.5, tlen=210, kl=0.00114, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.12it/s, pg=-0.0969, ret=0.000143, glen=50.1, tlen=210, kl=0.00117, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.13it/s, pg=-0.0969, ret=0.000143, glen=50.1, tlen=210, kl=0.00117, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.13it/s, pg=-0.0631, ret=6.26e-5, glen=49.4, tlen=209, kl=0.00117, act_lr=1e-6, ent=0.88]  Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.15it/s, pg=-0.0631, ret=6.26e-5, glen=49.4, tlen=209, kl=0.00117, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.15it/s, pg=0.0405, ret=-9.83e-5, glen=50.4, tlen=210, kl=0.00121, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.13it/s, pg=0.0405, ret=-9.83e-5, glen=50.4, tlen=210, kl=0.00121, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.13it/s, pg=0.0984, ret=-0.000169, glen=51.4, tlen=212, kl=0.00126, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.0984, ret=-0.000169, glen=51.4, tlen=212, kl=0.00126, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.113, ret=-0.000207, glen=51.7, tlen=212, kl=0.00115, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=0.113, ret=-0.000207, glen=51.7, tlen=212, kl=0.00115, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.19, ret=-0.000249, glen=51.4, tlen=212, kl=0.00116, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:30,  1.12it/s, pg=0.19, ret=-0.000249, glen=51.4, tlen=212, kl=0.00116, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:30,  1.12it/s, pg=-0.0834, ret=0.000128, glen=49.9, tlen=210, kl=0.00109, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:29,  1.14it/s, pg=-0.0834, ret=0.000128, glen=49.9, tlen=210, kl=0.00109, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:29,  1.14it/s, pg=0.0991, ret=-0.000112, glen=48.1, tlen=208, kl=0.00122, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.15it/s, pg=0.0991, ret=-0.000112, glen=48.1, tlen=208, kl=0.00122, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.15it/s, pg=0.00415, ret=2.3e-5, glen=51.2, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.923]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.16it/s, pg=0.00415, ret=2.3e-5, glen=51.2, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.16it/s, pg=-0.008, ret=2.55e-5, glen=51, tlen=211, kl=0.00121, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.16it/s, pg=-0.008, ret=2.55e-5, glen=51, tlen=211, kl=0.00121, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.16it/s, pg=-0.062, ret=9.6e-5, glen=50.8, tlen=211, kl=0.0012, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.16it/s, pg=-0.062, ret=9.6e-5, glen=50.8, tlen=211, kl=0.0012, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:24,  1.16it/s, pg=-0.0929, ret=0.000139, glen=51.6, tlen=212, kl=0.00126, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.0929, ret=0.000139, glen=51.6, tlen=212, kl=0.00126, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.0179, ret=4.15e-5, glen=52.5, tlen=213, kl=0.00128, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=-0.0179, ret=4.15e-5, glen=52.5, tlen=213, kl=0.00128, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.0587, ret=4.38e-5, glen=51.7, tlen=212, kl=0.00128, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.0587, ret=4.38e-5, glen=51.7, tlen=212, kl=0.00128, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.0307, ret=2.3e-5, glen=51.1, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.0307, ret=2.3e-5, glen=51.1, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=0.0176, ret=1.59e-5, glen=49.6, tlen=210, kl=0.00114, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.0176, ret=1.59e-5, glen=49.6, tlen=210, kl=0.00114, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0757, ret=0.000117, glen=51.4, tlen=212, kl=0.00119, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0757, ret=0.000117, glen=51.4, tlen=212, kl=0.00119, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.097, ret=-0.000108, glen=52.1, tlen=212, kl=0.00126, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.097, ret=-0.000108, glen=52.1, tlen=212, kl=0.00126, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.17it/s, pg=-0.0886, ret=0.000142, glen=50.1, tlen=210, kl=0.00121, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.14it/s, pg=-0.0886, ret=0.000142, glen=50.1, tlen=210, kl=0.00121, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.14it/s, pg=-0.068, ret=0.000104, glen=49.4, tlen=209, kl=0.00124, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.15it/s, pg=-0.068, ret=0.000104, glen=49.4, tlen=209, kl=0.00124, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.15it/s, pg=0.00458, ret=9.34e-6, glen=49.5, tlen=210, kl=0.00113, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.16it/s, pg=0.00458, ret=9.34e-6, glen=49.5, tlen=210, kl=0.00113, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.16it/s, pg=-0.032, ret=2.9e-5, glen=49.7, tlen=210, kl=0.0011, act_lr=1e-6, ent=0.952]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.16it/s, pg=-0.032, ret=2.9e-5, glen=49.7, tlen=210, kl=0.0011, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.16it/s, pg=-0.00476, ret=2.39e-5, glen=50, tlen=210, kl=0.00119, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=-0.00476, ret=2.39e-5, glen=50, tlen=210, kl=0.00119, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.0477, ret=-0.000102, glen=49.2, tlen=209, kl=0.00103, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.0477, ret=-0.000102, glen=49.2, tlen=209, kl=0.00103, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=0.0876, ret=-0.000115, glen=49.4, tlen=209, kl=0.00116, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=0.0876, ret=-0.000115, glen=49.4, tlen=209, kl=0.00116, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=-0.014, ret=1.24e-5, glen=50.7, tlen=211, kl=0.00118, act_lr=1e-6, ent=0.921]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.014, ret=1.24e-5, glen=50.7, tlen=211, kl=0.00118, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=0.163, ret=-0.000216, glen=51.6, tlen=212, kl=0.00123, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.163, ret=-0.000216, glen=51.6, tlen=212, kl=0.00123, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.12it/s, pg=0.0129, ret=8.09e-6, glen=49.3, tlen=209, kl=0.00121, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.0129, ret=8.09e-6, glen=49.3, tlen=209, kl=0.00121, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.182, ret=-0.000212, glen=51.4, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.182, ret=-0.000212, glen=51.4, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.00598, ret=5.3e-6, glen=50.9, tlen=211, kl=0.00122, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=-0.00598, ret=5.3e-6, glen=50.9, tlen=211, kl=0.00122, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=0.0757, ret=-9.72e-5, glen=49.5, tlen=210, kl=0.00114, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.0757, ret=-9.72e-5, glen=49.5, tlen=210, kl=0.00114, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.0295, ret=-9.83e-6, glen=52.4, tlen=213, kl=0.00128, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=0.0295, ret=-9.83e-6, glen=52.4, tlen=213, kl=0.00128, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.0284, ret=1.72e-5, glen=52.3, tlen=213, kl=0.00132, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.16it/s, pg=-0.0284, ret=1.72e-5, glen=52.3, tlen=213, kl=0.00132, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.16it/s, pg=-0.0154, ret=3.01e-5, glen=51, tlen=211, kl=0.00122, act_lr=1e-6, ent=0.923]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.16it/s, pg=-0.0154, ret=3.01e-5, glen=51, tlen=211, kl=0.00122, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.16it/s, pg=0.0385, ret=-6.25e-6, glen=50.5, tlen=210, kl=0.00125, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=0.0385, ret=-6.25e-6, glen=50.5, tlen=210, kl=0.00125, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0436, ret=-0.000103, glen=50.1, tlen=210, kl=0.00118, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0436, ret=-0.000103, glen=50.1, tlen=210, kl=0.00118, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.072, ret=0.000113, glen=50.5, tlen=211, kl=0.00109, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.072, ret=0.000113, glen=50.5, tlen=211, kl=0.00109, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.109, ret=0.000157, glen=51.5, tlen=212, kl=0.00118, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.109, ret=0.000157, glen=51.5, tlen=212, kl=0.00118, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.027, ret=1.15e-6, glen=50.6, tlen=211, kl=0.00122, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.027, ret=1.15e-6, glen=50.6, tlen=211, kl=0.00122, act_lr=1e-6, ent=0.962]
2025-07-24 20:12:06.716 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0861, ret=0.000118, glen=51.1, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0861, ret=0.000118, glen=51.1, tlen=211, kl=0.00119, act_lr=1e-6, ent=0.931]
2025-07-24 20:12:07.398 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:12:09.654 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.26s
2025-07-24 20:12:09.995 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.76s
2025-07-24 20:12:10.052 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0010438398881392045, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9346027171069925, 'kl': 0.001199440522627397, 'response_length': 50.704096273942426, 'total_length': 210.86131564053622, 'teacher_total_length': 222.81115618619052, 'return': -5.588825512280561e-07, 'policy_update_steps': 1.0}

Episode [8/20]:  15%|‚ñà‚ñå        | 2/13 [03:26<18:54, 103.13s/it][A2025-07-24 20:12:10.092 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:12:41.439 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:12:41.621 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:12:41.622 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.53s
2025-07-24 20:12:43.270 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0023,avg_pass_at_n: 1.0000,avg_num_tokens: 51.7698,std_num_tokens: 14.2415,avg_correct_num_tokens: 51.7225,std_correct_num_tokens: 14.2196,avg_incorrect_num_tokens: 56.2824,std_incorrect_num_tokens: 15.5417
2025-07-24 20:12:43.625 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.00s
2025-07-24 20:12:46.060 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.43s
2025-07-24 20:13:08.704 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 176
2025-07-24 20:13:08.704 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.64s
2025-07-24 20:13:10.218 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.08s
2025-07-24 20:13:10.219 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 5.2720265970988706e-05, avg_kl: 0.0019451921636408026, avg_response_length: 51.7913818359375, avg_orm_score: 0.0, avg_custom_rewards: 5.2720265970988706e-05
2025-07-24 20:13:10.262 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter93_replay_buffer.jsonl
2025-07-24 20:13:11.438 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.0226, ret=1.04e-5, glen=52.6, tlen=214, kl=0.00221, act_lr=1e-6, ent=0.99]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=0.0226, ret=1.04e-5, glen=52.6, tlen=214, kl=0.00221, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=0.0619, ret=-0.000117, glen=52, tlen=213, kl=0.00189, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=0.0619, ret=-0.000117, glen=52, tlen=213, kl=0.00189, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=-0.0366, ret=0.000105, glen=50.9, tlen=212, kl=0.00194, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.13it/s, pg=-0.0366, ret=0.000105, glen=50.9, tlen=212, kl=0.00194, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.13it/s, pg=-0.0574, ret=0.000116, glen=52.2, tlen=213, kl=0.00169, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.11it/s, pg=-0.0574, ret=0.000116, glen=52.2, tlen=213, kl=0.00169, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.11it/s, pg=-0.0205, ret=8.21e-5, glen=52.6, tlen=214, kl=0.00203, act_lr=1e-6, ent=0.984] Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:35,  1.10it/s, pg=-0.0205, ret=8.21e-5, glen=52.6, tlen=214, kl=0.00203, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:35,  1.10it/s, pg=-0.067, ret=0.000104, glen=53.4, tlen=215, kl=0.00175, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.12it/s, pg=-0.067, ret=0.000104, glen=53.4, tlen=215, kl=0.00175, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.12it/s, pg=0.0872, ret=-9.11e-5, glen=51.5, tlen=213, kl=0.00222, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.14it/s, pg=0.0872, ret=-9.11e-5, glen=51.5, tlen=213, kl=0.00222, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.14it/s, pg=-0.0105, ret=3.85e-6, glen=52.1, tlen=213, kl=0.0019, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=-0.0105, ret=3.85e-6, glen=52.1, tlen=213, kl=0.0019, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=0.0511, ret=-8.95e-5, glen=52.1, tlen=213, kl=0.00203, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=0.0511, ret=-8.95e-5, glen=52.1, tlen=213, kl=0.00203, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=-0.00275, ret=-1.73e-6, glen=51.7, tlen=213, kl=0.00175, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=-0.00275, ret=-1.73e-6, glen=51.7, tlen=213, kl=0.00175, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=0.0979, ret=-0.000218, glen=50.5, tlen=212, kl=0.00176, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.17it/s, pg=0.0979, ret=-0.000218, glen=50.5, tlen=212, kl=0.00176, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.17it/s, pg=-0.0541, ret=9.28e-5, glen=51.1, tlen=212, kl=0.00187, act_lr=1e-6, ent=0.944] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.14it/s, pg=-0.0541, ret=9.28e-5, glen=51.1, tlen=212, kl=0.00187, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.14it/s, pg=-0.0328, ret=0.000106, glen=51.1, tlen=213, kl=0.0019, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:27,  1.13it/s, pg=-0.0328, ret=0.000106, glen=51.1, tlen=213, kl=0.0019, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:27,  1.13it/s, pg=-0.126, ret=0.000257, glen=54.7, tlen=216, kl=0.00205, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=-0.126, ret=0.000257, glen=54.7, tlen=216, kl=0.00205, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=-0.0646, ret=0.000117, glen=50.7, tlen=212, kl=0.00179, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=-0.0646, ret=0.000117, glen=50.7, tlen=212, kl=0.00179, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.16it/s, pg=0.0273, ret=-6.53e-5, glen=52.8, tlen=213, kl=0.00204, act_lr=1e-6, ent=0.997] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.0273, ret=-6.53e-5, glen=52.8, tlen=213, kl=0.00204, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.0199, ret=-4.87e-5, glen=52.3, tlen=214, kl=0.00193, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.0199, ret=-4.87e-5, glen=52.3, tlen=214, kl=0.00193, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=0.0765, ret=-0.000141, glen=51.2, tlen=213, kl=0.00204, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=0.0765, ret=-0.000141, glen=51.2, tlen=213, kl=0.00204, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.00362, ret=1.3e-5, glen=53.1, tlen=214, kl=0.00207, act_lr=1e-6, ent=0.99]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.00362, ret=1.3e-5, glen=53.1, tlen=214, kl=0.00207, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=0.047, ret=-0.000123, glen=52.5, tlen=214, kl=0.00212, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.047, ret=-0.000123, glen=52.5, tlen=214, kl=0.00212, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.00537, ret=-2.65e-5, glen=51, tlen=212, kl=0.00184, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.00537, ret=-2.65e-5, glen=51, tlen=212, kl=0.00184, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.0588, ret=-0.000126, glen=51.4, tlen=212, kl=0.002, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0588, ret=-0.000126, glen=51.4, tlen=212, kl=0.002, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0238, ret=-4.92e-5, glen=51.2, tlen=212, kl=0.00199, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.18it/s, pg=0.0238, ret=-4.92e-5, glen=51.2, tlen=212, kl=0.00199, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=-0.0682, ret=0.000133, glen=51.2, tlen=213, kl=0.00187, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.18it/s, pg=-0.0682, ret=0.000133, glen=51.2, tlen=213, kl=0.00187, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.18it/s, pg=-0.0859, ret=0.000181, glen=51.7, tlen=212, kl=0.00179, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.16it/s, pg=-0.0859, ret=0.000181, glen=51.7, tlen=212, kl=0.00179, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.16it/s, pg=-0.101, ret=0.000168, glen=50.1, tlen=211, kl=0.00192, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.16it/s, pg=-0.101, ret=0.000168, glen=50.1, tlen=211, kl=0.00192, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.16it/s, pg=0.00964, ret=5.46e-6, glen=52.4, tlen=214, kl=0.00203, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.00964, ret=5.46e-6, glen=52.4, tlen=214, kl=0.00203, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.00159, ret=-4.6e-5, glen=50.7, tlen=212, kl=0.00229, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.00159, ret=-4.6e-5, glen=50.7, tlen=212, kl=0.00229, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=0.0875, ret=-0.000262, glen=49.4, tlen=210, kl=0.00175, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=0.0875, ret=-0.000262, glen=49.4, tlen=210, kl=0.00175, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=-0.0965, ret=0.000198, glen=52.3, tlen=213, kl=0.00197, act_lr=1e-6, ent=0.99] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.0965, ret=0.000198, glen=52.3, tlen=213, kl=0.00197, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.0609, ret=9.73e-5, glen=50, tlen=211, kl=0.00185, act_lr=1e-6, ent=0.958]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0609, ret=9.73e-5, glen=50, tlen=211, kl=0.00185, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.0834, ret=-0.000134, glen=52.6, tlen=214, kl=0.00192, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=0.0834, ret=-0.000134, glen=52.6, tlen=214, kl=0.00192, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=-0.0173, ret=5.17e-6, glen=51.2, tlen=212, kl=0.00201, act_lr=1e-6, ent=0.977] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=-0.0173, ret=5.17e-6, glen=51.2, tlen=212, kl=0.00201, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=0.049, ret=-2.09e-5, glen=52.4, tlen=214, kl=0.00222, act_lr=1e-6, ent=0.968] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=0.049, ret=-2.09e-5, glen=52.4, tlen=214, kl=0.00222, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=-0.0107, ret=-4.5e-5, glen=53.6, tlen=215, kl=0.00203, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.17it/s, pg=-0.0107, ret=-4.5e-5, glen=53.6, tlen=215, kl=0.00203, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.17it/s, pg=0.00696, ret=-1.06e-5, glen=50.5, tlen=211, kl=0.00197, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=0.00696, ret=-1.06e-5, glen=50.5, tlen=211, kl=0.00197, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.0535, ret=8.57e-5, glen=52.1, tlen=213, kl=0.00206, act_lr=1e-6, ent=0.978] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0535, ret=8.57e-5, glen=52.1, tlen=213, kl=0.00206, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=-0.00696, ret=-1.72e-5, glen=52.9, tlen=214, kl=0.00194, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.00696, ret=-1.72e-5, glen=52.9, tlen=214, kl=0.00194, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0331, ret=-1.85e-6, glen=54.8, tlen=216, kl=0.00171, act_lr=1e-6, ent=0.989]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.18it/s, pg=0.0331, ret=-1.85e-6, glen=54.8, tlen=216, kl=0.00171, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.18it/s, pg=0.00647, ret=-3.4e-5, glen=52.1, tlen=213, kl=0.00181, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.18it/s, pg=0.00647, ret=-3.4e-5, glen=52.1, tlen=213, kl=0.00181, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.18it/s, pg=0.00427, ret=-2.28e-5, glen=50.6, tlen=212, kl=0.00185, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.18it/s, pg=0.00427, ret=-2.28e-5, glen=50.6, tlen=212, kl=0.00185, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.18it/s, pg=-0.0404, ret=0.000261, glen=51.5, tlen=212, kl=0.00191, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=-0.0404, ret=0.000261, glen=51.5, tlen=212, kl=0.00191, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=0.127, ret=-0.000186, glen=49.8, tlen=211, kl=0.00193, act_lr=1e-6, ent=0.948] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=0.127, ret=-0.000186, glen=49.8, tlen=211, kl=0.00193, act_lr=1e-6, ent=0.948]
2025-07-24 20:13:49.894 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=0.00713, ret=-8.24e-5, glen=52.2, tlen=213, kl=0.00194, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=0.00713, ret=-8.24e-5, glen=52.2, tlen=213, kl=0.00194, act_lr=1e-6, ent=0.935]
2025-07-24 20:13:50.569 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:13:52.839 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-24 20:13:53.169 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.61s
2025-07-24 20:13:53.188 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007248791781338779, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9645304652777585, 'kl': 0.0019451921636408026, 'response_length': 51.79138174923983, 'total_length': 212.86869153109464, 'teacher_total_length': 224.81671073219994, 'return': 4.119815499011152e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:09<17:11, 103.13s/it][A2025-07-24 20:13:53.236 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:14:23.857 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:14:24.043 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:14:24.044 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.81s
2025-07-24 20:14:25.912 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0017,avg_pass_at_n: 1.0000,avg_num_tokens: 50.1699,std_num_tokens: 13.4396,avg_correct_num_tokens: 50.1353,std_correct_num_tokens: 13.3839,avg_incorrect_num_tokens: 53.5976,std_incorrect_num_tokens: 17.7900
2025-07-24 20:14:26.332 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.29s
2025-07-24 20:14:28.721 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.39s
2025-07-24 20:14:51.230 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 20:14:51.231 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.51s
2025-07-24 20:14:52.489 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 20:14:52.489 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.2025877909584978e-05, avg_kl: 0.003430596713362069, avg_response_length: 50.19987555755966, avg_orm_score: 0.0, avg_custom_rewards: 2.2025877909584978e-05
2025-07-24 20:14:52.521 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter94_replay_buffer.jsonl
2025-07-24 20:14:53.646 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=-0.0955, ret=0.000243, glen=50.5, tlen=211, kl=0.00399, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.01it/s, pg=-0.0955, ret=0.000243, glen=50.5, tlen=211, kl=0.00399, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.01it/s, pg=0.0896, ret=-0.000187, glen=50.2, tlen=210, kl=0.00354, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=0.0896, ret=-0.000187, glen=50.2, tlen=210, kl=0.00354, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=0.0735, ret=-5.01e-5, glen=49.2, tlen=210, kl=0.00394, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:37,  1.09it/s, pg=0.0735, ret=-5.01e-5, glen=49.2, tlen=210, kl=0.00394, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:37,  1.09it/s, pg=0.0259, ret=-1.14e-5, glen=50.6, tlen=211, kl=0.00325, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.11it/s, pg=0.0259, ret=-1.14e-5, glen=50.6, tlen=211, kl=0.00325, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.11it/s, pg=-0.0836, ret=9.73e-5, glen=50.6, tlen=211, kl=0.00346, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:35,  1.11it/s, pg=-0.0836, ret=9.73e-5, glen=50.6, tlen=211, kl=0.00346, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:35,  1.11it/s, pg=-0.0868, ret=0.000181, glen=50.2, tlen=211, kl=0.00369, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:34,  1.11it/s, pg=-0.0868, ret=0.000181, glen=50.2, tlen=211, kl=0.00369, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:34,  1.11it/s, pg=-0.0516, ret=8.64e-5, glen=50.5, tlen=211, kl=0.00368, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:33,  1.09it/s, pg=-0.0516, ret=8.64e-5, glen=50.5, tlen=211, kl=0.00368, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:33,  1.09it/s, pg=0.16, ret=-0.000215, glen=51.4, tlen=212, kl=0.00313, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=0.16, ret=-0.000215, glen=51.4, tlen=212, kl=0.00313, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:32,  1.12it/s, pg=-0.105, ret=0.000223, glen=49.2, tlen=210, kl=0.0035, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=-0.105, ret=0.000223, glen=49.2, tlen=210, kl=0.0035, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=-0.0198, ret=-1.08e-5, glen=50.3, tlen=210, kl=0.00386, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=-0.0198, ret=-1.08e-5, glen=50.3, tlen=210, kl=0.00386, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=-0.112, ret=0.000233, glen=50.8, tlen=211, kl=0.00362, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.15it/s, pg=-0.112, ret=0.000233, glen=50.8, tlen=211, kl=0.00362, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.15it/s, pg=0.165, ret=-0.000469, glen=51, tlen=211, kl=0.00368, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=0.165, ret=-0.000469, glen=51, tlen=211, kl=0.00368, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.0656, ret=0.000101, glen=50.5, tlen=211, kl=0.00288, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:27,  1.14it/s, pg=-0.0656, ret=0.000101, glen=50.5, tlen=211, kl=0.00288, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:27,  1.14it/s, pg=0.00226, ret=-8.87e-5, glen=51.2, tlen=211, kl=0.00338, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=0.00226, ret=-8.87e-5, glen=51.2, tlen=211, kl=0.00338, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=-0.00879, ret=-1.95e-5, glen=51.6, tlen=212, kl=0.00357, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=-0.00879, ret=-1.95e-5, glen=51.6, tlen=212, kl=0.00357, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.16it/s, pg=0.0442, ret=-0.000134, glen=50.1, tlen=211, kl=0.00302, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.0442, ret=-0.000134, glen=50.1, tlen=211, kl=0.00302, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.109, ret=0.00022, glen=48.5, tlen=209, kl=0.00319, act_lr=1e-6, ent=0.944]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=-0.109, ret=0.00022, glen=48.5, tlen=209, kl=0.00319, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.00714, ret=-2.74e-5, glen=51.7, tlen=212, kl=0.00393, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.14it/s, pg=-0.00714, ret=-2.74e-5, glen=51.7, tlen=212, kl=0.00393, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.14it/s, pg=0.024, ret=-2.42e-5, glen=49.5, tlen=210, kl=0.00349, act_lr=1e-6, ent=0.907] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.15it/s, pg=0.024, ret=-2.42e-5, glen=49.5, tlen=210, kl=0.00349, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.15it/s, pg=-0.0409, ret=6.88e-5, glen=52.3, tlen=213, kl=0.00418, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.16it/s, pg=-0.0409, ret=6.88e-5, glen=52.3, tlen=213, kl=0.00418, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.16it/s, pg=0.0686, ret=-6.9e-5, glen=50.8, tlen=211, kl=0.00321, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.16it/s, pg=0.0686, ret=-6.9e-5, glen=50.8, tlen=211, kl=0.00321, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.16it/s, pg=-0.0468, ret=9.82e-5, glen=50.1, tlen=211, kl=0.00329, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.16it/s, pg=-0.0468, ret=9.82e-5, glen=50.1, tlen=211, kl=0.00329, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.16it/s, pg=-0.0775, ret=0.000135, glen=48.6, tlen=209, kl=0.00335, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.0775, ret=0.000135, glen=48.6, tlen=209, kl=0.00335, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.097, ret=0.000187, glen=50.6, tlen=211, kl=0.00342, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=-0.097, ret=0.000187, glen=50.6, tlen=211, kl=0.00342, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.0369, ret=6.86e-5, glen=51.4, tlen=212, kl=0.0034, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.0369, ret=6.86e-5, glen=51.4, tlen=212, kl=0.0034, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.0156, ret=-0.00011, glen=49.8, tlen=210, kl=0.00299, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.0156, ret=-0.00011, glen=49.8, tlen=210, kl=0.00299, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.0372, ret=-0.000199, glen=48.9, tlen=209, kl=0.00315, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.0372, ret=-0.000199, glen=48.9, tlen=209, kl=0.00315, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.0842, ret=-0.000102, glen=49.9, tlen=210, kl=0.00348, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.0842, ret=-0.000102, glen=49.9, tlen=210, kl=0.00348, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.0211, ret=8.13e-5, glen=50, tlen=211, kl=0.00365, act_lr=1e-6, ent=0.903]   Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:13,  1.07it/s, pg=-0.0211, ret=8.13e-5, glen=50, tlen=211, kl=0.00365, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:13,  1.07it/s, pg=0.14, ret=-0.000231, glen=49.1, tlen=209, kl=0.00325, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.14, ret=-0.000231, glen=49.1, tlen=209, kl=0.00325, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.0266, ret=2.79e-5, glen=48.9, tlen=209, kl=0.00364, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0266, ret=2.79e-5, glen=48.9, tlen=209, kl=0.00364, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.12it/s, pg=0.011, ret=-1.83e-5, glen=51.6, tlen=212, kl=0.00359, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.011, ret=-1.83e-5, glen=51.6, tlen=212, kl=0.00359, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.0167, ret=-3.87e-5, glen=51.9, tlen=212, kl=0.003, act_lr=1e-6, ent=0.937] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.0167, ret=-3.87e-5, glen=51.9, tlen=212, kl=0.003, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.024, ret=6.37e-5, glen=50.7, tlen=211, kl=0.00302, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=-0.024, ret=6.37e-5, glen=50.7, tlen=211, kl=0.00302, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=-0.0225, ret=-8.44e-6, glen=51.3, tlen=212, kl=0.0039, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0225, ret=-8.44e-6, glen=51.3, tlen=212, kl=0.0039, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.0515, ret=-8.28e-5, glen=50.4, tlen=211, kl=0.00328, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=0.0515, ret=-8.28e-5, glen=50.4, tlen=211, kl=0.00328, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.0998, ret=0.000209, glen=49.2, tlen=209, kl=0.0031, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0998, ret=0.000209, glen=49.2, tlen=209, kl=0.0031, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=0.0629, ret=-0.000126, glen=50, tlen=210, kl=0.00318, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0629, ret=-0.000126, glen=50, tlen=210, kl=0.00318, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=-0.0238, ret=6.49e-5, glen=49.9, tlen=210, kl=0.0031, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0238, ret=6.49e-5, glen=49.9, tlen=210, kl=0.0031, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0194, ret=-6.65e-6, glen=47.8, tlen=208, kl=0.00307, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0194, ret=-6.65e-6, glen=47.8, tlen=208, kl=0.00307, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0748, ret=0.000129, glen=49.7, tlen=210, kl=0.0036, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0748, ret=0.000129, glen=49.7, tlen=210, kl=0.0036, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.095, ret=-2.03e-5, glen=49.8, tlen=210, kl=0.00368, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.18it/s, pg=0.095, ret=-2.03e-5, glen=49.8, tlen=210, kl=0.00368, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.18it/s, pg=0.142, ret=-0.000197, glen=49.8, tlen=211, kl=0.00323, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=0.142, ret=-0.000197, glen=49.8, tlen=211, kl=0.00323, act_lr=1e-6, ent=0.932]
2025-07-24 20:15:32.230 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.38s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=0.03, ret=-0.000127, glen=48.8, tlen=209, kl=0.00333, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=0.03, ret=-0.000127, glen=48.8, tlen=209, kl=0.00333, act_lr=1e-6, ent=0.918]
2025-07-24 20:15:32.910 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 20:15:35.230 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-24 20:15:35.562 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.84s
2025-07-24 20:15:35.568 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0005087987943129106, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9185676276683807, 'kl': 0.0034293261441317472, 'response_length': 50.207033764232285, 'total_length': 210.63655229048297, 'teacher_total_length': 222.64507605812767, 'return': -1.2451935369674836e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [06:51<15:25, 102.84s/it][A2025-07-24 20:15:35.612 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:16:06.134 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:16:06.311 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:16:06.311 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.70s
2025-07-24 20:16:08.056 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 49.8370,std_num_tokens: 13.5629,avg_correct_num_tokens: 49.8264,std_correct_num_tokens: 13.5527,avg_incorrect_num_tokens: 51.1875,std_incorrect_num_tokens: 14.7403
2025-07-24 20:16:08.508 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.20s
2025-07-24 20:16:11.047 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.54s
2025-07-24 20:16:33.631 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 20:16:33.632 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.58s
2025-07-24 20:16:35.116 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.10s
2025-07-24 20:16:35.116 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.243540146056263e-05, avg_kl: 0.008733815160290948, avg_response_length: 49.85091636920797, avg_orm_score: 0.0, avg_custom_rewards: 3.243540146056263e-05
2025-07-24 20:16:35.146 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter95_replay_buffer.jsonl
2025-07-24 20:16:36.274 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.00751, ret=4.35e-5, glen=51, tlen=211, kl=0.00806, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=-0.00751, ret=4.35e-5, glen=51, tlen=211, kl=0.00806, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.01s/it, pg=-0.0251, ret=3.7e-5, glen=48.3, tlen=209, kl=0.00761, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.08it/s, pg=-0.0251, ret=3.7e-5, glen=48.3, tlen=209, kl=0.00761, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.08it/s, pg=0.000854, ret=5.58e-5, glen=50.1, tlen=211, kl=0.00745, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:38,  1.08it/s, pg=0.000854, ret=5.58e-5, glen=50.1, tlen=211, kl=0.00745, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:38,  1.08it/s, pg=-0.0292, ret=4.5e-5, glen=49.8, tlen=210, kl=0.00909, act_lr=1e-6, ent=0.967]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:36,  1.11it/s, pg=-0.0292, ret=4.5e-5, glen=49.8, tlen=210, kl=0.00909, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:36,  1.11it/s, pg=0.0137, ret=3.89e-5, glen=49.4, tlen=210, kl=0.00913, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.13it/s, pg=0.0137, ret=3.89e-5, glen=49.4, tlen=210, kl=0.00913, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.13it/s, pg=-0.0745, ret=0.000138, glen=51.2, tlen=212, kl=0.0106, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.0745, ret=0.000138, glen=51.2, tlen=212, kl=0.0106, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=0.0837, ret=-0.000147, glen=50.3, tlen=211, kl=0.0083, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.13it/s, pg=0.0837, ret=-0.000147, glen=50.3, tlen=211, kl=0.0083, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.13it/s, pg=-0.0117, ret=-4.55e-5, glen=49.7, tlen=210, kl=0.00735, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.11it/s, pg=-0.0117, ret=-4.55e-5, glen=49.7, tlen=210, kl=0.00735, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:32,  1.11it/s, pg=0.035, ret=-0.00012, glen=48.4, tlen=208, kl=0.00867, act_lr=1e-6, ent=0.926]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:31,  1.12it/s, pg=0.035, ret=-0.00012, glen=48.4, tlen=208, kl=0.00867, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:31,  1.12it/s, pg=-0.0233, ret=3.32e-5, glen=50.6, tlen=211, kl=0.00955, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.14it/s, pg=-0.0233, ret=3.32e-5, glen=50.6, tlen=211, kl=0.00955, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.14it/s, pg=-0.0775, ret=0.000147, glen=50, tlen=211, kl=0.00908, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.15it/s, pg=-0.0775, ret=0.000147, glen=50, tlen=211, kl=0.00908, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.15it/s, pg=-0.00732, ret=8.81e-5, glen=49.1, tlen=209, kl=0.0077, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=-0.00732, ret=8.81e-5, glen=49.1, tlen=209, kl=0.0077, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.0996, ret=0.000171, glen=50.2, tlen=211, kl=0.0101, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.16it/s, pg=-0.0996, ret=0.000171, glen=50.2, tlen=211, kl=0.0101, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.16it/s, pg=-0.0336, ret=7.39e-5, glen=49.2, tlen=209, kl=0.00893, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.16it/s, pg=-0.0336, ret=7.39e-5, glen=49.2, tlen=209, kl=0.00893, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.16it/s, pg=-0.0195, ret=7.47e-6, glen=49, tlen=209, kl=0.00932, act_lr=1e-6, ent=0.949]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0195, ret=7.47e-6, glen=49, tlen=209, kl=0.00932, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:24,  1.17it/s, pg=-0.0139, ret=4.6e-5, glen=51.9, tlen=212, kl=0.00919, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.0139, ret=4.6e-5, glen=51.9, tlen=212, kl=0.00919, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=0.00818, ret=7.77e-6, glen=49.5, tlen=210, kl=0.00997, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.00818, ret=7.77e-6, glen=49.5, tlen=210, kl=0.00997, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.0774, ret=0.00014, glen=48.9, tlen=209, kl=0.0082, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.0774, ret=0.00014, glen=48.9, tlen=209, kl=0.0082, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=0.135, ret=-0.000177, glen=49.4, tlen=210, kl=0.0075, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=0.135, ret=-0.000177, glen=49.4, tlen=210, kl=0.0075, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=0.0221, ret=-3.89e-5, glen=50.6, tlen=211, kl=0.012, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.0221, ret=-3.89e-5, glen=50.6, tlen=211, kl=0.012, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.101, ret=0.000184, glen=50.6, tlen=211, kl=0.00831, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.15it/s, pg=-0.101, ret=0.000184, glen=50.6, tlen=211, kl=0.00831, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.15it/s, pg=0.187, ret=-0.000256, glen=49.5, tlen=209, kl=0.00854, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:19,  1.15it/s, pg=0.187, ret=-0.000256, glen=49.5, tlen=209, kl=0.00854, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:19,  1.15it/s, pg=-0.0857, ret=0.000163, glen=49.3, tlen=209, kl=0.00898, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=-0.0857, ret=0.000163, glen=49.3, tlen=209, kl=0.00898, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=-0.0834, ret=0.000148, glen=49.5, tlen=209, kl=0.00917, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.16it/s, pg=-0.0834, ret=0.000148, glen=49.5, tlen=209, kl=0.00917, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.16it/s, pg=-0.0105, ret=5.68e-5, glen=50.7, tlen=210, kl=0.0087, act_lr=1e-6, ent=0.924]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.14it/s, pg=-0.0105, ret=5.68e-5, glen=50.7, tlen=210, kl=0.0087, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.14it/s, pg=0.0369, ret=-9.76e-5, glen=51.8, tlen=212, kl=0.00841, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.15it/s, pg=0.0369, ret=-9.76e-5, glen=51.8, tlen=212, kl=0.00841, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.15it/s, pg=0.0194, ret=-1.35e-5, glen=48.6, tlen=209, kl=0.00726, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.16it/s, pg=0.0194, ret=-1.35e-5, glen=48.6, tlen=209, kl=0.00726, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.16it/s, pg=-0.00537, ret=-4.2e-5, glen=48.8, tlen=209, kl=0.00912, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.16it/s, pg=-0.00537, ret=-4.2e-5, glen=48.8, tlen=209, kl=0.00912, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.16it/s, pg=-0.0939, ret=0.000175, glen=49.7, tlen=210, kl=0.00826, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.06it/s, pg=-0.0939, ret=0.000175, glen=49.7, tlen=210, kl=0.00826, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.06it/s, pg=-0.0229, ret=6.22e-5, glen=51.7, tlen=212, kl=0.00871, act_lr=1e-6, ent=0.974] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.0229, ret=6.22e-5, glen=51.7, tlen=212, kl=0.00871, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.045, ret=4.83e-5, glen=49.5, tlen=210, kl=0.00814, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.045, ret=4.83e-5, glen=49.5, tlen=210, kl=0.00814, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.12it/s, pg=0.0372, ret=-9.41e-5, glen=48.3, tlen=208, kl=0.00846, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.0372, ret=-9.41e-5, glen=48.3, tlen=208, kl=0.00846, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.0179, ret=-7.34e-5, glen=48.6, tlen=209, kl=0.00767, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.0179, ret=-7.34e-5, glen=48.6, tlen=209, kl=0.00767, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.0177, ret=2.95e-5, glen=49.3, tlen=209, kl=0.00782, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.0177, ret=2.95e-5, glen=49.3, tlen=209, kl=0.00782, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=-0.0488, ret=4.57e-5, glen=50.2, tlen=211, kl=0.00851, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0488, ret=4.57e-5, glen=50.2, tlen=211, kl=0.00851, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.00357, ret=-5.18e-5, glen=50.4, tlen=211, kl=0.00831, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=0.00357, ret=-5.18e-5, glen=50.4, tlen=211, kl=0.00831, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.0599, ret=0.000108, glen=47.5, tlen=207, kl=0.00852, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0599, ret=0.000108, glen=47.5, tlen=207, kl=0.00852, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=0.28, ret=-0.000507, glen=49.9, tlen=210, kl=0.0112, act_lr=1e-6, ent=0.963]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.28, ret=-0.000507, glen=49.9, tlen=210, kl=0.0112, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=0.273, ret=-0.000433, glen=50, tlen=210, kl=0.0105, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.273, ret=-0.000433, glen=50, tlen=210, kl=0.0105, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0507, ret=-4.49e-5, glen=51.2, tlen=212, kl=0.00763, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0507, ret=-4.49e-5, glen=51.2, tlen=212, kl=0.00763, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0109, ret=-4.74e-7, glen=51, tlen=212, kl=0.0079, act_lr=1e-6, ent=0.956]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0109, ret=-4.74e-7, glen=51, tlen=212, kl=0.0079, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.0952, ret=0.000174, glen=51.3, tlen=212, kl=0.00723, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.0952, ret=0.000174, glen=51.3, tlen=212, kl=0.00723, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.04, ret=-6.92e-5, glen=49.9, tlen=210, kl=0.011, act_lr=1e-6, ent=0.929]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=0.04, ret=-6.92e-5, glen=49.9, tlen=210, kl=0.011, act_lr=1e-6, ent=0.929]
2025-07-24 20:17:14.852 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.38s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0732, ret=8.28e-5, glen=48.4, tlen=209, kl=0.0085, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0732, ret=8.28e-5, glen=48.4, tlen=209, kl=0.0085, act_lr=1e-6, ent=0.936]
2025-07-24 20:17:15.695 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:17:18.304 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.61s
2025-07-24 20:17:18.636 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.28s
2025-07-24 20:17:18.642 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00021674416281960228, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9445066411386837, 'kl': 0.008739991621537642, 'response_length': 49.82633235237815, 'total_length': 210.06823869185015, 'teacher_total_length': 222.08328420465642, 'return': 3.112270819242853e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [08:35<13:43, 102.92s/it][A2025-07-24 20:17:18.687 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:17:48.790 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:17:48.965 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:17:48.965 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.28s
2025-07-24 20:17:50.868 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 49.6599,std_num_tokens: 12.9925,avg_correct_num_tokens: 49.6503,std_correct_num_tokens: 12.9776,avg_incorrect_num_tokens: 50.7971,std_incorrect_num_tokens: 14.5886
2025-07-24 20:17:51.276 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.31s
2025-07-24 20:17:53.674 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.40s
2025-07-24 20:18:16.234 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 20:18:16.234 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.56s
2025-07-24 20:18:17.408 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.77s
2025-07-24 20:18:17.409 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.2137248567466077e-05, avg_kl: 0.023293330751616378, avg_response_length: 49.69546302707716, avg_orm_score: 0.0, avg_custom_rewards: 2.2137248567466077e-05
2025-07-24 20:18:17.436 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter96_replay_buffer.jsonl
2025-07-24 20:18:18.562 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0897, ret=0.00017, glen=50.5, tlen=211, kl=0.0198, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:44,  1.04s/it, pg=-0.0897, ret=0.00017, glen=50.5, tlen=211, kl=0.0198, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:44,  1.04s/it, pg=0.079, ret=-0.000245, glen=49.4, tlen=210, kl=0.0219, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:39,  1.06it/s, pg=0.079, ret=-0.000245, glen=49.4, tlen=210, kl=0.0219, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:39,  1.06it/s, pg=-0.0283, ret=7.46e-5, glen=48.5, tlen=209, kl=0.0182, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:37,  1.11it/s, pg=-0.0283, ret=7.46e-5, glen=48.5, tlen=209, kl=0.0182, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:37,  1.11it/s, pg=0.102, ret=-0.000158, glen=50, tlen=210, kl=0.0208, act_lr=1e-6, ent=0.948]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.13it/s, pg=0.102, ret=-0.000158, glen=50, tlen=210, kl=0.0208, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.13it/s, pg=0.0948, ret=-0.000169, glen=48.1, tlen=208, kl=0.0186, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:33,  1.15it/s, pg=0.0948, ret=-0.000169, glen=48.1, tlen=208, kl=0.0186, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:33,  1.15it/s, pg=0.111, ret=-0.000166, glen=49, tlen=209, kl=0.0174, act_lr=1e-6, ent=0.943]  Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.12it/s, pg=0.111, ret=-0.000166, glen=49, tlen=209, kl=0.0174, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.12it/s, pg=-0.0486, ret=8.1e-5, glen=49.8, tlen=210, kl=0.0199, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.13it/s, pg=-0.0486, ret=8.1e-5, glen=49.8, tlen=210, kl=0.0199, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.13it/s, pg=0.144, ret=-0.000206, glen=48.9, tlen=209, kl=0.0187, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=0.144, ret=-0.000206, glen=48.9, tlen=209, kl=0.0187, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=0.0438, ret=-3.71e-5, glen=50.8, tlen=212, kl=0.0208, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=0.0438, ret=-3.71e-5, glen=50.8, tlen=212, kl=0.0208, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.0182, ret=-3.22e-5, glen=49.3, tlen=210, kl=0.0186, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=0.0182, ret=-3.22e-5, glen=49.3, tlen=210, kl=0.0186, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=-0.0174, ret=2.63e-5, glen=49.4, tlen=210, kl=0.0205, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.0174, ret=2.63e-5, glen=49.4, tlen=210, kl=0.0205, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=-0.0442, ret=4.91e-5, glen=47.8, tlen=208, kl=0.14, act_lr=1e-6, ent=0.903]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.17it/s, pg=-0.0442, ret=4.91e-5, glen=47.8, tlen=208, kl=0.14, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.17it/s, pg=0.00903, ret=-8.48e-5, glen=49.3, tlen=210, kl=0.0182, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.00903, ret=-8.48e-5, glen=49.3, tlen=210, kl=0.0182, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=-0.0981, ret=0.00019, glen=47.8, tlen=208, kl=0.0186, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=-0.0981, ret=0.00019, glen=47.8, tlen=208, kl=0.0186, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=-0.0923, ret=0.000169, glen=49.8, tlen=210, kl=0.0198, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0923, ret=0.000169, glen=49.8, tlen=210, kl=0.0198, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0372, ret=9.65e-5, glen=49.8, tlen=211, kl=0.0188, act_lr=1e-6, ent=0.961] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:23,  1.17it/s, pg=-0.0372, ret=9.65e-5, glen=49.8, tlen=211, kl=0.0188, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=0.0563, ret=-4.37e-5, glen=50.1, tlen=211, kl=0.0162, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.0563, ret=-4.37e-5, glen=50.1, tlen=211, kl=0.0162, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.00797, ret=-2.17e-5, glen=51.4, tlen=212, kl=0.0207, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.00797, ret=-2.17e-5, glen=51.4, tlen=212, kl=0.0207, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.0167, ret=4.3e-5, glen=49.1, tlen=210, kl=0.0199, act_lr=1e-6, ent=0.946]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.0167, ret=4.3e-5, glen=49.1, tlen=210, kl=0.0199, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=0.0707, ret=-0.000135, glen=49.5, tlen=210, kl=0.0517, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.0707, ret=-0.000135, glen=49.5, tlen=210, kl=0.0517, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0359, ret=5.11e-5, glen=49.6, tlen=210, kl=0.0181, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0359, ret=5.11e-5, glen=49.6, tlen=210, kl=0.0181, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=-0.0428, ret=5.49e-5, glen=49.3, tlen=210, kl=0.0212, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.0428, ret=5.49e-5, glen=49.3, tlen=210, kl=0.0212, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.0309, ret=5.58e-5, glen=50.3, tlen=212, kl=0.0213, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.17it/s, pg=-0.0309, ret=5.58e-5, glen=50.3, tlen=212, kl=0.0213, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=0.0583, ret=-1.77e-5, glen=50.4, tlen=211, kl=0.0208, act_lr=1e-6, ent=0.98] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.0583, ret=-1.77e-5, glen=50.4, tlen=211, kl=0.0208, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.0159, ret=2.48e-5, glen=50, tlen=211, kl=0.0186, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.0159, ret=2.48e-5, glen=50, tlen=211, kl=0.0186, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.0513, ret=-3.08e-5, glen=48.2, tlen=209, kl=0.0178, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.0513, ret=-3.08e-5, glen=48.2, tlen=209, kl=0.0178, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.0176, ret=-3.17e-5, glen=49.8, tlen=210, kl=0.0235, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.0176, ret=-3.17e-5, glen=49.8, tlen=210, kl=0.0235, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.0377, ret=-4.17e-5, glen=49.3, tlen=210, kl=0.0201, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.0377, ret=-4.17e-5, glen=49.3, tlen=210, kl=0.0201, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=0.0399, ret=-3.27e-5, glen=51, tlen=212, kl=0.02, act_lr=1e-6, ent=0.93]     Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=0.0399, ret=-3.27e-5, glen=51, tlen=212, kl=0.02, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=-0.0183, ret=4.84e-5, glen=49.9, tlen=210, kl=0.0193, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=-0.0183, ret=4.84e-5, glen=49.9, tlen=210, kl=0.0193, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.0265, ret=8.3e-5, glen=51.2, tlen=212, kl=0.0195, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0265, ret=8.3e-5, glen=51.2, tlen=212, kl=0.0195, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.118, ret=0.000221, glen=50.7, tlen=212, kl=0.0171, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=-0.118, ret=0.000221, glen=50.7, tlen=212, kl=0.0171, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=0.05, ret=-0.000166, glen=51.1, tlen=211, kl=0.0203, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.05, ret=-0.000166, glen=51.1, tlen=211, kl=0.0203, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=0.0476, ret=-0.000152, glen=48.7, tlen=209, kl=0.0208, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.16it/s, pg=0.0476, ret=-0.000152, glen=48.7, tlen=209, kl=0.0208, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.16it/s, pg=0.00818, ret=-2.78e-5, glen=50.4, tlen=211, kl=0.0206, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.00818, ret=-2.78e-5, glen=50.4, tlen=211, kl=0.0206, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0867, ret=0.000164, glen=51, tlen=212, kl=0.021, act_lr=1e-6, ent=0.952]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.17it/s, pg=-0.0867, ret=0.000164, glen=51, tlen=212, kl=0.021, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.17it/s, pg=-0.11, ret=0.000205, glen=50, tlen=211, kl=0.0219, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.11, ret=0.000205, glen=50, tlen=211, kl=0.0219, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0819, ret=0.00016, glen=49, tlen=209, kl=0.0233, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:32<00:05,  1.17it/s, pg=-0.0819, ret=0.00016, glen=49, tlen=209, kl=0.0233, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0069, ret=-6.45e-5, glen=49.7, tlen=210, kl=0.0187, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.0069, ret=-6.45e-5, glen=49.7, tlen=210, kl=0.0187, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.101, ret=0.000192, glen=50.4, tlen=211, kl=0.0211, act_lr=1e-6, ent=0.946] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.101, ret=0.000192, glen=50.4, tlen=211, kl=0.0211, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0333, ret=4.78e-5, glen=48.9, tlen=210, kl=0.0172, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0333, ret=4.78e-5, glen=48.9, tlen=210, kl=0.0172, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.0249, ret=-4.22e-5, glen=49.6, tlen=210, kl=0.0172, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.0249, ret=-4.22e-5, glen=49.6, tlen=210, kl=0.0172, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.127, ret=-0.000222, glen=49.7, tlen=211, kl=0.0219, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=0.127, ret=-0.000222, glen=49.7, tlen=211, kl=0.0219, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0559, ret=7.94e-5, glen=49.6, tlen=210, kl=0.0211, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=-0.0559, ret=7.94e-5, glen=49.6, tlen=210, kl=0.0211, act_lr=1e-6, ent=0.933]
2025-07-24 20:18:56.914 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.19s
2025-07-24 20:18:57.780 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:19:00.367 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 20:19:00.791 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.18s
2025-07-24 20:19:00.809 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0011783946644176137, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.941198855638504, 'kl': 0.023217634721235794, 'response_length': 49.68199651891535, 'total_length': 210.25645065307617, 'teacher_total_length': 222.23821640014648, 'return': 3.6053714790217453e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [10:17<11:58, 102.67s/it][A2025-07-24 20:19:00.858 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:19:30.902 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:19:31.083 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:19:31.084 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.23s
2025-07-24 20:19:32.930 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 49.3344,std_num_tokens: 13.2728,avg_correct_num_tokens: 49.3387,std_correct_num_tokens: 13.2794,avg_incorrect_num_tokens: 48.8060,std_incorrect_num_tokens: 12.4460
2025-07-24 20:19:33.340 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.26s
2025-07-24 20:19:35.771 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.43s
2025-07-24 20:19:58.450 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 173
2025-07-24 20:19:58.450 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.68s
2025-07-24 20:19:59.716 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 20:19:59.717 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.3326401041680678e-05, avg_kl: 0.057583318280346824, avg_response_length: 49.35006358857789, avg_orm_score: 0.0, avg_custom_rewards: 1.3326401041680678e-05
2025-07-24 20:19:59.744 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter97_replay_buffer.jsonl
2025-07-24 20:20:00.803 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=-0.0242, ret=1.7e-5, glen=47.9, tlen=208, kl=0.0486, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.00it/s, pg=-0.0242, ret=1.7e-5, glen=47.9, tlen=208, kl=0.0486, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.00it/s, pg=-0.0819, ret=0.000161, glen=48.1, tlen=208, kl=0.0844, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=-0.0819, ret=0.000161, glen=48.1, tlen=208, kl=0.0844, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=-0.0316, ret=7.32e-5, glen=49.6, tlen=210, kl=0.0703, act_lr=1e-6, ent=0.958] Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=-0.0316, ret=7.32e-5, glen=49.6, tlen=210, kl=0.0703, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=-0.0371, ret=8.35e-5, glen=50.4, tlen=211, kl=0.0851, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:36,  1.11it/s, pg=-0.0371, ret=8.35e-5, glen=50.4, tlen=211, kl=0.0851, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:36,  1.11it/s, pg=-0.0621, ret=9.2e-5, glen=50.7, tlen=211, kl=0.0482, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.13it/s, pg=-0.0621, ret=9.2e-5, glen=50.7, tlen=211, kl=0.0482, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.13it/s, pg=-0.0996, ret=0.00018, glen=49.6, tlen=210, kl=0.09, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.0996, ret=0.00018, glen=49.6, tlen=210, kl=0.09, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=-0.00323, ret=6.23e-5, glen=51, tlen=211, kl=0.0489, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:33,  1.12it/s, pg=-0.00323, ret=6.23e-5, glen=51, tlen=211, kl=0.0489, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:33,  1.12it/s, pg=0.0188, ret=-7.06e-5, glen=48.9, tlen=209, kl=0.0486, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.0188, ret=-7.06e-5, glen=48.9, tlen=209, kl=0.0486, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=-0.0348, ret=4.77e-5, glen=51.5, tlen=212, kl=0.0504, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.15it/s, pg=-0.0348, ret=4.77e-5, glen=51.5, tlen=212, kl=0.0504, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=-0.0359, ret=5.48e-5, glen=50.7, tlen=211, kl=0.0708, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.16it/s, pg=-0.0359, ret=5.48e-5, glen=50.7, tlen=211, kl=0.0708, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.16it/s, pg=0.298, ret=-0.000565, glen=48.8, tlen=209, kl=0.0544, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=0.298, ret=-0.000565, glen=48.8, tlen=209, kl=0.0544, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=-0.037, ret=3.93e-5, glen=50.2, tlen=210, kl=0.0695, act_lr=1e-6, ent=0.907] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=-0.037, ret=3.93e-5, glen=50.2, tlen=210, kl=0.0695, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.0453, ret=8.26e-5, glen=48.6, tlen=209, kl=0.0453, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:27,  1.14it/s, pg=-0.0453, ret=8.26e-5, glen=48.6, tlen=209, kl=0.0453, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:27,  1.14it/s, pg=0.00476, ret=-2.17e-5, glen=49.4, tlen=210, kl=0.0397, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=0.00476, ret=-2.17e-5, glen=49.4, tlen=210, kl=0.0397, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=-0.071, ret=0.000138, glen=50.5, tlen=211, kl=0.0572, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=-0.071, ret=0.000138, glen=50.5, tlen=211, kl=0.0572, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.16it/s, pg=-0.0667, ret=0.000129, glen=49.1, tlen=209, kl=0.0513, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.0667, ret=0.000129, glen=49.1, tlen=209, kl=0.0513, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.0264, ret=3.84e-5, glen=49.4, tlen=209, kl=0.074, act_lr=1e-6, ent=0.912]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.16it/s, pg=-0.0264, ret=3.84e-5, glen=49.4, tlen=209, kl=0.074, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=-0.0323, ret=5.8e-5, glen=48.5, tlen=209, kl=0.0669, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=-0.0323, ret=5.8e-5, glen=48.5, tlen=209, kl=0.0669, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=-0.0823, ret=0.000154, glen=49.3, tlen=210, kl=0.0557, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.0823, ret=0.000154, glen=49.3, tlen=210, kl=0.0557, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0183, ret=1.33e-5, glen=49.9, tlen=210, kl=0.0444, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.15it/s, pg=-0.0183, ret=1.33e-5, glen=49.9, tlen=210, kl=0.0444, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.15it/s, pg=-0.0671, ret=0.000112, glen=48.1, tlen=208, kl=0.0431, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.16it/s, pg=-0.0671, ret=0.000112, glen=48.1, tlen=208, kl=0.0431, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.16it/s, pg=-0.0148, ret=-8.48e-6, glen=47.5, tlen=208, kl=0.0497, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.16it/s, pg=-0.0148, ret=-8.48e-6, glen=47.5, tlen=208, kl=0.0497, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.16it/s, pg=0.00519, ret=1.19e-5, glen=47.8, tlen=208, kl=0.0551, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=0.00519, ret=1.19e-5, glen=47.8, tlen=208, kl=0.0551, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=0.0964, ret=-0.000184, glen=49.9, tlen=210, kl=0.041, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.0964, ret=-0.000184, glen=49.9, tlen=210, kl=0.041, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.0573, ret=9.56e-5, glen=49.6, tlen=210, kl=0.0483, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.0573, ret=9.56e-5, glen=49.6, tlen=210, kl=0.0483, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.138, ret=-0.000273, glen=50.3, tlen=210, kl=0.0634, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.138, ret=-0.000273, glen=50.3, tlen=210, kl=0.0634, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.0879, ret=-0.000137, glen=48.2, tlen=208, kl=0.0393, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.0879, ret=-0.000137, glen=48.2, tlen=208, kl=0.0393, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.035, ret=5.77e-5, glen=47.6, tlen=208, kl=0.0499, act_lr=1e-6, ent=0.904]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.035, ret=5.77e-5, glen=47.6, tlen=208, kl=0.0499, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.0215, ret=-4.52e-6, glen=48.4, tlen=209, kl=0.0391, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.06it/s, pg=-0.0215, ret=-4.52e-6, glen=48.4, tlen=209, kl=0.0391, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.06it/s, pg=0.103, ret=-0.000165, glen=49.5, tlen=210, kl=0.0496, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.09it/s, pg=0.103, ret=-0.000165, glen=49.5, tlen=210, kl=0.0496, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.09it/s, pg=-0.0314, ret=8.29e-5, glen=48.5, tlen=208, kl=0.0727, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.11it/s, pg=-0.0314, ret=8.29e-5, glen=48.5, tlen=208, kl=0.0727, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.11it/s, pg=-0.00488, ret=2.83e-5, glen=49.7, tlen=210, kl=0.0471, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=-0.00488, ret=2.83e-5, glen=49.7, tlen=210, kl=0.0471, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.116, ret=-0.000183, glen=48.5, tlen=209, kl=0.0469, act_lr=1e-6, ent=0.935] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.14it/s, pg=0.116, ret=-0.000183, glen=48.5, tlen=209, kl=0.0469, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.14it/s, pg=-0.0992, ret=0.000195, glen=50.4, tlen=211, kl=0.0952, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.0992, ret=0.000195, glen=50.4, tlen=211, kl=0.0952, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=0.193, ret=-0.000247, glen=49.4, tlen=210, kl=0.0461, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.193, ret=-0.000247, glen=49.4, tlen=210, kl=0.0461, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0453, ret=8.75e-5, glen=48, tlen=208, kl=0.0533, act_lr=1e-6, ent=0.912]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.0453, ret=8.75e-5, glen=48, tlen=208, kl=0.0533, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.0525, ret=7.88e-5, glen=49.2, tlen=209, kl=0.0732, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.16it/s, pg=-0.0525, ret=7.88e-5, glen=49.2, tlen=209, kl=0.0732, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.16it/s, pg=-0.0598, ret=7.9e-5, glen=49.7, tlen=210, kl=0.0411, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0598, ret=7.9e-5, glen=49.7, tlen=210, kl=0.0411, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.138, ret=-0.000186, glen=49.6, tlen=210, kl=0.0583, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=0.138, ret=-0.000186, glen=49.6, tlen=210, kl=0.0583, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0103, ret=-6.3e-5, glen=52.2, tlen=213, kl=0.0748, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0103, ret=-6.3e-5, glen=52.2, tlen=213, kl=0.0748, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0348, ret=5.31e-5, glen=50.5, tlen=211, kl=0.0484, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0348, ret=5.31e-5, glen=50.5, tlen=211, kl=0.0484, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.0242, ret=-4.91e-5, glen=50.6, tlen=211, kl=0.067, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.0242, ret=-4.91e-5, glen=50.6, tlen=211, kl=0.067, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.0295, ret=-0.000131, glen=47.4, tlen=208, kl=0.0576, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=0.0295, ret=-0.000131, glen=47.4, tlen=208, kl=0.0576, act_lr=1e-6, ent=0.959]
2025-07-24 20:20:39.545 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.38s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0758, ret=0.000139, glen=48.3, tlen=208, kl=0.0619, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0758, ret=0.000139, glen=48.3, tlen=208, kl=0.0619, act_lr=1e-6, ent=0.921]
2025-07-24 20:20:40.366 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 20:20:42.937 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 20:20:43.285 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.24s
2025-07-24 20:20:43.319 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0012775767933238637, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9339861395684156, 'kl': 0.057393854314630684, 'response_length': 49.349665034901015, 'total_length': 209.5587272644043, 'teacher_total_length': 221.54551592740145, 'return': 3.5394947107389188e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [11:59<10:15, 102.61s/it][A2025-07-24 20:20:43.367 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:21:13.712 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:21:13.897 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 20:21:13.898 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.53s
2025-07-24 20:21:15.561 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 49.6265,std_num_tokens: 12.8234,avg_correct_num_tokens: 49.6112,std_correct_num_tokens: 12.8226,avg_incorrect_num_tokens: 51.1341,std_incorrect_num_tokens: 12.8184
2025-07-24 20:21:15.942 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.04s
2025-07-24 20:21:18.368 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.43s
2025-07-24 20:21:40.924 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 20:21:40.925 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.55s
2025-07-24 20:21:42.464 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.07s
2025-07-24 20:21:42.465 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 7.348218076359267e-06, avg_kl: 0.07515883171695402, avg_response_length: 49.636228144853966, avg_orm_score: 0.0, avg_custom_rewards: 7.348218076359267e-06
2025-07-24 20:21:42.493 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter98_replay_buffer.jsonl
2025-07-24 20:21:43.630 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.14s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s, pg=0.00415, ret=-2.28e-5, glen=50.4, tlen=211, kl=0.0771, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:00<00:42,  1.00it/s, pg=0.00415, ret=-2.28e-5, glen=50.4, tlen=211, kl=0.0771, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:42,  1.00it/s, pg=-0.00415, ret=1.24e-5, glen=50.2, tlen=211, kl=0.0695, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=-0.00415, ret=1.24e-5, glen=50.2, tlen=211, kl=0.0695, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=-0.037, ret=8.38e-5, glen=49.5, tlen=210, kl=0.0605, act_lr=1e-6, ent=0.924]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=-0.037, ret=8.38e-5, glen=49.5, tlen=210, kl=0.0605, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=-0.0394, ret=7.42e-5, glen=49.2, tlen=210, kl=0.0532, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.12it/s, pg=-0.0394, ret=7.42e-5, glen=49.2, tlen=210, kl=0.0532, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.12it/s, pg=0.0612, ret=-1.67e-5, glen=48.9, tlen=210, kl=0.0599, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:35,  1.10it/s, pg=0.0612, ret=-1.67e-5, glen=48.9, tlen=210, kl=0.0599, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:35,  1.10it/s, pg=0.102, ret=-0.000262, glen=48.9, tlen=209, kl=0.0568, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.12it/s, pg=0.102, ret=-0.000262, glen=48.9, tlen=209, kl=0.0568, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.12it/s, pg=-0.112, ret=0.00023, glen=50.7, tlen=211, kl=0.0707, act_lr=1e-6, ent=0.943] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.14it/s, pg=-0.112, ret=0.00023, glen=50.7, tlen=211, kl=0.0707, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.14it/s, pg=-0.11, ret=0.000217, glen=47.5, tlen=208, kl=0.0667, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=-0.11, ret=0.000217, glen=47.5, tlen=208, kl=0.0667, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:31,  1.15it/s, pg=-0.00549, ret=9.17e-6, glen=48.8, tlen=209, kl=0.0605, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=-0.00549, ret=9.17e-6, glen=48.8, tlen=209, kl=0.0605, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=-0.0531, ret=0.000138, glen=49.3, tlen=210, kl=0.0627, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.14it/s, pg=-0.0531, ret=0.000138, glen=49.3, tlen=210, kl=0.0627, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.14it/s, pg=0.187, ret=-0.000446, glen=50.4, tlen=211, kl=0.0563, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.15it/s, pg=0.187, ret=-0.000446, glen=50.4, tlen=211, kl=0.0563, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.15it/s, pg=-0.0248, ret=7.57e-5, glen=51.4, tlen=212, kl=0.0687, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=-0.0248, ret=7.57e-5, glen=51.4, tlen=212, kl=0.0687, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=-0.111, ret=0.000212, glen=48.4, tlen=209, kl=0.063, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.16it/s, pg=-0.111, ret=0.000212, glen=48.4, tlen=209, kl=0.063, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.16it/s, pg=-0.0612, ret=0.000116, glen=49.1, tlen=210, kl=0.0577, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.16it/s, pg=-0.0612, ret=0.000116, glen=49.1, tlen=210, kl=0.0577, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.16it/s, pg=-0.0221, ret=7.82e-5, glen=49.6, tlen=211, kl=0.0601, act_lr=1e-6, ent=0.956] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0221, ret=7.82e-5, glen=49.6, tlen=211, kl=0.0601, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:24,  1.17it/s, pg=-0.00757, ret=4.6e-5, glen=50.1, tlen=211, kl=0.0732, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.00757, ret=4.6e-5, glen=50.1, tlen=211, kl=0.0732, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=0.0557, ret=-6.72e-5, glen=49.6, tlen=211, kl=0.0778, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.0557, ret=-6.72e-5, glen=49.6, tlen=211, kl=0.0778, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=0.0502, ret=-0.000116, glen=49.6, tlen=210, kl=0.0568, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=0.0502, ret=-0.000116, glen=49.6, tlen=210, kl=0.0568, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=0.0258, ret=-7.14e-5, glen=49.9, tlen=211, kl=0.102, act_lr=1e-6, ent=0.944]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=0.0258, ret=-7.14e-5, glen=49.9, tlen=211, kl=0.102, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0925, ret=0.000189, glen=49.5, tlen=210, kl=0.0645, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0925, ret=0.000189, glen=49.5, tlen=210, kl=0.0645, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.012, ret=-4.03e-5, glen=49.3, tlen=210, kl=0.0976, act_lr=1e-6, ent=0.977] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.012, ret=-4.03e-5, glen=49.3, tlen=210, kl=0.0976, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.053, ret=-0.000138, glen=49.8, tlen=210, kl=0.0683, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:19,  1.15it/s, pg=0.053, ret=-0.000138, glen=49.8, tlen=210, kl=0.0683, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:19,  1.15it/s, pg=0.203, ret=-0.000354, glen=50.4, tlen=212, kl=0.0526, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.12it/s, pg=0.203, ret=-0.000354, glen=50.4, tlen=212, kl=0.0526, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.12it/s, pg=-0.0723, ret=9.9e-5, glen=50.2, tlen=211, kl=0.0804, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.13it/s, pg=-0.0723, ret=9.9e-5, glen=50.2, tlen=211, kl=0.0804, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.13it/s, pg=0.00464, ret=-1.77e-6, glen=50.1, tlen=211, kl=0.0579, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.14it/s, pg=0.00464, ret=-1.77e-6, glen=50.1, tlen=211, kl=0.0579, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.14it/s, pg=-0.0117, ret=1.95e-5, glen=51, tlen=213, kl=0.0648, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.13it/s, pg=-0.0117, ret=1.95e-5, glen=51, tlen=213, kl=0.0648, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.13it/s, pg=0.0231, ret=-3.7e-5, glen=51, tlen=212, kl=0.061, act_lr=1e-6, ent=0.946]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.15it/s, pg=0.0231, ret=-3.7e-5, glen=51, tlen=212, kl=0.061, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.15it/s, pg=-0.0195, ret=3.4e-5, glen=49.1, tlen=210, kl=0.188, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.15it/s, pg=-0.0195, ret=3.4e-5, glen=49.1, tlen=210, kl=0.188, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.15it/s, pg=-0.0149, ret=3.37e-5, glen=50, tlen=211, kl=0.0696, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.06it/s, pg=-0.0149, ret=3.37e-5, glen=50, tlen=211, kl=0.0696, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.06it/s, pg=0.196, ret=-0.000352, glen=49.3, tlen=210, kl=0.0584, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.09it/s, pg=0.196, ret=-0.000352, glen=49.3, tlen=210, kl=0.0584, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.09it/s, pg=0.121, ret=-0.000348, glen=49.1, tlen=210, kl=0.0685, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.11it/s, pg=0.121, ret=-0.000348, glen=49.1, tlen=210, kl=0.0685, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.11it/s, pg=0.123, ret=-0.000262, glen=50.8, tlen=211, kl=0.189, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.123, ret=-0.000262, glen=50.8, tlen=211, kl=0.189, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.00354, ret=-1.63e-5, glen=48.8, tlen=209, kl=0.0645, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.00354, ret=-1.63e-5, glen=48.8, tlen=209, kl=0.0645, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.108, ret=0.000236, glen=51.7, tlen=212, kl=0.0742, act_lr=1e-6, ent=0.958] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.108, ret=0.000236, glen=51.7, tlen=212, kl=0.0742, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=-0.0192, ret=-2.3e-5, glen=49.8, tlen=211, kl=0.103, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0192, ret=-2.3e-5, glen=49.8, tlen=211, kl=0.103, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0896, ret=0.00018, glen=48.2, tlen=209, kl=0.08, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.0896, ret=0.00018, glen=48.2, tlen=209, kl=0.08, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=0.0126, ret=-2.6e-5, glen=48.6, tlen=209, kl=0.0734, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=0.0126, ret=-2.6e-5, glen=48.6, tlen=209, kl=0.0734, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=-0.0345, ret=8.26e-5, glen=49.7, tlen=210, kl=0.0643, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0345, ret=8.26e-5, glen=49.7, tlen=210, kl=0.0643, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=0.00342, ret=7.4e-5, glen=48.8, tlen=210, kl=0.0579, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.00342, ret=7.4e-5, glen=48.8, tlen=210, kl=0.0579, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0308, ret=7.78e-5, glen=49.9, tlen=211, kl=0.0789, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.0308, ret=7.78e-5, glen=49.9, tlen=211, kl=0.0789, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0515, ret=8.09e-5, glen=50.2, tlen=211, kl=0.104, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0515, ret=8.09e-5, glen=50.2, tlen=211, kl=0.104, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.0934, ret=0.000196, glen=49.7, tlen=210, kl=0.116, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.0934, ret=0.000196, glen=49.7, tlen=210, kl=0.116, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.109, ret=0.000217, glen=49, tlen=210, kl=0.0529, act_lr=1e-6, ent=0.968] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.109, ret=0.000217, glen=49, tlen=210, kl=0.0529, act_lr=1e-6, ent=0.968]
2025-07-24 20:22:22.244 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.43s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=0.0889, ret=-0.000156, glen=48.3, tlen=209, kl=0.0752, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=0.0889, ret=-0.000156, glen=48.3, tlen=209, kl=0.0752, act_lr=1e-6, ent=0.93]
2025-07-24 20:22:22.922 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:22:24.980 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.06s
2025-07-24 20:22:25.315 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.62s
2025-07-24 20:22:25.321 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0006421696056019176, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9422129961577329, 'kl': 0.07538951526988637, 'response_length': 49.63419619473544, 'total_length': 210.4387664794922, 'teacher_total_length': 222.45355016534978, 'return': 1.303142033231614e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [13:41<08:32, 102.42s/it][A2025-07-24 20:22:25.367 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:22:55.853 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:22:56.029 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:22:56.029 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.66s
2025-07-24 20:22:57.806 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 49.9452,std_num_tokens: 12.9033,avg_correct_num_tokens: 49.9326,std_correct_num_tokens: 12.9133,avg_incorrect_num_tokens: 51.5968,std_incorrect_num_tokens: 11.4038
2025-07-24 20:22:58.246 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.22s
2025-07-24 20:23:00.684 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.44s
2025-07-24 20:23:23.578 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 174
2025-07-24 20:23:23.578 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.89s
2025-07-24 20:23:24.691 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.75s
2025-07-24 20:23:24.691 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.0676202953033064e-05, avg_kl: 0.1320660470545977, avg_response_length: 49.94588257800574, avg_orm_score: 0.0, avg_custom_rewards: 1.0676202953033064e-05
2025-07-24 20:23:24.716 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter99_replay_buffer.jsonl
2025-07-24 20:23:25.840 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0538, ret=8.02e-5, glen=50.1, tlen=210, kl=0.15, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=-0.0538, ret=8.02e-5, glen=50.1, tlen=210, kl=0.15, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.00s/it, pg=0.0929, ret=-0.000202, glen=51.6, tlen=212, kl=0.12, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.09it/s, pg=0.0929, ret=-0.000202, glen=51.6, tlen=212, kl=0.12, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.09it/s, pg=0.0352, ret=-5.47e-5, glen=48.2, tlen=209, kl=0.0908, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.13it/s, pg=0.0352, ret=-5.47e-5, glen=48.2, tlen=209, kl=0.0908, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.13it/s, pg=0.0532, ret=-0.000107, glen=48.3, tlen=209, kl=0.129, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:34,  1.14it/s, pg=0.0532, ret=-0.000107, glen=48.3, tlen=209, kl=0.129, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:34,  1.14it/s, pg=-0.105, ret=0.000183, glen=50, tlen=211, kl=0.157, act_lr=1e-6, ent=0.912]   Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:33,  1.15it/s, pg=-0.105, ret=0.000183, glen=50, tlen=211, kl=0.157, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:33,  1.15it/s, pg=-0.0394, ret=4.04e-5, glen=50, tlen=211, kl=0.127, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.0394, ret=4.04e-5, glen=50, tlen=211, kl=0.127, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=0.182, ret=-0.00027, glen=49.8, tlen=210, kl=0.0814, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.14it/s, pg=0.182, ret=-0.00027, glen=49.8, tlen=210, kl=0.0814, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.14it/s, pg=0.0233, ret=-4.29e-5, glen=50.7, tlen=211, kl=0.0847, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=0.0233, ret=-4.29e-5, glen=50.7, tlen=211, kl=0.0847, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=0.0468, ret=-4.01e-5, glen=48.5, tlen=209, kl=0.114, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.13it/s, pg=0.0468, ret=-4.01e-5, glen=48.5, tlen=209, kl=0.114, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=-0.0867, ret=0.000152, glen=50.1, tlen=211, kl=0.258, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=-0.0867, ret=0.000152, glen=50.1, tlen=211, kl=0.258, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=0.00854, ret=6.36e-6, glen=51.7, tlen=212, kl=0.122, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:29,  1.13it/s, pg=0.00854, ret=6.36e-6, glen=51.7, tlen=212, kl=0.122, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:29,  1.13it/s, pg=0.0731, ret=-0.000179, glen=50.8, tlen=212, kl=0.101, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:28,  1.14it/s, pg=0.0731, ret=-0.000179, glen=50.8, tlen=212, kl=0.101, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:28,  1.14it/s, pg=-0.0211, ret=1.74e-5, glen=50.4, tlen=211, kl=0.135, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.15it/s, pg=-0.0211, ret=1.74e-5, glen=50.4, tlen=211, kl=0.135, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.15it/s, pg=-0.109, ret=0.00025, glen=48.8, tlen=209, kl=0.0962, act_lr=1e-6, ent=0.863]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.13it/s, pg=-0.109, ret=0.00025, glen=48.8, tlen=209, kl=0.0962, act_lr=1e-6, ent=0.863]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.13it/s, pg=0.0791, ret=-0.000159, glen=51, tlen=211, kl=0.458, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.14it/s, pg=0.0791, ret=-0.000159, glen=51, tlen=211, kl=0.458, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.14it/s, pg=-0.101, ret=0.000184, glen=50.4, tlen=211, kl=0.107, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.15it/s, pg=-0.101, ret=0.000184, glen=50.4, tlen=211, kl=0.107, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.15it/s, pg=0.0923, ret=-0.000156, glen=50.7, tlen=211, kl=0.0825, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.16it/s, pg=0.0923, ret=-0.000156, glen=50.7, tlen=211, kl=0.0825, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=-0.024, ret=4.75e-5, glen=49, tlen=209, kl=0.101, act_lr=1e-6, ent=0.898]     Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.024, ret=4.75e-5, glen=49, tlen=209, kl=0.101, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.0657, ret=0.000115, glen=50.4, tlen=211, kl=0.102, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=-0.0657, ret=0.000115, glen=50.4, tlen=211, kl=0.102, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=-0.0383, ret=5.05e-5, glen=48.8, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0383, ret=5.05e-5, glen=48.8, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0238, ret=4.44e-5, glen=50, tlen=211, kl=0.0974, act_lr=1e-6, ent=0.895]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0238, ret=4.44e-5, glen=50, tlen=211, kl=0.0974, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=-0.107, ret=0.000195, glen=51, tlen=211, kl=0.146, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.18it/s, pg=-0.107, ret=0.000195, glen=51, tlen=211, kl=0.146, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.18it/s, pg=0.0264, ret=-7.25e-5, glen=51.3, tlen=212, kl=0.207, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=0.0264, ret=-7.25e-5, glen=51.3, tlen=212, kl=0.207, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.18it/s, pg=-0.1, ret=0.000189, glen=50.8, tlen=212, kl=0.163, act_lr=1e-6, ent=0.906]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.18it/s, pg=-0.1, ret=0.000189, glen=50.8, tlen=212, kl=0.163, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.18it/s, pg=-0.00653, ret=2.39e-5, glen=48.8, tlen=209, kl=0.0858, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.18it/s, pg=-0.00653, ret=2.39e-5, glen=48.8, tlen=209, kl=0.0858, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.18it/s, pg=0.307, ret=-0.000518, glen=49.6, tlen=211, kl=0.0922, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.18it/s, pg=0.307, ret=-0.000518, glen=49.6, tlen=211, kl=0.0922, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.18it/s, pg=-0.0552, ret=7.15e-5, glen=50.8, tlen=212, kl=0.0938, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=-0.0552, ret=7.15e-5, glen=50.8, tlen=212, kl=0.0938, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.017, ret=1.98e-5, glen=49.2, tlen=209, kl=0.107, act_lr=1e-6, ent=0.943]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.017, ret=1.98e-5, glen=49.2, tlen=209, kl=0.107, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=0.0291, ret=-8.06e-5, glen=49, tlen=209, kl=0.103, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:13,  1.07it/s, pg=0.0291, ret=-8.06e-5, glen=49, tlen=209, kl=0.103, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:13,  1.07it/s, pg=0.0908, ret=-0.000136, glen=50.5, tlen=211, kl=0.177, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.0908, ret=-0.000136, glen=50.5, tlen=211, kl=0.177, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.00964, ret=5.47e-5, glen=49.9, tlen=210, kl=0.137, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.00964, ret=5.47e-5, glen=49.9, tlen=210, kl=0.137, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=0.076, ret=-9.13e-5, glen=49.6, tlen=210, kl=0.0803, act_lr=1e-6, ent=0.908] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.14it/s, pg=0.076, ret=-9.13e-5, glen=49.6, tlen=210, kl=0.0803, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.14it/s, pg=-0.0131, ret=1.81e-5, glen=49.8, tlen=210, kl=0.137, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=-0.0131, ret=1.81e-5, glen=49.8, tlen=210, kl=0.137, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.0819, ret=0.000142, glen=50.4, tlen=211, kl=0.128, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.0819, ret=0.000142, glen=50.4, tlen=211, kl=0.128, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=-0.00464, ret=7.02e-6, glen=50.2, tlen=211, kl=0.0984, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.00464, ret=7.02e-6, glen=50.2, tlen=211, kl=0.0984, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.0653, ret=-0.000126, glen=50.1, tlen=211, kl=0.154, act_lr=1e-6, ent=0.887] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.14it/s, pg=0.0653, ret=-0.000126, glen=50.1, tlen=211, kl=0.154, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.14it/s, pg=-0.0941, ret=0.000162, glen=49, tlen=210, kl=0.107, act_lr=1e-6, ent=0.926]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.15it/s, pg=-0.0941, ret=0.000162, glen=49, tlen=210, kl=0.107, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.15it/s, pg=0.0559, ret=-5.8e-5, glen=49.2, tlen=209, kl=0.098, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.16it/s, pg=0.0559, ret=-5.8e-5, glen=49.2, tlen=209, kl=0.098, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.16it/s, pg=-0.0872, ret=0.000152, glen=49.8, tlen=210, kl=0.123, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.16it/s, pg=-0.0872, ret=0.000152, glen=49.8, tlen=210, kl=0.123, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.16it/s, pg=-0.0133, ret=3.41e-5, glen=50.3, tlen=211, kl=0.131, act_lr=1e-6, ent=0.903] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.0133, ret=3.41e-5, glen=50.3, tlen=211, kl=0.131, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0366, ret=4.1e-5, glen=49.8, tlen=211, kl=0.0775, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0366, ret=4.1e-5, glen=49.8, tlen=211, kl=0.0775, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.108, ret=0.000187, glen=49.9, tlen=210, kl=0.1, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.108, ret=0.000187, glen=49.9, tlen=210, kl=0.1, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.048, ret=8.59e-5, glen=49.8, tlen=211, kl=0.0953, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.048, ret=8.59e-5, glen=49.8, tlen=211, kl=0.0953, act_lr=1e-6, ent=0.914]
2025-07-24 20:24:04.324 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.32s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=0.0489, ret=-0.000105, glen=49.6, tlen=210, kl=0.338, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=0.0489, ret=-0.000105, glen=49.6, tlen=210, kl=0.338, act_lr=1e-6, ent=0.876]
2025-07-24 20:24:04.996 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:24:07.359 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 20:24:07.706 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.82s
2025-07-24 20:24:07.712 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0014707391912286932, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9111322476105257, 'kl': 0.13169722140512682, 'response_length': 49.95024499026212, 'total_length': 210.42024092240766, 'teacher_total_length': 222.5338502363725, 'return': 3.558190531872573e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [15:24<06:49, 102.41s/it][A2025-07-24 20:24:13.784 | INFO     | orz.ppo.trainer:train:183 - Successfully save model weights, training continue.
2025-07-24 20:24:13.785 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:36,  1.75it/s, est. speed input: 315.56 toks/s, output: 35.06 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 163/172 [00:02<00:00, 109.78it/s, est. speed input: 13744.35 toks/s, output: 3396.57 toks/s]
2025-07-24 20:24:17.721 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 209.5342,strategyqa_test/accuracy: 0.5182,eval_accuracy: 0.5182
2025-07-24 20:24:18.027 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:24:48.211 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:24:48.390 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:24:48.391 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.36s
2025-07-24 20:24:49.983 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0009,avg_pass_at_n: 1.0000,avg_num_tokens: 49.4427,std_num_tokens: 12.6920,avg_correct_num_tokens: 49.4363,std_correct_num_tokens: 12.6983,avg_incorrect_num_tokens: 50.1486,std_incorrect_num_tokens: 11.9590
2025-07-24 20:24:50.401 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.01s
2025-07-24 20:24:52.994 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.59s
2025-07-24 20:25:15.697 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 173
2025-07-24 20:25:15.698 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.70s
2025-07-24 20:25:16.826 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.72s
2025-07-24 20:25:16.826 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.8126078251469343e-05, avg_kl: 0.18256356123554912, avg_response_length: 49.46999603888892, avg_orm_score: 0.0, avg_custom_rewards: 1.8126078251469343e-05
2025-07-24 20:25:16.855 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter100_replay_buffer.jsonl
2025-07-24 20:25:17.978 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 148/172 [00:01<00:00, 128.24it/s, est. speed input: 13667.21 toks/s, output: 3302.14 toks/s][32m [repeated 48x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 55.13it/s, est. speed input: 9988.33 toks/s, output: 2479.36 toks/s]  [32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0162, ret=-2.88e-5, glen=48.9, tlen=210, kl=0.648, act_lr=1e-6, ent=0.873]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=-0.0162, ret=-2.88e-5, glen=48.9, tlen=210, kl=0.648, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=-0.0453, ret=0.000103, glen=49.4, tlen=210, kl=0.155, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.08it/s, pg=-0.0453, ret=0.000103, glen=49.4, tlen=210, kl=0.155, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.08it/s, pg=0.0137, ret=-3.89e-5, glen=50.4, tlen=211, kl=0.156, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=0.0137, ret=-3.89e-5, glen=50.4, tlen=211, kl=0.156, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=0.00232, ret=-4.07e-5, glen=50.2, tlen=211, kl=0.149, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.12it/s, pg=0.00232, ret=-4.07e-5, glen=50.2, tlen=211, kl=0.149, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.12it/s, pg=-0.0195, ret=-1.36e-5, glen=48.2, tlen=208, kl=0.158, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:35,  1.11it/s, pg=-0.0195, ret=-1.36e-5, glen=48.2, tlen=208, kl=0.158, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:35,  1.11it/s, pg=-0.0486, ret=7.58e-5, glen=51.4, tlen=212, kl=0.207, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:34,  1.10it/s, pg=-0.0486, ret=7.58e-5, glen=51.4, tlen=212, kl=0.207, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:34,  1.10it/s, pg=0.126, ret=-0.000139, glen=48.2, tlen=208, kl=0.125, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.12it/s, pg=0.126, ret=-0.000139, glen=48.2, tlen=208, kl=0.125, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.12it/s, pg=-0.0387, ret=6.35e-5, glen=49.9, tlen=211, kl=0.152, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=-0.0387, ret=6.35e-5, glen=49.9, tlen=211, kl=0.152, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:31,  1.14it/s, pg=0.0204, ret=-5.1e-5, glen=48.7, tlen=209, kl=0.133, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.0204, ret=-5.1e-5, glen=48.7, tlen=209, kl=0.133, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.15it/s, pg=0.0145, ret=-2.19e-5, glen=49.5, tlen=210, kl=0.137, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:30,  1.13it/s, pg=0.0145, ret=-2.19e-5, glen=49.5, tlen=210, kl=0.137, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:30,  1.13it/s, pg=-0.0465, ret=7.73e-5, glen=50.3, tlen=211, kl=0.305, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.14it/s, pg=-0.0465, ret=7.73e-5, glen=50.3, tlen=211, kl=0.305, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.14it/s, pg=-0.0948, ret=0.000187, glen=49, tlen=209, kl=0.132, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:28,  1.13it/s, pg=-0.0948, ret=0.000187, glen=49, tlen=209, kl=0.132, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:28,  1.13it/s, pg=-0.0247, ret=7.7e-5, glen=49.5, tlen=209, kl=0.114, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:27,  1.14it/s, pg=-0.0247, ret=7.7e-5, glen=49.5, tlen=209, kl=0.114, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:27,  1.14it/s, pg=0.0845, ret=-0.000176, glen=50.1, tlen=210, kl=0.196, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=0.0845, ret=-0.000176, glen=50.1, tlen=210, kl=0.196, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=-0.045, ret=5.66e-5, glen=48.4, tlen=209, kl=0.0966, act_lr=1e-6, ent=0.896] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.16it/s, pg=-0.045, ret=5.66e-5, glen=48.4, tlen=209, kl=0.0966, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.16it/s, pg=-0.082, ret=0.000118, glen=49.6, tlen=210, kl=0.121, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.082, ret=0.000118, glen=49.6, tlen=210, kl=0.121, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.0332, ret=-6.42e-5, glen=47.9, tlen=208, kl=0.169, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.16it/s, pg=0.0332, ret=-6.42e-5, glen=47.9, tlen=208, kl=0.169, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=-0.0826, ret=0.000168, glen=49.1, tlen=209, kl=0.266, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=-0.0826, ret=0.000168, glen=49.1, tlen=209, kl=0.266, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=-0.0297, ret=7.2e-5, glen=49, tlen=209, kl=0.472, act_lr=1e-6, ent=0.869]    Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:22,  1.14it/s, pg=-0.0297, ret=7.2e-5, glen=49, tlen=209, kl=0.472, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:22,  1.14it/s, pg=-0.0377, ret=9.02e-5, glen=51.2, tlen=212, kl=0.179, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.15it/s, pg=-0.0377, ret=9.02e-5, glen=51.2, tlen=212, kl=0.179, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.15it/s, pg=0.0626, ret=-5.5e-5, glen=50, tlen=210, kl=0.2, act_lr=1e-6, ent=0.877]     Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.15it/s, pg=0.0626, ret=-5.5e-5, glen=50, tlen=210, kl=0.2, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.15it/s, pg=0.0113, ret=-3.72e-5, glen=50.3, tlen=211, kl=0.102, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.16it/s, pg=0.0113, ret=-3.72e-5, glen=50.3, tlen=211, kl=0.102, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.16it/s, pg=-0.106, ret=0.000198, glen=49.1, tlen=209, kl=0.134, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.106, ret=0.000198, glen=49.1, tlen=209, kl=0.134, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:21<00:17,  1.17it/s, pg=0.00208, ret=-6.23e-6, glen=50.2, tlen=210, kl=0.324, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=0.00208, ret=-6.23e-6, glen=50.2, tlen=210, kl=0.324, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.0209, ret=5.82e-5, glen=49.1, tlen=209, kl=0.296, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.0209, ret=5.82e-5, glen=49.1, tlen=209, kl=0.296, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.0529, ret=-6.55e-5, glen=49.7, tlen=210, kl=0.168, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.0529, ret=-6.55e-5, glen=49.7, tlen=210, kl=0.168, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.0282, ret=-7.97e-6, glen=49.8, tlen=210, kl=0.151, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=0.0282, ret=-7.97e-6, glen=49.8, tlen=210, kl=0.151, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=0.113, ret=-0.000263, glen=50.1, tlen=211, kl=0.122, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.113, ret=-0.000263, glen=50.1, tlen=211, kl=0.122, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=0.0366, ret=-0.000134, glen=49.8, tlen=211, kl=0.177, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=0.0366, ret=-0.000134, glen=49.8, tlen=211, kl=0.177, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.0555, ret=-0.000124, glen=47.3, tlen=207, kl=0.0984, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.0555, ret=-0.000124, glen=47.3, tlen=207, kl=0.0984, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.0207, ret=-4.7e-6, glen=49.1, tlen=209, kl=0.15, act_lr=1e-6, ent=0.892]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0207, ret=-4.7e-6, glen=49.1, tlen=209, kl=0.15, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.12it/s, pg=0.0719, ret=-9.63e-5, glen=49, tlen=209, kl=0.151, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.0719, ret=-9.63e-5, glen=49, tlen=209, kl=0.151, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.06, ret=-0.000148, glen=47.3, tlen=207, kl=0.143, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.06, ret=-0.000148, glen=47.3, tlen=207, kl=0.143, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=0.0551, ret=-0.000104, glen=50.7, tlen=211, kl=0.192, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=0.0551, ret=-0.000104, glen=50.7, tlen=211, kl=0.192, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=-0.0336, ret=7.07e-5, glen=49.9, tlen=210, kl=0.273, act_lr=1e-6, ent=0.929] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=-0.0336, ret=7.07e-5, glen=49.9, tlen=210, kl=0.273, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.0321, ret=6.94e-5, glen=47.7, tlen=208, kl=0.102, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.0321, ret=6.94e-5, glen=47.7, tlen=208, kl=0.102, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.0474, ret=7.96e-5, glen=50.5, tlen=211, kl=0.125, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0474, ret=7.96e-5, glen=50.5, tlen=211, kl=0.125, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=0.0227, ret=-3.92e-5, glen=48.8, tlen=209, kl=0.115, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=0.0227, ret=-3.92e-5, glen=48.8, tlen=209, kl=0.115, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=0.0267, ret=-2.97e-5, glen=48.3, tlen=208, kl=0.0958, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0267, ret=-2.97e-5, glen=48.3, tlen=208, kl=0.0958, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0271, ret=6.92e-5, glen=49.7, tlen=210, kl=0.115, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.0271, ret=6.92e-5, glen=49.7, tlen=210, kl=0.115, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.0392, ret=-2.24e-5, glen=49.3, tlen=210, kl=0.188, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=0.0392, ret=-2.24e-5, glen=49.3, tlen=210, kl=0.188, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.00452, ret=6.82e-6, glen=51, tlen=212, kl=0.13, act_lr=1e-6, ent=0.88]    Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.00452, ret=6.82e-6, glen=51, tlen=212, kl=0.13, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.0146, ret=5.87e-5, glen=49.9, tlen=210, kl=0.252, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.0146, ret=5.87e-5, glen=49.9, tlen=210, kl=0.252, act_lr=1e-6, ent=0.901]
2025-07-24 20:25:56.575 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.42s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0961, ret=0.000197, glen=50.8, tlen=211, kl=0.163, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0961, ret=0.000197, glen=50.8, tlen=211, kl=0.163, act_lr=1e-6, ent=0.94]
2025-07-24 20:25:57.400 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:25:59.985 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 20:26:00.318 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.29s
2025-07-24 20:26:00.324 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.001669797030362216, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9006215889345516, 'kl': 0.1826144138520414, 'response_length': 49.46141797846014, 'total_length': 209.7301992936568, 'teacher_total_length': 221.7012675892223, 'return': 4.21897328289395e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [17:16<05:16, 105.56s/it][A2025-07-24 20:26:00.366 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:26:30.368 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:26:30.547 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:26:30.547 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.18s
2025-07-24 20:26:32.376 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0009,avg_pass_at_n: 1.0000,avg_num_tokens: 49.1962,std_num_tokens: 12.8042,avg_correct_num_tokens: 49.1894,std_correct_num_tokens: 12.7998,avg_incorrect_num_tokens: 49.7629,std_incorrect_num_tokens: 13.1532
2025-07-24 20:26:32.804 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.26s
2025-07-24 20:26:35.187 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.38s
2025-07-24 20:26:57.737 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 173
2025-07-24 20:26:57.738 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.55s
2025-07-24 20:26:59.022 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 20:26:59.023 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -5.909248095038051e-05, avg_kl: 0.17785432848627167, avg_response_length: 49.21118856441079, avg_orm_score: 0.0, avg_custom_rewards: -5.909248095038051e-05
2025-07-24 20:26:59.049 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter101_replay_buffer.jsonl
2025-07-24 20:27:00.165 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.12s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.127, ret=-0.000175, glen=47.8, tlen=208, kl=0.0837, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:46,  1.08s/it, pg=0.127, ret=-0.000175, glen=47.8, tlen=208, kl=0.0837, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:46,  1.08s/it, pg=-0.15, ret=0.000321, glen=50.1, tlen=211, kl=0.151, act_lr=1e-6, ent=0.954]  Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:39,  1.05it/s, pg=-0.15, ret=0.000321, glen=50.1, tlen=211, kl=0.151, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:39,  1.05it/s, pg=-0.0437, ret=0.000117, glen=50.9, tlen=211, kl=2.47, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:38,  1.06it/s, pg=-0.0437, ret=0.000117, glen=50.9, tlen=211, kl=2.47, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:38,  1.06it/s, pg=-0.00989, ret=8.93e-5, glen=49, tlen=209, kl=0.0939, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:37,  1.07it/s, pg=-0.00989, ret=8.93e-5, glen=49, tlen=209, kl=0.0939, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:37,  1.07it/s, pg=-0.102, ret=0.000179, glen=48.9, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:35,  1.10it/s, pg=-0.102, ret=0.000179, glen=48.9, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:35,  1.10it/s, pg=-0.0502, ret=7.35e-5, glen=48.4, tlen=209, kl=0.107, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.12it/s, pg=-0.0502, ret=7.35e-5, glen=48.4, tlen=209, kl=0.107, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.12it/s, pg=-0.102, ret=0.000226, glen=50.1, tlen=210, kl=0.113, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.14it/s, pg=-0.102, ret=0.000226, glen=50.1, tlen=210, kl=0.113, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.14it/s, pg=0.0414, ret=-0.000122, glen=49.9, tlen=211, kl=0.117, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.15it/s, pg=0.0414, ret=-0.000122, glen=49.9, tlen=211, kl=0.117, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:31,  1.15it/s, pg=0.0459, ret=-8.56e-5, glen=49, tlen=209, kl=0.0956, act_lr=1e-6, ent=0.952]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.16it/s, pg=0.0459, ret=-8.56e-5, glen=49, tlen=209, kl=0.0956, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:09<00:30,  1.16it/s, pg=0.0178, ret=-7.15e-5, glen=48.6, tlen=209, kl=0.108, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:30,  1.13it/s, pg=0.0178, ret=-7.15e-5, glen=48.6, tlen=209, kl=0.108, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:30,  1.13it/s, pg=-0.0122, ret=3.03e-5, glen=48.9, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.14it/s, pg=-0.0122, ret=3.03e-5, glen=48.9, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.14it/s, pg=-0.094, ret=0.00021, glen=48.2, tlen=209, kl=0.148, act_lr=1e-6, ent=0.973]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.15it/s, pg=-0.094, ret=0.00021, glen=48.2, tlen=209, kl=0.148, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.15it/s, pg=-0.00989, ret=-2.21e-5, glen=49.3, tlen=209, kl=0.15, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.16it/s, pg=-0.00989, ret=-2.21e-5, glen=49.3, tlen=209, kl=0.15, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.16it/s, pg=0.132, ret=-0.00034, glen=49, tlen=209, kl=0.132, act_lr=1e-6, ent=0.964]    Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.16it/s, pg=0.132, ret=-0.00034, glen=49, tlen=209, kl=0.132, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.16it/s, pg=-0.0504, ret=0.000117, glen=48, tlen=208, kl=0.0817, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.15it/s, pg=-0.0504, ret=0.000117, glen=48, tlen=208, kl=0.0817, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.15it/s, pg=-0.0383, ret=3.18e-5, glen=51.1, tlen=211, kl=0.108, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.0383, ret=3.18e-5, glen=51.1, tlen=211, kl=0.108, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:15<00:24,  1.16it/s, pg=-0.0416, ret=1.36e-5, glen=48.4, tlen=209, kl=0.0901, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=-0.0416, ret=1.36e-5, glen=48.4, tlen=209, kl=0.0901, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=0.156, ret=-0.000356, glen=49.5, tlen=210, kl=0.0873, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.17it/s, pg=0.156, ret=-0.000356, glen=49.5, tlen=210, kl=0.0873, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.17it/s, pg=0.0698, ret=-0.000189, glen=49.9, tlen=210, kl=0.136, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=0.0698, ret=-0.000189, glen=49.9, tlen=210, kl=0.136, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=0.041, ret=-3.23e-5, glen=48.4, tlen=208, kl=0.145, act_lr=1e-6, ent=0.974]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.041, ret=-3.23e-5, glen=48.4, tlen=208, kl=0.145, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=0.102, ret=-0.000178, glen=48.2, tlen=209, kl=0.116, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=0.102, ret=-0.000178, glen=48.2, tlen=209, kl=0.116, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.0381, ret=-0.000111, glen=49.9, tlen=210, kl=0.141, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0381, ret=-0.000111, glen=49.9, tlen=210, kl=0.141, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.17it/s, pg=-0.0643, ret=0.000171, glen=49.9, tlen=210, kl=0.163, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.0643, ret=0.000171, glen=49.9, tlen=210, kl=0.163, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.0995, ret=0.000213, glen=49.4, tlen=210, kl=0.128, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=-0.0995, ret=0.000213, glen=49.4, tlen=210, kl=0.128, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.000122, ret=1.77e-5, glen=50.5, tlen=211, kl=0.111, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.000122, ret=1.77e-5, glen=50.5, tlen=211, kl=0.111, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.16, ret=-0.000355, glen=48.9, tlen=209, kl=0.097, act_lr=1e-6, ent=0.932]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.16, ret=-0.000355, glen=48.9, tlen=209, kl=0.097, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=0.0036, ret=-0.000103, glen=49.3, tlen=209, kl=0.093, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.18it/s, pg=0.0036, ret=-0.000103, glen=49.3, tlen=209, kl=0.093, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.18it/s, pg=0.0917, ret=-0.00018, glen=48.6, tlen=209, kl=0.107, act_lr=1e-6, ent=0.943] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=0.0917, ret=-0.00018, glen=48.6, tlen=209, kl=0.107, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.0389, ret=0.000124, glen=48.6, tlen=209, kl=0.104, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.00it/s, pg=-0.0389, ret=0.000124, glen=48.6, tlen=209, kl=0.104, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.00it/s, pg=0.00879, ret=-4.97e-5, glen=49.7, tlen=210, kl=0.115, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:13,  1.05it/s, pg=0.00879, ret=-4.97e-5, glen=49.7, tlen=210, kl=0.115, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:13,  1.05it/s, pg=-0.0578, ret=0.000133, glen=48.9, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:12,  1.08it/s, pg=-0.0578, ret=0.000133, glen=48.9, tlen=209, kl=0.0992, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:12,  1.08it/s, pg=0.013, ret=-0.00011, glen=49.1, tlen=209, kl=0.14, act_lr=1e-6, ent=0.966]    Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.11it/s, pg=0.013, ret=-0.00011, glen=49.1, tlen=209, kl=0.14, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:29<00:10,  1.11it/s, pg=0.0175, ret=4.16e-5, glen=48.6, tlen=209, kl=0.0833, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.12it/s, pg=0.0175, ret=4.16e-5, glen=48.6, tlen=209, kl=0.0833, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.12it/s, pg=-0.124, ret=0.00028, glen=49.1, tlen=210, kl=0.1, act_lr=1e-6, ent=0.962]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.14it/s, pg=-0.124, ret=0.00028, glen=49.1, tlen=210, kl=0.1, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.14it/s, pg=0.0563, ret=-0.000112, glen=48.5, tlen=208, kl=0.0975, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.15it/s, pg=0.0563, ret=-0.000112, glen=48.5, tlen=208, kl=0.0975, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.15it/s, pg=0.00464, ret=-6.3e-5, glen=48.7, tlen=209, kl=0.492, act_lr=1e-6, ent=0.968]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=0.00464, ret=-6.3e-5, glen=48.7, tlen=209, kl=0.492, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.0703, ret=0.000147, glen=49.4, tlen=210, kl=0.102, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.16it/s, pg=-0.0703, ret=0.000147, glen=49.4, tlen=210, kl=0.102, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.16it/s, pg=-0.026, ret=0.000105, glen=48.9, tlen=210, kl=0.118, act_lr=1e-6, ent=0.944] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.026, ret=0.000105, glen=48.9, tlen=210, kl=0.118, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=0.0326, ret=-4.62e-5, glen=49, tlen=210, kl=0.106, act_lr=1e-6, ent=0.937]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0326, ret=-4.62e-5, glen=49, tlen=210, kl=0.106, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:35<00:04,  1.17it/s, pg=0.00391, ret=-2.51e-6, glen=52.3, tlen=213, kl=0.0935, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.00391, ret=-2.51e-6, glen=52.3, tlen=213, kl=0.0935, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.0992, ret=-0.000229, glen=48.7, tlen=209, kl=0.124, act_lr=1e-6, ent=0.954] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=0.0992, ret=-0.000229, glen=48.7, tlen=209, kl=0.124, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.0409, ret=0.000106, glen=51.1, tlen=212, kl=0.193, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.0409, ret=0.000106, glen=51.1, tlen=212, kl=0.193, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.0107, ret=-1.97e-5, glen=48.2, tlen=208, kl=0.133, act_lr=1e-6, ent=0.968] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=0.0107, ret=-1.97e-5, glen=48.2, tlen=208, kl=0.133, act_lr=1e-6, ent=0.968]
2025-07-24 20:27:38.930 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.59s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.00916, ret=5.08e-5, glen=48.5, tlen=209, kl=0.0968, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.00916, ret=5.08e-5, glen=48.5, tlen=209, kl=0.0968, act_lr=1e-6, ent=0.957]
2025-07-24 20:27:39.786 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 20:27:42.340 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.55s
2025-07-24 20:27:42.671 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.45s
2025-07-24 20:27:42.689 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0008479031649502841, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9592708579518578, 'kl': 0.17661216042258523, 'response_length': 49.2140007019043, 'total_length': 209.54466178200462, 'teacher_total_length': 221.47298951582476, 'return': -3.5385287860249677e-06, 'policy_update_steps': 1.0}

Episode [8/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [18:59<03:29, 104.58s/it][A2025-07-24 20:27:42.737 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:28:12.666 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:28:12.843 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:28:12.844 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 30.11s
2025-07-24 20:28:14.603 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 48.8822,std_num_tokens: 12.0806,avg_correct_num_tokens: 48.8946,std_correct_num_tokens: 12.0948,avg_incorrect_num_tokens: 47.7528,std_incorrect_num_tokens: 10.6500
2025-07-24 20:28:15.056 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.21s
2025-07-24 20:28:17.763 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.71s
2025-07-24 20:28:39.968 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 172
2025-07-24 20:28:39.969 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.20s
2025-07-24 20:28:41.183 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 20:28:41.184 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.2336486983004698e-05, avg_kl: 0.36321169831031974, avg_response_length: 48.89718071250029, avg_orm_score: 0.0, avg_custom_rewards: -1.2336486983004698e-05
2025-07-24 20:28:41.213 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter102_replay_buffer.jsonl
2025-07-24 20:28:42.310 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.10s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/43 [00:00<?, ?it/s, pg=-0.0482, ret=7.14e-5, glen=49.4, tlen=210, kl=0.105, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:00<00:41,  1.02it/s, pg=-0.0482, ret=7.14e-5, glen=49.4, tlen=210, kl=0.105, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/43 [00:01<00:41,  1.02it/s, pg=-0.0601, ret=0.000118, glen=49.9, tlen=210, kl=0.09, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:01<00:37,  1.10it/s, pg=-0.0601, ret=0.000118, glen=49.9, tlen=210, kl=0.09, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/43 [00:02<00:37,  1.10it/s, pg=0.0592, ret=-0.0001, glen=49.6, tlen=210, kl=0.102, act_lr=1e-6, ent=0.9]   Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:02<00:35,  1.13it/s, pg=0.0592, ret=-0.0001, glen=49.6, tlen=210, kl=0.102, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/43 [00:03<00:35,  1.13it/s, pg=-0.0163, ret=2.2e-5, glen=48.2, tlen=208, kl=0.114, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:03<00:34,  1.12it/s, pg=-0.0163, ret=2.2e-5, glen=48.2, tlen=208, kl=0.114, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/43 [00:04<00:34,  1.12it/s, pg=0.085, ret=-0.000242, glen=49.7, tlen=209, kl=0.112, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:04<00:33,  1.14it/s, pg=0.085, ret=-0.000242, glen=49.7, tlen=209, kl=0.112, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 5/43 [00:05<00:33,  1.14it/s, pg=0.0466, ret=-4.45e-5, glen=48.7, tlen=208, kl=0.119, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:05<00:32,  1.15it/s, pg=0.0466, ret=-4.45e-5, glen=48.7, tlen=208, kl=0.119, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 6/43 [00:06<00:32,  1.15it/s, pg=-0.0664, ret=0.000104, glen=47.6, tlen=207, kl=8.63, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:06<00:31,  1.15it/s, pg=-0.0664, ret=0.000104, glen=47.6, tlen=207, kl=8.63, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 7/43 [00:07<00:31,  1.15it/s, pg=-0.0499, ret=0.000127, glen=49.1, tlen=209, kl=2.11, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.0499, ret=0.000127, glen=49.1, tlen=209, kl=2.11, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 8/43 [00:07<00:30,  1.16it/s, pg=-0.0211, ret=4.16e-5, glen=50.5, tlen=210, kl=0.131, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:07<00:29,  1.16it/s, pg=-0.0211, ret=4.16e-5, glen=50.5, tlen=210, kl=0.131, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 9/43 [00:08<00:29,  1.16it/s, pg=-0.0848, ret=0.000163, glen=48.2, tlen=208, kl=0.113, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:08<00:28,  1.17it/s, pg=-0.0848, ret=0.000163, glen=48.2, tlen=208, kl=0.113, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/43 [00:09<00:28,  1.17it/s, pg=-0.0657, ret=0.000119, glen=49.5, tlen=209, kl=0.126, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:09<00:27,  1.17it/s, pg=-0.0657, ret=0.000119, glen=49.5, tlen=209, kl=0.126, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 11/43 [00:10<00:27,  1.17it/s, pg=-0.0985, ret=0.000218, glen=49, tlen=209, kl=0.099, act_lr=1e-6, ent=0.935]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:10<00:26,  1.17it/s, pg=-0.0985, ret=0.000218, glen=49, tlen=209, kl=0.099, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 12/43 [00:11<00:26,  1.17it/s, pg=-0.094, ret=0.000211, glen=49, tlen=209, kl=0.107, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:11<00:25,  1.17it/s, pg=-0.094, ret=0.000211, glen=49, tlen=209, kl=0.107, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 13/43 [00:12<00:25,  1.17it/s, pg=-0.00708, ret=1.9e-6, glen=49, tlen=209, kl=0.133, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=-0.00708, ret=1.9e-6, glen=49, tlen=209, kl=0.133, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 14/43 [00:12<00:24,  1.17it/s, pg=-0.085, ret=0.000124, glen=47.8, tlen=208, kl=0.134, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:12<00:23,  1.17it/s, pg=-0.085, ret=0.000124, glen=47.8, tlen=208, kl=0.134, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 15/43 [00:13<00:23,  1.17it/s, pg=0.0209, ret=5.65e-7, glen=48.9, tlen=209, kl=0.142, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:13<00:23,  1.17it/s, pg=0.0209, ret=5.65e-7, glen=48.9, tlen=209, kl=0.142, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 16/43 [00:14<00:23,  1.17it/s, pg=-0.00958, ret=-2.1e-5, glen=48.5, tlen=209, kl=0.155, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:14<00:22,  1.17it/s, pg=-0.00958, ret=-2.1e-5, glen=48.5, tlen=209, kl=0.155, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 17/43 [00:15<00:22,  1.17it/s, pg=0.121, ret=-0.000247, glen=50.3, tlen=210, kl=0.111, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:15<00:21,  1.17it/s, pg=0.121, ret=-0.000247, glen=50.3, tlen=210, kl=0.111, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 18/43 [00:16<00:21,  1.17it/s, pg=-0.0494, ret=0.000103, glen=47.1, tlen=207, kl=0.104, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:16<00:20,  1.17it/s, pg=-0.0494, ret=0.000103, glen=47.1, tlen=207, kl=0.104, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 19/43 [00:17<00:20,  1.17it/s, pg=-0.0284, ret=9.47e-5, glen=49.7, tlen=209, kl=0.0946, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:17<00:19,  1.17it/s, pg=-0.0284, ret=9.47e-5, glen=49.7, tlen=209, kl=0.0946, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 20/43 [00:18<00:19,  1.17it/s, pg=0.0616, ret=-0.000195, glen=49.6, tlen=210, kl=0.105, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:18<00:19,  1.13it/s, pg=0.0616, ret=-0.000195, glen=49.6, tlen=210, kl=0.105, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21/43 [00:19<00:19,  1.13it/s, pg=0, ret=3.39e-6, glen=46.9, tlen=207, kl=0.0837, act_lr=1e-6, ent=0.916]      Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:18,  1.13it/s, pg=0, ret=3.39e-6, glen=46.9, tlen=207, kl=0.0837, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/43 [00:19<00:18,  1.13it/s, pg=-0.0397, ret=7.12e-5, glen=49, tlen=209, kl=0.13, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:19<00:17,  1.14it/s, pg=-0.0397, ret=7.12e-5, glen=49, tlen=209, kl=0.13, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 23/43 [00:20<00:17,  1.14it/s, pg=0.144, ret=-0.000274, glen=48.7, tlen=209, kl=0.104, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:20<00:16,  1.15it/s, pg=0.144, ret=-0.000274, glen=48.7, tlen=209, kl=0.104, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 24/43 [00:21<00:16,  1.15it/s, pg=-0.0498, ret=9.24e-5, glen=50.3, tlen=210, kl=0.0852, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:21<00:15,  1.16it/s, pg=-0.0498, ret=9.24e-5, glen=50.3, tlen=210, kl=0.0852, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 25/43 [00:22<00:15,  1.16it/s, pg=0.00659, ret=2.1e-5, glen=49.1, tlen=209, kl=0.0964, act_lr=1e-6, ent=0.907] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:22<00:14,  1.16it/s, pg=0.00659, ret=2.1e-5, glen=49.1, tlen=209, kl=0.0964, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 26/43 [00:23<00:14,  1.16it/s, pg=0.0923, ret=-0.000156, glen=49.2, tlen=209, kl=0.102, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:23<00:13,  1.17it/s, pg=0.0923, ret=-0.000156, glen=49.2, tlen=209, kl=0.102, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 27/43 [00:24<00:13,  1.17it/s, pg=0.066, ret=-0.000142, glen=50, tlen=210, kl=0.108, act_lr=1e-6, ent=0.931]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:24<00:12,  1.17it/s, pg=0.066, ret=-0.000142, glen=50, tlen=210, kl=0.108, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 28/43 [00:25<00:12,  1.17it/s, pg=0.000122, ret=-3.56e-5, glen=48, tlen=208, kl=0.111, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:25<00:13,  1.06it/s, pg=0.000122, ret=-3.56e-5, glen=48, tlen=208, kl=0.111, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 29/43 [00:26<00:13,  1.06it/s, pg=0.0247, ret=-3.81e-5, glen=48.6, tlen=209, kl=0.0918, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:26<00:11,  1.09it/s, pg=0.0247, ret=-3.81e-5, glen=48.6, tlen=209, kl=0.0918, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 30/43 [00:27<00:11,  1.09it/s, pg=0.0127, ret=6.14e-6, glen=48, tlen=208, kl=0.131, act_lr=1e-6, ent=0.909]    Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=0.0127, ret=6.14e-6, glen=48, tlen=208, kl=0.131, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 31/43 [00:27<00:10,  1.12it/s, pg=-0.0443, ret=0.000132, glen=49.1, tlen=209, kl=0.165, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:27<00:09,  1.13it/s, pg=-0.0443, ret=0.000132, glen=49.1, tlen=209, kl=0.165, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 32/43 [00:28<00:09,  1.13it/s, pg=0.211, ret=-0.000335, glen=49.1, tlen=209, kl=0.121, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:28<00:08,  1.14it/s, pg=0.211, ret=-0.000335, glen=49.1, tlen=209, kl=0.121, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 33/43 [00:29<00:08,  1.14it/s, pg=-0.0116, ret=2.43e-5, glen=49.2, tlen=209, kl=0.353, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:29<00:08,  1.12it/s, pg=-0.0116, ret=2.43e-5, glen=49.2, tlen=209, kl=0.353, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 34/43 [00:30<00:08,  1.12it/s, pg=0.0919, ret=-0.000211, glen=48.9, tlen=209, kl=0.0957, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:30<00:07,  1.14it/s, pg=0.0919, ret=-0.000211, glen=48.9, tlen=209, kl=0.0957, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 35/43 [00:31<00:07,  1.14it/s, pg=0.0536, ret=-0.000139, glen=48.2, tlen=208, kl=0.152, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:31<00:06,  1.15it/s, pg=0.0536, ret=-0.000139, glen=48.2, tlen=208, kl=0.152, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 36/43 [00:32<00:06,  1.15it/s, pg=0.15, ret=-0.000329, glen=49.2, tlen=209, kl=0.105, act_lr=1e-6, ent=0.934]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:32<00:05,  1.16it/s, pg=0.15, ret=-0.000329, glen=49.2, tlen=209, kl=0.105, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 37/43 [00:33<00:05,  1.16it/s, pg=0.0363, ret=-0.000166, glen=48.4, tlen=208, kl=0.101, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.16it/s, pg=0.0363, ret=-0.000166, glen=48.4, tlen=208, kl=0.101, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 38/43 [00:33<00:04,  1.16it/s, pg=-0.0518, ret=8.31e-5, glen=50, tlen=211, kl=0.141, act_lr=1e-6, ent=0.939]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:33<00:03,  1.17it/s, pg=-0.0518, ret=8.31e-5, glen=50, tlen=211, kl=0.141, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 39/43 [00:34<00:03,  1.17it/s, pg=-0.109, ret=0.000234, glen=47.4, tlen=208, kl=0.1, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:34<00:02,  1.17it/s, pg=-0.109, ret=0.000234, glen=47.4, tlen=208, kl=0.1, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 40/43 [00:35<00:02,  1.17it/s, pg=-0.0853, ret=0.00018, glen=48.6, tlen=209, kl=0.126, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:35<00:01,  1.17it/s, pg=-0.0853, ret=0.00018, glen=48.6, tlen=209, kl=0.126, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 41/43 [00:36<00:01,  1.17it/s, pg=-0.0645, ret=0.000141, glen=48.7, tlen=208, kl=0.0834, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:36<00:00,  1.17it/s, pg=-0.0645, ret=0.000141, glen=48.7, tlen=208, kl=0.0834, act_lr=1e-6, ent=0.907]
2025-07-24 20:29:19.929 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 37.44s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.17it/s, pg=-0.0526, ret=0.000146, glen=48.7, tlen=209, kl=0.1, act_lr=1e-6, ent=0.934]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [00:37<00:00,  1.12it/s, pg=-0.0526, ret=0.000146, glen=48.7, tlen=209, kl=0.1, act_lr=1e-6, ent=0.934]
2025-07-24 20:29:20.755 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:29:23.312 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 20:29:23.644 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.28s
2025-07-24 20:29:23.650 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0002318981081940407, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9242571938869565, 'kl': 0.36321169831031974, 'response_length': 48.89718060160792, 'total_length': 208.78898549634357, 'teacher_total_length': 220.77956904921422, 'return': -5.171941031517764e-07, 'policy_update_steps': 1.0}

Episode [8/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [20:40<01:43, 103.48s/it][A2025-07-24 20:29:23.655 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:33,  1.81it/s, est. speed input: 327.48 toks/s, output: 34.38 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:01<00:02, 60.83it/s, est. speed input: 6391.47 toks/s, output: 1138.00 toks/s]Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:01<00:01, 75.05it/s, est. speed input: 7549.99 toks/s, output: 1421.55 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/172 [00:02<00:00, 119.60it/s, est. speed input: 14096.56 toks/s, output: 3521.72 toks/s]
2025-07-24 20:29:27.274 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 215.4745,strategyqa_test/accuracy: 0.5284,eval_accuracy: 0.5284
2025-07-24 20:29:27.519 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:29:43.458 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:29:43.630 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:29:43.631 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 16.11s
2025-07-24 20:29:44.693 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 49.3396,std_num_tokens: 12.1243,avg_correct_num_tokens: 49.3586,std_correct_num_tokens: 12.1415,avg_incorrect_num_tokens: 48.1739,std_incorrect_num_tokens: 10.9584
2025-07-24 20:29:45.005 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.37s
2025-07-24 20:29:46.405 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.40s
2025-07-24 20:29:58.537 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 91
2025-07-24 20:29:58.537 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.13s
2025-07-24 20:29:59.504 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.54s
2025-07-24 20:29:59.505 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0001460695021995599, avg_kl: 0.17251620449862637, avg_response_length: 49.33677807483044, avg_orm_score: 0.0, avg_custom_rewards: 0.0001460695021995599
2025-07-24 20:29:59.559 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter103_replay_buffer.jsonl
2025-07-24 20:30:00.126 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.57s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 154/172 [00:02<00:00, 111.79it/s, est. speed input: 12534.23 toks/s, output: 3137.92 toks/s][32m [repeated 45x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:02<00:00, 61.39it/s, est. speed input: 11122.95 toks/s, output: 2911.64 toks/s] [32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/23 [00:01<?, ?it/s, pg=-0.113, ret=0.000277, glen=48.7, tlen=209, kl=0.231, act_lr=1e-6, ent=0.875]Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:01<00:23,  1.06s/it, pg=-0.113, ret=0.000277, glen=48.7, tlen=209, kl=0.231, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/23 [00:01<00:23,  1.06s/it, pg=0.0876, ret=-0.000279, glen=49, tlen=209, kl=0.129, act_lr=1e-6, ent=0.895] Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:01<00:20,  1.04it/s, pg=0.0876, ret=-0.000279, glen=49, tlen=209, kl=0.129, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 2/23 [00:02<00:20,  1.04it/s, pg=0.107, ret=-0.000297, glen=50.3, tlen=211, kl=0.106, act_lr=1e-6, ent=0.872]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:02<00:18,  1.06it/s, pg=0.107, ret=-0.000297, glen=50.3, tlen=211, kl=0.106, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 3/23 [00:03<00:18,  1.06it/s, pg=-0.0903, ret=0.000235, glen=49.1, tlen=209, kl=0.0901, act_lr=1e-6, ent=0.872]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:03<00:17,  1.10it/s, pg=-0.0903, ret=0.000235, glen=49.1, tlen=209, kl=0.0901, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/23 [00:04<00:17,  1.10it/s, pg=-0.0571, ret=8.74e-5, glen=49.6, tlen=210, kl=0.115, act_lr=1e-6, ent=0.892]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:04<00:16,  1.12it/s, pg=-0.0571, ret=8.74e-5, glen=49.6, tlen=210, kl=0.115, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:05<00:16,  1.12it/s, pg=-0.117, ret=0.000326, glen=49.7, tlen=210, kl=0.105, act_lr=1e-6, ent=0.856]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:05<00:14,  1.14it/s, pg=-0.117, ret=0.000326, glen=49.7, tlen=210, kl=0.105, act_lr=1e-6, ent=0.856]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:06<00:14,  1.14it/s, pg=0.0601, ret=-0.00014, glen=49.8, tlen=210, kl=0.108, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:06<00:13,  1.15it/s, pg=0.0601, ret=-0.00014, glen=49.8, tlen=210, kl=0.108, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:07<00:13,  1.15it/s, pg=0.169, ret=-0.000426, glen=50.6, tlen=211, kl=0.101, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:07<00:12,  1.15it/s, pg=0.169, ret=-0.000426, glen=50.6, tlen=211, kl=0.101, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:08<00:12,  1.15it/s, pg=0.0131, ret=1.09e-7, glen=48.8, tlen=210, kl=0.893, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:08<00:12,  1.13it/s, pg=0.0131, ret=1.09e-7, glen=48.8, tlen=210, kl=0.893, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:08<00:12,  1.13it/s, pg=0.0061, ret=3.09e-5, glen=49.9, tlen=210, kl=0.0908, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:08<00:11,  1.14it/s, pg=0.0061, ret=3.09e-5, glen=49.9, tlen=210, kl=0.0908, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:09<00:11,  1.14it/s, pg=-0.0259, ret=0.000289, glen=47.6, tlen=208, kl=0.121, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:09<00:10,  1.15it/s, pg=-0.0259, ret=0.000289, glen=47.6, tlen=208, kl=0.121, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:10<00:10,  1.15it/s, pg=-0.0851, ret=0.000185, glen=49.6, tlen=210, kl=0.129, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:10<00:09,  1.16it/s, pg=-0.0851, ret=0.000185, glen=49.6, tlen=210, kl=0.129, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:11<00:09,  1.16it/s, pg=0.136, ret=-0.000383, glen=49.7, tlen=210, kl=0.134, act_lr=1e-6, ent=0.884] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:11<00:08,  1.16it/s, pg=0.136, ret=-0.000383, glen=49.7, tlen=210, kl=0.134, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:12<00:08,  1.16it/s, pg=0.021, ret=-3.49e-5, glen=49.7, tlen=210, kl=0.146, act_lr=1e-6, ent=0.891] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:12<00:07,  1.15it/s, pg=0.021, ret=-3.49e-5, glen=49.7, tlen=210, kl=0.146, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:13<00:07,  1.15it/s, pg=0.0104, ret=-3.21e-5, glen=48, tlen=208, kl=0.0842, act_lr=1e-6, ent=0.847]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:13<00:06,  1.16it/s, pg=0.0104, ret=-3.21e-5, glen=48, tlen=208, kl=0.0842, act_lr=1e-6, ent=0.847]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:14<00:06,  1.16it/s, pg=-0.0406, ret=9.4e-5, glen=50.3, tlen=210, kl=0.117, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:14<00:06,  1.16it/s, pg=-0.0406, ret=9.4e-5, glen=50.3, tlen=210, kl=0.117, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:14<00:06,  1.16it/s, pg=0.00585, ret=-0.000108, glen=48.7, tlen=209, kl=0.102, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:14<00:05,  1.16it/s, pg=0.00585, ret=-0.000108, glen=48.7, tlen=209, kl=0.102, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:15<00:05,  1.16it/s, pg=-0.0492, ret=9.95e-5, glen=48.9, tlen=210, kl=0.102, act_lr=1e-6, ent=0.883]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:15<00:04,  1.14it/s, pg=-0.0492, ret=9.95e-5, glen=48.9, tlen=210, kl=0.102, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:16<00:04,  1.14it/s, pg=-0.0135, ret=7.61e-5, glen=49.2, tlen=209, kl=0.152, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:16<00:03,  1.15it/s, pg=-0.0135, ret=7.61e-5, glen=49.2, tlen=209, kl=0.152, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:17<00:03,  1.15it/s, pg=-0.00104, ret=5.81e-5, glen=49.9, tlen=210, kl=0.125, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:17<00:02,  1.15it/s, pg=-0.00104, ret=5.81e-5, glen=49.9, tlen=210, kl=0.125, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:18<00:02,  1.15it/s, pg=0.00271, ret=8.75e-5, glen=49, tlen=209, kl=0.157, act_lr=1e-6, ent=0.879]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:18<00:01,  1.16it/s, pg=0.00271, ret=8.75e-5, glen=49, tlen=209, kl=0.157, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:19<00:01,  1.16it/s, pg=-0.0399, ret=9.98e-5, glen=49.4, tlen=210, kl=0.122, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:19<00:00,  1.16it/s, pg=-0.0399, ret=9.98e-5, glen=49.4, tlen=210, kl=0.122, act_lr=1e-6, ent=0.885]
2025-07-24 20:30:20.505 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:20<00:00,  1.16it/s, pg=0.0352, ret=-5.92e-5, glen=49, tlen=209, kl=0.488, act_lr=1e-6, ent=0.854]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:20<00:00,  1.09it/s, pg=0.0352, ret=-5.92e-5, glen=49, tlen=209, kl=0.488, act_lr=1e-6, ent=0.854]
2025-07-24 20:30:21.177 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:30:23.448 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-24 20:30:23.779 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 23.61s
2025-07-24 20:30:23.783 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0009691404259723166, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8814886601074882, 'kl': 0.17165474528851715, 'response_length': 49.32926708719005, 'total_length': 209.5142935047979, 'teacher_total_length': 221.50706481933594, 'return': 8.166049076902235e-06, 'policy_update_steps': 1.0}

Episode [8/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [21:40<00:00, 90.35s/it] [A[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 

[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, [36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,590] [INFO] [utils.py:782:see_memory_usage] MA 5.87 GB         Max_MA 5.87 GB         CA 6.9 GB         Max_CA 7 GB [36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7a8815ac0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
2025-07-24 20:30:32.746 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7a8815ac0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,591] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,592] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:10:23,626] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:08:43,046] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:12:06,706] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:15:32,223] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:17:14,845] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:18:56,904] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:20:39,537] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:22:22,237] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:24:04,316] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:24:04,316] [INFO] [logging.py:128:log_dist] [Rank 0] step=100, skipped=0, lr=[1e-06, 1e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:24:04,317] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=4.502029302437949, CurrSamplesPerSec=4.54861841010114, MemAllocated=13.89GB, MaxMemAllocated=25.96GB
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:25:56,568] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:27:38,923] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:29:19,921] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:30:20,499] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:30,424] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:30,611] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 3051, num_elems = 15.99B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,123] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,123] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,131] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,132] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,469] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,470] [INFO] [utils.py:782:see_memory_usage] MA 6.6 GB         Max_MA 11.56 GB         CA 7.63 GB         Max_CA 44 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,470] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.64 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [9/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [8/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [21:49<00:00, 100.70s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,740] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,741] [INFO] [utils.py:782:see_memory_usage] MA 6.6 GB         Max_MA 6.6 GB         CA 7.63 GB         Max_CA 8 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,741] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.64 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7a83fc9b0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,744] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,744] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 20:30:33.021 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:31:04.658 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:31:04.839 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:31:04.840 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.82s
2025-07-24 20:31:06.533 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0017,avg_pass_at_n: 1.0000,avg_num_tokens: 51.8848,std_num_tokens: 12.7447,avg_correct_num_tokens: 51.8931,std_correct_num_tokens: 12.7300,avg_incorrect_num_tokens: 51.2330,std_incorrect_num_tokens: 13.8355
2025-07-24 20:31:06.965 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.12s
2025-07-24 20:31:09.384 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.42s
2025-07-24 20:31:32.907 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 20:31:32.908 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.52s
2025-07-24 20:31:34.242 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.74s
2025-07-24 20:31:34.242 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.6499005034565926e-05, avg_kl: 0.0, avg_response_length: 51.90278479439872, avg_orm_score: 0.0, avg_custom_rewards: 2.6499005034565926e-05
2025-07-24 20:31:34.275 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter104_replay_buffer.jsonl
2025-07-24 20:31:35.417 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.15s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.101, ret=0.000227, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:44,  1.03s/it, pg=-0.101, ret=0.000227, glen=51.2, tlen=212, kl=0, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:44,  1.03s/it, pg=-0.0248, ret=-2.47e-6, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:39,  1.08it/s, pg=-0.0248, ret=-2.47e-6, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:39,  1.08it/s, pg=0.0829, ret=-0.000192, glen=52.6, tlen=213, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:37,  1.10it/s, pg=0.0829, ret=-0.000192, glen=52.6, tlen=213, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:37,  1.10it/s, pg=0.0409, ret=-0.000144, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:36,  1.11it/s, pg=0.0409, ret=-0.000144, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:36,  1.11it/s, pg=-0.0892, ret=0.000216, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.13it/s, pg=-0.0892, ret=0.000216, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.13it/s, pg=-0.0195, ret=9.82e-5, glen=52, tlen=212, kl=0, act_lr=1e-6, ent=0.904]   Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.14it/s, pg=-0.0195, ret=9.82e-5, glen=52, tlen=212, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.14it/s, pg=-0.00446, ret=5.18e-5, glen=50.1, tlen=210, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:32,  1.15it/s, pg=-0.00446, ret=5.18e-5, glen=50.1, tlen=210, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:32,  1.15it/s, pg=-0.0757, ret=0.000163, glen=53.6, tlen=214, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.13it/s, pg=-0.0757, ret=0.000163, glen=53.6, tlen=214, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:31,  1.13it/s, pg=-0.0306, ret=1.15e-5, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.921] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=-0.0306, ret=1.15e-5, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=0.0442, ret=-0.000157, glen=51.9, tlen=212, kl=0, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=0.0442, ret=-0.000157, glen=51.9, tlen=212, kl=0, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=-0.0629, ret=0.00015, glen=53.7, tlen=214, kl=0, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.0629, ret=0.00015, glen=53.7, tlen=214, kl=0, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=0.0212, ret=-2.41e-5, glen=52.9, tlen=213, kl=0, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=0.0212, ret=-2.41e-5, glen=52.9, tlen=213, kl=0, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=0.0593, ret=-0.000114, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.0593, ret=-0.000114, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=-0.0104, ret=-5.53e-5, glen=52.6, tlen=214, kl=0, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=-0.0104, ret=-5.53e-5, glen=52.6, tlen=214, kl=0, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=-0.0277, ret=3.02e-5, glen=53.3, tlen=214, kl=0, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0277, ret=3.02e-5, glen=53.3, tlen=214, kl=0, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=0.219, ret=-0.000368, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:23,  1.17it/s, pg=0.219, ret=-0.000368, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.0911, ret=0.000131, glen=52.2, tlen=213, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:22,  1.17it/s, pg=-0.0911, ret=0.000131, glen=52.2, tlen=213, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:22,  1.17it/s, pg=0.234, ret=-0.000477, glen=52.5, tlen=213, kl=0, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=0.234, ret=-0.000477, glen=52.5, tlen=213, kl=0, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=-0.0276, ret=5.41e-5, glen=53.9, tlen=214, kl=0, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=-0.0276, ret=5.41e-5, glen=53.9, tlen=214, kl=0, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=-0.129, ret=0.000299, glen=52.2, tlen=213, kl=0, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.129, ret=0.000299, glen=52.2, tlen=213, kl=0, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0289, ret=9.59e-5, glen=52.9, tlen=214, kl=0, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0289, ret=9.59e-5, glen=52.9, tlen=214, kl=0, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.0592, ret=-0.00016, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0592, ret=-0.00016, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0532, ret=-6.37e-5, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:19<00:17,  1.17it/s, pg=0.0532, ret=-6.37e-5, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.0831, ret=0.000164, glen=52, tlen=212, kl=0, act_lr=1e-6, ent=0.896] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.18it/s, pg=-0.0831, ret=0.000164, glen=52, tlen=212, kl=0, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.18it/s, pg=-0.103, ret=0.000249, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.18it/s, pg=-0.103, ret=0.000249, glen=50.8, tlen=211, kl=0, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.18it/s, pg=-0.0938, ret=0.000198, glen=52.2, tlen=212, kl=0, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.18it/s, pg=-0.0938, ret=0.000198, glen=52.2, tlen=212, kl=0, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.18it/s, pg=0.0494, ret=-0.000103, glen=51.5, tlen=212, kl=0, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.18it/s, pg=0.0494, ret=-0.000103, glen=51.5, tlen=212, kl=0, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.18it/s, pg=0.0749, ret=-0.000125, glen=52.2, tlen=213, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.18it/s, pg=0.0749, ret=-0.000125, glen=52.2, tlen=213, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.18it/s, pg=-0.0427, ret=0.000118, glen=51.7, tlen=213, kl=0, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.04it/s, pg=-0.0427, ret=0.000118, glen=51.7, tlen=213, kl=0, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.04it/s, pg=-0.0328, ret=0.000101, glen=50.2, tlen=210, kl=0, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.08it/s, pg=-0.0328, ret=0.000101, glen=50.2, tlen=210, kl=0, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.08it/s, pg=0.00783, ret=-8.13e-5, glen=51.3, tlen=212, kl=0, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.11it/s, pg=0.00783, ret=-8.13e-5, glen=51.3, tlen=212, kl=0, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.11it/s, pg=-0.0275, ret=3.71e-5, glen=51.2, tlen=211, kl=0, act_lr=1e-6, ent=0.894] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.13it/s, pg=-0.0275, ret=3.71e-5, glen=51.2, tlen=211, kl=0, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.00385, ret=3.71e-5, glen=51.5, tlen=212, kl=0, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.14it/s, pg=0.00385, ret=3.71e-5, glen=51.5, tlen=212, kl=0, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.14it/s, pg=-0.024, ret=8.64e-5, glen=52.6, tlen=213, kl=0, act_lr=1e-6, ent=0.881] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.024, ret=8.64e-5, glen=52.6, tlen=213, kl=0, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=0.159, ret=-0.000274, glen=49.7, tlen=210, kl=0, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.159, ret=-0.000274, glen=49.7, tlen=210, kl=0, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=-0.062, ret=0.000171, glen=52.8, tlen=213, kl=0, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=-0.062, ret=0.000171, glen=52.8, tlen=213, kl=0, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.0144, ret=1.49e-5, glen=52.9, tlen=213, kl=0, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:05,  1.17it/s, pg=-0.0144, ret=1.49e-5, glen=52.9, tlen=213, kl=0, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:05,  1.17it/s, pg=-0.0827, ret=0.000162, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0827, ret=0.000162, glen=52.4, tlen=213, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.00455, ret=-2.65e-5, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.00455, ret=-2.65e-5, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0186, ret=6.01e-5, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.867]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=-0.0186, ret=6.01e-5, glen=50.7, tlen=211, kl=0, act_lr=1e-6, ent=0.867]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.0609, ret=-0.00015, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.866]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.18it/s, pg=0.0609, ret=-0.00015, glen=51.7, tlen=212, kl=0, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.18it/s, pg=-0.0474, ret=9.88e-5, glen=50.3, tlen=211, kl=0, act_lr=1e-6, ent=0.863]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.0474, ret=9.88e-5, glen=50.3, tlen=211, kl=0, act_lr=1e-6, ent=0.863]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.188, ret=-0.000401, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.18it/s, pg=0.188, ret=-0.000401, glen=52.1, tlen=213, kl=0, act_lr=1e-6, ent=0.965]
2025-07-24 20:32:13.948 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.31s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.18it/s, pg=-0.0312, ret=3.01e-5, glen=50.4, tlen=210, kl=0, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.13it/s, pg=-0.0312, ret=3.01e-5, glen=50.4, tlen=210, kl=0, act_lr=1e-6, ent=0.92]
2025-07-24 20:32:14.628 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 20:32:16.642 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.01s
2025-07-24 20:32:17.034 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.51s
2025-07-24 20:32:17.040 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007352070374922318, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9048817090012811, 'kl': 0.0, 'response_length': 51.89680914445357, 'total_length': 212.38166046142578, 'teacher_total_length': 224.3215429132635, 'return': 3.1227059893849813e-06, 'policy_update_steps': 1.0}
Episode [9/20]:   8%|‚ñä         | 1/13 [01:44<20:51, 104.30s/it]2025-07-24 20:32:17.085 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:32:48.145 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:32:48.326 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:32:48.327 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.24s
2025-07-24 20:32:50.230 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0009,avg_pass_at_n: 1.0000,avg_num_tokens: 51.3857,std_num_tokens: 13.1307,avg_correct_num_tokens: 51.3843,std_correct_num_tokens: 13.1337,avg_incorrect_num_tokens: 51.5109,std_incorrect_num_tokens: 12.8634
2025-07-24 20:32:50.641 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.31s
2025-07-24 20:32:53.039 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.39s
2025-07-24 20:33:15.728 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 20:33:15.728 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.69s
2025-07-24 20:33:17.015 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.80s
2025-07-24 20:33:17.016 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.609245049101966e-05, avg_kl: 0.003659188406808036, avg_response_length: 51.40344508579799, avg_orm_score: 0.0, avg_custom_rewards: -1.609245049101966e-05
2025-07-24 20:33:17.044 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter105_replay_buffer.jsonl
2025-07-24 20:33:18.196 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0264, ret=7.66e-5, glen=52.8, tlen=213, kl=0.0039, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=-0.0264, ret=7.66e-5, glen=52.8, tlen=213, kl=0.0039, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=0.0287, ret=-8.39e-5, glen=50.9, tlen=211, kl=0.00414, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:38,  1.08it/s, pg=0.0287, ret=-8.39e-5, glen=50.9, tlen=211, kl=0.00414, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:38,  1.08it/s, pg=-0.00174, ret=-6.37e-5, glen=51, tlen=212, kl=0.00357, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=-0.00174, ret=-6.37e-5, glen=51, tlen=212, kl=0.00357, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=-0.106, ret=0.000222, glen=49.9, tlen=210, kl=0.0039, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.14it/s, pg=-0.106, ret=0.000222, glen=49.9, tlen=210, kl=0.0039, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.14it/s, pg=0.0297, ret=-5.94e-5, glen=54.5, tlen=215, kl=0.00294, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:33,  1.15it/s, pg=0.0297, ret=-5.94e-5, glen=54.5, tlen=215, kl=0.00294, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:33,  1.15it/s, pg=-0.0992, ret=0.000216, glen=51.4, tlen=212, kl=0.00335, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.13it/s, pg=-0.0992, ret=0.000216, glen=51.4, tlen=212, kl=0.00335, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.13it/s, pg=-0.0662, ret=0.000135, glen=51.5, tlen=212, kl=0.00371, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:33,  1.11it/s, pg=-0.0662, ret=0.000135, glen=51.5, tlen=212, kl=0.00371, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:33,  1.11it/s, pg=-0.037, ret=8.67e-5, glen=50, tlen=211, kl=0.00351, act_lr=1e-6, ent=0.911]    Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=-0.037, ret=8.67e-5, glen=50, tlen=211, kl=0.00351, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:32,  1.12it/s, pg=0.0159, ret=4.97e-5, glen=52.1, tlen=212, kl=0.00376, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=0.0159, ret=4.97e-5, glen=52.1, tlen=212, kl=0.00376, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=-0.0202, ret=8.07e-5, glen=53.7, tlen=214, kl=0.00345, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=-0.0202, ret=8.07e-5, glen=53.7, tlen=214, kl=0.00345, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=0.123, ret=-0.000173, glen=50.2, tlen=211, kl=0.00321, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=0.123, ret=-0.000173, glen=50.2, tlen=211, kl=0.00321, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=0.0493, ret=-0.00015, glen=52, tlen=212, kl=0.00357, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=0.0493, ret=-0.00015, glen=52, tlen=212, kl=0.00357, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=0.0222, ret=1.8e-5, glen=53.1, tlen=214, kl=0.00438, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.0222, ret=1.8e-5, glen=53.1, tlen=214, kl=0.00438, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=0.0563, ret=-0.000124, glen=51.4, tlen=212, kl=0.00422, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.17it/s, pg=0.0563, ret=-0.000124, glen=51.4, tlen=212, kl=0.00422, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.17it/s, pg=-0.0645, ret=0.000143, glen=51.6, tlen=212, kl=0.00375, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=-0.0645, ret=0.000143, glen=51.6, tlen=212, kl=0.00375, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.17it/s, pg=0.031, ret=-1.95e-5, glen=50.4, tlen=211, kl=0.00432, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:13<00:23,  1.17it/s, pg=0.031, ret=-1.95e-5, glen=50.4, tlen=211, kl=0.00432, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:23,  1.17it/s, pg=-0.0126, ret=4.22e-5, glen=50.8, tlen=212, kl=0.00364, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.16it/s, pg=-0.0126, ret=4.22e-5, glen=50.8, tlen=212, kl=0.00364, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.16it/s, pg=0.0316, ret=-9.84e-5, glen=52.4, tlen=213, kl=0.00363, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=0.0316, ret=-9.84e-5, glen=52.4, tlen=213, kl=0.00363, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=0.0681, ret=-0.000175, glen=50.2, tlen=210, kl=0.00346, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=0.0681, ret=-0.000175, glen=50.2, tlen=210, kl=0.00346, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=-0.0765, ret=0.000172, glen=50.9, tlen=211, kl=0.00327, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=-0.0765, ret=0.000172, glen=50.9, tlen=211, kl=0.00327, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=0.0479, ret=-0.000118, glen=50.7, tlen=211, kl=0.0036, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=0.0479, ret=-0.000118, glen=50.7, tlen=211, kl=0.0036, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=-0.111, ret=0.000239, glen=51.8, tlen=212, kl=0.00347, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=-0.111, ret=0.000239, glen=51.8, tlen=212, kl=0.00347, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.17it/s, pg=0.0544, ret=-0.000115, glen=52.7, tlen=213, kl=0.00411, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.13it/s, pg=0.0544, ret=-0.000115, glen=52.7, tlen=213, kl=0.00411, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.13it/s, pg=-0.117, ret=0.000256, glen=49.6, tlen=210, kl=0.00369, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.14it/s, pg=-0.117, ret=0.000256, glen=49.6, tlen=210, kl=0.00369, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.14it/s, pg=-0.115, ret=0.000249, glen=50.1, tlen=211, kl=0.00344, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.13it/s, pg=-0.115, ret=0.000249, glen=50.1, tlen=211, kl=0.00344, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.13it/s, pg=-0.0714, ret=0.000168, glen=51.3, tlen=212, kl=0.00336, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.14it/s, pg=-0.0714, ret=0.000168, glen=51.3, tlen=212, kl=0.00336, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.14it/s, pg=0.0083, ret=1.45e-5, glen=50.5, tlen=211, kl=0.0039, act_lr=1e-6, ent=0.911]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.15it/s, pg=0.0083, ret=1.45e-5, glen=50.5, tlen=211, kl=0.0039, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.15it/s, pg=-0.015, ret=2.56e-6, glen=52.6, tlen=213, kl=0.00398, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.16it/s, pg=-0.015, ret=2.56e-6, glen=52.6, tlen=213, kl=0.00398, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.16it/s, pg=-0.0461, ret=0.000101, glen=52.8, tlen=213, kl=0.00388, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.06it/s, pg=-0.0461, ret=0.000101, glen=52.8, tlen=213, kl=0.00388, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.06it/s, pg=-0.000122, ret=-1.28e-6, glen=50.8, tlen=211, kl=0.00385, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.09it/s, pg=-0.000122, ret=-1.28e-6, glen=50.8, tlen=211, kl=0.00385, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.09it/s, pg=-0.0541, ret=0.000147, glen=53.1, tlen=214, kl=0.00327, act_lr=1e-6, ent=0.943]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0541, ret=0.000147, glen=53.1, tlen=214, kl=0.00327, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.12it/s, pg=0.243, ret=-0.000423, glen=51.7, tlen=213, kl=0.00364, act_lr=1e-6, ent=0.929] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.243, ret=-0.000423, glen=51.7, tlen=213, kl=0.00364, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.107, ret=-0.000237, glen=51.4, tlen=212, kl=0.00349, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.15it/s, pg=0.107, ret=-0.000237, glen=51.4, tlen=212, kl=0.00349, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.15it/s, pg=-0.0373, ret=5.91e-5, glen=50.4, tlen=211, kl=0.004, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.12it/s, pg=-0.0373, ret=5.91e-5, glen=50.4, tlen=211, kl=0.004, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.12it/s, pg=0.011, ret=-0.000102, glen=49.7, tlen=210, kl=0.00405, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.13it/s, pg=0.011, ret=-0.000102, glen=49.7, tlen=210, kl=0.00405, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.13it/s, pg=0.0911, ret=-0.000214, glen=51.9, tlen=212, kl=0.00389, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.14it/s, pg=0.0911, ret=-0.000214, glen=51.9, tlen=212, kl=0.00389, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.14it/s, pg=-0.0125, ret=1.53e-5, glen=50.6, tlen=211, kl=0.00303, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.15it/s, pg=-0.0125, ret=1.53e-5, glen=50.6, tlen=211, kl=0.00303, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.15it/s, pg=-0.0411, ret=9.2e-5, glen=50.7, tlen=211, kl=0.00361, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.16it/s, pg=-0.0411, ret=9.2e-5, glen=50.7, tlen=211, kl=0.00361, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.16it/s, pg=0.144, ret=-0.000313, glen=51.6, tlen=212, kl=0.00299, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.144, ret=-0.000313, glen=51.6, tlen=212, kl=0.00299, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0056, ret=-6.31e-5, glen=50.8, tlen=211, kl=0.00348, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0056, ret=-6.31e-5, glen=50.8, tlen=211, kl=0.00348, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=0.00378, ret=-8.49e-5, glen=51.5, tlen=212, kl=0.00368, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=0.00378, ret=-8.49e-5, glen=51.5, tlen=212, kl=0.00368, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.0344, ret=-0.000145, glen=51.6, tlen=212, kl=0.0034, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.0344, ret=-0.000145, glen=51.6, tlen=212, kl=0.0034, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.0349, ret=0.000123, glen=52.7, tlen=213, kl=0.00379, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.0349, ret=0.000123, glen=52.7, tlen=213, kl=0.00379, act_lr=1e-6, ent=0.94]
2025-07-24 20:33:56.858 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.50s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0447, ret=7.79e-5, glen=50.1, tlen=211, kl=0.00349, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0447, ret=7.79e-5, glen=50.1, tlen=211, kl=0.00349, act_lr=1e-6, ent=0.928]
2025-07-24 20:33:57.688 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:34:00.205 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 20:34:00.554 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.31s
2025-07-24 20:34:00.587 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00010923905806107955, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9271052005616102, 'kl': 0.0036538297479802914, 'response_length': 51.394020167264074, 'total_length': 211.92616202614525, 'teacher_total_length': 223.95411023226652, 'return': 4.7156423864759167e-07, 'policy_update_steps': 1.0}
Episode [9/20]:  15%|‚ñà‚ñå        | 2/13 [03:27<19:02, 103.86s/it]2025-07-24 20:34:00.639 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:34:32.431 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:34:32.615 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:34:32.615 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.98s
2025-07-24 20:34:34.249 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 52.1107,std_num_tokens: 12.7716,avg_correct_num_tokens: 52.0871,std_correct_num_tokens: 12.7610,avg_incorrect_num_tokens: 54.1489,std_incorrect_num_tokens: 13.5041
2025-07-24 20:34:34.685 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.07s
2025-07-24 20:34:37.336 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.65s
2025-07-24 20:34:59.912 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 20:34:59.912 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.57s
2025-07-24 20:35:01.182 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 20:35:01.183 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -6.3651387712785175e-06, avg_kl: 0.0089306640625, avg_response_length: 52.12676744733538, avg_orm_score: 0.0, avg_custom_rewards: -6.3651387712785175e-06
2025-07-24 20:35:01.211 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter106_replay_buffer.jsonl
2025-07-24 20:35:02.388 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=0.0515, ret=-8e-5, glen=51.7, tlen=211, kl=0.00802, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:45,  1.05s/it, pg=0.0515, ret=-8e-5, glen=51.7, tlen=211, kl=0.00802, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:45,  1.05s/it, pg=0.0624, ret=-0.000208, glen=51.7, tlen=212, kl=0.00902, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:39,  1.07it/s, pg=0.0624, ret=-0.000208, glen=51.7, tlen=212, kl=0.00902, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:39,  1.07it/s, pg=-0.0791, ret=0.000117, glen=53.5, tlen=213, kl=0.00742, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.11it/s, pg=-0.0791, ret=0.000117, glen=53.5, tlen=213, kl=0.00742, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.11it/s, pg=0.0293, ret=-6.27e-5, glen=52, tlen=213, kl=0.00864, act_lr=1e-6, ent=0.942]   Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.14it/s, pg=0.0293, ret=-6.27e-5, glen=52, tlen=213, kl=0.00864, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.14it/s, pg=0.0232, ret=-2.68e-5, glen=53, tlen=214, kl=0.0102, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.14it/s, pg=0.0232, ret=-2.68e-5, glen=53, tlen=214, kl=0.0102, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.14it/s, pg=-0.082, ret=0.000147, glen=51.9, tlen=212, kl=0.00849, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:32,  1.15it/s, pg=-0.082, ret=0.000147, glen=51.9, tlen=212, kl=0.00849, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:32,  1.15it/s, pg=-0.0234, ret=5.31e-6, glen=51.9, tlen=212, kl=0.00941, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:31,  1.16it/s, pg=-0.0234, ret=5.31e-6, glen=51.9, tlen=212, kl=0.00941, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:31,  1.16it/s, pg=0.0508, ret=-9.55e-5, glen=52.3, tlen=212, kl=0.0105, act_lr=1e-6, ent=0.894] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.0508, ret=-9.55e-5, glen=52.3, tlen=212, kl=0.0105, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:31,  1.14it/s, pg=0.165, ret=-0.000177, glen=53.9, tlen=214, kl=0.0091, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:07<00:30,  1.14it/s, pg=0.165, ret=-0.000177, glen=53.9, tlen=214, kl=0.0091, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.14it/s, pg=0.0575, ret=-8.43e-5, glen=51.8, tlen=212, kl=0.00851, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.15it/s, pg=0.0575, ret=-8.43e-5, glen=51.8, tlen=212, kl=0.00851, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.15it/s, pg=-0.0295, ret=8.92e-5, glen=52.4, tlen=212, kl=0.0106, act_lr=1e-6, ent=0.932] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:28,  1.16it/s, pg=-0.0295, ret=8.92e-5, glen=52.4, tlen=212, kl=0.0106, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:28,  1.16it/s, pg=0.0123, ret=-1.37e-5, glen=51.6, tlen=212, kl=0.0123, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:27,  1.16it/s, pg=0.0123, ret=-1.37e-5, glen=51.6, tlen=212, kl=0.0123, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:27,  1.16it/s, pg=0.12, ret=-0.000268, glen=53.6, tlen=214, kl=0.0116, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.17it/s, pg=0.12, ret=-0.000268, glen=53.6, tlen=214, kl=0.0116, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.17it/s, pg=-0.00995, ret=2.96e-5, glen=53.1, tlen=213, kl=0.00915, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:26,  1.15it/s, pg=-0.00995, ret=2.96e-5, glen=53.1, tlen=213, kl=0.00915, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:26,  1.15it/s, pg=0.0673, ret=-0.000129, glen=50.6, tlen=211, kl=0.0105, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:25,  1.15it/s, pg=0.0673, ret=-0.000129, glen=50.6, tlen=211, kl=0.0105, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:25,  1.15it/s, pg=0.0285, ret=-4.16e-5, glen=51.9, tlen=212, kl=0.00819, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.14it/s, pg=0.0285, ret=-4.16e-5, glen=51.9, tlen=212, kl=0.00819, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.14it/s, pg=-0.102, ret=0.000204, glen=50.9, tlen=212, kl=0.00787, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.15it/s, pg=-0.102, ret=0.000204, glen=50.9, tlen=212, kl=0.00787, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.15it/s, pg=-0.0695, ret=0.000174, glen=51.4, tlen=212, kl=0.00812, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=-0.0695, ret=0.000174, glen=51.4, tlen=212, kl=0.00812, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=-0.081, ret=0.000172, glen=52.5, tlen=213, kl=0.00769, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.16it/s, pg=-0.081, ret=0.000172, glen=52.5, tlen=213, kl=0.00769, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.16it/s, pg=-0.00354, ret=5.35e-5, glen=51.4, tlen=211, kl=0.00747, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.16it/s, pg=-0.00354, ret=5.35e-5, glen=51.4, tlen=211, kl=0.00747, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.16it/s, pg=0.0482, ret=-8.41e-5, glen=54.7, tlen=215, kl=0.00733, act_lr=1e-6, ent=0.929] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.16it/s, pg=0.0482, ret=-8.41e-5, glen=54.7, tlen=215, kl=0.00733, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.16it/s, pg=0.123, ret=-0.00031, glen=53.7, tlen=214, kl=0.00748, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.123, ret=-0.00031, glen=53.7, tlen=214, kl=0.00748, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.17it/s, pg=-0.0689, ret=0.000137, glen=50.5, tlen=211, kl=0.0099, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=-0.0689, ret=0.000137, glen=50.5, tlen=211, kl=0.0099, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:17,  1.17it/s, pg=0.119, ret=-0.000246, glen=52.5, tlen=213, kl=0.0094, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.17it/s, pg=0.119, ret=-0.000246, glen=52.5, tlen=213, kl=0.0094, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.17it/s, pg=-0.0604, ret=0.000114, glen=51.9, tlen=212, kl=0.00756, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.17it/s, pg=-0.0604, ret=0.000114, glen=51.9, tlen=212, kl=0.00756, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.17it/s, pg=0.148, ret=-0.000353, glen=51.3, tlen=212, kl=0.00984, act_lr=1e-6, ent=0.88] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=0.148, ret=-0.000353, glen=51.3, tlen=212, kl=0.00984, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=-0.105, ret=0.000218, glen=51.7, tlen=212, kl=0.00887, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=-0.105, ret=0.000218, glen=51.7, tlen=212, kl=0.00887, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.00452, ret=4.14e-5, glen=53.3, tlen=214, kl=0.0093, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.00452, ret=4.14e-5, glen=53.3, tlen=214, kl=0.0093, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=-0.0381, ret=9.02e-5, glen=51.8, tlen=212, kl=0.0076, act_lr=1e-6, ent=0.932] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=-0.0381, ret=9.02e-5, glen=51.8, tlen=212, kl=0.0076, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.0131, ret=-5.83e-5, glen=52.7, tlen=213, kl=0.0067, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.0131, ret=-5.83e-5, glen=52.7, tlen=213, kl=0.0067, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.0267, ret=5.28e-5, glen=52.4, tlen=213, kl=0.00858, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.0267, ret=5.28e-5, glen=52.4, tlen=213, kl=0.00858, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.12it/s, pg=-0.00525, ret=3.36e-5, glen=52, tlen=213, kl=0.00943, act_lr=1e-6, ent=0.966] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:27<00:10,  1.13it/s, pg=-0.00525, ret=3.36e-5, glen=52, tlen=213, kl=0.00943, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.13it/s, pg=0.0272, ret=-0.00012, glen=52, tlen=212, kl=0.00764, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.14it/s, pg=0.0272, ret=-0.00012, glen=52, tlen=212, kl=0.00764, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.14it/s, pg=-0.112, ret=0.000243, glen=51.3, tlen=212, kl=0.0124, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.112, ret=0.000243, glen=51.3, tlen=212, kl=0.0124, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=0, ret=-3.33e-5, glen=53.6, tlen=214, kl=0.00995, act_lr=1e-6, ent=0.916]    Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0, ret=-3.33e-5, glen=53.6, tlen=214, kl=0.00995, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.00647, ret=-0.000111, glen=51, tlen=212, kl=0.00787, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=0.00647, ret=-0.000111, glen=51, tlen=212, kl=0.00787, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=0.0837, ret=-0.000172, glen=51.1, tlen=211, kl=0.00785, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.16it/s, pg=0.0837, ret=-0.000172, glen=51.1, tlen=211, kl=0.00785, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.16it/s, pg=-0.0944, ret=0.000187, glen=50.9, tlen=211, kl=0.00716, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0944, ret=0.000187, glen=50.9, tlen=211, kl=0.00716, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0154, ret=3.82e-5, glen=52.5, tlen=213, kl=0.00861, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:33<00:04,  1.17it/s, pg=-0.0154, ret=3.82e-5, glen=52.5, tlen=213, kl=0.00861, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0425, ret=-0.000116, glen=52, tlen=213, kl=0.0106, act_lr=1e-6, ent=0.928]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0425, ret=-0.000116, glen=52, tlen=213, kl=0.0106, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.133, ret=0.000288, glen=54, tlen=215, kl=0.00725, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.133, ret=0.000288, glen=54, tlen=215, kl=0.00725, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=-0.084, ret=0.000179, glen=51.4, tlen=212, kl=0.0101, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=-0.084, ret=0.000179, glen=51.4, tlen=212, kl=0.0101, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=0.0548, ret=-8.07e-5, glen=50.7, tlen=211, kl=0.00858, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=0.0548, ret=-8.07e-5, glen=50.7, tlen=211, kl=0.00858, act_lr=1e-6, ent=0.892]
2025-07-24 20:35:41.080 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.32s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.149, ret=0.000293, glen=52.5, tlen=213, kl=0.00948, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.149, ret=0.000293, glen=52.5, tlen=213, kl=0.00948, act_lr=1e-6, ent=0.908]
2025-07-24 20:35:41.748 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 20:35:44.117 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.37s
2025-07-24 20:35:44.464 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.82s
2025-07-24 20:35:44.534 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0009519403631036931, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9215727597475052, 'kl': 0.008915814486416903, 'response_length': 52.143463134765625, 'total_length': 212.44947884299538, 'teacher_total_length': 224.5107862299139, 'return': 8.37936733494809e-07, 'policy_update_steps': 1.0}
Episode [9/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:11<17:18, 103.90s/it]2025-07-24 20:35:44.570 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:36:15.736 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:36:15.920 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:36:15.921 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 31.35s
2025-07-24 20:36:17.821 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0150,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 52.0674,std_num_tokens: 12.4463,avg_correct_num_tokens: 52.0910,std_correct_num_tokens: 12.4482,avg_incorrect_num_tokens: 48.8667,std_incorrect_num_tokens: 11.7523
2025-07-24 20:36:18.175 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.25s
2025-07-24 20:36:20.638 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.46s
2025-07-24 20:36:43.496 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 175
2025-07-24 20:36:43.497 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.86s
2025-07-24 20:36:44.827 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.86s
2025-07-24 20:36:44.827 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 9.577563032507896e-06, avg_kl: 0.028456333705357145, avg_response_length: 52.0827700151716, avg_orm_score: 0.0, avg_custom_rewards: 9.577563032507896e-06
2025-07-24 20:36:44.858 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter107_replay_buffer.jsonl
2025-07-24 20:36:46.028 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/44 [00:01<?, ?it/s, pg=-0.0205, ret=1.35e-5, glen=50.4, tlen=211, kl=0.0227, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=-0.0205, ret=1.35e-5, glen=50.4, tlen=211, kl=0.0227, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/44 [00:01<00:43,  1.02s/it, pg=0.00793, ret=-6.2e-5, glen=51.3, tlen=211, kl=0.0249, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:01<00:39,  1.08it/s, pg=0.00793, ret=-6.2e-5, glen=51.3, tlen=211, kl=0.0249, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñç         | 2/44 [00:02<00:39,  1.08it/s, pg=0.15, ret=-0.000311, glen=51.6, tlen=212, kl=0.0224, act_lr=1e-6, ent=0.887] Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:02<00:36,  1.12it/s, pg=0.15, ret=-0.000311, glen=51.6, tlen=212, kl=0.0224, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/44 [00:03<00:36,  1.12it/s, pg=-0.0289, ret=3.89e-5, glen=50.2, tlen=210, kl=0.0334, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:03<00:35,  1.14it/s, pg=-0.0289, ret=3.89e-5, glen=50.2, tlen=210, kl=0.0334, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/44 [00:04<00:35,  1.14it/s, pg=-0.024, ret=5.01e-5, glen=53.7, tlen=214, kl=0.0375, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:04<00:34,  1.11it/s, pg=-0.024, ret=5.01e-5, glen=53.7, tlen=214, kl=0.0375, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 5/44 [00:05<00:34,  1.11it/s, pg=0.0106, ret=2.37e-5, glen=53.5, tlen=214, kl=0.0221, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:05<00:33,  1.13it/s, pg=0.0106, ret=2.37e-5, glen=53.5, tlen=214, kl=0.0221, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 6/44 [00:06<00:33,  1.13it/s, pg=-0.0354, ret=7.6e-5, glen=52.8, tlen=213, kl=0.0284, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:06<00:33,  1.12it/s, pg=-0.0354, ret=7.6e-5, glen=52.8, tlen=213, kl=0.0284, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/44 [00:07<00:33,  1.12it/s, pg=0.0591, ret=-5.84e-5, glen=51.8, tlen=212, kl=0.0267, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:07<00:32,  1.12it/s, pg=0.0591, ret=-5.84e-5, glen=51.8, tlen=212, kl=0.0267, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/44 [00:08<00:32,  1.12it/s, pg=0.0344, ret=-5.89e-5, glen=53, tlen=213, kl=0.029, act_lr=1e-6, ent=0.899]   Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=0.0344, ret=-5.89e-5, glen=53, tlen=213, kl=0.029, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/44 [00:08<00:30,  1.13it/s, pg=-0.0289, ret=7.43e-5, glen=51.7, tlen=211, kl=0.0265, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:08<00:29,  1.14it/s, pg=-0.0289, ret=7.43e-5, glen=51.7, tlen=211, kl=0.0265, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 10/44 [00:09<00:29,  1.14it/s, pg=-0.0266, ret=4.83e-5, glen=50.3, tlen=210, kl=0.0337, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:09<00:29,  1.13it/s, pg=-0.0266, ret=4.83e-5, glen=50.3, tlen=210, kl=0.0337, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 11/44 [00:10<00:29,  1.13it/s, pg=-0.0295, ret=6.18e-5, glen=52.1, tlen=212, kl=0.028, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:10<00:28,  1.14it/s, pg=-0.0295, ret=6.18e-5, glen=52.1, tlen=212, kl=0.028, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/44 [00:11<00:28,  1.14it/s, pg=-0.105, ret=0.000186, glen=51.4, tlen=211, kl=0.0181, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:11<00:26,  1.15it/s, pg=-0.105, ret=0.000186, glen=51.4, tlen=211, kl=0.0181, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 13/44 [00:12<00:26,  1.15it/s, pg=0.0812, ret=-0.000182, glen=52.2, tlen=212, kl=0.0307, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:12<00:25,  1.16it/s, pg=0.0812, ret=-0.000182, glen=52.2, tlen=212, kl=0.0307, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 14/44 [00:13<00:25,  1.16it/s, pg=0.217, ret=-0.000318, glen=52.2, tlen=213, kl=0.0368, act_lr=1e-6, ent=0.903] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:13<00:24,  1.16it/s, pg=0.217, ret=-0.000318, glen=52.2, tlen=213, kl=0.0368, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 15/44 [00:14<00:24,  1.16it/s, pg=-0.0712, ret=0.000138, glen=53.3, tlen=214, kl=0.0327, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=-0.0712, ret=0.000138, glen=53.3, tlen=214, kl=0.0327, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 16/44 [00:14<00:24,  1.16it/s, pg=0.0327, ret=-7.25e-5, glen=51, tlen=211, kl=0.0183, act_lr=1e-6, ent=0.914]   Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:14<00:23,  1.17it/s, pg=0.0327, ret=-7.25e-5, glen=51, tlen=211, kl=0.0183, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 17/44 [00:15<00:23,  1.17it/s, pg=-0.074, ret=0.000139, glen=53.3, tlen=213, kl=0.0271, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:15<00:22,  1.16it/s, pg=-0.074, ret=0.000139, glen=53.3, tlen=213, kl=0.0271, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 18/44 [00:16<00:22,  1.16it/s, pg=0.0153, ret=-7.86e-5, glen=52.1, tlen=212, kl=0.0228, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:16<00:21,  1.17it/s, pg=0.0153, ret=-7.86e-5, glen=52.1, tlen=212, kl=0.0228, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 19/44 [00:17<00:21,  1.17it/s, pg=0.0216, ret=-5.73e-5, glen=52.8, tlen=213, kl=0.0208, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:17<00:20,  1.17it/s, pg=0.0216, ret=-5.73e-5, glen=52.8, tlen=213, kl=0.0208, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 20/44 [00:18<00:20,  1.17it/s, pg=-0.0438, ret=5.72e-5, glen=53.1, tlen=213, kl=0.0461, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:18<00:19,  1.17it/s, pg=-0.0438, ret=5.72e-5, glen=53.1, tlen=213, kl=0.0461, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/44 [00:19<00:19,  1.17it/s, pg=0.0529, ret=-0.000106, glen=52, tlen=212, kl=0.0283, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:19<00:18,  1.17it/s, pg=0.0529, ret=-0.000106, glen=52, tlen=212, kl=0.0283, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 22/44 [00:20<00:18,  1.17it/s, pg=0.0292, ret=2.83e-5, glen=52.3, tlen=213, kl=0.0256, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=0.0292, ret=2.83e-5, glen=52.3, tlen=213, kl=0.0256, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 23/44 [00:20<00:18,  1.16it/s, pg=0.091, ret=-9.73e-5, glen=52, tlen=212, kl=0.0308, act_lr=1e-6, ent=0.879]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:20<00:17,  1.16it/s, pg=0.091, ret=-9.73e-5, glen=52, tlen=212, kl=0.0308, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 24/44 [00:21<00:17,  1.16it/s, pg=-0.0247, ret=1.19e-5, glen=53.9, tlen=214, kl=0.0334, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:21<00:16,  1.16it/s, pg=-0.0247, ret=1.19e-5, glen=53.9, tlen=214, kl=0.0334, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 25/44 [00:22<00:16,  1.16it/s, pg=-0.0605, ret=0.000115, glen=52.2, tlen=212, kl=0.0233, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:22<00:15,  1.17it/s, pg=-0.0605, ret=0.000115, glen=52.2, tlen=212, kl=0.0233, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 26/44 [00:23<00:15,  1.17it/s, pg=-0.0794, ret=0.000152, glen=50.9, tlen=211, kl=0.0164, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:23<00:14,  1.17it/s, pg=-0.0794, ret=0.000152, glen=50.9, tlen=211, kl=0.0164, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 27/44 [00:24<00:14,  1.17it/s, pg=-0.0852, ret=0.000162, glen=52.9, tlen=213, kl=0.0292, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:24<00:13,  1.17it/s, pg=-0.0852, ret=0.000162, glen=52.9, tlen=213, kl=0.0292, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 28/44 [00:25<00:13,  1.17it/s, pg=0.0985, ret=-0.000151, glen=53.2, tlen=213, kl=0.0256, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:25<00:14,  1.07it/s, pg=0.0985, ret=-0.000151, glen=53.2, tlen=213, kl=0.0256, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 29/44 [00:26<00:14,  1.07it/s, pg=0.108, ret=-0.000196, glen=52, tlen=212, kl=0.0202, act_lr=1e-6, ent=0.889]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:26<00:12,  1.10it/s, pg=0.108, ret=-0.000196, glen=52, tlen=212, kl=0.0202, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 30/44 [00:27<00:12,  1.10it/s, pg=-0.0829, ret=0.000147, glen=52.9, tlen=213, kl=0.0328, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:27<00:11,  1.10it/s, pg=-0.0829, ret=0.000147, glen=52.9, tlen=213, kl=0.0328, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 31/44 [00:28<00:11,  1.10it/s, pg=0.0635, ret=-0.000159, glen=52.2, tlen=212, kl=0.0317, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.12it/s, pg=0.0635, ret=-0.000159, glen=52.2, tlen=212, kl=0.0317, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 32/44 [00:28<00:10,  1.12it/s, pg=-0.0631, ret=0.000118, glen=51.8, tlen=212, kl=0.0277, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:28<00:09,  1.14it/s, pg=-0.0631, ret=0.000118, glen=51.8, tlen=212, kl=0.0277, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 33/44 [00:29<00:09,  1.14it/s, pg=-0.00732, ret=-5.07e-5, glen=50.8, tlen=211, kl=0.0266, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:29<00:08,  1.15it/s, pg=-0.00732, ret=-5.07e-5, glen=50.8, tlen=211, kl=0.0266, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 34/44 [00:30<00:08,  1.15it/s, pg=0.0369, ret=-7.78e-5, glen=52.5, tlen=213, kl=0.0349, act_lr=1e-6, ent=0.919]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:30<00:07,  1.16it/s, pg=0.0369, ret=-7.78e-5, glen=52.5, tlen=213, kl=0.0349, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 35/44 [00:31<00:07,  1.16it/s, pg=0.0421, ret=-4.6e-5, glen=52, tlen=212, kl=0.0248, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:31<00:06,  1.16it/s, pg=0.0421, ret=-4.6e-5, glen=52, tlen=212, kl=0.0248, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 36/44 [00:32<00:06,  1.16it/s, pg=-0.083, ret=0.000156, glen=51.3, tlen=211, kl=0.0202, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:32<00:06,  1.16it/s, pg=-0.083, ret=0.000156, glen=51.3, tlen=211, kl=0.0202, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 37/44 [00:33<00:06,  1.16it/s, pg=-0.0332, ret=5.66e-5, glen=51.4, tlen=211, kl=0.0291, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:33<00:05,  1.17it/s, pg=-0.0332, ret=5.66e-5, glen=51.4, tlen=211, kl=0.0291, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 38/44 [00:34<00:05,  1.17it/s, pg=-0.0604, ret=7.43e-5, glen=51.7, tlen=211, kl=0.0493, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=-0.0604, ret=7.43e-5, glen=51.7, tlen=211, kl=0.0493, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 39/44 [00:34<00:04,  1.17it/s, pg=0.0312, ret=-3.95e-5, glen=51.3, tlen=212, kl=0.0294, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:34<00:03,  1.17it/s, pg=0.0312, ret=-3.95e-5, glen=51.3, tlen=212, kl=0.0294, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 40/44 [00:35<00:03,  1.17it/s, pg=-0.0474, ret=4.87e-5, glen=52.3, tlen=213, kl=0.0357, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:35<00:02,  1.17it/s, pg=-0.0474, ret=4.87e-5, glen=52.3, tlen=213, kl=0.0357, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 41/44 [00:36<00:02,  1.17it/s, pg=0.0608, ret=-7.96e-5, glen=51.4, tlen=212, kl=0.0248, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:36<00:01,  1.17it/s, pg=0.0608, ret=-7.96e-5, glen=51.4, tlen=212, kl=0.0248, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 42/44 [00:37<00:01,  1.17it/s, pg=-0.0695, ret=0.00013, glen=52.3, tlen=213, kl=0.0311, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:37<00:00,  1.17it/s, pg=-0.0695, ret=0.00013, glen=52.3, tlen=213, kl=0.0311, act_lr=1e-6, ent=0.914]
2025-07-24 20:37:24.642 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.43s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.17it/s, pg=-0.0265, ret=6.83e-5, glen=52.3, tlen=213, kl=0.0305, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 43/44 [00:38<00:00,  1.12it/s, pg=-0.0265, ret=6.83e-5, glen=52.3, tlen=213, kl=0.0305, act_lr=1e-6, ent=0.921]
2025-07-24 20:37:25.308 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:37:27.511 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.20s
2025-07-24 20:37:27.864 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 41.77s
2025-07-24 20:37:27.870 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0007497614080255681, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9101240838115866, 'kl': 0.02840423583984375, 'response_length': 52.077585220336914, 'total_length': 212.26270155473188, 'teacher_total_length': 224.32028753107244, 'return': -6.037128266391598e-07, 'policy_update_steps': 1.0}
Episode [9/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [06:55<15:33, 103.68s/it]2025-07-24 20:37:27.919 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:38:00.725 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:38:00.956 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.23s
2025-07-24 20:38:00.957 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.04s
2025-07-24 20:38:02.759 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0012,avg_pass_at_n: 1.0000,avg_num_tokens: 53.8521,std_num_tokens: 13.0744,avg_correct_num_tokens: 53.8254,std_correct_num_tokens: 13.0503,avg_incorrect_num_tokens: 56.9014,std_incorrect_num_tokens: 15.2834
2025-07-24 20:38:03.185 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.23s
2025-07-24 20:38:06.138 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.95s
2025-07-24 20:38:29.248 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 177
2025-07-24 20:38:29.249 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.11s
2025-07-24 20:38:30.588 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 20:38:30.589 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -7.313349952468764e-06, avg_kl: 0.033316574527718926, avg_response_length: 53.87144899098887, avg_orm_score: 0.0, avg_custom_rewards: -7.313349952468764e-06
2025-07-24 20:38:30.618 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter108_replay_buffer.jsonl
2025-07-24 20:38:31.793 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0365, ret=6.04e-5, glen=55.1, tlen=216, kl=0.0237, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.0365, ret=6.04e-5, glen=55.1, tlen=216, kl=0.0237, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.00806, ret=4.2e-5, glen=54, tlen=215, kl=0.0254, act_lr=1e-6, ent=0.933]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=0.00806, ret=4.2e-5, glen=54, tlen=215, kl=0.0254, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.12, ret=0.000231, glen=53.2, tlen=214, kl=0.0385, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.12it/s, pg=-0.12, ret=0.000231, glen=53.2, tlen=214, kl=0.0385, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.12it/s, pg=0.00781, ret=-1.29e-5, glen=55.2, tlen=216, kl=0.028, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:37,  1.10it/s, pg=0.00781, ret=-1.29e-5, glen=55.2, tlen=216, kl=0.028, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:37,  1.10it/s, pg=0.000488, ret=-5.71e-5, glen=54.2, tlen=215, kl=0.0352, act_lr=1e-6, ent=0.87]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.12it/s, pg=0.000488, ret=-5.71e-5, glen=54.2, tlen=215, kl=0.0352, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.12it/s, pg=-0.101, ret=0.000197, glen=53.7, tlen=214, kl=0.0409, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.14it/s, pg=-0.101, ret=0.000197, glen=53.7, tlen=214, kl=0.0409, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.14it/s, pg=-0.0885, ret=0.000171, glen=53.1, tlen=214, kl=0.057, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.15it/s, pg=-0.0885, ret=0.000171, glen=53.1, tlen=214, kl=0.057, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.15it/s, pg=0.135, ret=-0.000279, glen=54.6, tlen=216, kl=0.0346, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.16it/s, pg=0.135, ret=-0.000279, glen=54.6, tlen=216, kl=0.0346, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.16it/s, pg=0.134, ret=-0.000266, glen=53, tlen=214, kl=0.0325, act_lr=1e-6, ent=0.936]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=0.134, ret=-0.000266, glen=53, tlen=214, kl=0.0325, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=-0.00305, ret=-4.62e-5, glen=54, tlen=215, kl=0.0352, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.14it/s, pg=-0.00305, ret=-4.62e-5, glen=54, tlen=215, kl=0.0352, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.14it/s, pg=-0.0965, ret=0.000186, glen=54, tlen=215, kl=0.0265, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:30,  1.13it/s, pg=-0.0965, ret=0.000186, glen=54, tlen=215, kl=0.0265, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:30,  1.13it/s, pg=-0.037, ret=6.29e-5, glen=55.1, tlen=216, kl=0.0301, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.14it/s, pg=-0.037, ret=6.29e-5, glen=55.1, tlen=216, kl=0.0301, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.14it/s, pg=0.117, ret=-0.000286, glen=56.8, tlen=218, kl=0.0255, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=0.117, ret=-0.000286, glen=56.8, tlen=218, kl=0.0255, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=0.126, ret=-0.000175, glen=53.7, tlen=214, kl=0.019, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=0.126, ret=-0.000175, glen=53.7, tlen=214, kl=0.019, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=0.221, ret=-0.000398, glen=53.2, tlen=213, kl=0.0246, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.16it/s, pg=0.221, ret=-0.000398, glen=53.2, tlen=213, kl=0.0246, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.16it/s, pg=-0.0923, ret=0.000164, glen=55.3, tlen=216, kl=0.0304, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.16it/s, pg=-0.0923, ret=0.000164, glen=55.3, tlen=216, kl=0.0304, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.16it/s, pg=0.0322, ret=-1.14e-5, glen=52.6, tlen=213, kl=0.0434, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.17it/s, pg=0.0322, ret=-1.14e-5, glen=52.6, tlen=213, kl=0.0434, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.17it/s, pg=-0.0222, ret=6.07e-5, glen=54.4, tlen=215, kl=0.0386, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=-0.0222, ret=6.07e-5, glen=54.4, tlen=215, kl=0.0386, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=0.114, ret=-0.000188, glen=54.6, tlen=216, kl=0.0287, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=0.114, ret=-0.000188, glen=54.6, tlen=216, kl=0.0287, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=0.0316, ret=-5.14e-5, glen=54, tlen=215, kl=0.0366, act_lr=1e-6, ent=0.904]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=0.0316, ret=-5.14e-5, glen=54, tlen=215, kl=0.0366, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=0.0309, ret=-7.65e-5, glen=52.8, tlen=214, kl=0.0292, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=0.0309, ret=-7.65e-5, glen=52.8, tlen=214, kl=0.0292, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=0.309, ret=-0.000614, glen=54.2, tlen=216, kl=0.058, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.309, ret=-0.000614, glen=54.2, tlen=216, kl=0.058, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.17it/s, pg=-0.0509, ret=7.36e-5, glen=55.1, tlen=216, kl=0.0332, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0509, ret=7.36e-5, glen=55.1, tlen=216, kl=0.0332, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0636, ret=7.74e-5, glen=54.7, tlen=216, kl=0.0227, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0636, ret=7.74e-5, glen=54.7, tlen=216, kl=0.0227, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0408, ret=4.94e-5, glen=52.1, tlen=213, kl=0.0265, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0408, ret=4.94e-5, glen=52.1, tlen=213, kl=0.0265, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0615, ret=7.77e-5, glen=51.7, tlen=212, kl=0.0438, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0615, ret=7.77e-5, glen=51.7, tlen=212, kl=0.0438, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.0117, ret=5.82e-5, glen=53.4, tlen=214, kl=0.0324, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.18it/s, pg=0.0117, ret=5.82e-5, glen=53.4, tlen=214, kl=0.0324, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.18it/s, pg=0.0592, ret=-9.2e-5, glen=53.2, tlen=214, kl=0.044, act_lr=1e-6, ent=0.892] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=0.0592, ret=-9.2e-5, glen=53.2, tlen=214, kl=0.044, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0187, ret=7.9e-5, glen=53.4, tlen=214, kl=0.0282, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0187, ret=7.9e-5, glen=53.4, tlen=214, kl=0.0282, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.105, ret=0.000203, glen=52.9, tlen=214, kl=0.0298, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.105, ret=0.000203, glen=52.9, tlen=214, kl=0.0298, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0964, ret=0.000182, glen=53.5, tlen=214, kl=0.0247, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0964, ret=0.000182, glen=53.5, tlen=214, kl=0.0247, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.0135, ret=-3.46e-5, glen=52.1, tlen=213, kl=0.0428, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.13it/s, pg=0.0135, ret=-3.46e-5, glen=52.1, tlen=213, kl=0.0428, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.0994, ret=0.000201, glen=54.1, tlen=215, kl=0.0437, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.14it/s, pg=-0.0994, ret=0.000201, glen=54.1, tlen=215, kl=0.0437, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=-0.0364, ret=4.61e-5, glen=52.8, tlen=214, kl=0.0369, act_lr=1e-6, ent=0.893] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=-0.0364, ret=4.61e-5, glen=52.8, tlen=214, kl=0.0369, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=-0.0417, ret=7.87e-5, glen=53.4, tlen=214, kl=0.0247, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0417, ret=7.87e-5, glen=53.4, tlen=214, kl=0.0247, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0286, ret=7.85e-5, glen=53.7, tlen=215, kl=0.0277, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.0286, ret=7.85e-5, glen=53.7, tlen=215, kl=0.0277, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0114, ret=-2.46e-5, glen=53.8, tlen=215, kl=0.0319, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0114, ret=-2.46e-5, glen=53.8, tlen=215, kl=0.0319, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0709, ret=0.000131, glen=54.1, tlen=215, kl=0.0333, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0709, ret=0.000131, glen=54.1, tlen=215, kl=0.0333, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0193, ret=8.31e-5, glen=55.2, tlen=216, kl=0.0303, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0193, ret=8.31e-5, glen=55.2, tlen=216, kl=0.0303, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0491, ret=-5.15e-5, glen=53.9, tlen=215, kl=0.0304, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=0.0491, ret=-5.15e-5, glen=53.9, tlen=215, kl=0.0304, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0533, ret=5.91e-5, glen=54.1, tlen=215, kl=0.0362, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0533, ret=5.91e-5, glen=54.1, tlen=215, kl=0.0362, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.00446, ret=-5.09e-5, glen=53.2, tlen=214, kl=0.0369, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.00446, ret=-5.09e-5, glen=53.2, tlen=214, kl=0.0369, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=0.0835, ret=-0.000149, glen=53.9, tlen=214, kl=0.0274, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.0835, ret=-0.000149, glen=53.9, tlen=214, kl=0.0274, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0826, ret=0.000159, glen=53.6, tlen=214, kl=0.0397, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.0826, ret=0.000159, glen=53.6, tlen=214, kl=0.0397, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.18it/s, pg=-0.0496, ret=8.01e-5, glen=54.3, tlen=215, kl=0.0363, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.0496, ret=8.01e-5, glen=54.3, tlen=215, kl=0.0363, act_lr=1e-6, ent=0.947]
2025-07-24 20:39:11.091 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.12s
2025-07-24 20:39:11.870 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.71s
2025-07-24 20:39:14.425 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.55s
2025-07-24 20:39:14.758 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.91s
2025-07-24 20:39:14.764 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.001076846652560764, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9186079435878329, 'kl': 0.033452351888020836, 'response_length': 53.8494886610243, 'total_length': 214.70554334852432, 'teacher_total_length': 226.69826117621528, 'return': 6.596100219111476e-07, 'policy_update_steps': 1.0}
Episode [9/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [08:42<13:58, 104.84s/it]2025-07-24 20:39:14.809 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:39:47.225 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:39:47.408 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:39:47.408 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.60s
[36m(get_reflection_patt pid=1437881)[0m ern_score
2025-07-24 20:39:49.342 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 53.7345,std_num_tokens: 13.0526,avg_correct_num_tokens: 53.7317,std_correct_num_tokens: 13.0127,avg_incorrect_num_tokens: 54.0400,std_incorrect_num_tokens: 16.8233
2025-07-24 20:39:49.739 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.33s
2025-07-24 20:39:52.168 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.43s
2025-07-24 20:40:15.314 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 177
2025-07-24 20:40:15.315 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.14s
2025-07-24 20:40:16.583 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 20:40:16.583 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.934898812008106e-05, avg_kl: 0.05523612674346751, avg_response_length: 53.77167241586804, avg_orm_score: 0.0, avg_custom_rewards: 1.934898812008106e-05
2025-07-24 20:40:16.621 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter109_replay_buffer.jsonl
2025-07-24 20:40:17.788 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.116, ret=0.000234, glen=53.6, tlen=213, kl=0.0864, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:45,  1.02s/it, pg=-0.116, ret=0.000234, glen=53.6, tlen=213, kl=0.0864, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:45,  1.02s/it, pg=0.0437, ret=-4.89e-5, glen=54.7, tlen=215, kl=0.0641, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=0.0437, ret=-4.89e-5, glen=54.7, tlen=215, kl=0.0641, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0172, ret=4.62e-5, glen=52.4, tlen=212, kl=0.034, act_lr=1e-6, ent=0.872] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.08it/s, pg=-0.0172, ret=4.62e-5, glen=52.4, tlen=212, kl=0.034, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.08it/s, pg=-0.0869, ret=0.000175, glen=53.9, tlen=214, kl=0.0519, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.11it/s, pg=-0.0869, ret=0.000175, glen=53.9, tlen=214, kl=0.0519, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.11it/s, pg=-0.00732, ret=1.86e-5, glen=53.8, tlen=214, kl=0.035, act_lr=1e-6, ent=0.897] Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.11it/s, pg=-0.00732, ret=1.86e-5, glen=53.8, tlen=214, kl=0.035, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.11it/s, pg=-0.0121, ret=5.3e-5, glen=53.7, tlen=214, kl=0.0452, act_lr=1e-6, ent=0.872] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.13it/s, pg=-0.0121, ret=5.3e-5, glen=53.7, tlen=214, kl=0.0452, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.13it/s, pg=0.073, ret=-0.000146, glen=54.4, tlen=215, kl=0.0504, act_lr=1e-6, ent=0.864]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=0.073, ret=-0.000146, glen=54.4, tlen=215, kl=0.0504, act_lr=1e-6, ent=0.864]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=-0.077, ret=0.000119, glen=54.1, tlen=214, kl=0.0549, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.077, ret=0.000119, glen=54.1, tlen=214, kl=0.0549, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.15it/s, pg=-0.00226, ret=-2.99e-5, glen=54, tlen=214, kl=0.0665, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=-0.00226, ret=-2.99e-5, glen=54, tlen=214, kl=0.0665, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=-0.0664, ret=9.65e-5, glen=53.8, tlen=214, kl=0.0633, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.14it/s, pg=-0.0664, ret=9.65e-5, glen=53.8, tlen=214, kl=0.0633, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.14it/s, pg=-0.0949, ret=0.000183, glen=54.6, tlen=215, kl=0.0322, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=-0.0949, ret=0.000183, glen=54.6, tlen=215, kl=0.0322, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=0.0237, ret=-0.000116, glen=53.1, tlen=214, kl=0.0566, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=0.0237, ret=-0.000116, glen=53.1, tlen=214, kl=0.0566, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=0.00409, ret=6.12e-5, glen=54.7, tlen=215, kl=0.0555, act_lr=1e-6, ent=0.875] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=0.00409, ret=6.12e-5, glen=54.7, tlen=215, kl=0.0555, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=0.0865, ret=-0.00018, glen=53.4, tlen=214, kl=0.036, act_lr=1e-6, ent=0.867] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0865, ret=-0.00018, glen=53.4, tlen=214, kl=0.036, act_lr=1e-6, ent=0.867]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=0.0825, ret=-0.000153, glen=55, tlen=215, kl=0.0437, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.0825, ret=-0.000153, glen=55, tlen=215, kl=0.0437, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.17it/s, pg=0.0712, ret=-3.7e-5, glen=53.9, tlen=214, kl=0.0309, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=0.0712, ret=-3.7e-5, glen=53.9, tlen=214, kl=0.0309, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=0.245, ret=-0.000468, glen=52, tlen=212, kl=0.056, act_lr=1e-6, ent=0.906]  Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.15it/s, pg=0.245, ret=-0.000468, glen=52, tlen=212, kl=0.056, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.15it/s, pg=-0.0334, ret=6.06e-5, glen=54.2, tlen=215, kl=0.0381, act_lr=1e-6, ent=0.866]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.16it/s, pg=-0.0334, ret=6.06e-5, glen=54.2, tlen=215, kl=0.0381, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.16it/s, pg=0.148, ret=-0.000294, glen=53.6, tlen=214, kl=0.0561, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.16it/s, pg=0.148, ret=-0.000294, glen=53.6, tlen=214, kl=0.0561, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.16it/s, pg=0.0284, ret=-0.000107, glen=55.9, tlen=216, kl=0.0585, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=0.0284, ret=-0.000107, glen=55.9, tlen=216, kl=0.0585, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.0263, ret=2.44e-5, glen=52.8, tlen=213, kl=0.0402, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.16it/s, pg=-0.0263, ret=2.44e-5, glen=52.8, tlen=213, kl=0.0402, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.16it/s, pg=-0.0139, ret=5.16e-5, glen=52.3, tlen=213, kl=0.0677, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.16it/s, pg=-0.0139, ret=5.16e-5, glen=52.3, tlen=213, kl=0.0677, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.16it/s, pg=0.0867, ret=-0.000172, glen=53.3, tlen=213, kl=0.0453, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=0.0867, ret=-0.000172, glen=53.3, tlen=213, kl=0.0453, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0571, ret=9.52e-5, glen=54.3, tlen=215, kl=0.0717, act_lr=1e-6, ent=0.88]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0571, ret=9.52e-5, glen=54.3, tlen=215, kl=0.0717, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.196, ret=-0.000284, glen=54, tlen=214, kl=0.0304, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.196, ret=-0.000284, glen=54, tlen=214, kl=0.0304, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0216, ret=6.21e-5, glen=53.7, tlen=214, kl=0.0591, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0216, ret=6.21e-5, glen=53.7, tlen=214, kl=0.0591, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.00256, ret=-7.38e-6, glen=55.4, tlen=216, kl=0.0557, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.00256, ret=-7.38e-6, glen=55.4, tlen=216, kl=0.0557, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0284, ret=6.27e-5, glen=53.4, tlen=213, kl=0.0356, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0284, ret=6.27e-5, glen=53.4, tlen=213, kl=0.0356, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.079, ret=-0.000167, glen=54.9, tlen=215, kl=0.114, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=0.079, ret=-0.000167, glen=54.9, tlen=215, kl=0.114, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=0.00616, ret=-2.12e-5, glen=52.3, tlen=213, kl=0.0785, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=0.00616, ret=-2.12e-5, glen=52.3, tlen=213, kl=0.0785, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0286, ret=6.86e-5, glen=53.9, tlen=214, kl=0.0439, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0286, ret=6.86e-5, glen=53.9, tlen=214, kl=0.0439, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=-0.0802, ret=0.000114, glen=53.3, tlen=214, kl=0.0758, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0802, ret=0.000114, glen=53.3, tlen=214, kl=0.0758, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0322, ret=5.99e-5, glen=52.9, tlen=213, kl=0.0457, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0322, ret=5.99e-5, glen=52.9, tlen=213, kl=0.0457, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.0248, ret=-2.03e-5, glen=55.1, tlen=216, kl=0.0465, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.0248, ret=-2.03e-5, glen=55.1, tlen=216, kl=0.0465, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.028, ret=4.99e-5, glen=53.1, tlen=213, kl=0.0916, act_lr=1e-6, ent=0.892] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.028, ret=4.99e-5, glen=53.1, tlen=213, kl=0.0916, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.04, ret=8.84e-5, glen=53.4, tlen=213, kl=0.0301, act_lr=1e-6, ent=0.904] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.04, ret=8.84e-5, glen=53.4, tlen=213, kl=0.0301, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=-0.0391, ret=4.82e-5, glen=54.7, tlen=215, kl=0.037, act_lr=1e-6, ent=0.87]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0391, ret=4.82e-5, glen=54.7, tlen=215, kl=0.037, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.122, ret=0.000225, glen=54.5, tlen=215, kl=0.0511, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.122, ret=0.000225, glen=54.5, tlen=215, kl=0.0511, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0126, ret=2.85e-5, glen=52.7, tlen=213, kl=0.0312, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0126, ret=2.85e-5, glen=52.7, tlen=213, kl=0.0312, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0392, ret=7.02e-5, glen=52.9, tlen=213, kl=0.0359, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0392, ret=7.02e-5, glen=52.9, tlen=213, kl=0.0359, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.0172, ret=-5.31e-5, glen=54, tlen=214, kl=0.127, act_lr=1e-6, ent=0.917]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=0.0172, ret=-5.31e-5, glen=54, tlen=214, kl=0.127, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0985, ret=0.000193, glen=53.6, tlen=214, kl=0.095, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0985, ret=0.000193, glen=53.6, tlen=214, kl=0.095, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=0.0892, ret=-0.000157, glen=53.5, tlen=213, kl=0.0643, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=0.0892, ret=-0.000157, glen=53.5, tlen=213, kl=0.0643, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=-0.0697, ret=0.00012, glen=52.7, tlen=213, kl=0.0454, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.0697, ret=0.00012, glen=52.7, tlen=213, kl=0.0454, act_lr=1e-6, ent=0.919]
2025-07-24 20:40:57.129 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.18it/s, pg=-0.0677, ret=0.000108, glen=55.6, tlen=216, kl=0.0308, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0677, ret=0.000108, glen=55.6, tlen=216, kl=0.0308, act_lr=1e-6, ent=0.877]
2025-07-24 20:40:57.981 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 20:40:59.875 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 1.89s
2025-07-24 20:41:00.223 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.39s
2025-07-24 20:41:00.229 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00035976833767361113, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8938758982552423, 'kl': 0.05476311598387029, 'response_length': 53.79958648681641, 'total_length': 214.01390041775173, 'teacher_total_length': 225.97880316840278, 'return': 1.2616819326972797e-06, 'policy_update_steps': 1.0}
Episode [9/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [10:27<12:15, 105.05s/it]2025-07-24 20:41:00.234 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435059)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:34,  1.80it/s, est. speed input: 323.71 toks/s, output: 34.17 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 163/172 [00:02<00:00, 107.75it/s, est. speed input: 12533.94 toks/s, output: 3559.37 toks/s]
2025-07-24 20:41:04.327 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 254.5298,strategyqa_test/accuracy: 0.5197,eval_accuracy: 0.5197
2025-07-24 20:41:04.599 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:41:37.382 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:41:37.554 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:41:37.555 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.96s
2025-07-24 20:41:39.230 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0150,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 54.5310,std_num_tokens: 13.3825,avg_correct_num_tokens: 54.5409,std_correct_num_tokens: 13.3917,avg_incorrect_num_tokens: 53.4167,std_incorrect_num_tokens: 12.2460
2025-07-24 20:41:39.650 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.09s
2025-07-24 20:41:42.313 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.66s
2025-07-24 20:42:05.350 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 20:42:05.350 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.03s
2025-07-24 20:42:06.608 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 20:42:06.609 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.709702913011058e-05, avg_kl: 0.04073582338483146, avg_response_length: 54.528992963640874, avg_orm_score: 0.0, avg_custom_rewards: 2.709702913011058e-05
2025-07-24 20:42:06.635 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter110_replay_buffer.jsonl
2025-07-24 20:42:07.822 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:02<00:00, 88.64it/s, est. speed input: 11729.91 toks/s, output: 3402.08 toks/s][32m [repeated 53x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 52.67it/s, est. speed input: 9570.87 toks/s, output: 2945.91 toks/s] [32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=0.0262, ret=-4.46e-5, glen=54.1, tlen=215, kl=0.0287, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.0262, ret=-4.46e-5, glen=54.1, tlen=215, kl=0.0287, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.035, ret=7.11e-5, glen=54.4, tlen=215, kl=0.0308, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.035, ret=7.11e-5, glen=54.4, tlen=215, kl=0.0308, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=0.0435, ret=-0.000142, glen=52.6, tlen=213, kl=0.0328, act_lr=1e-6, ent=0.857]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.08it/s, pg=0.0435, ret=-0.000142, glen=52.6, tlen=213, kl=0.0328, act_lr=1e-6, ent=0.857]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.08it/s, pg=0.01, ret=-1.36e-5, glen=54.8, tlen=215, kl=0.0412, act_lr=1e-6, ent=0.923]   Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:37,  1.11it/s, pg=0.01, ret=-1.36e-5, glen=54.8, tlen=215, kl=0.0412, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:37,  1.11it/s, pg=-0.087, ret=0.000179, glen=54.7, tlen=215, kl=0.0369, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.10it/s, pg=-0.087, ret=0.000179, glen=54.7, tlen=215, kl=0.0369, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.10it/s, pg=-0.0469, ret=8.49e-5, glen=54.4, tlen=214, kl=0.0268, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.12it/s, pg=-0.0469, ret=8.49e-5, glen=54.4, tlen=214, kl=0.0268, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.12it/s, pg=-0.0219, ret=5.76e-5, glen=53.7, tlen=214, kl=0.0694, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=-0.0219, ret=5.76e-5, glen=53.7, tlen=214, kl=0.0694, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=0.0593, ret=-0.000129, glen=55.5, tlen=216, kl=0.0763, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:33,  1.12it/s, pg=0.0593, ret=-0.000129, glen=55.5, tlen=216, kl=0.0763, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:33,  1.12it/s, pg=0.0421, ret=-5.39e-5, glen=56.6, tlen=217, kl=0.0619, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=0.0421, ret=-5.39e-5, glen=56.6, tlen=217, kl=0.0619, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.13it/s, pg=-0.00452, ret=-3.76e-7, glen=55.4, tlen=216, kl=0.04, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.14it/s, pg=-0.00452, ret=-3.76e-7, glen=55.4, tlen=216, kl=0.04, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.14it/s, pg=-0.0083, ret=6.29e-5, glen=56, tlen=216, kl=0.0339, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=-0.0083, ret=6.29e-5, glen=56, tlen=216, kl=0.0339, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=-0.00494, ret=2.81e-5, glen=54, tlen=214, kl=0.0315, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.14it/s, pg=-0.00494, ret=2.81e-5, glen=54, tlen=214, kl=0.0315, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.14it/s, pg=-0.0585, ret=9.72e-5, glen=54.3, tlen=215, kl=0.0529, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=-0.0585, ret=9.72e-5, glen=54.3, tlen=215, kl=0.0529, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=-0.106, ret=0.000211, glen=54.4, tlen=215, kl=0.0264, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:27,  1.14it/s, pg=-0.106, ret=0.000211, glen=54.4, tlen=215, kl=0.0264, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:27,  1.14it/s, pg=-0.0189, ret=7.02e-5, glen=54.9, tlen=215, kl=0.0338, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.15it/s, pg=-0.0189, ret=7.02e-5, glen=54.9, tlen=215, kl=0.0338, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:26,  1.15it/s, pg=0.191, ret=-0.000359, glen=54.7, tlen=216, kl=0.0586, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.15it/s, pg=0.191, ret=-0.000359, glen=54.7, tlen=216, kl=0.0586, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:15<00:25,  1.15it/s, pg=0.000122, ret=6.64e-5, glen=54.4, tlen=215, kl=0.03, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.16it/s, pg=0.000122, ret=6.64e-5, glen=54.4, tlen=215, kl=0.03, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.16it/s, pg=0.0879, ret=-0.000199, glen=54.8, tlen=216, kl=0.0339, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.16it/s, pg=0.0879, ret=-0.000199, glen=54.8, tlen=216, kl=0.0339, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.16it/s, pg=0.0664, ret=-0.000126, glen=54, tlen=214, kl=0.0791, act_lr=1e-6, ent=0.906]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=0.0664, ret=-0.000126, glen=54, tlen=214, kl=0.0791, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0398, ret=4.69e-5, glen=53.6, tlen=214, kl=0.0707, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0398, ret=4.69e-5, glen=53.6, tlen=214, kl=0.0707, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.0304, ret=6.99e-5, glen=53.9, tlen=214, kl=0.0249, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.0304, ret=6.99e-5, glen=53.9, tlen=214, kl=0.0249, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.0874, ret=0.000271, glen=53, tlen=213, kl=0.0737, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0874, ret=0.000271, glen=53, tlen=213, kl=0.0737, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.17it/s, pg=-0.0481, ret=0.000106, glen=53.6, tlen=214, kl=0.0278, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0481, ret=0.000106, glen=53.6, tlen=214, kl=0.0278, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=0.127, ret=-0.000263, glen=53.5, tlen=214, kl=0.0329, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=0.127, ret=-0.000263, glen=53.5, tlen=214, kl=0.0329, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.00623, ret=4.25e-5, glen=55.5, tlen=216, kl=0.0253, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.00623, ret=4.25e-5, glen=55.5, tlen=216, kl=0.0253, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=0.00061, ret=-8.28e-6, glen=52.1, tlen=213, kl=0.0321, act_lr=1e-6, ent=0.9] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=0.00061, ret=-8.28e-6, glen=52.1, tlen=213, kl=0.0321, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.0266, ret=-5.91e-5, glen=55.6, tlen=216, kl=0.0271, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.18it/s, pg=0.0266, ret=-5.91e-5, glen=55.6, tlen=216, kl=0.0271, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.18it/s, pg=-0.108, ret=0.000212, glen=54.9, tlen=216, kl=0.0425, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.18it/s, pg=-0.108, ret=0.000212, glen=54.9, tlen=216, kl=0.0425, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.18it/s, pg=-0.0599, ret=0.000123, glen=53.6, tlen=214, kl=0.0414, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0599, ret=0.000123, glen=53.6, tlen=214, kl=0.0414, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0137, ret=3.15e-5, glen=54.8, tlen=215, kl=0.043, act_lr=1e-6, ent=0.949]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0137, ret=3.15e-5, glen=54.8, tlen=215, kl=0.043, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=0.198, ret=-0.000452, glen=54.7, tlen=215, kl=0.0604, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.198, ret=-0.000452, glen=54.7, tlen=215, kl=0.0604, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=0.00415, ret=-3.51e-5, glen=55, tlen=215, kl=0.0546, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.00415, ret=-3.51e-5, glen=55, tlen=215, kl=0.0546, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0995, ret=0.000195, glen=53.8, tlen=214, kl=0.0479, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0995, ret=0.000195, glen=53.8, tlen=214, kl=0.0479, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.00427, ret=-2.36e-5, glen=52.7, tlen=213, kl=0.0451, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.00427, ret=-2.36e-5, glen=52.7, tlen=213, kl=0.0451, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.0978, ret=-0.000157, glen=52.4, tlen=212, kl=0.0425, act_lr=1e-6, ent=0.88] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.0978, ret=-0.000157, glen=52.4, tlen=212, kl=0.0425, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.0583, ret=-7.85e-5, glen=54.6, tlen=215, kl=0.0263, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=0.0583, ret=-7.85e-5, glen=54.6, tlen=215, kl=0.0263, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=-0.0507, ret=4.28e-5, glen=55.8, tlen=216, kl=0.0413, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0507, ret=4.28e-5, glen=55.8, tlen=216, kl=0.0413, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=0.0198, ret=-8.11e-5, glen=55.1, tlen=216, kl=0.0274, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0198, ret=-8.11e-5, glen=55.1, tlen=216, kl=0.0274, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:05,  1.17it/s, pg=-0.00171, ret=4.8e-5, glen=55.3, tlen=216, kl=0.0414, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.00171, ret=4.8e-5, glen=55.3, tlen=216, kl=0.0414, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0559, ret=9e-5, glen=55, tlen=215, kl=0.033, act_lr=1e-6, ent=0.917]      Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0559, ret=9e-5, glen=55, tlen=215, kl=0.033, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0292, ret=3.51e-5, glen=54.2, tlen=215, kl=0.0254, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0292, ret=3.51e-5, glen=54.2, tlen=215, kl=0.0254, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=0.0271, ret=-3.36e-5, glen=54.7, tlen=215, kl=0.0275, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=0.0271, ret=-3.36e-5, glen=54.7, tlen=215, kl=0.0275, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=0.053, ret=-3.89e-5, glen=58.4, tlen=220, kl=0.0339, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.053, ret=-3.89e-5, glen=58.4, tlen=220, kl=0.0339, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0986, ret=0.000186, glen=53.9, tlen=214, kl=0.0347, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0986, ret=0.000186, glen=53.9, tlen=214, kl=0.0347, act_lr=1e-6, ent=0.9]
2025-07-24 20:42:47.197 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.20s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=0.0083, ret=-5.15e-5, glen=55, tlen=215, kl=0.0312, act_lr=1e-6, ent=0.908] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=0.0083, ret=-5.15e-5, glen=55, tlen=215, kl=0.0312, act_lr=1e-6, ent=0.908]
2025-07-24 20:42:47.868 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:42:50.079 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.21s
2025-07-24 20:42:50.434 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.56s
2025-07-24 20:42:50.440 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0009649488661024305, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9082330783208211, 'kl': 0.040863715277777775, 'response_length': 54.50311838785807, 'total_length': 214.88553636338975, 'teacher_total_length': 226.92497219509548, 'return': 1.7241635456836472e-06, 'policy_update_steps': 1.0}
Episode [9/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [12:17<10:40, 106.74s/it]2025-07-24 20:42:50.490 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:43:23.777 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:43:23.951 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:43:23.951 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.46s
2025-07-24 20:43:25.654 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 55.9166,std_num_tokens: 13.1977,avg_correct_num_tokens: 55.8960,std_correct_num_tokens: 13.1988,avg_incorrect_num_tokens: 58.8103,std_incorrect_num_tokens: 12.7082
2025-07-24 20:43:26.063 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.11s
2025-07-24 20:43:28.527 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.46s
2025-07-24 20:43:51.679 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 20:43:51.679 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.15s
2025-07-24 20:43:52.925 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 20:43:52.925 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -2.057726097324591e-05, avg_kl: 0.04351635193556882, avg_response_length: 55.940149907315714, avg_orm_score: 0.0, avg_custom_rewards: -2.057726097324591e-05
2025-07-24 20:43:52.962 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter111_replay_buffer.jsonl
2025-07-24 20:43:54.172 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=0.169, ret=-0.000302, glen=57.3, tlen=218, kl=0.0216, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.169, ret=-0.000302, glen=57.3, tlen=218, kl=0.0216, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=0.121, ret=-0.000202, glen=57, tlen=217, kl=0.042, act_lr=1e-6, ent=0.922]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=0.121, ret=-0.000202, glen=57, tlen=217, kl=0.042, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.0708, ret=0.000132, glen=55, tlen=215, kl=0.0452, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.12it/s, pg=-0.0708, ret=0.000132, glen=55, tlen=215, kl=0.0452, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.12it/s, pg=-0.103, ret=0.000178, glen=55.3, tlen=215, kl=0.0387, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:35,  1.14it/s, pg=-0.103, ret=0.000178, glen=55.3, tlen=215, kl=0.0387, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:35,  1.14it/s, pg=-0.00885, ret=4.25e-5, glen=56.2, tlen=216, kl=0.0515, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:34,  1.15it/s, pg=-0.00885, ret=4.25e-5, glen=56.2, tlen=216, kl=0.0515, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:34,  1.15it/s, pg=-0.0654, ret=0.000118, glen=59.3, tlen=219, kl=0.0517, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.14it/s, pg=-0.0654, ret=0.000118, glen=59.3, tlen=219, kl=0.0517, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.14it/s, pg=-0.0896, ret=0.000162, glen=54, tlen=214, kl=0.0385, act_lr=1e-6, ent=0.935]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.15it/s, pg=-0.0896, ret=0.000162, glen=54, tlen=214, kl=0.0385, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.15it/s, pg=-0.092, ret=0.000159, glen=54.9, tlen=215, kl=0.0545, act_lr=1e-6, ent=0.879]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.16it/s, pg=-0.092, ret=0.000159, glen=54.9, tlen=215, kl=0.0545, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.16it/s, pg=-0.0341, ret=5.22e-5, glen=54.8, tlen=215, kl=0.0406, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.16it/s, pg=-0.0341, ret=5.22e-5, glen=54.8, tlen=215, kl=0.0406, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.16it/s, pg=-0.0259, ret=1.95e-5, glen=56.9, tlen=217, kl=0.0479, act_lr=1e-6, ent=0.857]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=-0.0259, ret=1.95e-5, glen=56.9, tlen=217, kl=0.0479, act_lr=1e-6, ent=0.857]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=-0.0264, ret=1.34e-5, glen=54.1, tlen=214, kl=0.0743, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.17it/s, pg=-0.0264, ret=1.34e-5, glen=54.1, tlen=214, kl=0.0743, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.17it/s, pg=-0.00781, ret=3.14e-5, glen=57.3, tlen=218, kl=0.0255, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.00781, ret=3.14e-5, glen=57.3, tlen=218, kl=0.0255, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=0.0166, ret=-9.08e-5, glen=54.3, tlen=214, kl=0.0604, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=0.0166, ret=-9.08e-5, glen=54.3, tlen=214, kl=0.0604, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=-0.017, ret=6.33e-5, glen=55.4, tlen=215, kl=0.0549, act_lr=1e-6, ent=0.904] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.017, ret=6.33e-5, glen=55.4, tlen=215, kl=0.0549, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0782, ret=-0.00011, glen=55.8, tlen=216, kl=0.0298, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:12<00:25,  1.17it/s, pg=0.0782, ret=-0.00011, glen=55.8, tlen=216, kl=0.0298, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.014, ret=8.69e-6, glen=54.9, tlen=215, kl=0.0368, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=-0.014, ret=8.69e-6, glen=54.9, tlen=215, kl=0.0368, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0806, ret=0.000136, glen=53.5, tlen=213, kl=0.0322, act_lr=1e-6, ent=0.863]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.18it/s, pg=-0.0806, ret=0.000136, glen=53.5, tlen=213, kl=0.0322, act_lr=1e-6, ent=0.863]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.18it/s, pg=0.0726, ret=-0.000108, glen=54.2, tlen=213, kl=0.0393, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:22,  1.17it/s, pg=0.0726, ret=-0.000108, glen=54.2, tlen=213, kl=0.0393, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:22,  1.17it/s, pg=-0.0913, ret=0.000159, glen=55.8, tlen=216, kl=0.0455, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.18it/s, pg=-0.0913, ret=0.000159, glen=55.8, tlen=216, kl=0.0455, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.18it/s, pg=0.073, ret=-0.000195, glen=57.7, tlen=217, kl=0.0359, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.18it/s, pg=0.073, ret=-0.000195, glen=57.7, tlen=217, kl=0.0359, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.18it/s, pg=-0.0841, ret=0.000155, glen=55.3, tlen=215, kl=0.0291, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.18it/s, pg=-0.0841, ret=0.000155, glen=55.3, tlen=215, kl=0.0291, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.18it/s, pg=0.0673, ret=-0.00013, glen=55.5, tlen=215, kl=0.0292, act_lr=1e-6, ent=0.897] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:18<00:19,  1.18it/s, pg=0.0673, ret=-0.00013, glen=55.5, tlen=215, kl=0.0292, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.18it/s, pg=-0.00775, ret=2.68e-6, glen=57.1, tlen=217, kl=0.0457, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.18it/s, pg=-0.00775, ret=2.68e-6, glen=57.1, tlen=217, kl=0.0457, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.18it/s, pg=0.0162, ret=-3.83e-5, glen=56.2, tlen=217, kl=0.0465, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.18it/s, pg=0.0162, ret=-3.83e-5, glen=56.2, tlen=217, kl=0.0465, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.18it/s, pg=-0.0101, ret=4.45e-5, glen=56.8, tlen=217, kl=0.0371, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:16,  1.18it/s, pg=-0.0101, ret=4.45e-5, glen=56.8, tlen=217, kl=0.0371, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:16,  1.18it/s, pg=-0.046, ret=9.19e-5, glen=54.9, tlen=215, kl=0.0374, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.16it/s, pg=-0.046, ret=9.19e-5, glen=54.9, tlen=215, kl=0.0374, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.16it/s, pg=0.0886, ret=-0.000198, glen=57, tlen=217, kl=0.025, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.16it/s, pg=0.0886, ret=-0.000198, glen=57, tlen=217, kl=0.025, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.16it/s, pg=-0.0847, ret=0.000148, glen=56.4, tlen=216, kl=0.03, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0847, ret=0.000148, glen=56.4, tlen=216, kl=0.03, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.1, ret=-0.00021, glen=55.3, tlen=216, kl=0.0297, act_lr=1e-6, ent=0.894] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=0.1, ret=-0.00021, glen=55.3, tlen=216, kl=0.0297, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0186, ret=5.01e-5, glen=56.2, tlen=216, kl=0.0371, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0186, ret=5.01e-5, glen=56.2, tlen=216, kl=0.0371, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.103, ret=0.000182, glen=55.5, tlen=215, kl=0.0307, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:26<00:12,  1.12it/s, pg=-0.103, ret=0.000182, glen=55.5, tlen=215, kl=0.0307, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0901, ret=0.000161, glen=56.1, tlen=216, kl=0.164, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.0901, ret=0.000161, glen=56.1, tlen=216, kl=0.164, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=0.105, ret=-0.000191, glen=55.7, tlen=216, kl=0.0266, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=0.105, ret=-0.000191, glen=55.7, tlen=216, kl=0.0266, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.0688, ret=-0.000105, glen=57.3, tlen=217, kl=0.0329, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.0688, ret=-0.000105, glen=57.3, tlen=217, kl=0.0329, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.187, ret=-0.000219, glen=56.8, tlen=217, kl=0.06, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.187, ret=-0.000219, glen=56.8, tlen=217, kl=0.06, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0854, ret=0.000148, glen=55.9, tlen=216, kl=0.054, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0854, ret=0.000148, glen=55.9, tlen=216, kl=0.054, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=0.0312, ret=-4.25e-5, glen=55.6, tlen=216, kl=0.0296, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:31<00:06,  1.17it/s, pg=0.0312, ret=-4.25e-5, glen=55.6, tlen=216, kl=0.0296, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.121, ret=-0.000219, glen=58.3, tlen=218, kl=0.0309, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=0.121, ret=-0.000219, glen=58.3, tlen=218, kl=0.0309, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0303, ret=2.63e-5, glen=55.2, tlen=215, kl=0.0496, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0303, ret=2.63e-5, glen=55.2, tlen=215, kl=0.0496, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0308, ret=-8.36e-5, glen=54.8, tlen=215, kl=0.0239, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.18it/s, pg=0.0308, ret=-8.36e-5, glen=54.8, tlen=215, kl=0.0239, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.18it/s, pg=-0.0263, ret=4.67e-5, glen=56.1, tlen=216, kl=0.0236, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.18it/s, pg=-0.0263, ret=4.67e-5, glen=56.1, tlen=216, kl=0.0236, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.18it/s, pg=-0.0863, ret=0.000149, glen=56, tlen=216, kl=0.0443, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0863, ret=0.000149, glen=56, tlen=216, kl=0.0443, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0133, ret=4.72e-5, glen=55.9, tlen=216, kl=0.0991, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0133, ret=4.72e-5, glen=55.9, tlen=216, kl=0.0991, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.00659, ret=-4.91e-5, glen=56.9, tlen=217, kl=0.0355, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:37<00:00,  1.17it/s, pg=0.00659, ret=-4.91e-5, glen=56.9, tlen=217, kl=0.0355, act_lr=1e-6, ent=0.919]
2025-07-24 20:44:33.314 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.93s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=0.0608, ret=-8.55e-5, glen=56.7, tlen=217, kl=0.0298, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=0.0608, ret=-8.55e-5, glen=56.7, tlen=217, kl=0.0298, act_lr=1e-6, ent=0.925]
2025-07-24 20:44:33.991 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:44:36.334 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.34s
2025-07-24 20:44:36.683 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.42s
2025-07-24 20:44:36.689 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 2.44140625e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9055470241440667, 'kl': 0.04328715006510417, 'response_length': 55.9408090379503, 'total_length': 215.90585666232639, 'teacher_total_length': 227.917579820421, 'return': -1.2051044905092566e-06, 'policy_update_steps': 1.0}
Episode [9/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [14:03<08:52, 106.58s/it]2025-07-24 20:44:36.732 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:45:11.052 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:45:11.225 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:45:11.225 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.49s
2025-07-24 20:45:12.929 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0009,avg_pass_at_n: 1.0000,avg_num_tokens: 57.1942,std_num_tokens: 13.4884,avg_correct_num_tokens: 57.1670,std_correct_num_tokens: 13.4523,avg_incorrect_num_tokens: 60.1053,std_incorrect_num_tokens: 16.6485
2025-07-24 20:45:13.375 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.15s
2025-07-24 20:45:16.163 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.78s
2025-07-24 20:45:39.297 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 180
2025-07-24 20:45:39.298 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.13s
2025-07-24 20:45:40.505 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 20:45:40.506 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.0494784004986288e-05, avg_kl: 0.05217929416232639, avg_response_length: 57.19199589623345, avg_orm_score: 0.0, avg_custom_rewards: 3.0494784004986288e-05
2025-07-24 20:45:40.537 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter112_replay_buffer.jsonl
2025-07-24 20:45:41.763 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.23s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=0.133, ret=-0.000305, glen=57.1, tlen=217, kl=0.0357, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.01it/s, pg=0.133, ret=-0.000305, glen=57.1, tlen=217, kl=0.0357, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.01it/s, pg=0.0413, ret=-2.91e-5, glen=56.7, tlen=217, kl=0.0301, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=0.0413, ret=-2.91e-5, glen=56.7, tlen=217, kl=0.0301, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.113, ret=0.000211, glen=57.5, tlen=218, kl=0.0343, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:39,  1.07it/s, pg=-0.113, ret=0.000211, glen=57.5, tlen=218, kl=0.0343, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:39,  1.07it/s, pg=-0.0408, ret=5.63e-5, glen=57, tlen=217, kl=0.0565, act_lr=1e-6, ent=0.884]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:38,  1.07it/s, pg=-0.0408, ret=5.63e-5, glen=57, tlen=217, kl=0.0565, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:38,  1.07it/s, pg=0.132, ret=-0.000281, glen=57.7, tlen=218, kl=0.0413, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:36,  1.10it/s, pg=0.132, ret=-0.000281, glen=57.7, tlen=218, kl=0.0413, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:36,  1.10it/s, pg=0.00122, ret=6.68e-6, glen=57.4, tlen=218, kl=0.0446, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:35,  1.09it/s, pg=0.00122, ret=6.68e-6, glen=57.4, tlen=218, kl=0.0446, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:35,  1.09it/s, pg=-0.0299, ret=0.000105, glen=58.1, tlen=218, kl=0.0512, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:34,  1.11it/s, pg=-0.0299, ret=0.000105, glen=58.1, tlen=218, kl=0.0512, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:34,  1.11it/s, pg=-0.0988, ret=0.000194, glen=57.3, tlen=218, kl=0.0395, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=-0.0988, ret=0.000194, glen=57.3, tlen=218, kl=0.0395, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.13it/s, pg=-0.00293, ret=-6.4e-5, glen=56.9, tlen=217, kl=0.0579, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=-0.00293, ret=-6.4e-5, glen=56.9, tlen=217, kl=0.0579, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=-0.086, ret=0.00018, glen=57.8, tlen=218, kl=0.0446, act_lr=1e-6, ent=0.896]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=-0.086, ret=0.00018, glen=57.8, tlen=218, kl=0.0446, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=0.0928, ret=-4.62e-5, glen=56.2, tlen=216, kl=0.04, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=0.0928, ret=-4.62e-5, glen=56.2, tlen=216, kl=0.04, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.0857, ret=0.000168, glen=59.1, tlen=220, kl=0.0366, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.14it/s, pg=-0.0857, ret=0.000168, glen=59.1, tlen=220, kl=0.0366, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.14it/s, pg=-0.105, ret=0.000217, glen=57.6, tlen=218, kl=0.0305, act_lr=1e-6, ent=0.881] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:28,  1.14it/s, pg=-0.105, ret=0.000217, glen=57.6, tlen=218, kl=0.0305, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:28,  1.14it/s, pg=0.123, ret=-0.000255, glen=55.6, tlen=216, kl=0.0821, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.15it/s, pg=0.123, ret=-0.000255, glen=55.6, tlen=216, kl=0.0821, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.15it/s, pg=0.066, ret=-0.00024, glen=56.9, tlen=217, kl=0.0682, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.16it/s, pg=0.066, ret=-0.00024, glen=56.9, tlen=217, kl=0.0682, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.16it/s, pg=-0.0403, ret=8.64e-5, glen=59.2, tlen=219, kl=0.069, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.16it/s, pg=-0.0403, ret=8.64e-5, glen=59.2, tlen=219, kl=0.069, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:15<00:24,  1.16it/s, pg=-0.115, ret=0.00022, glen=55.3, tlen=215, kl=0.0422, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.14it/s, pg=-0.115, ret=0.00022, glen=55.3, tlen=215, kl=0.0422, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.14it/s, pg=-0.0332, ret=9.23e-5, glen=57.5, tlen=218, kl=0.0346, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.15it/s, pg=-0.0332, ret=9.23e-5, glen=57.5, tlen=218, kl=0.0346, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.15it/s, pg=0.00922, ret=-4.14e-5, glen=56.7, tlen=217, kl=0.0308, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.16it/s, pg=0.00922, ret=-4.14e-5, glen=56.7, tlen=217, kl=0.0308, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.16it/s, pg=-0.0258, ret=3.88e-5, glen=55.8, tlen=216, kl=0.0545, act_lr=1e-6, ent=0.897] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=-0.0258, ret=3.88e-5, glen=55.8, tlen=216, kl=0.0545, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=-0.0762, ret=0.000118, glen=57.6, tlen=218, kl=0.0359, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.0762, ret=0.000118, glen=57.6, tlen=218, kl=0.0359, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=0.108, ret=-0.00027, glen=58, tlen=218, kl=0.117, act_lr=1e-6, ent=0.921]     Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=0.108, ret=-0.00027, glen=58, tlen=218, kl=0.117, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.17it/s, pg=0.149, ret=-0.000285, glen=57.1, tlen=217, kl=0.0399, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=0.149, ret=-0.000285, glen=57.1, tlen=217, kl=0.0399, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:21<00:18,  1.17it/s, pg=-0.0475, ret=6.09e-5, glen=57, tlen=217, kl=0.0374, act_lr=1e-6, ent=0.933]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0475, ret=6.09e-5, glen=57, tlen=217, kl=0.0374, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.0787, ret=-7.76e-5, glen=56.7, tlen=217, kl=0.0399, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.14it/s, pg=0.0787, ret=-7.76e-5, glen=56.7, tlen=217, kl=0.0399, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.14it/s, pg=0.0491, ret=-0.000151, glen=57.3, tlen=217, kl=0.0323, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.15it/s, pg=0.0491, ret=-0.000151, glen=57.3, tlen=217, kl=0.0323, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.15it/s, pg=-0.00317, ret=-4.45e-5, glen=57.5, tlen=218, kl=0.033, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.16it/s, pg=-0.00317, ret=-4.45e-5, glen=57.5, tlen=218, kl=0.033, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.16it/s, pg=-0.0924, ret=0.000183, glen=56.5, tlen=216, kl=0.131, act_lr=1e-6, ent=0.9]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.16it/s, pg=-0.0924, ret=0.000183, glen=56.5, tlen=216, kl=0.131, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.16it/s, pg=0.198, ret=-0.000255, glen=56.9, tlen=217, kl=0.0325, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.06it/s, pg=0.198, ret=-0.000255, glen=56.9, tlen=217, kl=0.0325, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.06it/s, pg=0.0123, ret=-4.06e-5, glen=56.1, tlen=216, kl=0.149, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=0.0123, ret=-4.06e-5, glen=56.1, tlen=216, kl=0.149, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=-0.0358, ret=5.35e-5, glen=56.6, tlen=217, kl=0.0373, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.11it/s, pg=-0.0358, ret=5.35e-5, glen=56.6, tlen=217, kl=0.0373, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.11it/s, pg=0.0713, ret=-0.000146, glen=57.9, tlen=219, kl=0.0455, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=0.0713, ret=-0.000146, glen=57.9, tlen=219, kl=0.0455, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:29<00:11,  1.13it/s, pg=-0.0198, ret=5.35e-5, glen=55.8, tlen=216, kl=0.0553, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=-0.0198, ret=5.35e-5, glen=55.8, tlen=216, kl=0.0553, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.14it/s, pg=-0.0922, ret=0.000166, glen=56.9, tlen=217, kl=0.0397, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=-0.0922, ret=0.000166, glen=56.9, tlen=217, kl=0.0397, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=-0.0225, ret=8.52e-5, glen=56.2, tlen=216, kl=0.0326, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0225, ret=8.52e-5, glen=56.2, tlen=216, kl=0.0326, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.126, ret=0.000255, glen=58.2, tlen=218, kl=0.0318, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.126, ret=0.000255, glen=58.2, tlen=218, kl=0.0318, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=0.00549, ret=-1.48e-5, glen=57, tlen=217, kl=0.0344, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.00549, ret=-1.48e-5, glen=57, tlen=217, kl=0.0344, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=0.0271, ret=-2.21e-5, glen=58.1, tlen=218, kl=0.0517, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:06,  1.15it/s, pg=0.0271, ret=-2.21e-5, glen=58.1, tlen=218, kl=0.0517, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:06,  1.15it/s, pg=-0.101, ret=0.000198, glen=58, tlen=218, kl=0.0312, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.16it/s, pg=-0.101, ret=0.000198, glen=58, tlen=218, kl=0.0312, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:35<00:05,  1.16it/s, pg=0.073, ret=-0.000141, glen=57.8, tlen=218, kl=0.052, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.16it/s, pg=0.073, ret=-0.000141, glen=57.8, tlen=218, kl=0.052, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.16it/s, pg=-0.0204, ret=6.88e-5, glen=57.9, tlen=218, kl=0.033, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0204, ret=6.88e-5, glen=57.9, tlen=218, kl=0.033, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0573, ret=8.89e-5, glen=57.5, tlen=218, kl=0.0436, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0573, ret=8.89e-5, glen=57.5, tlen=218, kl=0.0436, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0583, ret=0.000108, glen=59, tlen=219, kl=0.188, act_lr=1e-6, ent=0.894]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0583, ret=0.000108, glen=59, tlen=219, kl=0.188, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=0.159, ret=1.61e-5, glen=57.1, tlen=218, kl=0.0359, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=0.159, ret=1.61e-5, glen=57.1, tlen=218, kl=0.0359, act_lr=1e-6, ent=0.947]
2025-07-24 20:46:21.373 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.44s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=0, ret=-1.29e-5, glen=55.9, tlen=216, kl=0.0635, act_lr=1e-6, ent=0.897]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=0, ret=-1.29e-5, glen=55.9, tlen=216, kl=0.0635, act_lr=1e-6, ent=0.897]
2025-07-24 20:46:22.200 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 20:46:24.796 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 20:46:25.144 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.33s
2025-07-24 20:46:25.150 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 4.238552517361111e-08, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9103015210893419, 'kl': 0.05217929416232639, 'response_length': 57.191995324028866, 'total_length': 217.42520650227866, 'teacher_total_length': 229.38905131022136, 'return': 6.843536620080057e-06, 'policy_update_steps': 1.0}
Episode [9/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [15:52<07:08, 107.17s/it]2025-07-24 20:46:25.194 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:47:00.536 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:47:00.800 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.26s
2025-07-24 20:47:00.800 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.61s
2025-07-24 20:47:02.802 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 59.9698,std_num_tokens: 13.8100,avg_correct_num_tokens: 59.9575,std_correct_num_tokens: 13.8030,avg_incorrect_num_tokens: 61.8333,std_incorrect_num_tokens: 14.7054
2025-07-24 20:47:03.218 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.42s
2025-07-24 20:47:05.741 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.52s
2025-07-24 20:47:29.373 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 20:47:29.374 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.63s
2025-07-24 20:47:30.641 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.88s
2025-07-24 20:47:30.641 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4594377397180914e-05, avg_kl: 0.04756919106284341, avg_response_length: 60.00135262457879, avg_orm_score: 0.0, avg_custom_rewards: 1.4594377397180914e-05
2025-07-24 20:47:30.668 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter113_replay_buffer.jsonl
2025-07-24 20:47:31.935 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.27s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0336, ret=4.81e-5, glen=58.6, tlen=219, kl=0.0364, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.00s/it, pg=-0.0336, ret=4.81e-5, glen=58.6, tlen=219, kl=0.0364, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.00s/it, pg=-0.0136, ret=3.6e-5, glen=58.7, tlen=218, kl=0.122, act_lr=1e-6, ent=0.909]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0136, ret=3.6e-5, glen=58.7, tlen=218, kl=0.122, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0236, ret=4.71e-5, glen=60.6, tlen=221, kl=0.041, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.13it/s, pg=-0.0236, ret=4.71e-5, glen=60.6, tlen=221, kl=0.041, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.13it/s, pg=-0.0961, ret=0.000165, glen=63.6, tlen=224, kl=0.0361, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.13it/s, pg=-0.0961, ret=0.000165, glen=63.6, tlen=224, kl=0.0361, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.13it/s, pg=-0.0022, ret=2.51e-5, glen=60.9, tlen=221, kl=0.044, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:35,  1.14it/s, pg=-0.0022, ret=2.51e-5, glen=60.9, tlen=221, kl=0.044, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:35,  1.14it/s, pg=-0.0945, ret=0.000156, glen=59.1, tlen=219, kl=0.0595, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.15it/s, pg=-0.0945, ret=0.000156, glen=59.1, tlen=219, kl=0.0595, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.15it/s, pg=0.102, ret=-0.000225, glen=60, tlen=220, kl=0.0588, act_lr=1e-6, ent=0.943]   Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=0.102, ret=-0.000225, glen=60, tlen=220, kl=0.0588, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=-0.0842, ret=0.000151, glen=58.7, tlen=219, kl=0.0396, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.17it/s, pg=-0.0842, ret=0.000151, glen=58.7, tlen=219, kl=0.0396, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.17it/s, pg=-0.0972, ret=0.000171, glen=58.6, tlen=219, kl=0.0529, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.17it/s, pg=-0.0972, ret=0.000171, glen=58.6, tlen=219, kl=0.0529, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.17it/s, pg=-0.0851, ret=0.000146, glen=58.8, tlen=219, kl=0.0394, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.17it/s, pg=-0.0851, ret=0.000146, glen=58.8, tlen=219, kl=0.0394, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.17it/s, pg=0.0399, ret=-4.44e-5, glen=59, tlen=219, kl=0.0439, act_lr=1e-6, ent=0.928]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:29,  1.18it/s, pg=0.0399, ret=-4.44e-5, glen=59, tlen=219, kl=0.0439, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:29,  1.18it/s, pg=0.163, ret=-0.000239, glen=60.4, tlen=220, kl=0.0454, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:28,  1.18it/s, pg=0.163, ret=-0.000239, glen=60.4, tlen=220, kl=0.0454, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:28,  1.18it/s, pg=0.0083, ret=1.06e-5, glen=58.7, tlen=219, kl=0.0303, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.18it/s, pg=0.0083, ret=1.06e-5, glen=58.7, tlen=219, kl=0.0303, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.18it/s, pg=-0.0189, ret=6.43e-7, glen=59.2, tlen=219, kl=0.0246, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.18it/s, pg=-0.0189, ret=6.43e-7, glen=59.2, tlen=219, kl=0.0246, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.18it/s, pg=0.17, ret=-0.00032, glen=60.2, tlen=220, kl=0.0717, act_lr=1e-6, ent=0.926]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:12<00:26,  1.17it/s, pg=0.17, ret=-0.00032, glen=60.2, tlen=220, kl=0.0717, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.0266, ret=5.79e-6, glen=60.9, tlen=221, kl=0.0338, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.16it/s, pg=0.0266, ret=5.79e-6, glen=60.9, tlen=221, kl=0.0338, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=-0.0668, ret=0.000157, glen=60, tlen=220, kl=0.0372, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:25,  1.16it/s, pg=-0.0668, ret=0.000157, glen=60, tlen=220, kl=0.0372, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.16it/s, pg=0.101, ret=-0.000205, glen=59, tlen=219, kl=0.0477, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=0.101, ret=-0.000205, glen=59, tlen=219, kl=0.0477, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=-0.0768, ret=8.63e-5, glen=60.6, tlen=221, kl=0.0422, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0768, ret=8.63e-5, glen=60.6, tlen=221, kl=0.0422, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=0.139, ret=-0.000201, glen=59, tlen=219, kl=0.0488, act_lr=1e-6, ent=0.882]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.139, ret=-0.000201, glen=59, tlen=219, kl=0.0488, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0227, ret=-2.7e-5, glen=59.8, tlen=220, kl=0.0446, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0227, ret=-2.7e-5, glen=59.8, tlen=220, kl=0.0446, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0769, ret=-0.000217, glen=59.8, tlen=220, kl=0.0361, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:18<00:20,  1.17it/s, pg=0.0769, ret=-0.000217, glen=59.8, tlen=220, kl=0.0361, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0462, ret=5.18e-5, glen=60.8, tlen=221, kl=0.0437, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.18it/s, pg=-0.0462, ret=5.18e-5, glen=60.8, tlen=221, kl=0.0437, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=0.12, ret=-0.000118, glen=62.9, tlen=223, kl=0.0335, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=0.12, ret=-0.000118, glen=62.9, tlen=223, kl=0.0335, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=-0.0333, ret=2.36e-5, glen=59.9, tlen=220, kl=0.0549, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=-0.0333, ret=2.36e-5, glen=59.9, tlen=220, kl=0.0549, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=0.00775, ret=-3.17e-5, glen=61.1, tlen=221, kl=0.0413, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:16,  1.18it/s, pg=0.00775, ret=-3.17e-5, glen=61.1, tlen=221, kl=0.0413, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:16,  1.18it/s, pg=-0.012, ret=4.87e-6, glen=60.6, tlen=221, kl=0.0823, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=-0.012, ret=4.87e-6, glen=60.6, tlen=221, kl=0.0823, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=0.017, ret=-8.86e-5, glen=59.2, tlen=219, kl=0.0378, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.16it/s, pg=0.017, ret=-8.86e-5, glen=59.2, tlen=219, kl=0.0378, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.16it/s, pg=-0.0851, ret=0.000148, glen=61.5, tlen=222, kl=0.0373, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:17,  1.02s/it, pg=-0.0851, ret=0.000148, glen=61.5, tlen=222, kl=0.0373, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:17,  1.02s/it, pg=-0.000488, ret=1.55e-5, glen=61.3, tlen=221, kl=0.0312, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:15,  1.03it/s, pg=-0.000488, ret=1.55e-5, glen=61.3, tlen=221, kl=0.0312, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:15,  1.03it/s, pg=-0.0783, ret=0.000135, glen=61.2, tlen=221, kl=0.0552, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.07it/s, pg=-0.0783, ret=0.000135, glen=61.2, tlen=221, kl=0.0552, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.07it/s, pg=-0.0328, ret=2.89e-5, glen=60.9, tlen=221, kl=0.0482, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.10it/s, pg=-0.0328, ret=2.89e-5, glen=60.9, tlen=221, kl=0.0482, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.10it/s, pg=0.0279, ret=-7.73e-5, glen=61.7, tlen=222, kl=0.0352, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.12it/s, pg=0.0279, ret=-7.73e-5, glen=61.7, tlen=222, kl=0.0352, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.12it/s, pg=-0.0698, ret=0.000121, glen=61.1, tlen=221, kl=0.103, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.0698, ret=0.000121, glen=61.1, tlen=221, kl=0.103, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=0.0607, ret=-0.000104, glen=59.6, tlen=220, kl=0.0603, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=0.0607, ret=-0.000104, glen=59.6, tlen=220, kl=0.0603, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=-0.0153, ret=1.21e-5, glen=60.4, tlen=220, kl=0.0366, act_lr=1e-6, ent=0.929] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0153, ret=1.21e-5, glen=60.4, tlen=220, kl=0.0366, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0154, ret=1.44e-5, glen=58.9, tlen=219, kl=0.0369, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0154, ret=1.44e-5, glen=58.9, tlen=219, kl=0.0369, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.093, ret=0.000156, glen=59.6, tlen=220, kl=0.0302, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.093, ret=0.000156, glen=59.6, tlen=220, kl=0.0302, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0905, ret=0.000156, glen=60.2, tlen=220, kl=0.0391, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0905, ret=0.000156, glen=60.2, tlen=220, kl=0.0391, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.0121, ret=-2.22e-6, glen=58.9, tlen=219, kl=0.0547, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.0121, ret=-2.22e-6, glen=58.9, tlen=219, kl=0.0547, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.00739, ret=3.3e-5, glen=59.1, tlen=219, kl=0.0417, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.00739, ret=3.3e-5, glen=59.1, tlen=219, kl=0.0417, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0488, ret=-9.82e-5, glen=59, tlen=219, kl=0.0409, act_lr=1e-6, ent=0.917]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0488, ret=-9.82e-5, glen=59, tlen=219, kl=0.0409, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0736, ret=-0.00011, glen=59.5, tlen=220, kl=0.0407, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0736, ret=-0.00011, glen=59.5, tlen=220, kl=0.0407, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0841, ret=0.000149, glen=60.5, tlen=221, kl=0.0565, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0841, ret=0.000149, glen=60.5, tlen=221, kl=0.0565, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=0.0876, ret=-0.000111, glen=60.6, tlen=221, kl=0.0549, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=0.0876, ret=-0.000111, glen=60.6, tlen=221, kl=0.0549, act_lr=1e-6, ent=0.931]
2025-07-24 20:48:12.158 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.02s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=0.0383, ret=-2.63e-5, glen=58.4, tlen=218, kl=0.0477, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=0.0383, ret=-2.63e-5, glen=58.4, tlen=218, kl=0.0477, act_lr=1e-6, ent=0.928]
2025-07-24 20:48:12.845 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 20:48:15.111 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-24 20:48:15.461 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.44s
2025-07-24 20:48:15.468 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00027333135190217393, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9161047028458636, 'kl': 0.04739844296937403, 'response_length': 60.018246194590695, 'total_length': 220.1147407863451, 'teacher_total_length': 232.11176598590353, 'return': 2.436577826459973e-07, 'policy_update_steps': 1.0}
Episode [9/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [17:42<05:24, 108.14s/it]2025-07-24 20:48:15.502 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:48:50.684 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:48:50.862 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:48:50.862 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.36s
2025-07-24 20:48:52.613 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 59.1975,std_num_tokens: 14.0438,avg_correct_num_tokens: 59.1792,std_correct_num_tokens: 14.0372,avg_incorrect_num_tokens: 61.9630,std_incorrect_num_tokens: 14.7447
2025-07-24 20:48:53.041 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.18s
2025-07-24 20:48:55.940 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.90s
2025-07-24 20:49:19.521 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 20:49:19.522 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.58s
2025-07-24 20:49:20.775 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 20:49:20.775 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.0800713592996963e-05, avg_kl: 0.05193421080872253, avg_response_length: 59.2426267351423, avg_orm_score: 0.0, avg_custom_rewards: 2.0800713592996963e-05
2025-07-24 20:49:20.800 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter114_replay_buffer.jsonl
2025-07-24 20:49:22.050 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.25s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=0.052, ret=-7.97e-5, glen=59, tlen=219, kl=0.0472, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.01it/s, pg=0.052, ret=-7.97e-5, glen=59, tlen=219, kl=0.0472, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.01it/s, pg=-0.0598, ret=0.000105, glen=58.6, tlen=219, kl=0.0635, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0598, ret=0.000105, glen=58.6, tlen=219, kl=0.0635, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0097, ret=-1.16e-5, glen=60, tlen=221, kl=0.0393, act_lr=1e-6, ent=0.904]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.09it/s, pg=-0.0097, ret=-1.16e-5, glen=60, tlen=221, kl=0.0393, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.09it/s, pg=0.0417, ret=-3.27e-5, glen=57.7, tlen=218, kl=0.0474, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=0.0417, ret=-3.27e-5, glen=57.7, tlen=218, kl=0.0474, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=0.0485, ret=-0.000132, glen=58.4, tlen=219, kl=0.0307, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.12it/s, pg=0.0485, ret=-0.000132, glen=58.4, tlen=219, kl=0.0307, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.12it/s, pg=-0.0856, ret=0.000148, glen=58.3, tlen=219, kl=0.0469, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=-0.0856, ret=0.000148, glen=58.3, tlen=219, kl=0.0469, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=0.00952, ret=-8.84e-5, glen=58.3, tlen=219, kl=0.0493, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:35,  1.11it/s, pg=0.00952, ret=-8.84e-5, glen=58.3, tlen=219, kl=0.0493, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:35,  1.11it/s, pg=-0.0872, ret=0.000215, glen=60.7, tlen=222, kl=0.0593, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=-0.0872, ret=0.000215, glen=60.7, tlen=222, kl=0.0593, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.13it/s, pg=-0.0201, ret=1.36e-5, glen=60.1, tlen=220, kl=0.0351, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:33,  1.12it/s, pg=-0.0201, ret=1.36e-5, glen=60.1, tlen=220, kl=0.0351, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:09<00:33,  1.12it/s, pg=-0.0483, ret=4.01e-5, glen=58.7, tlen=219, kl=0.0568, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:32,  1.10it/s, pg=-0.0483, ret=4.01e-5, glen=58.7, tlen=219, kl=0.0568, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:32,  1.10it/s, pg=0.00995, ret=-1.65e-5, glen=61.4, tlen=222, kl=0.0414, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:31,  1.11it/s, pg=0.00995, ret=-1.65e-5, glen=61.4, tlen=222, kl=0.0414, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:31,  1.11it/s, pg=0.145, ret=-0.000279, glen=57.7, tlen=218, kl=0.0297, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:30,  1.13it/s, pg=0.145, ret=-0.000279, glen=57.7, tlen=218, kl=0.0297, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:30,  1.13it/s, pg=-0.0131, ret=1.68e-5, glen=60.9, tlen=221, kl=0.0457, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.14it/s, pg=-0.0131, ret=1.68e-5, glen=60.9, tlen=221, kl=0.0457, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.14it/s, pg=0.0157, ret=-9.82e-5, glen=58.7, tlen=219, kl=0.0439, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.15it/s, pg=0.0157, ret=-9.82e-5, glen=58.7, tlen=219, kl=0.0439, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.15it/s, pg=0.0137, ret=1.17e-5, glen=60.2, tlen=221, kl=0.0418, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:27,  1.14it/s, pg=0.0137, ret=1.17e-5, glen=60.2, tlen=221, kl=0.0418, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:27,  1.14it/s, pg=0.005, ret=3.72e-5, glen=59.3, tlen=220, kl=0.0388, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.15it/s, pg=0.005, ret=3.72e-5, glen=59.3, tlen=220, kl=0.0388, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:15<00:26,  1.15it/s, pg=0.0151, ret=2.7e-5, glen=59.7, tlen=220, kl=0.0521, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.16it/s, pg=0.0151, ret=2.7e-5, glen=59.7, tlen=220, kl=0.0521, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.16it/s, pg=0.0496, ret=-0.0001, glen=58.9, tlen=220, kl=0.0412, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=0.0496, ret=-0.0001, glen=58.9, tlen=220, kl=0.0412, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=-0.0183, ret=1.74e-5, glen=60.7, tlen=221, kl=0.0336, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0183, ret=1.74e-5, glen=60.7, tlen=221, kl=0.0336, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=0.0486, ret=-9.9e-5, glen=58.2, tlen=219, kl=0.0566, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.0486, ret=-9.9e-5, glen=58.2, tlen=219, kl=0.0566, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0213, ret=1.43e-5, glen=58.8, tlen=219, kl=0.0611, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0213, ret=1.43e-5, glen=58.8, tlen=219, kl=0.0611, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.0835, ret=-7.42e-5, glen=57.7, tlen=218, kl=0.038, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:21,  1.14it/s, pg=0.0835, ret=-7.42e-5, glen=57.7, tlen=218, kl=0.038, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:21,  1.14it/s, pg=0.0162, ret=7.32e-6, glen=58.6, tlen=219, kl=0.0522, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:20,  1.15it/s, pg=0.0162, ret=7.32e-6, glen=58.6, tlen=219, kl=0.0522, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:21<00:20,  1.15it/s, pg=0.0732, ret=-0.000136, glen=61.6, tlen=222, kl=0.0534, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:19,  1.15it/s, pg=0.0732, ret=-0.000136, glen=61.6, tlen=222, kl=0.0534, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:19,  1.15it/s, pg=-0.0178, ret=-7.72e-6, glen=59, tlen=220, kl=0.0608, act_lr=1e-6, ent=0.909]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.0178, ret=-7.72e-6, glen=59, tlen=220, kl=0.0608, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=0.114, ret=-0.000232, glen=60.8, tlen=221, kl=0.0555, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.114, ret=-0.000232, glen=60.8, tlen=221, kl=0.0555, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0152, ret=2.58e-5, glen=59.4, tlen=220, kl=0.0549, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0152, ret=2.58e-5, glen=59.4, tlen=220, kl=0.0549, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=0.107, ret=-9.68e-5, glen=60.9, tlen=222, kl=0.0363, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=0.107, ret=-9.68e-5, glen=60.9, tlen=222, kl=0.0363, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0858, ret=0.00016, glen=59.9, tlen=221, kl=0.0392, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0858, ret=0.00016, glen=59.9, tlen=221, kl=0.0392, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.146, ret=-0.000235, glen=59.1, tlen=219, kl=0.0389, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.146, ret=-0.000235, glen=59.1, tlen=219, kl=0.0389, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0756, ret=0.000137, glen=60.1, tlen=221, kl=0.0571, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0756, ret=0.000137, glen=60.1, tlen=221, kl=0.0571, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=0.0442, ret=-2.94e-5, glen=58.2, tlen=219, kl=0.0471, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.0442, ret=-2.94e-5, glen=58.2, tlen=219, kl=0.0471, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:29<00:12,  1.14it/s, pg=-0.0941, ret=0.000157, glen=58.4, tlen=219, kl=0.0282, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0941, ret=0.000157, glen=58.4, tlen=219, kl=0.0282, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.101, ret=0.000166, glen=58.1, tlen=218, kl=0.0675, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.101, ret=0.000166, glen=58.1, tlen=218, kl=0.0675, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=0.000671, ret=4.83e-6, glen=58.6, tlen=219, kl=0.0513, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.000671, ret=4.83e-6, glen=58.6, tlen=219, kl=0.0513, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0383, ret=4.05e-5, glen=58.7, tlen=220, kl=0.0686, act_lr=1e-6, ent=0.946] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0383, ret=4.05e-5, glen=58.7, tlen=220, kl=0.0686, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0742, ret=0.000133, glen=60.3, tlen=221, kl=0.0693, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0742, ret=0.000133, glen=60.3, tlen=221, kl=0.0693, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=0.0374, ret=-9.47e-5, glen=59.5, tlen=220, kl=0.117, act_lr=1e-6, ent=0.947]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0374, ret=-9.47e-5, glen=59.5, tlen=220, kl=0.117, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0771, ret=0.000129, glen=59.1, tlen=220, kl=0.0468, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0771, ret=0.000129, glen=59.1, tlen=220, kl=0.0468, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.02, ret=1.84e-5, glen=57.9, tlen=218, kl=0.0428, act_lr=1e-6, ent=0.895]   Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.02, ret=1.84e-5, glen=57.9, tlen=218, kl=0.0428, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0425, ret=-7.58e-5, glen=58.4, tlen=219, kl=0.0781, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0425, ret=-7.58e-5, glen=58.4, tlen=219, kl=0.0781, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0154, ret=1.19e-5, glen=58.1, tlen=219, kl=0.0626, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0154, ret=1.19e-5, glen=58.1, tlen=219, kl=0.0626, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0146, ret=2.73e-5, glen=58.7, tlen=219, kl=0.123, act_lr=1e-6, ent=0.904]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0146, ret=2.73e-5, glen=58.7, tlen=219, kl=0.123, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.0121, ret=3.17e-6, glen=61.1, tlen=222, kl=0.0588, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.0121, ret=3.17e-6, glen=61.1, tlen=222, kl=0.0588, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.0864, ret=0.000145, glen=59.2, tlen=219, kl=0.0386, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0864, ret=0.000145, glen=59.2, tlen=219, kl=0.0386, act_lr=1e-6, ent=0.921]
2025-07-24 20:50:02.406 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=-0.0828, ret=0.000145, glen=58.9, tlen=219, kl=0.0466, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=-0.0828, ret=0.000145, glen=58.9, tlen=219, kl=0.0466, act_lr=1e-6, ent=0.921]
2025-07-24 20:50:03.269 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-24 20:50:05.756 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.49s
2025-07-24 20:50:06.087 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.99s
2025-07-24 20:50:06.093 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0009128736413043479, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9254181035186934, 'kl': 0.05203711468240489, 'response_length': 59.228980520497196, 'total_length': 219.6687134452488, 'teacher_total_length': 231.66885674518088, 'return': 8.085751529585611e-07, 'policy_update_steps': 1.0}
Episode [9/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [19:33<03:37, 108.90s/it]2025-07-24 20:50:06.137 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:50:41.611 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:50:41.790 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:50:41.790 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.65s
2025-07-24 20:50:43.488 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 60.0812,std_num_tokens: 14.1322,avg_correct_num_tokens: 60.1109,std_correct_num_tokens: 14.1157,avg_incorrect_num_tokens: 56.8243,std_incorrect_num_tokens: 15.4940
2025-07-24 20:50:43.893 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.10s
2025-07-24 20:50:46.422 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.53s
2025-07-24 20:51:10.030 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 20:51:10.031 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.61s
2025-07-24 20:51:11.621 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.14s
2025-07-24 20:51:11.621 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0003098274232524503, avg_kl: 0.06406023202698087, avg_response_length: 60.10399521374312, avg_orm_score: 0.0, avg_custom_rewards: -0.0003098274232524503
2025-07-24 20:51:11.672 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter115_replay_buffer.jsonl
2025-07-24 20:51:12.922 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.25s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=-0.0797, ret=0.000172, glen=60.8, tlen=222, kl=0.0551, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.01it/s, pg=-0.0797, ret=0.000172, glen=60.8, tlen=222, kl=0.0551, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.01it/s, pg=-0.00061, ret=-3.86e-6, glen=59.6, tlen=220, kl=0.0327, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.00061, ret=-3.86e-6, glen=59.6, tlen=220, kl=0.0327, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.01, ret=-2.6e-5, glen=58.4, tlen=220, kl=0.0687, act_lr=1e-6, ent=0.942]    Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.10it/s, pg=-0.01, ret=-2.6e-5, glen=58.4, tlen=220, kl=0.0687, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.10it/s, pg=0.0773, ret=-0.000204, glen=60.5, tlen=221, kl=0.0408, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.13it/s, pg=0.0773, ret=-0.000204, glen=60.5, tlen=221, kl=0.0408, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.13it/s, pg=0.0261, ret=-2.86e-5, glen=60.4, tlen=221, kl=0.0366, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:37,  1.11it/s, pg=0.0261, ret=-2.86e-5, glen=60.4, tlen=221, kl=0.0366, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:37,  1.11it/s, pg=0.000305, ret=-3.67e-5, glen=59.7, tlen=221, kl=0.0346, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=0.000305, ret=-3.67e-5, glen=59.7, tlen=221, kl=0.0346, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=-0.0518, ret=7.8e-5, glen=58.7, tlen=220, kl=0.0466, act_lr=1e-6, ent=0.924]   Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:35,  1.11it/s, pg=-0.0518, ret=7.8e-5, glen=58.7, tlen=220, kl=0.0466, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:35,  1.11it/s, pg=-0.00488, ret=2.09e-5, glen=60.9, tlen=222, kl=0.0601, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=-0.00488, ret=2.09e-5, glen=60.9, tlen=222, kl=0.0601, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.13it/s, pg=-0.0162, ret=2.26e-6, glen=61.4, tlen=222, kl=0.0715, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=-0.0162, ret=2.26e-6, glen=61.4, tlen=222, kl=0.0715, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=-0.0701, ret=0.000154, glen=60, tlen=221, kl=0.0423, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.15it/s, pg=-0.0701, ret=0.000154, glen=60, tlen=221, kl=0.0423, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.15it/s, pg=0.00525, ret=-3.28e-5, glen=60.2, tlen=221, kl=0.0578, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.16it/s, pg=0.00525, ret=-3.28e-5, glen=60.2, tlen=221, kl=0.0578, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.16it/s, pg=0.00772, ret=-1.55e-5, glen=59.8, tlen=221, kl=0.042, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=0.00772, ret=-1.55e-5, glen=59.8, tlen=221, kl=0.042, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=-0.000977, ret=5.79e-5, glen=59.9, tlen=221, kl=0.0633, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=-0.000977, ret=5.79e-5, glen=59.9, tlen=221, kl=0.0633, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=-0.0924, ret=0.000193, glen=59.9, tlen=220, kl=0.0374, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.15it/s, pg=-0.0924, ret=0.000193, glen=59.9, tlen=220, kl=0.0374, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.15it/s, pg=0.523, ret=-0.002, glen=60.5, tlen=222, kl=0.0619, act_lr=1e-6, ent=0.919]    Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.16it/s, pg=0.523, ret=-0.002, glen=60.5, tlen=222, kl=0.0619, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.16it/s, pg=0.104, ret=-0.00029, glen=59.8, tlen=221, kl=0.137, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=0.104, ret=-0.00029, glen=59.8, tlen=221, kl=0.137, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=-0.127, ret=0.00027, glen=61.1, tlen=222, kl=0.0496, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.127, ret=0.00027, glen=61.1, tlen=222, kl=0.0496, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0116, ret=-4.65e-5, glen=60.1, tlen=221, kl=0.0491, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0116, ret=-4.65e-5, glen=60.1, tlen=221, kl=0.0491, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=0.0319, ret=-4.76e-5, glen=61.5, tlen=222, kl=0.045, act_lr=1e-6, ent=0.902]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=0.0319, ret=-4.76e-5, glen=61.5, tlen=222, kl=0.045, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=0.0598, ret=-0.000108, glen=59.4, tlen=220, kl=0.317, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.0598, ret=-0.000108, glen=59.4, tlen=220, kl=0.317, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0759, ret=0.00012, glen=60.1, tlen=221, kl=0.0534, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=-0.0759, ret=0.00012, glen=60.1, tlen=221, kl=0.0534, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=-0.0608, ret=0.000147, glen=58.9, tlen=219, kl=0.0388, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0608, ret=0.000147, glen=58.9, tlen=219, kl=0.0388, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.17it/s, pg=-0.0426, ret=0.000117, glen=59.8, tlen=221, kl=0.0681, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0426, ret=0.000117, glen=59.8, tlen=221, kl=0.0681, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=0.000366, ret=2.66e-5, glen=60.1, tlen=221, kl=0.0364, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=0.000366, ret=2.66e-5, glen=60.1, tlen=221, kl=0.0364, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.0315, ret=-9.65e-6, glen=60.1, tlen=221, kl=0.0974, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=0.0315, ret=-9.65e-6, glen=60.1, tlen=221, kl=0.0974, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=0.0402, ret=-4.07e-5, glen=60.4, tlen=221, kl=0.0789, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.0402, ret=-4.07e-5, glen=60.4, tlen=221, kl=0.0789, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.00122, ret=3.29e-5, glen=58.5, tlen=219, kl=0.0574, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.00122, ret=3.29e-5, glen=58.5, tlen=219, kl=0.0574, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.104, ret=0.000218, glen=59.9, tlen=221, kl=0.087, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.104, ret=0.000218, glen=59.9, tlen=221, kl=0.087, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0727, ret=0.000156, glen=59.9, tlen=221, kl=0.0387, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0727, ret=0.000156, glen=59.9, tlen=221, kl=0.0387, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0505, ret=0.000125, glen=59.9, tlen=221, kl=0.0857, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0505, ret=0.000125, glen=59.9, tlen=221, kl=0.0857, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.107, ret=0.000224, glen=60, tlen=221, kl=0.113, act_lr=1e-6, ent=0.91]     Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.107, ret=0.000224, glen=60, tlen=221, kl=0.113, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0554, ret=7.63e-5, glen=60.4, tlen=222, kl=0.0491, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=-0.0554, ret=7.63e-5, glen=60.4, tlen=222, kl=0.0491, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0422, ret=5.97e-5, glen=60.4, tlen=221, kl=0.0543, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0422, ret=5.97e-5, glen=60.4, tlen=221, kl=0.0543, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=0.063, ret=-0.000161, glen=60.1, tlen=221, kl=0.054, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=0.063, ret=-0.000161, glen=60.1, tlen=221, kl=0.054, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=-0.0959, ret=0.000197, glen=59.6, tlen=220, kl=0.0509, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0959, ret=0.000197, glen=59.6, tlen=220, kl=0.0509, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=0.153, ret=-0.000379, glen=60.7, tlen=222, kl=0.0732, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=0.153, ret=-0.000379, glen=60.7, tlen=222, kl=0.0732, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=0.0405, ret=-0.000176, glen=61, tlen=223, kl=0.0453, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=0.0405, ret=-0.000176, glen=61, tlen=223, kl=0.0453, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=-0.03, ret=6.98e-5, glen=60.3, tlen=221, kl=0.0686, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.03, ret=6.98e-5, glen=60.3, tlen=221, kl=0.0686, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0508, ret=-7.51e-5, glen=60.8, tlen=222, kl=0.0654, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.0508, ret=-7.51e-5, glen=60.8, tlen=222, kl=0.0654, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.0616, ret=-6.34e-5, glen=60.3, tlen=221, kl=0.0377, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.0616, ret=-6.34e-5, glen=60.3, tlen=221, kl=0.0377, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0599, ret=-0.000156, glen=59.6, tlen=221, kl=0.0671, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0599, ret=-0.000156, glen=59.6, tlen=221, kl=0.0671, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0436, ret=-5.4e-5, glen=60.9, tlen=222, kl=0.0611, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0436, ret=-5.4e-5, glen=60.9, tlen=222, kl=0.0611, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0181, ret=1.93e-5, glen=59.6, tlen=221, kl=0.0533, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0181, ret=1.93e-5, glen=59.6, tlen=221, kl=0.0533, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.11, ret=0.00022, glen=60.3, tlen=221, kl=0.0629, act_lr=1e-6, ent=0.925]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.11, ret=0.00022, glen=60.3, tlen=221, kl=0.0629, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=0.159, ret=-0.000334, glen=60.2, tlen=221, kl=0.0398, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.159, ret=-0.000334, glen=60.2, tlen=221, kl=0.0398, act_lr=1e-6, ent=0.942]
2025-07-24 20:51:53.089 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.00s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.00323, ret=3.25e-6, glen=60.4, tlen=221, kl=0.0515, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=0.00323, ret=3.25e-6, glen=60.4, tlen=221, kl=0.0515, act_lr=1e-6, ent=0.935]
2025-07-24 20:51:53.731 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.58s
2025-07-24 20:51:55.993 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.26s
2025-07-24 20:51:56.325 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.35s
2025-07-24 20:51:56.331 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.004676487134850543, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9288548438445382, 'kl': 0.06386798361073369, 'response_length': 60.10632846666419, 'total_length': 221.08048281462297, 'teacher_total_length': 233.05180193030316, 'return': -3.330207267066251e-05, 'policy_update_steps': 1.0}
Episode [9/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [21:23<01:49, 109.31s/it]2025-07-24 20:51:56.337 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<02:12,  1.29it/s, est. speed input: 236.85 toks/s, output: 37.53 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 146/172 [00:02<00:00, 94.78it/s, est. speed input: 10979.06 toks/s, output: 3458.91 toks/s] Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:02<00:00, 99.43it/s, est. speed input: 11397.94 toks/s, output: 3623.83 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 161/172 [00:02<00:00, 81.78it/s, est. speed input: 11287.85 toks/s, output: 3548.58 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 49.62it/s, est. speed input: 10118.64 toks/s, output: 3386.80 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 55.75it/s, est. speed input: 10118.64 toks/s, output: 3386.80 toks/s]
2025-07-24 20:52:00.202 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 280.8006,strategyqa_test/accuracy: 0.5604,eval_accuracy: 0.5604
2025-07-24 20:52:00.504 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:52:18.931 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:52:19.104 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:52:19.104 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 18.60s
2025-07-24 20:52:20.117 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0140,avg_reflection_pattern_score: 0.0016,avg_pass_at_n: 1.0000,avg_num_tokens: 59.7253,std_num_tokens: 13.2838,avg_correct_num_tokens: 59.7368,std_correct_num_tokens: 13.2902,avg_incorrect_num_tokens: 57.3810,std_incorrect_num_tokens: 11.6761
2025-07-24 20:52:20.354 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.25s
2025-07-24 20:52:21.748 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.39s
2025-07-24 20:52:34.397 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 96
2025-07-24 20:52:34.398 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.65s
2025-07-24 20:52:35.292 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.47s
2025-07-24 20:52:35.292 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -4.2528336052782834e-07, avg_kl: 0.09544626871744792, avg_response_length: 59.84312307834625, avg_orm_score: 0.0, avg_custom_rewards: -4.2528336052782834e-07
2025-07-24 20:52:35.315 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter116_replay_buffer.jsonl
2025-07-24 20:52:35.974 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.66s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:02<00:00, 100.02it/s, est. speed input: 11597.21 toks/s, output: 3620.39 toks/s][32m [repeated 54x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:02<00:00, 58.06it/s, est. speed input: 10525.69 toks/s, output: 3391.85 toks/s][32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, pg=-0.00848, ret=-3.86e-6, glen=60.3, tlen=221, kl=0.0457, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:00<00:21,  1.06it/s, pg=-0.00848, ret=-3.86e-6, glen=60.3, tlen=221, kl=0.0457, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:01<00:21,  1.06it/s, pg=0.0826, ret=-0.000147, glen=59.7, tlen=220, kl=0.0574, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:01<00:19,  1.12it/s, pg=0.0826, ret=-0.000147, glen=59.7, tlen=220, kl=0.0574, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:02<00:19,  1.12it/s, pg=-0.076, ret=0.000126, glen=60.8, tlen=221, kl=0.0471, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:02<00:18,  1.11it/s, pg=-0.076, ret=0.000126, glen=60.8, tlen=221, kl=0.0471, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:03<00:18,  1.11it/s, pg=-0.00635, ret=-2.5e-5, glen=58.9, tlen=219, kl=0.087, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:03<00:17,  1.12it/s, pg=-0.00635, ret=-2.5e-5, glen=58.9, tlen=219, kl=0.087, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:04<00:17,  1.12it/s, pg=-0.0615, ret=9.68e-5, glen=59.6, tlen=220, kl=0.0599, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:04<00:17,  1.10it/s, pg=-0.0615, ret=9.68e-5, glen=59.6, tlen=220, kl=0.0599, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:05<00:17,  1.10it/s, pg=-0.0645, ret=0.0001, glen=57.1, tlen=218, kl=0.0682, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:05<00:15,  1.13it/s, pg=-0.0645, ret=0.0001, glen=57.1, tlen=218, kl=0.0682, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:06<00:15,  1.13it/s, pg=0.0161, ret=-2.34e-5, glen=58.6, tlen=219, kl=0.116, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:06<00:14,  1.14it/s, pg=0.0161, ret=-2.34e-5, glen=58.6, tlen=219, kl=0.116, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:07<00:14,  1.14it/s, pg=-0.0664, ret=0.000101, glen=61.9, tlen=222, kl=0.272, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:13,  1.15it/s, pg=-0.0664, ret=0.000101, glen=61.9, tlen=222, kl=0.272, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:13,  1.15it/s, pg=-0.104, ret=0.000168, glen=60.6, tlen=221, kl=0.0797, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:07<00:12,  1.16it/s, pg=-0.104, ret=0.000168, glen=60.6, tlen=221, kl=0.0797, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:08<00:12,  1.16it/s, pg=-0.0497, ret=7.98e-5, glen=61, tlen=221, kl=0.1, act_lr=1e-6, ent=0.94]      Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:08<00:12,  1.16it/s, pg=-0.0497, ret=7.98e-5, glen=61, tlen=221, kl=0.1, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:09<00:12,  1.16it/s, pg=0.0956, ret=-0.000143, glen=60.4, tlen=221, kl=0.0472, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:09<00:11,  1.13it/s, pg=0.0956, ret=-0.000143, glen=60.4, tlen=221, kl=0.0472, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:10<00:11,  1.13it/s, pg=0.0817, ret=-0.000129, glen=60.1, tlen=220, kl=0.263, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:10<00:10,  1.14it/s, pg=0.0817, ret=-0.000129, glen=60.1, tlen=220, kl=0.263, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:11<00:10,  1.14it/s, pg=0.237, ret=-0.000376, glen=58.8, tlen=219, kl=0.0551, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:11<00:09,  1.15it/s, pg=0.237, ret=-0.000376, glen=58.8, tlen=219, kl=0.0551, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:12<00:09,  1.15it/s, pg=-0.0618, ret=9.76e-5, glen=60.5, tlen=221, kl=0.0724, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:12<00:08,  1.16it/s, pg=-0.0618, ret=9.76e-5, glen=60.5, tlen=221, kl=0.0724, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:13<00:08,  1.16it/s, pg=0.0176, ret=-1.6e-5, glen=57.9, tlen=219, kl=0.299, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.16it/s, pg=0.0176, ret=-1.6e-5, glen=57.9, tlen=219, kl=0.299, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.16it/s, pg=0.00329, ret=-3.76e-5, glen=61.7, tlen=223, kl=0.0501, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:13<00:06,  1.17it/s, pg=0.00329, ret=-3.76e-5, glen=61.7, tlen=223, kl=0.0501, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:14<00:06,  1.17it/s, pg=-0.0728, ret=0.000116, glen=58.9, tlen=219, kl=0.0917, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:14<00:05,  1.17it/s, pg=-0.0728, ret=0.000116, glen=58.9, tlen=219, kl=0.0917, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:15<00:05,  1.17it/s, pg=-0.0047, ret=7.72e-6, glen=59.8, tlen=220, kl=0.0717, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:15<00:05,  1.17it/s, pg=-0.0047, ret=7.72e-6, glen=59.8, tlen=220, kl=0.0717, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:16<00:05,  1.17it/s, pg=-0.0675, ret=0.00011, glen=60, tlen=220, kl=0.139, act_lr=1e-6, ent=0.919]   Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:16<00:04,  1.14it/s, pg=-0.0675, ret=0.00011, glen=60, tlen=220, kl=0.139, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:17<00:04,  1.14it/s, pg=0.0491, ret=-3.28e-5, glen=61, tlen=221, kl=0.0496, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:17<00:03,  1.15it/s, pg=0.0491, ret=-3.28e-5, glen=61, tlen=221, kl=0.0496, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:18<00:03,  1.15it/s, pg=0.095, ret=-0.00015, glen=58.6, tlen=219, kl=0.0653, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:18<00:02,  1.16it/s, pg=0.095, ret=-0.00015, glen=58.6, tlen=219, kl=0.0653, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:19<00:02,  1.16it/s, pg=-0.00122, ret=-8.88e-6, glen=60.2, tlen=221, kl=0.0724, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:19<00:01,  1.16it/s, pg=-0.00122, ret=-8.88e-6, glen=60.2, tlen=221, kl=0.0724, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:20<00:01,  1.16it/s, pg=-0.0742, ret=0.000111, glen=60.9, tlen=221, kl=0.0399, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.17it/s, pg=-0.0742, ret=0.000111, glen=60.9, tlen=221, kl=0.0399, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.17it/s, pg=0.0513, ret=-2.25e-5, glen=59, tlen=219, kl=0.0414, act_lr=1e-6, ent=0.93]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.10it/s, pg=0.0513, ret=-2.25e-5, glen=59, tlen=219, kl=0.0414, act_lr=1e-6, ent=0.93]
2025-07-24 20:52:57.132 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.98s
2025-07-24 20:52:57.995 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 20:53:00.508 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-24 20:53:00.860 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 24.84s
2025-07-24 20:53:00.863 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00040419896443684894, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9253358170390129, 'kl': 0.09544626871744792, 'response_length': 59.84312359491984, 'total_length': 220.34208170572916, 'teacher_total_length': 232.2180550893148, 'return': -1.2280982749264998e-08, 'policy_update_steps': 1.0}
Episode [9/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [22:28<00:00, 95.74s/it] 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,741] [INFO] [utils.py:782:see_memory_usage] MA 6.6 GB         Max_MA 6.6 GB         CA 7.63 GB         Max_CA 8 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,741] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.64 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7a83fc9b0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,742] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,743] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,744] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,744] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
2025-07-24 20:53:09.903 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:32:13,941] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:30:32,123] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:33:56,850] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:35:41,070] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:37:24,634] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:39:11,083] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:42:47,189] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:44:33,307] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:46:21,366] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:48:12,150] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:50:02,399] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:51:53,081] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:52:57,124] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:07,581] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:07,775] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 3390, num_elems = 17.77B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,272] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,272] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,280] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,281] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,622] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,622] [INFO] [utils.py:782:see_memory_usage] MA 7.33 GB         Max_MA 12.29 GB         CA 8.37 GB         Max_CA 39 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,622] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.63 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [10/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [9/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [22:37<00:00, 104.40s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,897] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,898] [INFO] [utils.py:782:see_memory_usage] MA 7.33 GB         Max_MA 7.33 GB         CA 8.37 GB         Max_CA 8 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,898] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.63 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee79dc3f0b0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   global_rank .................. 0
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 20:53:10.192 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:53:45.908 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:53:46.090 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:53:46.090 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.90s
2025-07-24 20:53:47.979 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 60.2717,std_num_tokens: 13.9138,avg_correct_num_tokens: 60.3010,std_correct_num_tokens: 13.9190,avg_incorrect_num_tokens: 54.1538,std_incorrect_num_tokens: 11.2125
2025-07-24 20:53:48.396 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.31s
2025-07-24 20:53:50.905 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.51s
2025-07-24 20:54:15.283 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 20:54:15.284 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.38s
2025-07-24 20:54:16.521 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 20:54:16.521 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.5796186042038703e-05, avg_kl: 0.0, avg_response_length: 60.2904596223936, avg_orm_score: 0.0, avg_custom_rewards: 2.5796186042038703e-05
2025-07-24 20:54:16.559 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter117_replay_buffer.jsonl
2025-07-24 20:54:17.766 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0445, ret=7.14e-5, glen=60.1, tlen=220, kl=0, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:49,  1.10s/it, pg=-0.0445, ret=7.14e-5, glen=60.1, tlen=220, kl=0, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:49,  1.10s/it, pg=-0.0449, ret=7.33e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:42,  1.04it/s, pg=-0.0449, ret=7.33e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:42,  1.04it/s, pg=0.1, ret=-0.000119, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.899]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.10it/s, pg=0.1, ret=-0.000119, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.10it/s, pg=-0.0696, ret=0.000109, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.13it/s, pg=-0.0696, ret=0.000109, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.13it/s, pg=-0.075, ret=0.000115, glen=58.6, tlen=219, kl=0, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.12it/s, pg=-0.075, ret=0.000115, glen=58.6, tlen=219, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.12it/s, pg=0.232, ret=-0.00037, glen=59.3, tlen=220, kl=0, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.14it/s, pg=0.232, ret=-0.00037, glen=59.3, tlen=220, kl=0, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.14it/s, pg=-0.0574, ret=9.41e-5, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.15it/s, pg=-0.0574, ret=9.41e-5, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.15it/s, pg=-0.0464, ret=7.91e-5, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0464, ret=7.91e-5, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=0.00568, ret=-1.88e-5, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.16it/s, pg=0.00568, ret=-1.88e-5, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=0.0222, ret=-1.54e-5, glen=60, tlen=221, kl=0, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.17it/s, pg=0.0222, ret=-1.54e-5, glen=60, tlen=221, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.17it/s, pg=0.346, ret=-0.00048, glen=58.6, tlen=218, kl=0, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.16it/s, pg=0.346, ret=-0.00048, glen=58.6, tlen=218, kl=0, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.16it/s, pg=0.112, ret=-0.000142, glen=61.8, tlen=222, kl=0, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=0.112, ret=-0.000142, glen=61.8, tlen=222, kl=0, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=-0.0592, ret=0.000102, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=-0.0592, ret=0.000102, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=0.0152, ret=-3.93e-5, glen=62.2, tlen=223, kl=0, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=0.0152, ret=-3.93e-5, glen=62.2, tlen=223, kl=0, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.071, ret=0.000114, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.071, ret=0.000114, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.00153, ret=-2.44e-5, glen=61.2, tlen=222, kl=0, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.00153, ret=-2.44e-5, glen=61.2, tlen=222, kl=0, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=0.0319, ret=-3.16e-5, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=0.89]   Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=0.0319, ret=-3.16e-5, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0479, ret=8.78e-5, glen=61.9, tlen=222, kl=0, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0479, ret=8.78e-5, glen=61.9, tlen=222, kl=0, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0633, ret=0.000104, glen=60.1, tlen=220, kl=0, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0633, ret=0.000104, glen=60.1, tlen=220, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0119, ret=4.36e-7, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0119, ret=4.36e-7, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0562, ret=9.88e-5, glen=60.5, tlen=221, kl=0, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=-0.0562, ret=9.88e-5, glen=60.5, tlen=221, kl=0, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=-0.0723, ret=0.000116, glen=60, tlen=221, kl=0, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0723, ret=0.000116, glen=60, tlen=221, kl=0, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0526, ret=8.83e-5, glen=61.4, tlen=222, kl=0, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.17it/s, pg=-0.0526, ret=8.83e-5, glen=61.4, tlen=222, kl=0, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0504, ret=8.78e-5, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=0.898]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0504, ret=8.78e-5, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0869, ret=0.000133, glen=60.1, tlen=221, kl=0, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.0869, ret=0.000133, glen=60.1, tlen=221, kl=0, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=0.0833, ret=-0.000133, glen=57.9, tlen=218, kl=0, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.16it/s, pg=0.0833, ret=-0.000133, glen=57.9, tlen=218, kl=0, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.16it/s, pg=0.108, ret=-0.000136, glen=58.6, tlen=219, kl=0, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.108, ret=-0.000136, glen=58.6, tlen=219, kl=0, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.00414, ret=-9.44e-6, glen=60, tlen=220, kl=0, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.15it/s, pg=-0.00414, ret=-9.44e-6, glen=60, tlen=220, kl=0, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.15it/s, pg=-0.0709, ret=0.000109, glen=60.5, tlen=221, kl=0, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:16,  1.06it/s, pg=-0.0709, ret=0.000109, glen=60.5, tlen=221, kl=0, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:16,  1.06it/s, pg=-0.033, ret=3.3e-5, glen=61.9, tlen=222, kl=0, act_lr=1e-6, ent=0.898]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.09it/s, pg=-0.033, ret=3.3e-5, glen=61.9, tlen=222, kl=0, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.09it/s, pg=-0.0583, ret=9.17e-5, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.11it/s, pg=-0.0583, ret=9.17e-5, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.11it/s, pg=0.072, ret=-0.000125, glen=60.2, tlen=221, kl=0, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.13it/s, pg=0.072, ret=-0.000125, glen=60.2, tlen=221, kl=0, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=0.0209, ret=-1.03e-5, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.12it/s, pg=0.0209, ret=-1.03e-5, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.12it/s, pg=0.106, ret=-0.000238, glen=58.6, tlen=219, kl=0, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.13it/s, pg=0.106, ret=-0.000238, glen=58.6, tlen=219, kl=0, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.13it/s, pg=0.173, ret=-0.000258, glen=59.2, tlen=219, kl=0, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.13it/s, pg=0.173, ret=-0.000258, glen=59.2, tlen=219, kl=0, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.13it/s, pg=0.00641, ret=-2.45e-5, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.15it/s, pg=0.00641, ret=-2.45e-5, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.15it/s, pg=-0.0941, ret=0.000139, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.15it/s, pg=-0.0941, ret=0.000139, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.15it/s, pg=-0.0616, ret=9.79e-5, glen=60.9, tlen=222, kl=0, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.16it/s, pg=-0.0616, ret=9.79e-5, glen=60.9, tlen=222, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.16it/s, pg=-0.0509, ret=8.29e-5, glen=60.7, tlen=221, kl=0, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:06,  1.17it/s, pg=-0.0509, ret=8.29e-5, glen=60.7, tlen=221, kl=0, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:06,  1.17it/s, pg=0.00781, ret=-6.47e-6, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.00781, ret=-6.47e-6, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0647, ret=0.000102, glen=60.2, tlen=221, kl=0, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0647, ret=0.000102, glen=60.2, tlen=221, kl=0, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0698, ret=-0.000129, glen=60.3, tlen=221, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0698, ret=-0.000129, glen=60.3, tlen=221, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0524, ret=8.77e-5, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0524, ret=8.77e-5, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0579, ret=9.48e-5, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=0.875]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0579, ret=9.48e-5, glen=59.5, tlen=220, kl=0, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=0.0389, ret=-1.99e-5, glen=61, tlen=221, kl=0, act_lr=1e-6, ent=0.936]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.0389, ret=-1.99e-5, glen=61, tlen=221, kl=0, act_lr=1e-6, ent=0.936]
2025-07-24 20:54:58.195 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.0884, ret=-0.000142, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=0.0884, ret=-0.000142, glen=60.4, tlen=221, kl=0, act_lr=1e-6, ent=0.911]
2025-07-24 20:54:58.858 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 20:55:01.162 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.30s
2025-07-24 20:55:01.494 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.48s
2025-07-24 20:55:01.501 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0039194355840268345, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9030334016551143, 'kl': 0.0, 'response_length': 60.283679547517195, 'total_length': 220.66260130509087, 'teacher_total_length': 232.6207398124363, 'return': -3.4818191994892676e-06, 'policy_update_steps': 1.0}

Episode [10/20]:   8%|‚ñä         | 1/13 [01:51<22:19, 111.60s/it][A2025-07-24 20:55:01.545 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:55:37.240 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:55:37.429 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 20:55:37.430 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.89s
2025-07-24 20:55:39.463 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0141,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 61.0236,std_num_tokens: 13.6911,avg_correct_num_tokens: 60.9967,std_correct_num_tokens: 13.6814,avg_incorrect_num_tokens: 65.7826,std_incorrect_num_tokens: 14.5481
2025-07-24 20:55:39.891 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.46s
2025-07-24 20:55:42.425 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.53s
2025-07-24 20:56:05.948 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 20:56:05.948 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.52s
2025-07-24 20:56:07.204 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 20:56:07.204 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.6685750824017602e-07, avg_kl: 0.0018838350890112705, avg_response_length: 61.05210065581108, avg_orm_score: 0.0, avg_custom_rewards: -1.6685750824017602e-07
2025-07-24 20:56:07.235 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter118_replay_buffer.jsonl
2025-07-24 20:56:08.493 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=0.0784, ret=-0.00013, glen=63.7, tlen=224, kl=0.00206, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.02it/s, pg=0.0784, ret=-0.00013, glen=63.7, tlen=224, kl=0.00206, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.02it/s, pg=-0.0249, ret=3.9e-5, glen=60.9, tlen=221, kl=0.00233, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:39,  1.10it/s, pg=-0.0249, ret=3.9e-5, glen=60.9, tlen=221, kl=0.00233, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:39,  1.10it/s, pg=-0.0112, ret=-5.3e-6, glen=61.4, tlen=221, kl=0.00201, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.10it/s, pg=-0.0112, ret=-5.3e-6, glen=61.4, tlen=221, kl=0.00201, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.10it/s, pg=-0.0591, ret=9.16e-5, glen=61.9, tlen=222, kl=0.00169, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.13it/s, pg=-0.0591, ret=9.16e-5, glen=61.9, tlen=222, kl=0.00169, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.13it/s, pg=-0.016, ret=1.7e-6, glen=60.4, tlen=220, kl=0.00223, act_lr=1e-6, ent=0.889]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:35,  1.14it/s, pg=-0.016, ret=1.7e-6, glen=60.4, tlen=220, kl=0.00223, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:35,  1.14it/s, pg=0.131, ret=-0.000152, glen=61.9, tlen=222, kl=0.00183, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.15it/s, pg=0.131, ret=-0.000152, glen=61.9, tlen=222, kl=0.00183, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.15it/s, pg=-0.0823, ret=0.000134, glen=61.7, tlen=222, kl=0.00206, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=-0.0823, ret=0.000134, glen=61.7, tlen=222, kl=0.00206, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=-0.0365, ret=1.9e-5, glen=60.5, tlen=221, kl=0.00183, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0365, ret=1.9e-5, glen=60.5, tlen=221, kl=0.00183, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0475, ret=7.52e-5, glen=60.4, tlen=220, kl=0.0019, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.17it/s, pg=-0.0475, ret=7.52e-5, glen=60.4, tlen=220, kl=0.0019, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.17it/s, pg=-0.0687, ret=0.000108, glen=62.9, tlen=223, kl=0.00185, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.15it/s, pg=-0.0687, ret=0.000108, glen=62.9, tlen=223, kl=0.00185, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.15it/s, pg=0.0717, ret=-9.84e-5, glen=61.4, tlen=221, kl=0.00187, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.16it/s, pg=0.0717, ret=-9.84e-5, glen=61.4, tlen=221, kl=0.00187, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.16it/s, pg=0.0436, ret=-2.57e-5, glen=60, tlen=220, kl=0.00233, act_lr=1e-6, ent=0.934]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=0.0436, ret=-2.57e-5, glen=60, tlen=220, kl=0.00233, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=0.00433, ret=-3.86e-6, glen=59.7, tlen=220, kl=0.00194, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=0.00433, ret=-3.86e-6, glen=59.7, tlen=220, kl=0.00194, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=0.0116, ret=2.6e-5, glen=60.4, tlen=220, kl=0.00187, act_lr=1e-6, ent=0.908]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=0.0116, ret=2.6e-5, glen=60.4, tlen=220, kl=0.00187, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.0793, ret=0.000131, glen=63.6, tlen=224, kl=0.0017, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0793, ret=0.000131, glen=63.6, tlen=224, kl=0.0017, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0435, ret=6.37e-5, glen=61.9, tlen=221, kl=0.00172, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0435, ret=6.37e-5, glen=61.9, tlen=221, kl=0.00172, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0388, ret=4.83e-5, glen=59.7, tlen=220, kl=0.00169, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0388, ret=4.83e-5, glen=59.7, tlen=220, kl=0.00169, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.108, ret=0.000168, glen=61.3, tlen=221, kl=0.0017, act_lr=1e-6, ent=0.868] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.108, ret=0.000168, glen=61.3, tlen=221, kl=0.0017, act_lr=1e-6, ent=0.868]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0848, ret=0.000144, glen=61.1, tlen=221, kl=0.00231, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0848, ret=0.000144, glen=61.1, tlen=221, kl=0.00231, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=0.204, ret=-0.00034, glen=62, tlen=222, kl=0.00167, act_lr=1e-6, ent=0.898]    Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.204, ret=-0.00034, glen=62, tlen=222, kl=0.00167, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0452, ret=6.56e-5, glen=61.7, tlen=222, kl=0.00191, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.14it/s, pg=-0.0452, ret=6.56e-5, glen=61.7, tlen=222, kl=0.00191, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.14it/s, pg=0.153, ret=-0.000201, glen=59, tlen=219, kl=0.00181, act_lr=1e-6, ent=0.91]   Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=0.153, ret=-0.000201, glen=59, tlen=219, kl=0.00181, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=0.0713, ret=-9.65e-5, glen=60.6, tlen=220, kl=0.00227, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.16it/s, pg=0.0713, ret=-9.65e-5, glen=60.6, tlen=220, kl=0.00227, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=0.161, ret=-0.000224, glen=62.7, tlen=222, kl=0.00183, act_lr=1e-6, ent=0.871]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.16it/s, pg=0.161, ret=-0.000224, glen=62.7, tlen=222, kl=0.00183, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.16it/s, pg=-0.0845, ret=0.00014, glen=60.4, tlen=220, kl=0.00177, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.17it/s, pg=-0.0845, ret=0.00014, glen=60.4, tlen=220, kl=0.00177, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.17it/s, pg=-0.0125, ret=-5.83e-6, glen=60.1, tlen=220, kl=0.00187, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.0125, ret=-5.83e-6, glen=60.1, tlen=220, kl=0.00187, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.0995, ret=-0.000118, glen=60.6, tlen=221, kl=0.00181, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.13it/s, pg=0.0995, ret=-0.000118, glen=60.6, tlen=221, kl=0.00181, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.13it/s, pg=-0.0083, ret=-2.65e-5, glen=61.2, tlen=221, kl=0.00179, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.14it/s, pg=-0.0083, ret=-2.65e-5, glen=61.2, tlen=221, kl=0.00179, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.14it/s, pg=-0.0229, ret=-9.33e-6, glen=62.7, tlen=223, kl=0.00154, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:17,  1.01s/it, pg=-0.0229, ret=-9.33e-6, glen=62.7, tlen=223, kl=0.00154, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:17,  1.01s/it, pg=-0.0108, ret=5.98e-6, glen=60.4, tlen=221, kl=0.00161, act_lr=1e-6, ent=0.896] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:15,  1.04it/s, pg=-0.0108, ret=5.98e-6, glen=60.4, tlen=221, kl=0.00161, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:15,  1.04it/s, pg=0.00183, ret=-4.39e-5, glen=59.1, tlen=219, kl=0.00178, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.08it/s, pg=0.00183, ret=-4.39e-5, glen=59.1, tlen=219, kl=0.00178, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.08it/s, pg=-0.0822, ret=0.000127, glen=60.3, tlen=220, kl=0.00184, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=-0.0822, ret=0.000127, glen=60.3, tlen=220, kl=0.00184, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=0.0112, ret=-1.35e-5, glen=59.9, tlen=220, kl=0.0018, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.13it/s, pg=0.0112, ret=-1.35e-5, glen=59.9, tlen=220, kl=0.0018, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.103, ret=0.000166, glen=60.6, tlen=221, kl=0.00177, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.103, ret=0.000166, glen=60.6, tlen=221, kl=0.00177, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=-0.017, ret=2.57e-7, glen=61.6, tlen=221, kl=0.00167, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=-0.017, ret=2.57e-7, glen=61.6, tlen=221, kl=0.00167, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=0.0698, ret=-0.000147, glen=61.6, tlen=222, kl=0.00195, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=0.0698, ret=-0.000147, glen=61.6, tlen=222, kl=0.00195, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0637, ret=9.23e-5, glen=61.2, tlen=221, kl=0.00175, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0637, ret=9.23e-5, glen=61.2, tlen=221, kl=0.00175, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=0.00446, ret=1.63e-5, glen=60.5, tlen=221, kl=0.00195, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.00446, ret=1.63e-5, glen=60.5, tlen=221, kl=0.00195, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0014, ret=-1.64e-5, glen=62.3, tlen=223, kl=0.00215, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0014, ret=-1.64e-5, glen=62.3, tlen=223, kl=0.00215, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.00684, ret=2.19e-6, glen=60.7, tlen=221, kl=0.00193, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.00684, ret=2.19e-6, glen=60.7, tlen=221, kl=0.00193, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0848, ret=0.000132, glen=60.4, tlen=220, kl=0.00191, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0848, ret=0.000132, glen=60.4, tlen=220, kl=0.00191, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0813, ret=-0.000107, glen=62.8, tlen=223, kl=0.00207, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0813, ret=-0.000107, glen=62.8, tlen=223, kl=0.00207, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0815, ret=0.000127, glen=59.6, tlen=220, kl=0.00177, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0815, ret=0.000127, glen=59.6, tlen=220, kl=0.00177, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.0132, ret=-1.54e-5, glen=61, tlen=221, kl=0.00174, act_lr=1e-6, ent=0.893]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.0132, ret=-1.54e-5, glen=61, tlen=221, kl=0.00174, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.0597, ret=7.91e-5, glen=60.3, tlen=220, kl=0.0021, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0597, ret=7.91e-5, glen=60.3, tlen=220, kl=0.0021, act_lr=1e-6, ent=0.889]
2025-07-24 20:56:48.792 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.12s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.18it/s, pg=0.136, ret=-0.000197, glen=60.7, tlen=220, kl=0.00181, act_lr=1e-6, ent=0.869]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=0.136, ret=-0.000197, glen=60.7, tlen=220, kl=0.00181, act_lr=1e-6, ent=0.869]
2025-07-24 20:56:49.641 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 20:56:52.195 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.55s
2025-07-24 20:56:52.542 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.00s
2025-07-24 20:56:52.548 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005096767259680706, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8992573294950568, 'kl': 0.0018862019414487092, 'response_length': 61.05676941249681, 'total_length': 221.0412889563519, 'teacher_total_length': 233.003394085428, 'return': 6.17959911956543e-07, 'policy_update_steps': 1.0}

Episode [10/20]:  15%|‚ñà‚ñå        | 2/13 [03:42<20:24, 111.27s/it][A2025-07-24 20:56:52.594 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:57:28.349 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:57:28.517 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 20:57:28.517 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.92s
2025-07-24 20:57:30.426 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0009,avg_pass_at_n: 1.0000,avg_num_tokens: 60.8289,std_num_tokens: 13.2727,avg_correct_num_tokens: 60.8338,std_correct_num_tokens: 13.2621,avg_incorrect_num_tokens: 59.6061,std_incorrect_num_tokens: 15.6223
2025-07-24 20:57:30.712 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.19s
2025-07-24 20:57:33.494 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.78s
2025-07-24 20:57:57.030 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 20:57:57.031 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.54s
2025-07-24 20:57:58.338 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.87s
2025-07-24 20:57:58.338 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 9.631731705180283e-06, avg_kl: 0.0035678884370730873, avg_response_length: 60.8426764858225, avg_orm_score: 0.0, avg_custom_rewards: 9.631731705180283e-06
2025-07-24 20:57:58.368 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter119_replay_buffer.jsonl
2025-07-24 20:57:59.679 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.31s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.00751, ret=-1.86e-5, glen=59.9, tlen=220, kl=0.00316, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=0.00751, ret=-1.86e-5, glen=59.9, tlen=220, kl=0.00316, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0605, ret=8.13e-5, glen=61, tlen=222, kl=0.00376, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0605, ret=8.13e-5, glen=61, tlen=222, kl=0.00376, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=0.164, ret=-0.000145, glen=59.2, tlen=220, kl=0.00287, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=0.164, ret=-0.000145, glen=59.2, tlen=220, kl=0.00287, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0831, ret=0.00012, glen=61.8, tlen=223, kl=0.00337, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.11it/s, pg=-0.0831, ret=0.00012, glen=61.8, tlen=223, kl=0.00337, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.11it/s, pg=0.136, ret=-0.000143, glen=63.6, tlen=224, kl=0.00331, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=0.136, ret=-0.000143, glen=63.6, tlen=224, kl=0.00331, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=0.18, ret=-0.00027, glen=60.3, tlen=221, kl=0.00404, act_lr=1e-6, ent=0.883]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.15it/s, pg=0.18, ret=-0.00027, glen=60.3, tlen=221, kl=0.00404, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.15it/s, pg=-0.0338, ret=4.9e-5, glen=61, tlen=222, kl=0.00312, act_lr=1e-6, ent=0.883] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=-0.0338, ret=4.9e-5, glen=61, tlen=222, kl=0.00312, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=-0.0811, ret=0.000114, glen=59.7, tlen=220, kl=0.00374, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0811, ret=0.000114, glen=59.7, tlen=220, kl=0.00374, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=0.0271, ret=-5.93e-5, glen=59.2, tlen=220, kl=0.00313, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:32,  1.14it/s, pg=0.0271, ret=-5.93e-5, glen=59.2, tlen=220, kl=0.00313, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=-0.0914, ret=0.000139, glen=59.7, tlen=220, kl=0.00372, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:32,  1.12it/s, pg=-0.0914, ret=0.000139, glen=59.7, tlen=220, kl=0.00372, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:32,  1.12it/s, pg=0.037, ret=-3.86e-5, glen=61.2, tlen=222, kl=0.00388, act_lr=1e-6, ent=0.885]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.14it/s, pg=0.037, ret=-3.86e-5, glen=61.2, tlen=222, kl=0.00388, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.14it/s, pg=0.0853, ret=-3.65e-5, glen=61.2, tlen=222, kl=0.00436, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=0.0853, ret=-3.65e-5, glen=61.2, tlen=222, kl=0.00436, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=0.00891, ret=-4.24e-5, glen=60.3, tlen=221, kl=0.00374, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=0.00891, ret=-4.24e-5, glen=60.3, tlen=221, kl=0.00374, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0583, ret=8.37e-5, glen=61.3, tlen=222, kl=0.00351, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0583, ret=8.37e-5, glen=61.3, tlen=222, kl=0.00351, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=-3.05e-5, ret=-2.57e-5, glen=61.7, tlen=222, kl=0.00432, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-3.05e-5, ret=-2.57e-5, glen=61.7, tlen=222, kl=0.00432, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0469, ret=6.45e-5, glen=60.5, tlen=221, kl=0.00382, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0469, ret=6.45e-5, glen=60.5, tlen=221, kl=0.00382, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0485, ret=7.14e-5, glen=60.9, tlen=221, kl=0.00302, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0485, ret=7.14e-5, glen=60.9, tlen=221, kl=0.00302, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.0872, ret=-0.000157, glen=60.7, tlen=221, kl=0.00242, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.0872, ret=-0.000157, glen=60.7, tlen=221, kl=0.00242, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=0.22, ret=-0.000291, glen=61.5, tlen=222, kl=0.0035, act_lr=1e-6, ent=0.936]   Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=0.22, ret=-0.000291, glen=61.5, tlen=222, kl=0.0035, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0527, ret=7.1e-5, glen=61.3, tlen=222, kl=0.00375, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0527, ret=7.1e-5, glen=61.3, tlen=222, kl=0.00375, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0654, ret=9.13e-5, glen=61.1, tlen=222, kl=0.00372, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=-0.0654, ret=9.13e-5, glen=61.1, tlen=222, kl=0.00372, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.032, ret=-5.1e-5, glen=61.1, tlen=222, kl=0.0029, act_lr=1e-6, ent=0.906]   Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.032, ret=-5.1e-5, glen=61.1, tlen=222, kl=0.0029, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.011, ret=-4.21e-5, glen=60.5, tlen=221, kl=0.00415, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.17it/s, pg=0.011, ret=-4.21e-5, glen=60.5, tlen=221, kl=0.00415, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.08, ret=0.000106, glen=60.4, tlen=221, kl=0.00394, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.08, ret=0.000106, glen=60.4, tlen=221, kl=0.00394, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.126, ret=-0.000158, glen=61.9, tlen=223, kl=0.00303, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=0.126, ret=-0.000158, glen=61.9, tlen=223, kl=0.00303, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=0.0315, ret=-4.7e-5, glen=60.1, tlen=221, kl=0.00299, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.0315, ret=-4.7e-5, glen=60.1, tlen=221, kl=0.00299, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.0705, ret=-0.000139, glen=60.9, tlen=221, kl=0.00352, act_lr=1e-6, ent=0.879]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.0705, ret=-0.000139, glen=60.9, tlen=221, kl=0.00352, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=0.0477, ret=-4.95e-5, glen=59.9, tlen=221, kl=0.0043, act_lr=1e-6, ent=0.882]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.18it/s, pg=0.0477, ret=-4.95e-5, glen=59.9, tlen=221, kl=0.0043, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.18it/s, pg=-0.0635, ret=9.33e-5, glen=61.9, tlen=223, kl=0.00396, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:16,  1.02it/s, pg=-0.0635, ret=9.33e-5, glen=61.9, tlen=223, kl=0.00396, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:16,  1.02it/s, pg=0.0736, ret=-4.7e-5, glen=61.5, tlen=222, kl=0.00364, act_lr=1e-6, ent=0.892] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:15,  1.06it/s, pg=0.0736, ret=-4.7e-5, glen=61.5, tlen=222, kl=0.00364, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:15,  1.06it/s, pg=-0.0522, ret=7.33e-5, glen=60.8, tlen=222, kl=0.00378, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.08it/s, pg=-0.0522, ret=7.33e-5, glen=60.8, tlen=222, kl=0.00378, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.08it/s, pg=-0.0556, ret=8.34e-5, glen=61.5, tlen=222, kl=0.00313, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.10it/s, pg=-0.0556, ret=8.34e-5, glen=61.5, tlen=222, kl=0.00313, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.10it/s, pg=-0.0875, ret=0.000121, glen=59.6, tlen=220, kl=0.00421, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.13it/s, pg=-0.0875, ret=0.000121, glen=59.6, tlen=220, kl=0.00421, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=0.0781, ret=-0.000133, glen=60.9, tlen=222, kl=0.00367, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=0.0781, ret=-0.000133, glen=60.9, tlen=222, kl=0.00367, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=-0.0554, ret=7.59e-5, glen=60.3, tlen=221, kl=0.00298, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=-0.0554, ret=7.59e-5, glen=60.3, tlen=221, kl=0.00298, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=-0.0657, ret=9.26e-5, glen=59.4, tlen=220, kl=0.00377, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0657, ret=9.26e-5, glen=59.4, tlen=220, kl=0.00377, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0519, ret=7.91e-5, glen=61.2, tlen=222, kl=0.0034, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.14it/s, pg=-0.0519, ret=7.91e-5, glen=61.2, tlen=222, kl=0.0034, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.14it/s, pg=0.0939, ret=-0.000179, glen=62, tlen=222, kl=0.00305, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.15it/s, pg=0.0939, ret=-0.000179, glen=62, tlen=222, kl=0.00305, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.15it/s, pg=-0.0706, ret=9.84e-5, glen=60.6, tlen=221, kl=0.00365, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:06,  1.16it/s, pg=-0.0706, ret=9.84e-5, glen=60.6, tlen=221, kl=0.00365, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:06,  1.16it/s, pg=-0.0861, ret=0.00012, glen=59.6, tlen=220, kl=0.0045, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.16it/s, pg=-0.0861, ret=0.00012, glen=59.6, tlen=220, kl=0.0045, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.16it/s, pg=0.0166, ret=-3.28e-5, glen=60.4, tlen=221, kl=0.00499, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0166, ret=-3.28e-5, glen=60.4, tlen=221, kl=0.00499, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0657, ret=8.87e-5, glen=60.2, tlen=221, kl=0.00333, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0657, ret=8.87e-5, glen=60.2, tlen=221, kl=0.00333, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0694, ret=9.52e-5, glen=61.1, tlen=221, kl=0.00283, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0694, ret=9.52e-5, glen=61.1, tlen=221, kl=0.00283, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0673, ret=9.37e-5, glen=62.1, tlen=223, kl=0.00305, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0673, ret=9.37e-5, glen=62.1, tlen=223, kl=0.00305, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.0623, ret=8.16e-5, glen=61.5, tlen=222, kl=0.00325, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0623, ret=8.16e-5, glen=61.5, tlen=222, kl=0.00325, act_lr=1e-6, ent=0.9]
2025-07-24 20:58:40.014 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.14s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.18it/s, pg=0.0113, ret=-3.67e-5, glen=61.4, tlen=222, kl=0.00369, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=0.0113, ret=-3.67e-5, glen=61.4, tlen=222, kl=0.00369, act_lr=1e-6, ent=0.915]
2025-07-24 20:58:40.673 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 20:58:43.077 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.40s
2025-07-24 20:58:43.427 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.67s
2025-07-24 20:58:43.434 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00019935939622961956, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9038853645324707, 'kl': 0.003564502881920856, 'response_length': 60.83205778702445, 'total_length': 221.43240455959153, 'teacher_total_length': 233.3912011851435, 'return': 9.886297962309427e-07, 'policy_update_steps': 1.0}

Episode [10/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:33<18:30, 111.10s/it][A2025-07-24 20:58:43.439 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<02:20,  1.22it/s, est. speed input: 223.25 toks/s, output: 37.82 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   3%|‚ñé         | 6/172 [00:01<00:20,  8.27it/s, est. speed input: 1030.83 toks/s, output: 207.31 toks/s]Processed prompts:   6%|‚ñå         | 10/172 [00:01<00:11, 14.22it/s, est. speed input: 1559.49 toks/s, output: 329.90 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/172 [00:01<00:01, 73.63it/s, est. speed input: 5934.77 toks/s, output: 1604.69 toks/s]Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/172 [00:01<00:01, 90.39it/s, est. speed input: 6988.91 toks/s, output: 1990.53 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/172 [00:02<00:00, 96.38it/s, est. speed input: 10822.88 toks/s, output: 3649.61 toks/s] 
[36m(LLMActor pid=1435059)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:02<00:00, 83.61it/s, est. speed input: 10623.61 toks/s, output: 3559.70 toks/s]Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 165/172 [00:02<00:00, 84.69it/s, est. speed input: 10821.17 toks/s, output: 3658.84 toks/s]
2025-07-24 20:58:48.105 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 301.2853,strategyqa_test/accuracy: 0.5473,eval_accuracy: 0.5473
2025-07-24 20:58:48.397 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 20:59:25.027 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 20:59:25.205 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 20:59:25.206 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.81s
2025-07-24 20:59:27.128 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0140,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 62.7997,std_num_tokens: 13.8004,avg_correct_num_tokens: 62.7769,std_correct_num_tokens: 13.7639,avg_incorrect_num_tokens: 66.6667,std_incorrect_num_tokens: 18.5970
2025-07-24 20:59:27.556 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.35s
2025-07-24 20:59:30.086 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.53s
2025-07-24 20:59:54.118 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 20:59:54.119 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.03s
2025-07-24 20:59:55.392 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 20:59:55.393 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 5.154986425328094e-05, avg_kl: 0.004194682353251689, avg_response_length: 62.81871997730152, avg_orm_score: 0.0, avg_custom_rewards: 5.154986425328094e-05
2025-07-24 20:59:55.422 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter120_replay_buffer.jsonl
2025-07-24 20:59:56.795 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.38s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 146/172 [00:02<00:00, 96.79it/s, est. speed input: 9847.00 toks/s, output: 3247.32 toks/s][32m [repeated 54x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:03<00:00, 44.19it/s, est. speed input: 8010.87 toks/s, output: 2807.85 toks/s][32m [repeated 9x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:01<?, ?it/s, pg=0.0665, ret=-0.000143, glen=62.3, tlen=223, kl=0.00477, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.02s/it, pg=0.0665, ret=-0.000143, glen=62.3, tlen=223, kl=0.00477, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.02s/it, pg=0.0706, ret=-8.67e-5, glen=62.1, tlen=223, kl=0.00431, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.08it/s, pg=0.0706, ret=-8.67e-5, glen=62.1, tlen=223, kl=0.00431, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.08it/s, pg=-0.0061, ret=1.94e-5, glen=63.6, tlen=225, kl=0.00513, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:40,  1.09it/s, pg=-0.0061, ret=1.94e-5, glen=63.6, tlen=225, kl=0.00513, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:40,  1.09it/s, pg=0.0139, ret=3.15e-6, glen=63.1, tlen=224, kl=0.00396, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:38,  1.12it/s, pg=0.0139, ret=3.15e-6, glen=63.1, tlen=224, kl=0.00396, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:38,  1.12it/s, pg=0.0552, ret=-0.000116, glen=62.1, tlen=223, kl=0.00434, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:36,  1.14it/s, pg=0.0552, ret=-0.000116, glen=62.1, tlen=223, kl=0.00434, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:36,  1.14it/s, pg=0.104, ret=-8.29e-5, glen=63.4, tlen=224, kl=0.00447, act_lr=1e-6, ent=0.894]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:35,  1.15it/s, pg=0.104, ret=-8.29e-5, glen=63.4, tlen=224, kl=0.00447, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:35,  1.15it/s, pg=-0.0728, ret=0.000125, glen=62.3, tlen=223, kl=0.00437, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:34,  1.16it/s, pg=-0.0728, ret=0.000125, glen=62.3, tlen=223, kl=0.00437, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:07<00:34,  1.16it/s, pg=-0.0779, ret=0.00013, glen=62.9, tlen=224, kl=0.00456, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.16it/s, pg=-0.0779, ret=0.00013, glen=62.9, tlen=224, kl=0.00456, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.16it/s, pg=0.144, ret=-0.000266, glen=64.5, tlen=226, kl=0.00343, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:07<00:32,  1.17it/s, pg=0.144, ret=-0.000266, glen=64.5, tlen=226, kl=0.00343, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:32,  1.17it/s, pg=-0.0673, ret=0.000112, glen=62.5, tlen=223, kl=0.00357, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:31,  1.17it/s, pg=-0.0673, ret=0.000112, glen=62.5, tlen=223, kl=0.00357, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:31,  1.17it/s, pg=0.0765, ret=-0.000125, glen=61.8, tlen=222, kl=0.00483, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:31,  1.15it/s, pg=0.0765, ret=-0.000125, glen=61.8, tlen=222, kl=0.00483, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:31,  1.15it/s, pg=-0.112, ret=0.000187, glen=63.5, tlen=224, kl=0.0049, act_lr=1e-6, ent=0.919]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:30,  1.13it/s, pg=-0.112, ret=0.000187, glen=63.5, tlen=224, kl=0.0049, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:30,  1.13it/s, pg=0.162, ret=-0.000241, glen=63.9, tlen=225, kl=0.00373, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.14it/s, pg=0.162, ret=-0.000241, glen=63.9, tlen=225, kl=0.00373, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.14it/s, pg=-0.0839, ret=0.000138, glen=62.8, tlen=224, kl=0.00358, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:28,  1.15it/s, pg=-0.0839, ret=0.000138, glen=62.8, tlen=224, kl=0.00358, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:28,  1.15it/s, pg=-0.00458, ret=-9.31e-6, glen=63.7, tlen=224, kl=0.00515, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.00458, ret=-9.31e-6, glen=63.7, tlen=224, kl=0.00515, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.0842, ret=0.000141, glen=61.5, tlen=222, kl=0.00439, act_lr=1e-6, ent=0.929] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:13<00:26,  1.17it/s, pg=-0.0842, ret=0.000141, glen=61.5, tlen=222, kl=0.00439, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.17it/s, pg=-0.0706, ret=0.000119, glen=64.1, tlen=225, kl=0.00413, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:25,  1.17it/s, pg=-0.0706, ret=0.000119, glen=64.1, tlen=225, kl=0.00413, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:25,  1.17it/s, pg=-0.0111, ret=-1.27e-5, glen=63.1, tlen=224, kl=0.00344, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:24,  1.17it/s, pg=-0.0111, ret=-1.27e-5, glen=63.1, tlen=224, kl=0.00344, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:24,  1.17it/s, pg=-0.0732, ret=0.000117, glen=62.7, tlen=223, kl=0.00363, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:23,  1.17it/s, pg=-0.0732, ret=0.000117, glen=62.7, tlen=223, kl=0.00363, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:23,  1.17it/s, pg=-0.0452, ret=2.74e-5, glen=61.5, tlen=223, kl=0.00496, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.17it/s, pg=-0.0452, ret=2.74e-5, glen=61.5, tlen=223, kl=0.00496, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.17it/s, pg=0.115, ret=-0.000122, glen=63.2, tlen=223, kl=0.00462, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.17it/s, pg=0.115, ret=-0.000122, glen=63.2, tlen=223, kl=0.00462, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.17it/s, pg=0.0347, ret=8e-6, glen=62.2, tlen=223, kl=0.00397, act_lr=1e-6, ent=0.917]    Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=0.0347, ret=8e-6, glen=62.2, tlen=223, kl=0.00397, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.0178, ret=6.61e-6, glen=62.6, tlen=223, kl=0.00397, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:19<00:20,  1.17it/s, pg=-0.0178, ret=6.61e-6, glen=62.6, tlen=223, kl=0.00397, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=-0.0151, ret=-3.64e-12, glen=63.4, tlen=224, kl=0.00406, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.17it/s, pg=-0.0151, ret=-3.64e-12, glen=63.4, tlen=224, kl=0.00406, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.17it/s, pg=0.243, ret=-0.000288, glen=62, tlen=222, kl=0.00404, act_lr=1e-6, ent=0.933]    Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.17it/s, pg=0.243, ret=-0.000288, glen=62, tlen=222, kl=0.00404, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.17it/s, pg=0.041, ret=9.44e-6, glen=61.5, tlen=222, kl=0.00414, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:17,  1.17it/s, pg=0.041, ret=9.44e-6, glen=61.5, tlen=222, kl=0.00414, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:17,  1.17it/s, pg=-0.074, ret=0.000128, glen=63.3, tlen=224, kl=0.00423, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:17,  1.14it/s, pg=-0.074, ret=0.000128, glen=63.3, tlen=224, kl=0.00423, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:17,  1.14it/s, pg=-0.0872, ret=0.000144, glen=62.2, tlen=223, kl=0.00462, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.15it/s, pg=-0.0872, ret=0.000144, glen=62.2, tlen=223, kl=0.00462, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.15it/s, pg=-0.0862, ret=0.000134, glen=64, tlen=224, kl=0.00389, act_lr=1e-6, ent=0.921]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:17,  1.06it/s, pg=-0.0862, ret=0.000134, glen=64, tlen=224, kl=0.00389, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:17,  1.06it/s, pg=0.044, ret=-0.000114, glen=61.7, tlen=222, kl=0.00427, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.09it/s, pg=0.044, ret=-0.000114, glen=61.7, tlen=222, kl=0.00427, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:27<00:15,  1.09it/s, pg=0.0711, ret=-0.000137, glen=62.8, tlen=224, kl=0.00369, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.11it/s, pg=0.0711, ret=-0.000137, glen=62.8, tlen=224, kl=0.00369, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.11it/s, pg=-0.0655, ret=0.000111, glen=64.5, tlen=225, kl=0.00428, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:27<00:13,  1.13it/s, pg=-0.0655, ret=0.000111, glen=64.5, tlen=225, kl=0.00428, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.13it/s, pg=0.0791, ret=-0.000114, glen=63, tlen=223, kl=0.00329, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.15it/s, pg=0.0791, ret=-0.000114, glen=63, tlen=223, kl=0.00329, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.15it/s, pg=-0.0836, ret=0.000135, glen=60.8, tlen=222, kl=0.00414, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.15it/s, pg=-0.0836, ret=0.000135, glen=60.8, tlen=222, kl=0.00414, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.15it/s, pg=-0.0734, ret=0.000123, glen=62.5, tlen=223, kl=0.00413, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.16it/s, pg=-0.0734, ret=0.000123, glen=62.5, tlen=223, kl=0.00413, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.16it/s, pg=-0.0781, ret=0.000129, glen=61.7, tlen=223, kl=0.00641, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.17it/s, pg=-0.0781, ret=0.000129, glen=61.7, tlen=223, kl=0.00641, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.17it/s, pg=0.0522, ret=-0.000121, glen=62.4, tlen=223, kl=0.00398, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=0.0522, ret=-0.000121, glen=62.4, tlen=223, kl=0.00398, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:33<00:08,  1.17it/s, pg=-0.0769, ret=0.000133, glen=64.6, tlen=226, kl=0.0042, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=-0.0769, ret=0.000133, glen=64.6, tlen=226, kl=0.0042, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=0.0873, ret=-0.000228, glen=62, tlen=223, kl=0.00425, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:33<00:06,  1.17it/s, pg=0.0873, ret=-0.000228, glen=62, tlen=223, kl=0.00425, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=0.0629, ret=-0.0001, glen=62.4, tlen=223, kl=0.00496, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:06,  1.16it/s, pg=0.0629, ret=-0.0001, glen=62.4, tlen=223, kl=0.00496, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:06,  1.16it/s, pg=-0.0731, ret=0.000118, glen=61.4, tlen=222, kl=0.0051, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.16it/s, pg=-0.0731, ret=0.000118, glen=61.4, tlen=222, kl=0.0051, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.16it/s, pg=-0.0097, ret=4.22e-6, glen=63.7, tlen=224, kl=0.00342, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.17it/s, pg=-0.0097, ret=4.22e-6, glen=63.7, tlen=224, kl=0.00342, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.17it/s, pg=0.0879, ret=-0.000117, glen=63.2, tlen=224, kl=0.00378, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.17it/s, pg=0.0879, ret=-0.000117, glen=63.2, tlen=224, kl=0.00378, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.17it/s, pg=-0.0836, ret=0.000135, glen=65.8, tlen=226, kl=0.00289, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.17it/s, pg=-0.0836, ret=0.000135, glen=65.8, tlen=226, kl=0.00289, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:39<00:02,  1.17it/s, pg=-0.00513, ret=-2.48e-6, glen=61.9, tlen=222, kl=0.00401, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.17it/s, pg=-0.00513, ret=-2.48e-6, glen=61.9, tlen=222, kl=0.00401, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.17it/s, pg=0.00452, ret=2.51e-5, glen=62.9, tlen=224, kl=0.00396, act_lr=1e-6, ent=0.931]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:39<00:00,  1.17it/s, pg=0.00452, ret=2.51e-5, glen=62.9, tlen=224, kl=0.00396, act_lr=1e-6, ent=0.931]
2025-07-24 21:00:37.824 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.17it/s, pg=-0.0763, ret=0.000126, glen=63.4, tlen=224, kl=0.00378, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.13it/s, pg=-0.0763, ret=0.000126, glen=63.4, tlen=224, kl=0.00378, act_lr=1e-6, ent=0.906]
2025-07-24 21:00:38.494 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 21:00:40.786 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-24 21:00:41.134 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.28s
2025-07-24 21:00:41.187 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 2.856964760638298e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.91352346095633, 'kl': 0.004206555954953457, 'response_length': 62.81559542392163, 'total_length': 223.47662937894782, 'teacher_total_length': 235.4701791316905, 'return': 3.4312980131560717e-06, 'policy_update_steps': 1.0}

Episode [10/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [07:31<17:03, 113.72s/it][A2025-07-24 21:00:41.228 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:01:17.707 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:01:17.887 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:01:17.887 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.66s
2025-07-24 21:01:19.605 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0140,avg_reflection_pattern_score: 0.0020,avg_pass_at_n: 1.0000,avg_num_tokens: 62.7324,std_num_tokens: 13.7452,avg_correct_num_tokens: 62.7305,std_correct_num_tokens: 13.7414,avg_incorrect_num_tokens: 63.2759,std_incorrect_num_tokens: 14.7599
2025-07-24 21:01:20.026 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.14s
2025-07-24 21:01:22.793 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.76s
2025-07-24 21:01:46.365 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:01:46.365 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.57s
2025-07-24 21:01:47.608 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 21:01:47.609 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4173192460515091e-05, avg_kl: 0.007210607114045516, avg_response_length: 62.7493720261947, avg_orm_score: 0.0, avg_custom_rewards: 1.4173192460515091e-05
2025-07-24 21:01:47.642 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter121_replay_buffer.jsonl
2025-07-24 21:01:48.926 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.29s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=-0.00275, ret=-1.78e-5, glen=61.6, tlen=222, kl=0.00678, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.00it/s, pg=-0.00275, ret=-1.78e-5, glen=61.6, tlen=222, kl=0.00678, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.00it/s, pg=-0.0784, ret=0.000105, glen=64.9, tlen=225, kl=0.00458, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0784, ret=0.000105, glen=64.9, tlen=225, kl=0.00458, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=0.0182, ret=-6.23e-5, glen=63, tlen=223, kl=0.00762, act_lr=1e-6, ent=0.925]   Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.09it/s, pg=0.0182, ret=-6.23e-5, glen=63, tlen=223, kl=0.00762, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.09it/s, pg=-0.0535, ret=6.8e-5, glen=60.9, tlen=221, kl=0.00896, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.11it/s, pg=-0.0535, ret=6.8e-5, glen=60.9, tlen=221, kl=0.00896, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.11it/s, pg=-0.0022, ret=-3.47e-5, glen=63.8, tlen=224, kl=0.0092, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=-0.0022, ret=-3.47e-5, glen=63.8, tlen=224, kl=0.0092, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=0.058, ret=-5.73e-5, glen=63.4, tlen=223, kl=0.00686, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.12it/s, pg=0.058, ret=-5.73e-5, glen=63.4, tlen=223, kl=0.00686, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.12it/s, pg=-0.00867, ret=-1.42e-5, glen=62, tlen=222, kl=0.00765, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=-0.00867, ret=-1.42e-5, glen=62, tlen=222, kl=0.00765, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=-0.0795, ret=0.000108, glen=61.5, tlen=222, kl=0.00745, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=-0.0795, ret=0.000108, glen=61.5, tlen=222, kl=0.00745, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=-0.058, ret=7.11e-5, glen=62.9, tlen=223, kl=0.00687, act_lr=1e-6, ent=0.921]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:32,  1.15it/s, pg=-0.058, ret=7.11e-5, glen=62.9, tlen=223, kl=0.00687, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=0.0271, ret=-6.58e-5, glen=63.9, tlen=224, kl=0.00972, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.16it/s, pg=0.0271, ret=-6.58e-5, glen=63.9, tlen=224, kl=0.00972, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.16it/s, pg=-0.0516, ret=7.18e-5, glen=64.5, tlen=225, kl=0.00678, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.17it/s, pg=-0.0516, ret=7.18e-5, glen=64.5, tlen=225, kl=0.00678, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.17it/s, pg=0.0362, ret=-4.37e-5, glen=61.9, tlen=222, kl=0.0071, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.17it/s, pg=0.0362, ret=-4.37e-5, glen=61.9, tlen=222, kl=0.0071, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.17it/s, pg=0.0701, ret=-6.47e-5, glen=62, tlen=222, kl=0.00751, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=0.0701, ret=-6.47e-5, glen=62, tlen=222, kl=0.00751, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=0.113, ret=-0.000166, glen=61.3, tlen=221, kl=0.00869, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=0.113, ret=-0.000166, glen=61.3, tlen=221, kl=0.00869, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.0398, ret=5.61e-5, glen=64.5, tlen=224, kl=0.00558, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0398, ret=5.61e-5, glen=64.5, tlen=224, kl=0.00558, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0367, ret=5.09e-5, glen=63.2, tlen=223, kl=0.00777, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0367, ret=5.09e-5, glen=63.2, tlen=223, kl=0.00777, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=0.0158, ret=-2.29e-5, glen=62.5, tlen=223, kl=0.00656, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=0.0158, ret=-2.29e-5, glen=62.5, tlen=223, kl=0.00656, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.133, ret=-0.000133, glen=63.2, tlen=223, kl=0.0064, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.133, ret=-0.000133, glen=63.2, tlen=223, kl=0.0064, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0439, ret=5.32e-5, glen=62.3, tlen=222, kl=0.006, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.17it/s, pg=-0.0439, ret=5.32e-5, glen=62.3, tlen=222, kl=0.006, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.17it/s, pg=-0.0531, ret=7.01e-5, glen=63.1, tlen=223, kl=0.00722, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0531, ret=7.01e-5, glen=63.1, tlen=223, kl=0.00722, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0509, ret=7.07e-5, glen=60.8, tlen=221, kl=0.00993, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.14it/s, pg=-0.0509, ret=7.07e-5, glen=60.8, tlen=221, kl=0.00993, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.14it/s, pg=-0.067, ret=9.04e-5, glen=65, tlen=225, kl=0.00647, act_lr=1e-6, ent=0.911]   Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=-0.067, ret=9.04e-5, glen=65, tlen=225, kl=0.00647, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=-0.046, ret=6.21e-5, glen=62.3, tlen=222, kl=0.00552, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.15it/s, pg=-0.046, ret=6.21e-5, glen=62.3, tlen=222, kl=0.00552, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.15it/s, pg=0.151, ret=-0.000158, glen=61.5, tlen=222, kl=0.00644, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.16it/s, pg=0.151, ret=-0.000158, glen=61.5, tlen=222, kl=0.00644, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.16it/s, pg=-0.0761, ret=9.19e-5, glen=62.9, tlen=223, kl=0.00561, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.17it/s, pg=-0.0761, ret=9.19e-5, glen=62.9, tlen=223, kl=0.00561, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.17it/s, pg=-0.0668, ret=9.13e-5, glen=63.3, tlen=223, kl=0.00768, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.0668, ret=9.13e-5, glen=63.3, tlen=223, kl=0.00768, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0525, ret=6.05e-5, glen=61.8, tlen=222, kl=0.0086, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0525, ret=6.05e-5, glen=61.8, tlen=222, kl=0.0086, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=0.139, ret=-0.000168, glen=62.5, tlen=223, kl=0.00947, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=0.139, ret=-0.000168, glen=62.5, tlen=223, kl=0.00947, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=0.0434, ret=-4.41e-5, glen=62.2, tlen=222, kl=0.00732, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:16,  1.00it/s, pg=0.0434, ret=-4.41e-5, glen=62.2, tlen=222, kl=0.00732, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:16,  1.00it/s, pg=0.238, ret=-0.00032, glen=64.2, tlen=224, kl=0.00726, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:15,  1.05it/s, pg=0.238, ret=-0.00032, glen=64.2, tlen=224, kl=0.00726, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:15,  1.05it/s, pg=-0.0509, ret=6.41e-5, glen=62.1, tlen=223, kl=0.00657, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.08it/s, pg=-0.0509, ret=6.41e-5, glen=62.1, tlen=223, kl=0.00657, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.08it/s, pg=-0.0469, ret=5.94e-5, glen=63, tlen=223, kl=0.00703, act_lr=1e-6, ent=0.893]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=-0.0469, ret=5.94e-5, glen=63, tlen=223, kl=0.00703, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=0.0637, ret=-4.88e-5, glen=62.5, tlen=223, kl=0.0087, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.13it/s, pg=0.0637, ret=-4.88e-5, glen=62.5, tlen=223, kl=0.0087, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.0875, ret=0.000109, glen=63.5, tlen=224, kl=0.00903, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.0875, ret=0.000109, glen=63.5, tlen=224, kl=0.00903, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=-0.0618, ret=8.7e-5, glen=62, tlen=222, kl=0.00818, act_lr=1e-6, ent=0.911]    Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=-0.0618, ret=8.7e-5, glen=62, tlen=222, kl=0.00818, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=0.0194, ret=-3.13e-5, glen=62.9, tlen=223, kl=0.00495, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=0.0194, ret=-3.13e-5, glen=62.9, tlen=223, kl=0.00495, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0576, ret=7.71e-5, glen=63.1, tlen=223, kl=0.00692, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0576, ret=7.71e-5, glen=63.1, tlen=223, kl=0.00692, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.00208, ret=-3.42e-5, glen=61.9, tlen=222, kl=0.00825, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.00208, ret=-3.42e-5, glen=61.9, tlen=222, kl=0.00825, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0559, ret=7.21e-5, glen=62.1, tlen=222, kl=0.00726, act_lr=1e-6, ent=0.926]  Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0559, ret=7.21e-5, glen=62.1, tlen=222, kl=0.00726, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0504, ret=6.36e-5, glen=62.8, tlen=223, kl=0.0051, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0504, ret=6.36e-5, glen=62.8, tlen=223, kl=0.0051, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.114, ret=-0.000142, glen=64.7, tlen=225, kl=0.00605, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.114, ret=-0.000142, glen=64.7, tlen=225, kl=0.00605, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0612, ret=7.72e-5, glen=61.2, tlen=222, kl=0.0064, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0612, ret=7.72e-5, glen=61.2, tlen=222, kl=0.0064, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0498, ret=5.96e-5, glen=63.2, tlen=224, kl=0.00812, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0498, ret=5.96e-5, glen=63.2, tlen=224, kl=0.00812, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.0804, ret=-6.37e-5, glen=61, tlen=221, kl=0.0071, act_lr=1e-6, ent=0.936]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.0804, ret=-6.37e-5, glen=61, tlen=221, kl=0.0071, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.0595, ret=7.9e-5, glen=63.5, tlen=224, kl=0.00613, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0595, ret=7.9e-5, glen=63.5, tlen=224, kl=0.00613, act_lr=1e-6, ent=0.911]
2025-07-24 21:02:29.311 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=0.134, ret=-0.000147, glen=64, tlen=224, kl=0.00632, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=0.134, ret=-0.000147, glen=64, tlen=224, kl=0.00632, act_lr=1e-6, ent=0.933]
2025-07-24 21:02:30.141 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:02:32.715 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 21:02:33.064 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.06s
2025-07-24 21:02:33.070 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 7.297681725543478e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9235029583391936, 'kl': 0.007210607114045516, 'response_length': 62.7493723164434, 'total_length': 222.9378778208857, 'teacher_total_length': 234.99728493068528, 'return': 6.410541375415683e-07, 'policy_update_steps': 1.0}

Episode [10/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [09:23<15:04, 113.06s/it][A2025-07-24 21:02:33.122 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:03:10.163 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:03:10.340 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:03:10.340 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 37.22s
2025-07-24 21:03:12.252 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0137,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 63.3175,std_num_tokens: 13.8464,avg_correct_num_tokens: 63.2841,std_correct_num_tokens: 13.8356,avg_incorrect_num_tokens: 68.7600,std_incorrect_num_tokens: 14.5142
2025-07-24 21:03:12.670 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.33s
2025-07-24 21:03:15.240 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.57s
2025-07-24 21:03:39.310 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 21:03:39.310 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.07s
2025-07-24 21:03:40.567 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:03:40.567 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.2091735403078633e-05, avg_kl: 0.010109896273226351, avg_response_length: 63.34280869767473, avg_orm_score: 0.0, avg_custom_rewards: -1.2091735403078633e-05
2025-07-24 21:03:40.598 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter122_replay_buffer.jsonl
2025-07-24 21:03:41.877 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.28s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s, pg=-0.0072, ret=1.6e-5, glen=62.1, tlen=222, kl=0.0104, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:00<00:45,  1.01it/s, pg=-0.0072, ret=1.6e-5, glen=62.1, tlen=222, kl=0.0104, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:45,  1.01it/s, pg=-0.0645, ret=0.000107, glen=62.9, tlen=223, kl=0.0101, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.09it/s, pg=-0.0645, ret=0.000107, glen=62.9, tlen=223, kl=0.0101, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.09it/s, pg=0.0258, ret=-9.64e-5, glen=63.2, tlen=224, kl=0.00826, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:40,  1.08it/s, pg=0.0258, ret=-9.64e-5, glen=63.2, tlen=224, kl=0.00826, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:40,  1.08it/s, pg=-0.0833, ret=0.00014, glen=63.6, tlen=224, kl=0.00611, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:39,  1.08it/s, pg=-0.0833, ret=0.00014, glen=63.6, tlen=224, kl=0.00611, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:39,  1.08it/s, pg=0.135, ret=-0.00026, glen=65.3, tlen=226, kl=0.00974, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:37,  1.11it/s, pg=0.135, ret=-0.00026, glen=65.3, tlen=226, kl=0.00974, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:37,  1.11it/s, pg=0.0944, ret=-0.000124, glen=63.5, tlen=224, kl=0.0105, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:36,  1.13it/s, pg=0.0944, ret=-0.000124, glen=63.5, tlen=224, kl=0.0105, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:36,  1.13it/s, pg=0.00769, ret=-4.86e-6, glen=63.2, tlen=224, kl=0.00881, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:35,  1.13it/s, pg=0.00769, ret=-4.86e-6, glen=63.2, tlen=224, kl=0.00881, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:07<00:35,  1.13it/s, pg=-0.0372, ret=3.32e-5, glen=63.3, tlen=224, kl=0.0112, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:34,  1.14it/s, pg=-0.0372, ret=3.32e-5, glen=63.3, tlen=224, kl=0.0112, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:08<00:34,  1.14it/s, pg=0.0246, ret=-2.21e-5, glen=62.5, tlen=223, kl=0.00982, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.15it/s, pg=0.0246, ret=-2.21e-5, glen=62.5, tlen=223, kl=0.00982, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.15it/s, pg=0.0758, ret=-8.44e-5, glen=62.7, tlen=223, kl=0.00892, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:32,  1.16it/s, pg=0.0758, ret=-8.44e-5, glen=62.7, tlen=223, kl=0.00892, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:32,  1.16it/s, pg=0.0538, ret=-0.000106, glen=62.4, tlen=223, kl=0.0105, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:31,  1.14it/s, pg=0.0538, ret=-0.000106, glen=62.4, tlen=223, kl=0.0105, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:31,  1.14it/s, pg=-0.00909, ret=-1.71e-5, glen=61.3, tlen=222, kl=0.0142, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:30,  1.15it/s, pg=-0.00909, ret=-1.71e-5, glen=61.3, tlen=222, kl=0.0142, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:30,  1.15it/s, pg=-0.00885, ret=2.74e-5, glen=66.3, tlen=227, kl=0.00811, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.16it/s, pg=-0.00885, ret=2.74e-5, glen=66.3, tlen=227, kl=0.00811, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.16it/s, pg=-0.0219, ret=2.9e-5, glen=63.6, tlen=224, kl=0.00758, act_lr=1e-6, ent=0.935]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:29,  1.14it/s, pg=-0.0219, ret=2.9e-5, glen=63.6, tlen=224, kl=0.00758, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:29,  1.14it/s, pg=-0.0698, ret=0.000115, glen=63.9, tlen=225, kl=0.0118, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.15it/s, pg=-0.0698, ret=0.000115, glen=63.9, tlen=225, kl=0.0118, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:14<00:27,  1.15it/s, pg=-0.0889, ret=0.000146, glen=63.1, tlen=224, kl=0.011, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.15it/s, pg=-0.0889, ret=0.000146, glen=63.1, tlen=224, kl=0.011, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.15it/s, pg=-0.0768, ret=0.000126, glen=62.3, tlen=223, kl=0.00817, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:25,  1.16it/s, pg=-0.0768, ret=0.000126, glen=62.3, tlen=223, kl=0.00817, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:25,  1.16it/s, pg=0.0667, ret=-0.000173, glen=62.4, tlen=223, kl=0.0161, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:24,  1.16it/s, pg=0.0667, ret=-0.000173, glen=62.4, tlen=223, kl=0.0161, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:24,  1.16it/s, pg=-0.0453, ret=4.85e-5, glen=64.4, tlen=225, kl=0.00826, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:23,  1.17it/s, pg=-0.0453, ret=4.85e-5, glen=64.4, tlen=225, kl=0.00826, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:23,  1.17it/s, pg=-0.0979, ret=0.000157, glen=63.3, tlen=223, kl=0.00838, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.17it/s, pg=-0.0979, ret=0.000157, glen=63.3, tlen=223, kl=0.00838, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.17it/s, pg=-0.102, ret=0.000167, glen=62.2, tlen=223, kl=0.00962, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.17it/s, pg=-0.102, ret=0.000167, glen=62.2, tlen=223, kl=0.00962, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.17it/s, pg=0.00934, ret=-9.62e-6, glen=63, tlen=224, kl=0.0179, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=0.00934, ret=-9.62e-6, glen=63, tlen=224, kl=0.0179, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:20<00:21,  1.17it/s, pg=0.237, ret=-0.00037, glen=63.1, tlen=224, kl=0.00857, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=0.237, ret=-0.00037, glen=63.1, tlen=224, kl=0.00857, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=-0.0917, ret=0.00015, glen=63.3, tlen=224, kl=0.00758, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.18it/s, pg=-0.0917, ret=0.00015, glen=63.3, tlen=224, kl=0.00758, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.18it/s, pg=0.0637, ret=-0.00011, glen=62.9, tlen=224, kl=0.00852, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.18it/s, pg=0.0637, ret=-0.00011, glen=62.9, tlen=224, kl=0.00852, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.18it/s, pg=-0.0884, ret=0.000126, glen=64.9, tlen=226, kl=0.00823, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:17,  1.18it/s, pg=-0.0884, ret=0.000126, glen=64.9, tlen=226, kl=0.00823, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:17,  1.18it/s, pg=-0.01, ret=-9.24e-6, glen=64.7, tlen=225, kl=0.0216, act_lr=1e-6, ent=0.935]   Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:16,  1.18it/s, pg=-0.01, ret=-9.24e-6, glen=64.7, tlen=225, kl=0.0216, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:16,  1.18it/s, pg=-0.0312, ret=2.82e-5, glen=65.6, tlen=226, kl=0.0108, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.18it/s, pg=-0.0312, ret=2.82e-5, glen=65.6, tlen=226, kl=0.0108, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.18it/s, pg=-0.0225, ret=1.35e-5, glen=65.4, tlen=226, kl=0.00671, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:16,  1.07it/s, pg=-0.0225, ret=1.35e-5, glen=65.4, tlen=226, kl=0.00671, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:16,  1.07it/s, pg=0.0724, ret=-6.41e-5, glen=63, tlen=224, kl=0.00852, act_lr=1e-6, ent=0.914]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.10it/s, pg=0.0724, ret=-6.41e-5, glen=63, tlen=224, kl=0.00852, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:27<00:15,  1.10it/s, pg=0.0131, ret=-2.13e-6, glen=62.5, tlen=223, kl=0.0129, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.13it/s, pg=0.0131, ret=-2.13e-6, glen=62.5, tlen=223, kl=0.0129, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.13it/s, pg=-0.0581, ret=9.59e-5, glen=62.6, tlen=223, kl=0.00841, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:27<00:13,  1.14it/s, pg=-0.0581, ret=9.59e-5, glen=62.6, tlen=223, kl=0.00841, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=0.0509, ret=-1.24e-5, glen=63.9, tlen=225, kl=0.0107, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.15it/s, pg=0.0509, ret=-1.24e-5, glen=63.9, tlen=225, kl=0.0107, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.15it/s, pg=0.043, ret=-6.18e-5, glen=63.5, tlen=224, kl=0.00989, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.16it/s, pg=0.043, ret=-6.18e-5, glen=63.5, tlen=224, kl=0.00989, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.16it/s, pg=-0.0733, ret=0.000121, glen=63.2, tlen=223, kl=0.00837, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.17it/s, pg=-0.0733, ret=0.000121, glen=63.2, tlen=223, kl=0.00837, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.17it/s, pg=-0.0807, ret=0.000127, glen=63.5, tlen=224, kl=0.00911, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.17it/s, pg=-0.0807, ret=0.000127, glen=63.5, tlen=224, kl=0.00911, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.17it/s, pg=0.0797, ret=-0.000145, glen=65, tlen=226, kl=0.0103, act_lr=1e-6, ent=0.941]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=0.0797, ret=-0.000145, glen=65, tlen=226, kl=0.0103, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:33<00:08,  1.17it/s, pg=0.119, ret=-0.000181, glen=62.7, tlen=223, kl=0.00732, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=0.119, ret=-0.000181, glen=62.7, tlen=223, kl=0.00732, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=-0.101, ret=0.00017, glen=64.2, tlen=225, kl=0.0143, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:33<00:06,  1.17it/s, pg=-0.101, ret=0.00017, glen=64.2, tlen=225, kl=0.0143, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=0.0242, ret=-4.63e-6, glen=61.7, tlen=222, kl=0.00903, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:05,  1.18it/s, pg=0.0242, ret=-4.63e-6, glen=61.7, tlen=222, kl=0.00903, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:05,  1.18it/s, pg=-0.0697, ret=0.00011, glen=63.1, tlen=224, kl=0.0117, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.18it/s, pg=-0.0697, ret=0.00011, glen=63.1, tlen=224, kl=0.0117, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.18it/s, pg=0.116, ret=-0.000259, glen=63.3, tlen=224, kl=0.00868, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.18it/s, pg=0.116, ret=-0.000259, glen=63.3, tlen=224, kl=0.00868, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.18it/s, pg=-0.0703, ret=0.000121, glen=61.7, tlen=222, kl=0.0117, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.18it/s, pg=-0.0703, ret=0.000121, glen=61.7, tlen=222, kl=0.0117, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.18it/s, pg=-0.0782, ret=0.00013, glen=64.8, tlen=225, kl=0.0117, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.18it/s, pg=-0.0782, ret=0.00013, glen=64.8, tlen=225, kl=0.0117, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:39<00:02,  1.18it/s, pg=-0.0322, ret=4.83e-5, glen=61.4, tlen=222, kl=0.0097, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.18it/s, pg=-0.0322, ret=4.83e-5, glen=61.4, tlen=222, kl=0.0097, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.18it/s, pg=0.0979, ret=-0.000112, glen=63.3, tlen=224, kl=0.00871, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:39<00:00,  1.18it/s, pg=0.0979, ret=-0.000112, glen=63.3, tlen=224, kl=0.00871, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.18it/s, pg=0.113, ret=-0.000171, glen=62.9, tlen=223, kl=0.00808, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.13it/s, pg=0.113, ret=-0.000171, glen=62.9, tlen=223, kl=0.00808, act_lr=1e-6, ent=0.923]
2025-07-24 21:04:22.886 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.83s
2025-07-24 21:04:23.562 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 21:04:25.715 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.15s
2025-07-24 21:04:26.061 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.12s
2025-07-24 21:04:26.081 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 5.1944813829787234e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9282630872219166, 'kl': 0.01014287421043883, 'response_length': 63.32755141562604, 'total_length': 223.91341189120678, 'teacher_total_length': 235.94526964552858, 'return': -9.94400211988869e-07, 'policy_update_steps': 1.0}

Episode [10/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [11:16<13:11, 113.04s/it][A2025-07-24 21:04:26.130 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:05:02.978 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:05:03.155 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:05:03.156 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 37.03s
2025-07-24 21:05:04.957 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0141,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 63.1539,std_num_tokens: 14.4229,avg_correct_num_tokens: 63.1294,std_correct_num_tokens: 14.3774,avg_incorrect_num_tokens: 69.8333,std_incorrect_num_tokens: 22.7905
2025-07-24 21:05:05.383 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.23s
2025-07-24 21:05:07.945 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.56s
2025-07-24 21:05:31.972 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 21:05:31.973 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.03s
2025-07-24 21:05:33.324 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.92s
2025-07-24 21:05:33.325 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.1964614282588703e-06, avg_kl: 0.016301665434966216, avg_response_length: 63.17364714338973, avg_orm_score: 0.0, avg_custom_rewards: 3.1964614282588703e-06
2025-07-24 21:05:33.358 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter123_replay_buffer.jsonl
2025-07-24 21:05:34.645 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.29s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:01<?, ?it/s, pg=0.0406, ret=-2.82e-5, glen=64, tlen=225, kl=0.0102, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.01s/it, pg=0.0406, ret=-2.82e-5, glen=64, tlen=225, kl=0.0102, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.01s/it, pg=-0.0406, ret=5.57e-5, glen=63, tlen=224, kl=0.022, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.09it/s, pg=-0.0406, ret=5.57e-5, glen=63, tlen=224, kl=0.022, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.09it/s, pg=0.0201, ret=-3.16e-5, glen=63.5, tlen=224, kl=0.00947, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:39,  1.12it/s, pg=0.0201, ret=-3.16e-5, glen=63.5, tlen=224, kl=0.00947, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:39,  1.12it/s, pg=-0.00177, ret=-2.38e-5, glen=64.5, tlen=225, kl=0.0142, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:37,  1.14it/s, pg=-0.00177, ret=-2.38e-5, glen=64.5, tlen=225, kl=0.0142, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:37,  1.14it/s, pg=0.0476, ret=-4.78e-5, glen=61.4, tlen=222, kl=0.0131, act_lr=1e-6, ent=0.959]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:37,  1.13it/s, pg=0.0476, ret=-4.78e-5, glen=61.4, tlen=222, kl=0.0131, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:37,  1.13it/s, pg=-0.0809, ret=9.85e-5, glen=62.9, tlen=224, kl=0.016, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:36,  1.14it/s, pg=-0.0809, ret=9.85e-5, glen=62.9, tlen=224, kl=0.016, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:36,  1.14it/s, pg=0.0049, ret=-4.97e-5, glen=63.9, tlen=225, kl=0.0145, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:34,  1.15it/s, pg=0.0049, ret=-4.97e-5, glen=63.9, tlen=225, kl=0.0145, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:07<00:34,  1.15it/s, pg=-0.0528, ret=7.06e-5, glen=64.4, tlen=225, kl=0.019, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.16it/s, pg=-0.0528, ret=7.06e-5, glen=64.4, tlen=225, kl=0.019, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.16it/s, pg=-0.0476, ret=5.93e-5, glen=63.7, tlen=225, kl=0.0145, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:07<00:33,  1.14it/s, pg=-0.0476, ret=5.93e-5, glen=63.7, tlen=225, kl=0.0145, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.14it/s, pg=0.198, ret=-0.000162, glen=63, tlen=223, kl=0.0254, act_lr=1e-6, ent=0.902]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:32,  1.15it/s, pg=0.198, ret=-0.000162, glen=63, tlen=223, kl=0.0254, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:32,  1.15it/s, pg=-0.0508, ret=6.54e-5, glen=64, tlen=225, kl=0.0193, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:31,  1.16it/s, pg=-0.0508, ret=6.54e-5, glen=64, tlen=225, kl=0.0193, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:31,  1.16it/s, pg=-0.0656, ret=7.71e-5, glen=61, tlen=222, kl=0.0131, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:30,  1.16it/s, pg=-0.0656, ret=7.71e-5, glen=61, tlen=222, kl=0.0131, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:30,  1.16it/s, pg=-0.0457, ret=5.8e-5, glen=63.3, tlen=224, kl=0.0155, act_lr=1e-6, ent=0.9] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.15it/s, pg=-0.0457, ret=5.8e-5, glen=63.3, tlen=224, kl=0.0155, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.15it/s, pg=-0.0583, ret=7.52e-5, glen=60.4, tlen=221, kl=0.0233, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:28,  1.16it/s, pg=-0.0583, ret=7.52e-5, glen=60.4, tlen=221, kl=0.0233, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:28,  1.16it/s, pg=-0.0592, ret=7.9e-5, glen=62.7, tlen=223, kl=0.0278, act_lr=1e-6, ent=0.948] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.0592, ret=7.9e-5, glen=62.7, tlen=223, kl=0.0278, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.0923, ret=0.000117, glen=63.2, tlen=224, kl=0.0219, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:13<00:26,  1.17it/s, pg=-0.0923, ret=0.000117, glen=63.2, tlen=224, kl=0.0219, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.17it/s, pg=-0.0831, ret=0.000102, glen=62.5, tlen=223, kl=0.0114, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:25,  1.17it/s, pg=-0.0831, ret=0.000102, glen=62.5, tlen=223, kl=0.0114, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:25,  1.17it/s, pg=0.0126, ret=-5.33e-5, glen=62.7, tlen=223, kl=0.0286, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:25,  1.15it/s, pg=0.0126, ret=-5.33e-5, glen=62.7, tlen=223, kl=0.0286, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:25,  1.15it/s, pg=-0.0274, ret=-3.38e-5, glen=62.1, tlen=223, kl=0.0212, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:24,  1.15it/s, pg=-0.0274, ret=-3.38e-5, glen=62.1, tlen=223, kl=0.0212, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:24,  1.15it/s, pg=-0.0677, ret=8.47e-5, glen=64.9, tlen=226, kl=0.0152, act_lr=1e-6, ent=0.946] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.16it/s, pg=-0.0677, ret=8.47e-5, glen=64.9, tlen=226, kl=0.0152, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.16it/s, pg=0.0157, ret=-4.64e-5, glen=65, tlen=226, kl=0.0236, act_lr=1e-6, ent=0.919]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.17it/s, pg=0.0157, ret=-4.64e-5, glen=65, tlen=226, kl=0.0236, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.17it/s, pg=0.165, ret=-0.000152, glen=61.9, tlen=223, kl=0.0165, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=0.165, ret=-0.000152, glen=61.9, tlen=223, kl=0.0165, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.0555, ret=7.41e-5, glen=62, tlen=222, kl=0.0151, act_lr=1e-6, ent=0.936]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:19<00:20,  1.17it/s, pg=-0.0555, ret=7.41e-5, glen=62, tlen=222, kl=0.0151, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=0.135, ret=-0.000185, glen=64.3, tlen=225, kl=0.0113, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.17it/s, pg=0.135, ret=-0.000185, glen=64.3, tlen=225, kl=0.0113, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.17it/s, pg=-0.0928, ret=0.000118, glen=66.2, tlen=227, kl=0.0138, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.17it/s, pg=-0.0928, ret=0.000118, glen=66.2, tlen=227, kl=0.0138, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.17it/s, pg=-0.0596, ret=7.58e-5, glen=62.3, tlen=223, kl=0.011, act_lr=1e-6, ent=0.91]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:17,  1.18it/s, pg=-0.0596, ret=7.58e-5, glen=62.3, tlen=223, kl=0.011, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:17,  1.18it/s, pg=0.0632, ret=-3.28e-5, glen=61.5, tlen=222, kl=0.0149, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:16,  1.18it/s, pg=0.0632, ret=-3.28e-5, glen=61.5, tlen=222, kl=0.0149, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:16,  1.18it/s, pg=0.0123, ret=-5.33e-5, glen=64.3, tlen=225, kl=0.0157, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.17it/s, pg=0.0123, ret=-5.33e-5, glen=64.3, tlen=225, kl=0.0157, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.17it/s, pg=-0.0595, ret=7.77e-5, glen=62.9, tlen=223, kl=0.0155, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:16,  1.07it/s, pg=-0.0595, ret=7.77e-5, glen=62.9, tlen=223, kl=0.0155, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:16,  1.07it/s, pg=-0.0682, ret=9.41e-5, glen=64.6, tlen=225, kl=0.0122, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.10it/s, pg=-0.0682, ret=9.41e-5, glen=64.6, tlen=225, kl=0.0122, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:27<00:15,  1.10it/s, pg=-0.0597, ret=8.4e-5, glen=64.6, tlen=226, kl=0.0154, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.13it/s, pg=-0.0597, ret=8.4e-5, glen=64.6, tlen=226, kl=0.0154, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.13it/s, pg=0.125, ret=-0.000164, glen=63.4, tlen=224, kl=0.0111, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:27<00:13,  1.14it/s, pg=0.125, ret=-0.000164, glen=63.4, tlen=224, kl=0.0111, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=0.143, ret=-0.000177, glen=62.4, tlen=223, kl=0.0133, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.15it/s, pg=0.143, ret=-0.000177, glen=62.4, tlen=223, kl=0.0133, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.15it/s, pg=0.00476, ret=-4.45e-5, glen=61.9, tlen=223, kl=0.0185, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.16it/s, pg=0.00476, ret=-4.45e-5, glen=61.9, tlen=223, kl=0.0185, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.16it/s, pg=-0.0533, ret=7.02e-5, glen=61.4, tlen=222, kl=0.0144, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.16it/s, pg=-0.0533, ret=7.02e-5, glen=61.4, tlen=222, kl=0.0144, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.16it/s, pg=-0.0509, ret=6.02e-5, glen=61.5, tlen=223, kl=0.0132, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.17it/s, pg=-0.0509, ret=6.02e-5, glen=61.5, tlen=223, kl=0.0132, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.17it/s, pg=-0.0518, ret=6.68e-5, glen=62.3, tlen=223, kl=0.0111, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=-0.0518, ret=6.68e-5, glen=62.3, tlen=223, kl=0.0111, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=0.017, ret=-6.92e-5, glen=63.6, tlen=224, kl=0.0172, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:32<00:07,  1.17it/s, pg=0.017, ret=-6.92e-5, glen=63.6, tlen=224, kl=0.0172, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=0.173, ret=-0.000174, glen=63.9, tlen=225, kl=0.0233, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:33<00:06,  1.17it/s, pg=0.173, ret=-0.000174, glen=63.9, tlen=225, kl=0.0233, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=-0.0753, ret=9.71e-5, glen=62.3, tlen=223, kl=0.0116, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:05,  1.17it/s, pg=-0.0753, ret=9.71e-5, glen=62.3, tlen=223, kl=0.0116, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:05,  1.17it/s, pg=-0.0471, ret=5.89e-5, glen=61.8, tlen=223, kl=0.0136, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.17it/s, pg=-0.0471, ret=5.89e-5, glen=61.8, tlen=223, kl=0.0136, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.17it/s, pg=-0.00989, ret=-2.22e-5, glen=63.6, tlen=224, kl=0.0194, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.17it/s, pg=-0.00989, ret=-2.22e-5, glen=63.6, tlen=224, kl=0.0194, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.17it/s, pg=0.206, ret=-0.000168, glen=64.4, tlen=226, kl=0.011, act_lr=1e-6, ent=0.926]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.18it/s, pg=0.206, ret=-0.000168, glen=64.4, tlen=226, kl=0.011, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.18it/s, pg=0.0153, ret=-3.03e-5, glen=65.5, tlen=226, kl=0.0111, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.18it/s, pg=0.0153, ret=-3.03e-5, glen=65.5, tlen=226, kl=0.0111, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.18it/s, pg=0.0598, ret=-4.04e-5, glen=63.9, tlen=225, kl=0.0182, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:38<00:01,  1.18it/s, pg=0.0598, ret=-4.04e-5, glen=63.9, tlen=225, kl=0.0182, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.18it/s, pg=-0.0707, ret=8.8e-5, glen=62.5, tlen=223, kl=0.0138, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:39<00:00,  1.18it/s, pg=-0.0707, ret=8.8e-5, glen=62.5, tlen=223, kl=0.0138, act_lr=1e-6, ent=0.927]
2025-07-24 21:06:15.569 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.75s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.18it/s, pg=0.0229, ret=-5.54e-5, glen=64, tlen=225, kl=0.0207, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.13it/s, pg=0.0229, ret=-5.54e-5, glen=64, tlen=225, kl=0.0207, act_lr=1e-6, ent=0.91]
2025-07-24 21:06:16.425 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 21:06:18.962 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 21:06:19.298 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.59s
2025-07-24 21:06:19.304 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0009606544007646277, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9198195135339777, 'kl': 0.01621002846575798, 'response_length': 63.174542934336564, 'total_length': 223.9291079906707, 'teacher_total_length': 235.9045121213223, 'return': 1.3225756621513356e-06, 'policy_update_steps': 1.0}

Episode [10/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [13:09<11:18, 113.10s/it][A2025-07-24 21:06:19.405 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:06:55.623 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:06:55.797 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 21:06:55.798 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.39s
2025-07-24 21:06:57.564 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0142,avg_reflection_pattern_score: 0.0013,avg_pass_at_n: 1.0000,avg_num_tokens: 61.9789,std_num_tokens: 13.9037,avg_correct_num_tokens: 61.9717,std_correct_num_tokens: 13.6423,avg_incorrect_num_tokens: 63.3488,std_incorrect_num_tokens: 39.4490
2025-07-24 21:06:57.970 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.17s
2025-07-24 21:07:00.679 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.71s
2025-07-24 21:07:24.299 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:07:24.299 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.62s
2025-07-24 21:07:25.623 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.87s
2025-07-24 21:07:25.624 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.5284969856071733e-05, avg_kl: 0.021715910538383152, avg_response_length: 61.993125189905584, avg_orm_score: 0.0, avg_custom_rewards: 1.5284969856071733e-05
2025-07-24 21:07:25.651 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter124_replay_buffer.jsonl
2025-07-24 21:07:26.946 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.30s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0602, ret=9.27e-5, glen=62.5, tlen=223, kl=0.0161, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0602, ret=9.27e-5, glen=62.5, tlen=223, kl=0.0161, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=0.0472, ret=-0.000108, glen=63.5, tlen=224, kl=0.0186, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=0.0472, ret=-0.000108, glen=63.5, tlen=224, kl=0.0186, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0794, ret=0.000128, glen=61.3, tlen=222, kl=0.0206, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.09it/s, pg=-0.0794, ret=0.000128, glen=61.3, tlen=222, kl=0.0206, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.09it/s, pg=-0.0898, ret=0.000145, glen=62, tlen=223, kl=0.0239, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=-0.0898, ret=0.000145, glen=62, tlen=223, kl=0.0239, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=-0.0103, ret=-1.26e-5, glen=62.7, tlen=224, kl=0.0188, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.12it/s, pg=-0.0103, ret=-1.26e-5, glen=62.7, tlen=224, kl=0.0188, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.12it/s, pg=0.0126, ret=-2.22e-5, glen=61.7, tlen=222, kl=0.0186, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=0.0126, ret=-2.22e-5, glen=61.7, tlen=222, kl=0.0186, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=0.0486, ret=-0.000102, glen=62.9, tlen=223, kl=0.0149, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=0.0486, ret=-0.000102, glen=62.9, tlen=223, kl=0.0149, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=0.089, ret=-0.00017, glen=63, tlen=224, kl=0.0204, act_lr=1e-6, ent=0.896]    Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.15it/s, pg=0.089, ret=-0.00017, glen=63, tlen=224, kl=0.0204, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.15it/s, pg=-0.00693, ret=-7.72e-6, glen=61.8, tlen=222, kl=0.0252, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.16it/s, pg=-0.00693, ret=-7.72e-6, glen=61.8, tlen=222, kl=0.0252, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=-0.0623, ret=9.26e-5, glen=61.4, tlen=222, kl=0.0214, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=-0.0623, ret=9.26e-5, glen=61.4, tlen=222, kl=0.0214, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=0.000977, ret=2.99e-5, glen=62.1, tlen=222, kl=0.0155, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=0.000977, ret=2.99e-5, glen=62.1, tlen=222, kl=0.0155, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0848, ret=0.00014, glen=63.4, tlen=224, kl=0.0173, act_lr=1e-6, ent=0.908] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=-0.0848, ret=0.00014, glen=63.4, tlen=224, kl=0.0173, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=-0.0585, ret=9.64e-5, glen=62.4, tlen=223, kl=0.0141, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0585, ret=9.64e-5, glen=62.4, tlen=223, kl=0.0141, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0656, ret=0.000101, glen=63.5, tlen=224, kl=0.0188, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0656, ret=0.000101, glen=63.5, tlen=224, kl=0.0188, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=-0.000854, ret=-1.83e-5, glen=62.8, tlen=223, kl=0.0178, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.000854, ret=-1.83e-5, glen=62.8, tlen=223, kl=0.0178, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.126, ret=-0.000134, glen=62.6, tlen=223, kl=0.0167, act_lr=1e-6, ent=0.902]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=0.126, ret=-0.000134, glen=62.6, tlen=223, kl=0.0167, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0923, ret=0.000138, glen=61.5, tlen=222, kl=0.0253, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0923, ret=0.000138, glen=61.5, tlen=222, kl=0.0253, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0753, ret=0.000125, glen=60.4, tlen=221, kl=0.0229, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0753, ret=0.000125, glen=60.4, tlen=221, kl=0.0229, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=0.0191, ret=-3.09e-5, glen=60.6, tlen=221, kl=0.0355, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=0.0191, ret=-3.09e-5, glen=60.6, tlen=221, kl=0.0355, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.084, ret=0.000131, glen=61, tlen=221, kl=0.0201, act_lr=1e-6, ent=0.9]    Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.084, ret=0.000131, glen=61, tlen=221, kl=0.0201, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0819, ret=-0.000129, glen=62.9, tlen=223, kl=0.0186, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0819, ret=-0.000129, glen=62.9, tlen=223, kl=0.0186, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.061, ret=-0.000143, glen=61.1, tlen=221, kl=0.0168, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=0.061, ret=-0.000143, glen=61.1, tlen=221, kl=0.0168, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=-0.0552, ret=8.52e-5, glen=61.4, tlen=222, kl=0.0822, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.16it/s, pg=-0.0552, ret=8.52e-5, glen=61.4, tlen=222, kl=0.0822, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=0.0563, ret=-9.07e-5, glen=61.5, tlen=222, kl=0.0184, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.16it/s, pg=0.0563, ret=-9.07e-5, glen=61.5, tlen=222, kl=0.0184, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.16it/s, pg=-0.0787, ret=0.000128, glen=63.6, tlen=224, kl=0.0143, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.0787, ret=0.000128, glen=63.6, tlen=224, kl=0.0143, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=0.0853, ret=-0.000143, glen=62.6, tlen=223, kl=0.028, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.0853, ret=-0.000143, glen=62.6, tlen=223, kl=0.028, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.0464, ret=-0.000133, glen=61.9, tlen=222, kl=0.0197, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.0464, ret=-0.000133, glen=61.9, tlen=222, kl=0.0197, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=0.0274, ret=-3.28e-5, glen=60.7, tlen=221, kl=0.0309, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=0.0274, ret=-3.28e-5, glen=60.7, tlen=221, kl=0.0309, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=0.0159, ret=-3.35e-6, glen=63.7, tlen=224, kl=0.0161, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=0.0159, ret=-3.35e-6, glen=63.7, tlen=224, kl=0.0161, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.101, ret=-0.000163, glen=60.9, tlen=221, kl=0.0234, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.101, ret=-0.000163, glen=60.9, tlen=221, kl=0.0234, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.00803, ret=-1.3e-5, glen=61.4, tlen=222, kl=0.0212, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.00803, ret=-1.3e-5, glen=61.4, tlen=222, kl=0.0212, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.0617, ret=-0.00015, glen=60.6, tlen=221, kl=0.0184, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=0.0617, ret=-0.00015, glen=60.6, tlen=221, kl=0.0184, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0666, ret=0.00011, glen=62.3, tlen=222, kl=0.0259, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.12it/s, pg=-0.0666, ret=0.00011, glen=62.3, tlen=222, kl=0.0259, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.12it/s, pg=-0.0939, ret=0.00015, glen=63.2, tlen=224, kl=0.023, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.0939, ret=0.00015, glen=63.2, tlen=224, kl=0.023, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=-0.027, ret=7.21e-6, glen=62.4, tlen=223, kl=0.0218, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=-0.027, ret=7.21e-6, glen=62.4, tlen=223, kl=0.0218, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=0.011, ret=-1.14e-5, glen=62.7, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=0.011, ret=-1.14e-5, glen=62.7, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0268, ret=1.35e-5, glen=61.1, tlen=222, kl=0.02, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0268, ret=1.35e-5, glen=61.1, tlen=222, kl=0.02, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.0844, ret=0.000133, glen=61.5, tlen=222, kl=0.0235, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0844, ret=0.000133, glen=61.5, tlen=222, kl=0.0235, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.422, ret=-0.000222, glen=62.5, tlen=223, kl=0.0192, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.422, ret=-0.000222, glen=62.5, tlen=223, kl=0.0192, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0758, ret=0.000117, glen=61.5, tlen=222, kl=0.0202, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0758, ret=0.000117, glen=61.5, tlen=222, kl=0.0202, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0958, ret=-0.00025, glen=62.8, tlen=224, kl=0.018, act_lr=1e-6, ent=0.911]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0958, ret=-0.00025, glen=62.8, tlen=224, kl=0.018, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0622, ret=9.84e-5, glen=61, tlen=222, kl=0.022, act_lr=1e-6, ent=0.883]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0622, ret=9.84e-5, glen=61, tlen=222, kl=0.022, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.033, ret=-8.49e-5, glen=60, tlen=221, kl=0.0202, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.033, ret=-8.49e-5, glen=60, tlen=221, kl=0.0202, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0681, ret=0.000106, glen=63, tlen=224, kl=0.0159, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0681, ret=0.000106, glen=63, tlen=224, kl=0.0159, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=0.00604, ret=-1.93e-6, glen=60.5, tlen=221, kl=0.0173, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.00604, ret=-1.93e-6, glen=60.5, tlen=221, kl=0.0173, act_lr=1e-6, ent=0.92]
2025-07-24 21:08:07.141 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.02s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0564, ret=9.17e-5, glen=62, tlen=222, kl=0.0194, act_lr=1e-6, ent=0.896]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0564, ret=9.17e-5, glen=62, tlen=222, kl=0.0194, act_lr=1e-6, ent=0.896]
2025-07-24 21:08:07.811 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 21:08:10.176 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.37s
2025-07-24 21:08:10.517 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.51s
2025-07-24 21:08:10.524 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.000533394191576087, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9198242801686992, 'kl': 0.021715910538383152, 'response_length': 61.99312508624533, 'total_length': 222.417202161706, 'teacher_total_length': 234.37314672055453, 'return': 1.7090852729186579e-06, 'policy_update_steps': 1.0}

Episode [10/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [15:00<09:22, 112.50s/it][A2025-07-24 21:08:10.578 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:08:46.835 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:08:47.009 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 21:08:47.009 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.43s
2025-07-24 21:08:48.702 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 61.9152,std_num_tokens: 12.8964,avg_correct_num_tokens: 61.9071,std_correct_num_tokens: 12.8865,avg_incorrect_num_tokens: 64.1000,std_incorrect_num_tokens: 15.1995
2025-07-24 21:08:49.124 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.11s
2025-07-24 21:08:51.607 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.48s
2025-07-24 21:09:15.194 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:09:15.194 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.58s
2025-07-24 21:09:16.477 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:09:16.477 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.678950502537191e-05, avg_kl: 0.026360884956691578, avg_response_length: 61.922808667887814, avg_orm_score: 0.0, avg_custom_rewards: 1.678950502537191e-05
2025-07-24 21:09:16.505 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter125_replay_buffer.jsonl
2025-07-24 21:09:17.831 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.33s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.0204, ret=-4.26e-5, glen=61.8, tlen=222, kl=0.0237, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=0.0204, ret=-4.26e-5, glen=61.8, tlen=222, kl=0.0237, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.00604, ret=-4.57e-5, glen=61.9, tlen=222, kl=0.019, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.00604, ret=-4.57e-5, glen=61.9, tlen=222, kl=0.019, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0602, ret=8.11e-5, glen=62.5, tlen=223, kl=0.0191, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.0602, ret=8.11e-5, glen=62.5, tlen=223, kl=0.0191, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=0.0155, ret=3.51e-5, glen=60.8, tlen=221, kl=0.0298, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:36,  1.14it/s, pg=0.0155, ret=3.51e-5, glen=60.8, tlen=221, kl=0.0298, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:36,  1.14it/s, pg=-0.0464, ret=6.61e-5, glen=63.2, tlen=224, kl=0.0428, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.12it/s, pg=-0.0464, ret=6.61e-5, glen=63.2, tlen=224, kl=0.0428, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.12it/s, pg=-0.0584, ret=7.53e-5, glen=62.8, tlen=224, kl=0.0177, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.11it/s, pg=-0.0584, ret=7.53e-5, glen=62.8, tlen=224, kl=0.0177, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.11it/s, pg=0.0671, ret=-0.000145, glen=62.2, tlen=223, kl=0.0238, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:35,  1.10it/s, pg=0.0671, ret=-0.000145, glen=62.2, tlen=223, kl=0.0238, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:35,  1.10it/s, pg=0.04, ret=-4.33e-5, glen=61.4, tlen=222, kl=0.0263, act_lr=1e-6, ent=0.857]   Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:34,  1.10it/s, pg=0.04, ret=-4.33e-5, glen=61.4, tlen=222, kl=0.0263, act_lr=1e-6, ent=0.857]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:34,  1.10it/s, pg=-0.0535, ret=7.52e-5, glen=59.6, tlen=220, kl=0.0378, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.12it/s, pg=-0.0535, ret=7.52e-5, glen=59.6, tlen=220, kl=0.0378, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:09<00:32,  1.12it/s, pg=-0.0529, ret=6.92e-5, glen=61.9, tlen=222, kl=0.045, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:32,  1.11it/s, pg=-0.0529, ret=6.92e-5, glen=61.9, tlen=222, kl=0.045, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:32,  1.11it/s, pg=-0.0591, ret=7.67e-5, glen=62.7, tlen=223, kl=0.0233, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:31,  1.13it/s, pg=-0.0591, ret=7.67e-5, glen=62.7, tlen=223, kl=0.0233, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:31,  1.13it/s, pg=0.0201, ret=-3.82e-5, glen=61.2, tlen=222, kl=0.0192, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.14it/s, pg=0.0201, ret=-3.82e-5, glen=61.2, tlen=222, kl=0.0192, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.14it/s, pg=-0.00879, ret=-1.35e-5, glen=60.8, tlen=221, kl=0.0203, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.15it/s, pg=-0.00879, ret=-1.35e-5, glen=60.8, tlen=221, kl=0.0203, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.15it/s, pg=-0.0547, ret=6.89e-5, glen=62.1, tlen=222, kl=0.0483, act_lr=1e-6, ent=0.925]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.15it/s, pg=-0.0547, ret=6.89e-5, glen=62.1, tlen=222, kl=0.0483, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.15it/s, pg=-0.0642, ret=8.29e-5, glen=62, tlen=222, kl=0.028, act_lr=1e-6, ent=0.884]   Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.16it/s, pg=-0.0642, ret=8.29e-5, glen=62, tlen=222, kl=0.028, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.16it/s, pg=-0.0493, ret=6.32e-5, glen=62.6, tlen=223, kl=0.0214, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=-0.0493, ret=6.32e-5, glen=62.6, tlen=223, kl=0.0214, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:15<00:25,  1.16it/s, pg=0.0147, ret=-2.18e-5, glen=61.4, tlen=222, kl=0.0295, act_lr=1e-6, ent=0.872]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.15it/s, pg=0.0147, ret=-2.18e-5, glen=61.4, tlen=222, kl=0.0295, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.15it/s, pg=0.0504, ret=-4.18e-5, glen=59.9, tlen=220, kl=0.0412, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.14it/s, pg=0.0504, ret=-4.18e-5, glen=59.9, tlen=220, kl=0.0412, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.14it/s, pg=0.0677, ret=-3.02e-5, glen=62, tlen=223, kl=0.0186, act_lr=1e-6, ent=0.917]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.14it/s, pg=0.0677, ret=-3.02e-5, glen=62, tlen=223, kl=0.0186, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.14it/s, pg=-0.0646, ret=8.1e-5, glen=61.2, tlen=221, kl=0.03, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.15it/s, pg=-0.0646, ret=8.1e-5, glen=61.2, tlen=221, kl=0.03, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.15it/s, pg=-0.0796, ret=0.000107, glen=60.9, tlen=222, kl=0.0237, act_lr=1e-6, ent=0.87]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.16it/s, pg=-0.0796, ret=0.000107, glen=60.9, tlen=222, kl=0.0237, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.16it/s, pg=0.00806, ret=-3.35e-5, glen=62.3, tlen=223, kl=0.0224, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.16it/s, pg=0.00806, ret=-3.35e-5, glen=62.3, tlen=223, kl=0.0224, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.16it/s, pg=-0.0457, ret=6.5e-5, glen=62.2, tlen=222, kl=0.021, act_lr=1e-6, ent=0.929]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0457, ret=6.5e-5, glen=62.2, tlen=222, kl=0.021, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:21<00:19,  1.17it/s, pg=0.00909, ret=-3.97e-5, glen=61.6, tlen=222, kl=0.0263, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.00909, ret=-3.97e-5, glen=61.6, tlen=222, kl=0.0263, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.00439, ret=-3.43e-5, glen=62.1, tlen=223, kl=0.0249, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=0.00439, ret=-3.43e-5, glen=62.1, tlen=223, kl=0.0249, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=0.0269, ret=-4.86e-5, glen=62.9, tlen=223, kl=0.0216, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.15it/s, pg=0.0269, ret=-4.86e-5, glen=62.9, tlen=223, kl=0.0216, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.15it/s, pg=0.0634, ret=-6.29e-5, glen=61.8, tlen=222, kl=0.0288, act_lr=1e-6, ent=0.87] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=0.0634, ret=-6.29e-5, glen=61.8, tlen=222, kl=0.0288, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=0.139, ret=-0.000194, glen=62.1, tlen=222, kl=0.0235, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.16it/s, pg=0.139, ret=-0.000194, glen=62.1, tlen=222, kl=0.0235, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.16it/s, pg=-0.0798, ret=0.000103, glen=62.1, tlen=223, kl=0.0244, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:16,  1.06it/s, pg=-0.0798, ret=0.000103, glen=62.1, tlen=223, kl=0.0244, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:16,  1.06it/s, pg=0.0508, ret=-5.59e-5, glen=61.5, tlen=222, kl=0.022, act_lr=1e-6, ent=0.906]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.09it/s, pg=0.0508, ret=-5.59e-5, glen=61.5, tlen=222, kl=0.022, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.09it/s, pg=-0.0763, ret=0.000103, glen=63, tlen=223, kl=0.0263, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0763, ret=0.000103, glen=63, tlen=223, kl=0.0263, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=0.0312, ret=-3.75e-5, glen=60.6, tlen=221, kl=0.0212, act_lr=1e-6, ent=0.875]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=0.0312, ret=-3.75e-5, glen=60.6, tlen=221, kl=0.0212, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:29<00:12,  1.13it/s, pg=0.173, ret=-0.000188, glen=62, tlen=222, kl=0.0331, act_lr=1e-6, ent=0.924]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.14it/s, pg=0.173, ret=-0.000188, glen=62, tlen=222, kl=0.0331, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.14it/s, pg=0.0395, ret=-2.89e-5, glen=61.1, tlen=221, kl=0.0189, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=0.0395, ret=-2.89e-5, glen=61.1, tlen=221, kl=0.0189, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=-0.0535, ret=7e-5, glen=63.7, tlen=224, kl=0.0195, act_lr=1e-6, ent=0.9]     Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0535, ret=7e-5, glen=63.7, tlen=224, kl=0.0195, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=0.189, ret=-0.000202, glen=62.9, tlen=223, kl=0.0247, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=0.189, ret=-0.000202, glen=62.9, tlen=223, kl=0.0247, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0518, ret=7.04e-5, glen=62.9, tlen=224, kl=0.0424, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0518, ret=7.04e-5, glen=62.9, tlen=224, kl=0.0424, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=0.0426, ret=-4.1e-5, glen=63.3, tlen=224, kl=0.04, act_lr=1e-6, ent=0.922]   Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0426, ret=-4.1e-5, glen=63.3, tlen=224, kl=0.04, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0854, ret=0.000108, glen=62.8, tlen=223, kl=0.0229, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0854, ret=0.000108, glen=62.8, tlen=223, kl=0.0229, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:35<00:05,  1.17it/s, pg=0.0297, ret=-6.09e-5, glen=61.1, tlen=222, kl=0.0225, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0297, ret=-6.09e-5, glen=61.1, tlen=222, kl=0.0225, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.00763, ret=-1.59e-5, glen=61.5, tlen=222, kl=0.0227, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.00763, ret=-1.59e-5, glen=61.5, tlen=222, kl=0.0227, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0586, ret=-3.41e-5, glen=63.2, tlen=224, kl=0.0204, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0586, ret=-3.41e-5, glen=63.2, tlen=224, kl=0.0204, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0465, ret=-4.26e-5, glen=61.9, tlen=222, kl=0.0288, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=0.0465, ret=-4.26e-5, glen=61.9, tlen=222, kl=0.0288, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.18it/s, pg=-0.0335, ret=4.33e-5, glen=62.1, tlen=222, kl=0.02, act_lr=1e-6, ent=0.925]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0335, ret=4.33e-5, glen=62.1, tlen=222, kl=0.02, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=-0.0762, ret=9.54e-5, glen=61.8, tlen=222, kl=0.0207, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0762, ret=9.54e-5, glen=61.8, tlen=222, kl=0.0207, act_lr=1e-6, ent=0.893]
2025-07-24 21:09:58.282 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.18it/s, pg=-0.0647, ret=8.37e-5, glen=61.1, tlen=222, kl=0.0252, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=-0.0647, ret=8.37e-5, glen=61.1, tlen=222, kl=0.0252, act_lr=1e-6, ent=0.892]
2025-07-24 21:09:59.145 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 21:10:01.809 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.66s
2025-07-24 21:10:02.159 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.26s
2025-07-24 21:10:02.165 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0001983642578125, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9006133222061655, 'kl': 0.026360884956691578, 'response_length': 61.922808605691664, 'total_length': 222.39230313508406, 'teacher_total_length': 234.437964397928, 'return': 1.7715761793585008e-06, 'policy_update_steps': 1.0}

Episode [10/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [16:52<07:28, 112.23s/it][A2025-07-24 21:10:02.213 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:10:37.789 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:10:37.976 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 21:10:37.976 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.76s
2025-07-24 21:10:39.673 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 60.4463,std_num_tokens: 12.6157,avg_correct_num_tokens: 60.4536,std_correct_num_tokens: 12.6169,avg_incorrect_num_tokens: 58.5161,std_incorrect_num_tokens: 12.1360
2025-07-24 21:10:40.086 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.11s
2025-07-24 21:10:42.763 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.67s
2025-07-24 21:11:06.672 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:11:06.672 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.91s
2025-07-24 21:11:07.941 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.86s
2025-07-24 21:11:07.942 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.541161264007901e-05, avg_kl: 0.04025385288592896, avg_response_length: 60.466622733027556, avg_orm_score: 0.0, avg_custom_rewards: 1.541161264007901e-05
2025-07-24 21:11:07.973 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter126_replay_buffer.jsonl
2025-07-24 21:11:09.230 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=0.0764, ret=-6.18e-5, glen=60.7, tlen=221, kl=0.0299, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.02it/s, pg=0.0764, ret=-6.18e-5, glen=60.7, tlen=221, kl=0.0299, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.02it/s, pg=-0.0513, ret=7.66e-5, glen=59.3, tlen=220, kl=0.033, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.10it/s, pg=-0.0513, ret=7.66e-5, glen=59.3, tlen=220, kl=0.033, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.10it/s, pg=0.234, ret=-0.000302, glen=60.6, tlen=221, kl=0.0272, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.13it/s, pg=0.234, ret=-0.000302, glen=60.6, tlen=221, kl=0.0272, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.13it/s, pg=0.0146, ret=-4.46e-6, glen=62.2, tlen=223, kl=0.0303, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=0.0146, ret=-4.46e-6, glen=62.2, tlen=223, kl=0.0303, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=0.118, ret=-0.000145, glen=59.5, tlen=220, kl=0.0251, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.14it/s, pg=0.118, ret=-0.000145, glen=59.5, tlen=220, kl=0.0251, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.14it/s, pg=-0.0296, ret=4.63e-5, glen=59.9, tlen=220, kl=0.0373, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.15it/s, pg=-0.0296, ret=4.63e-5, glen=59.9, tlen=220, kl=0.0373, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.15it/s, pg=-0.0522, ret=7.14e-5, glen=59.7, tlen=220, kl=0.0357, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=-0.0522, ret=7.14e-5, glen=59.7, tlen=220, kl=0.0357, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=0.046, ret=-6.37e-5, glen=61.3, tlen=222, kl=0.0289, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=0.046, ret=-6.37e-5, glen=61.3, tlen=222, kl=0.0289, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=-0.0708, ret=9.45e-5, glen=61.6, tlen=223, kl=0.0717, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:32,  1.14it/s, pg=-0.0708, ret=9.45e-5, glen=61.6, tlen=223, kl=0.0717, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=0.0302, ret=-4.1e-6, glen=61.5, tlen=222, kl=0.0392, act_lr=1e-6, ent=0.941] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.15it/s, pg=0.0302, ret=-4.1e-6, glen=61.5, tlen=222, kl=0.0392, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.15it/s, pg=-0.0529, ret=7.8e-5, glen=60.9, tlen=222, kl=0.0211, act_lr=1e-6, ent=0.87] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.16it/s, pg=-0.0529, ret=7.8e-5, glen=60.9, tlen=222, kl=0.0211, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.16it/s, pg=0.0398, ret=-1.74e-5, glen=59.9, tlen=220, kl=0.0242, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=0.0398, ret=-1.74e-5, glen=59.9, tlen=220, kl=0.0242, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=0.11, ret=-0.000191, glen=61.7, tlen=222, kl=0.0481, act_lr=1e-6, ent=0.887] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=0.11, ret=-0.000191, glen=61.7, tlen=222, kl=0.0481, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=0.14, ret=-0.000156, glen=60, tlen=221, kl=0.037, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=0.14, ret=-0.000156, glen=60, tlen=221, kl=0.037, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.0146, ret=-2.81e-5, glen=61.6, tlen=222, kl=0.0286, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.0146, ret=-2.81e-5, glen=61.6, tlen=222, kl=0.0286, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0697, ret=9.15e-5, glen=60.6, tlen=221, kl=0.0323, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0697, ret=9.15e-5, glen=60.6, tlen=221, kl=0.0323, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=0.0551, ret=-5.4e-5, glen=59.8, tlen=220, kl=0.0237, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=0.0551, ret=-5.4e-5, glen=59.8, tlen=220, kl=0.0237, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.0939, ret=-0.000152, glen=60.1, tlen=221, kl=0.0443, act_lr=1e-6, ent=0.873]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.0939, ret=-0.000152, glen=60.1, tlen=221, kl=0.0443, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0014, ret=-2.31e-5, glen=60, tlen=220, kl=0.0333, act_lr=1e-6, ent=0.877]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0014, ret=-2.31e-5, glen=60, tlen=220, kl=0.0333, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.072, ret=9.64e-5, glen=60.4, tlen=221, kl=0.0398, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.072, ret=9.64e-5, glen=60.4, tlen=221, kl=0.0398, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0804, ret=-0.000139, glen=62.7, tlen=224, kl=0.0301, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0804, ret=-0.000139, glen=62.7, tlen=224, kl=0.0301, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=-0.00647, ret=-1.74e-5, glen=60.3, tlen=221, kl=0.0321, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.00647, ret=-1.74e-5, glen=60.3, tlen=221, kl=0.0321, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0492, ret=7.09e-5, glen=58.9, tlen=219, kl=0.0317, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.17it/s, pg=-0.0492, ret=7.09e-5, glen=58.9, tlen=219, kl=0.0317, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0359, ret=5.26e-5, glen=61.4, tlen=222, kl=0.0356, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:19,  1.15it/s, pg=-0.0359, ret=5.26e-5, glen=61.4, tlen=222, kl=0.0356, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:19,  1.15it/s, pg=-0.0613, ret=8.77e-5, glen=59.4, tlen=220, kl=0.054, act_lr=1e-6, ent=0.892] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.0613, ret=8.77e-5, glen=59.4, tlen=220, kl=0.054, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=0.0443, ret=-5.31e-5, glen=61.5, tlen=222, kl=0.0459, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.16it/s, pg=0.0443, ret=-5.31e-5, glen=61.5, tlen=222, kl=0.0459, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.16it/s, pg=-0.0626, ret=8.39e-5, glen=59.8, tlen=220, kl=0.045, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0626, ret=8.39e-5, glen=59.8, tlen=220, kl=0.045, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0361, ret=4.82e-5, glen=60.8, tlen=221, kl=0.12, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0361, ret=4.82e-5, glen=60.8, tlen=221, kl=0.12, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.045, ret=6.37e-5, glen=61.2, tlen=221, kl=0.0388, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.06it/s, pg=-0.045, ret=6.37e-5, glen=61.2, tlen=221, kl=0.0388, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.06it/s, pg=-0.0118, ret=8.67e-5, glen=59.9, tlen=220, kl=0.0706, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.09it/s, pg=-0.0118, ret=8.67e-5, glen=59.9, tlen=220, kl=0.0706, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.09it/s, pg=-0.0613, ret=8.58e-5, glen=61, tlen=221, kl=0.026, act_lr=1e-6, ent=0.91]    Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0613, ret=8.58e-5, glen=61, tlen=221, kl=0.026, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-6.1e-5, ret=-1.59e-5, glen=61.3, tlen=222, kl=0.0485, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=-6.1e-5, ret=-1.59e-5, glen=61.3, tlen=222, kl=0.0485, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0538, ret=7.52e-5, glen=60.4, tlen=221, kl=0.0286, act_lr=1e-6, ent=0.893] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0538, ret=7.52e-5, glen=60.4, tlen=221, kl=0.0286, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0061, ret=-3.86e-5, glen=60.4, tlen=221, kl=0.18, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.0061, ret=-3.86e-5, glen=60.4, tlen=221, kl=0.18, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=0.0253, ret=-5.35e-5, glen=60.3, tlen=221, kl=0.0255, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=0.0253, ret=-5.35e-5, glen=60.3, tlen=221, kl=0.0255, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=-0.0567, ret=7.25e-5, glen=58.8, tlen=219, kl=0.0262, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0567, ret=7.25e-5, glen=58.8, tlen=219, kl=0.0262, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0553, ret=7.52e-5, glen=60, tlen=220, kl=0.0289, act_lr=1e-6, ent=0.871]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0553, ret=7.52e-5, glen=60, tlen=220, kl=0.0289, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.0616, ret=8.87e-5, glen=60.8, tlen=222, kl=0.0247, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0616, ret=8.87e-5, glen=60.8, tlen=222, kl=0.0247, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0336, ret=-4.05e-5, glen=60.1, tlen=221, kl=0.0273, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.0336, ret=-4.05e-5, glen=60.1, tlen=221, kl=0.0273, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.0144, ret=-4.44e-5, glen=60, tlen=220, kl=0.0256, act_lr=1e-6, ent=0.882]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.0144, ret=-4.44e-5, glen=60, tlen=220, kl=0.0256, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0705, ret=9.38e-5, glen=60.7, tlen=222, kl=0.0308, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0705, ret=9.38e-5, glen=60.7, tlen=222, kl=0.0308, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0544, ret=7.21e-5, glen=59.7, tlen=220, kl=0.0288, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0544, ret=7.21e-5, glen=59.7, tlen=220, kl=0.0288, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.055, ret=7.52e-5, glen=59.7, tlen=220, kl=0.0285, act_lr=1e-6, ent=0.883] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.055, ret=7.52e-5, glen=59.7, tlen=220, kl=0.0285, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.00183, ret=4.83e-6, glen=61.4, tlen=222, kl=0.0277, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.00183, ret=4.83e-6, glen=61.4, tlen=222, kl=0.0277, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.05, ret=6.84e-5, glen=59.5, tlen=220, kl=0.0635, act_lr=1e-6, ent=0.901]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.17it/s, pg=-0.05, ret=6.84e-5, glen=59.5, tlen=220, kl=0.0635, act_lr=1e-6, ent=0.901]
2025-07-24 21:11:49.354 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.95s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.0388, ret=-3.28e-5, glen=60.1, tlen=221, kl=0.0313, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=0.0388, ret=-3.28e-5, glen=60.1, tlen=221, kl=0.0313, act_lr=1e-6, ent=0.893]
2025-07-24 21:11:50.024 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 21:11:52.407 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.38s
2025-07-24 21:11:52.763 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.49s
2025-07-24 21:11:52.769 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005459163499915081, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8942557858384174, 'kl': 0.040143220159022705, 'response_length': 60.458531255307406, 'total_length': 221.03153859014097, 'teacher_total_length': 232.96916165559188, 'return': 2.661248167303553e-06, 'policy_update_steps': 1.0}

Episode [10/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [18:42<05:35, 111.73s/it][A2025-07-24 21:11:52.815 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:12:27.886 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:12:28.062 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:12:28.062 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.25s
2025-07-24 21:12:29.783 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 59.6357,std_num_tokens: 12.6903,avg_correct_num_tokens: 59.6204,std_correct_num_tokens: 12.6781,avg_incorrect_num_tokens: 62.5349,std_incorrect_num_tokens: 14.5336
2025-07-24 21:12:30.101 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.04s
2025-07-24 21:12:32.591 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.49s
2025-07-24 21:12:56.217 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 181
2025-07-24 21:12:56.217 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.62s
2025-07-24 21:12:57.786 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.07s
2025-07-24 21:12:57.786 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 4.109305854454554e-06, avg_kl: 0.03932282674378453, avg_response_length: 59.65673004066088, avg_orm_score: 0.0, avg_custom_rewards: 4.109305854454554e-06
2025-07-24 21:12:57.815 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter127_replay_buffer.jsonl
2025-07-24 21:12:59.095 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.28s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.0543, ret=-1.93e-5, glen=60, tlen=220, kl=0.0732, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=0.0543, ret=-1.93e-5, glen=60, tlen=220, kl=0.0732, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=0.0448, ret=-3.87e-5, glen=61, tlen=221, kl=0.0316, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=0.0448, ret=-3.87e-5, glen=61, tlen=221, kl=0.0316, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0781, ret=0.000117, glen=58.9, tlen=219, kl=0.0343, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.09it/s, pg=-0.0781, ret=0.000117, glen=58.9, tlen=219, kl=0.0343, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.09it/s, pg=0.0194, ret=-4.44e-5, glen=59.2, tlen=219, kl=0.048, act_lr=1e-6, ent=0.923]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:39,  1.06it/s, pg=0.0194, ret=-4.44e-5, glen=59.2, tlen=219, kl=0.048, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:39,  1.06it/s, pg=-0.0533, ret=8.54e-5, glen=59, tlen=219, kl=0.0487, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:38,  1.07it/s, pg=-0.0533, ret=8.54e-5, glen=59, tlen=219, kl=0.0487, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:38,  1.07it/s, pg=0.0105, ret=-2.11e-5, glen=58.4, tlen=218, kl=0.0671, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:36,  1.10it/s, pg=0.0105, ret=-2.11e-5, glen=58.4, tlen=218, kl=0.0671, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:36,  1.10it/s, pg=0.023, ret=-3.32e-5, glen=57.4, tlen=217, kl=0.0298, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.12it/s, pg=0.023, ret=-3.32e-5, glen=57.4, tlen=217, kl=0.0298, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.12it/s, pg=0.00256, ret=-1.41e-5, glen=59.8, tlen=220, kl=0.134, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.14it/s, pg=0.00256, ret=-1.41e-5, glen=59.8, tlen=220, kl=0.134, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.14it/s, pg=0.0251, ret=-9.91e-5, glen=58.3, tlen=218, kl=0.0375, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=0.0251, ret=-9.91e-5, glen=58.3, tlen=218, kl=0.0375, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=0.263, ret=-0.000329, glen=58.9, tlen=219, kl=0.0318, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.16it/s, pg=0.263, ret=-0.000329, glen=58.9, tlen=219, kl=0.0318, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.16it/s, pg=-0.0146, ret=7.45e-6, glen=58.4, tlen=218, kl=0.0369, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.16it/s, pg=-0.0146, ret=7.45e-6, glen=58.4, tlen=218, kl=0.0369, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.16it/s, pg=-0.0872, ret=0.000133, glen=59.4, tlen=219, kl=0.0341, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.14it/s, pg=-0.0872, ret=0.000133, glen=59.4, tlen=219, kl=0.0341, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.14it/s, pg=-0.0643, ret=9.33e-5, glen=62.1, tlen=222, kl=0.0276, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.15it/s, pg=-0.0643, ret=9.33e-5, glen=62.1, tlen=222, kl=0.0276, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.15it/s, pg=-0.00928, ret=7.78e-6, glen=59.3, tlen=220, kl=0.0312, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.00928, ret=7.78e-6, glen=59.3, tlen=220, kl=0.0312, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.0885, ret=-0.000108, glen=60.6, tlen=220, kl=0.0264, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.16it/s, pg=0.0885, ret=-0.000108, glen=60.6, tlen=220, kl=0.0264, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.16it/s, pg=-0.0741, ret=0.000112, glen=59.5, tlen=219, kl=0.027, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0741, ret=0.000112, glen=59.5, tlen=219, kl=0.027, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0713, ret=0.000114, glen=59.3, tlen=219, kl=0.0355, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0713, ret=0.000114, glen=59.3, tlen=219, kl=0.0355, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.0833, ret=-0.000114, glen=60.2, tlen=220, kl=0.0362, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.0833, ret=-0.000114, glen=60.2, tlen=220, kl=0.0362, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=0.0366, ret=-8.1e-5, glen=60.7, tlen=221, kl=0.0327, act_lr=1e-6, ent=0.914]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.17it/s, pg=0.0366, ret=-8.1e-5, glen=60.7, tlen=221, kl=0.0327, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.17it/s, pg=0.0776, ret=-0.000133, glen=60.4, tlen=220, kl=0.0325, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.18it/s, pg=0.0776, ret=-0.000133, glen=60.4, tlen=220, kl=0.0325, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.18it/s, pg=-0.0644, ret=0.000101, glen=58.4, tlen=218, kl=0.0272, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=-0.0644, ret=0.000101, glen=58.4, tlen=218, kl=0.0272, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.18it/s, pg=0.0179, ret=1.93e-5, glen=60.2, tlen=220, kl=0.0345, act_lr=1e-6, ent=0.914]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=0.0179, ret=1.93e-5, glen=60.2, tlen=220, kl=0.0345, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.18it/s, pg=0.0912, ret=-0.000134, glen=60.1, tlen=220, kl=0.0247, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=0.0912, ret=-0.000134, glen=60.1, tlen=220, kl=0.0247, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=0.0263, ret=2.45e-6, glen=60, tlen=220, kl=0.0473, act_lr=1e-6, ent=0.918]    Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=0.0263, ret=2.45e-6, glen=60, tlen=220, kl=0.0473, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=-0.0849, ret=0.00013, glen=60.6, tlen=220, kl=0.0301, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=-0.0849, ret=0.00013, glen=60.6, tlen=220, kl=0.0301, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=-0.0345, ret=3.38e-6, glen=58.8, tlen=219, kl=0.0308, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.18it/s, pg=-0.0345, ret=3.38e-6, glen=58.8, tlen=219, kl=0.0308, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.18it/s, pg=-0.00159, ret=-1.91e-5, glen=59.7, tlen=219, kl=0.044, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.18it/s, pg=-0.00159, ret=-1.91e-5, glen=59.7, tlen=219, kl=0.044, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.18it/s, pg=0.11, ret=-0.00026, glen=60.2, tlen=221, kl=0.0421, act_lr=1e-6, ent=0.92]    Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.18it/s, pg=0.11, ret=-0.00026, glen=60.2, tlen=221, kl=0.0421, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.18it/s, pg=0.0356, ret=-4.06e-5, glen=58.9, tlen=219, kl=0.0629, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=0.0356, ret=-4.06e-5, glen=58.9, tlen=219, kl=0.0629, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.113, ret=-0.000139, glen=60.7, tlen=221, kl=0.023, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.113, ret=-0.000139, glen=60.7, tlen=221, kl=0.023, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=0.0536, ret=-4.9e-5, glen=59.7, tlen=219, kl=0.0373, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.0536, ret=-4.9e-5, glen=59.7, tlen=219, kl=0.0373, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=-0.0838, ret=0.000129, glen=59.7, tlen=220, kl=0.0384, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0838, ret=0.000129, glen=59.7, tlen=220, kl=0.0384, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0631, ret=0.000103, glen=60.8, tlen=220, kl=0.0269, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0631, ret=0.000103, glen=60.8, tlen=220, kl=0.0269, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0307, ret=2.36e-5, glen=59.6, tlen=220, kl=0.0465, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0307, ret=2.36e-5, glen=59.6, tlen=220, kl=0.0465, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=0.0832, ret=-8.68e-5, glen=61.7, tlen=221, kl=0.0284, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.0832, ret=-8.68e-5, glen=61.7, tlen=221, kl=0.0284, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0601, ret=9.15e-5, glen=59.1, tlen=219, kl=0.0383, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0601, ret=9.15e-5, glen=59.1, tlen=219, kl=0.0383, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.00961, ret=-2.14e-5, glen=61.5, tlen=222, kl=0.0265, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.00961, ret=-2.14e-5, glen=61.5, tlen=222, kl=0.0265, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=-0.093, ret=0.000133, glen=61.8, tlen=222, kl=0.0263, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.093, ret=0.000133, glen=61.8, tlen=222, kl=0.0263, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0955, ret=0.000156, glen=59.7, tlen=220, kl=0.0394, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0955, ret=0.000156, glen=59.7, tlen=220, kl=0.0394, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.083, ret=0.000122, glen=61, tlen=221, kl=0.0265, act_lr=1e-6, ent=0.933]   Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.083, ret=0.000122, glen=61, tlen=221, kl=0.0265, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.00446, ret=-1.35e-5, glen=57.8, tlen=218, kl=0.0336, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.00446, ret=-1.35e-5, glen=57.8, tlen=218, kl=0.0336, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0562, ret=-0.000116, glen=58.5, tlen=219, kl=0.0341, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.18it/s, pg=0.0562, ret=-0.000116, glen=58.5, tlen=219, kl=0.0341, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.18it/s, pg=-0.0721, ret=0.000112, glen=58.3, tlen=218, kl=0.0352, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=-0.0721, ret=0.000112, glen=58.3, tlen=218, kl=0.0352, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.18it/s, pg=-0.0788, ret=0.000123, glen=59, tlen=219, kl=0.0777, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0788, ret=0.000123, glen=59, tlen=219, kl=0.0777, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=-0.0673, ret=0.0001, glen=58.6, tlen=218, kl=0.0389, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0673, ret=0.0001, glen=58.6, tlen=218, kl=0.0389, act_lr=1e-6, ent=0.923]
2025-07-24 21:13:39.258 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.99s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0126, ret=1.64e-5, glen=59, tlen=219, kl=0.0304, act_lr=1e-6, ent=0.935] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0126, ret=1.64e-5, glen=59, tlen=219, kl=0.0304, act_lr=1e-6, ent=0.935]
2025-07-24 21:13:40.115 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 21:13:42.710 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 21:13:43.062 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.92s
2025-07-24 21:13:43.068 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0016326904296875, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9183533178723376, 'kl': 0.03929071841032609, 'response_length': 59.65725442637568, 'total_length': 219.49870698348334, 'teacher_total_length': 231.40302243440047, 'return': 2.614565979140183e-06, 'policy_update_steps': 1.0}

Episode [10/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [20:33<03:42, 111.29s/it][A2025-07-24 21:13:43.112 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:14:18.440 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:14:18.621 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:14:18.621 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.51s
2025-07-24 21:14:20.394 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 60.6125,std_num_tokens: 12.8758,avg_correct_num_tokens: 60.6047,std_correct_num_tokens: 12.8690,avg_incorrect_num_tokens: 62.6774,std_incorrect_num_tokens: 14.3940
2025-07-24 21:14:21.007 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 21:14:23.499 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.49s
2025-07-24 21:14:47.010 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:14:47.010 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.51s
2025-07-24 21:14:48.189 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 21:14:48.190 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.5399160937818348e-05, avg_kl: 0.03857355169911202, avg_response_length: 60.62447125794458, avg_orm_score: 0.0, avg_custom_rewards: 2.5399160937818348e-05
2025-07-24 21:14:48.216 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter128_replay_buffer.jsonl
2025-07-24 21:14:49.522 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.31s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0635, ret=8.25e-5, glen=59.9, tlen=220, kl=0.025, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0635, ret=8.25e-5, glen=59.9, tlen=220, kl=0.025, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.061, ret=7.85e-5, glen=60.5, tlen=221, kl=0.028, act_lr=1e-6, ent=0.876] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.061, ret=7.85e-5, glen=60.5, tlen=221, kl=0.028, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=0.0307, ret=-5.34e-5, glen=61.1, tlen=222, kl=0.0349, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:40,  1.06it/s, pg=0.0307, ret=-5.34e-5, glen=61.1, tlen=222, kl=0.0349, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:40,  1.06it/s, pg=-0.0705, ret=9.78e-5, glen=62, tlen=223, kl=0.0278, act_lr=1e-6, ent=0.912]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:40,  1.05it/s, pg=-0.0705, ret=9.78e-5, glen=62, tlen=223, kl=0.0278, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:40,  1.05it/s, pg=-0.0549, ret=7.08e-5, glen=61.1, tlen=221, kl=0.026, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:37,  1.09it/s, pg=-0.0549, ret=7.08e-5, glen=61.1, tlen=221, kl=0.026, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:37,  1.09it/s, pg=0.0609, ret=-5.98e-5, glen=60.1, tlen=220, kl=0.0424, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.11it/s, pg=0.0609, ret=-5.98e-5, glen=60.1, tlen=220, kl=0.0424, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.11it/s, pg=-0.0579, ret=7.11e-5, glen=60.2, tlen=220, kl=0.03, act_lr=1e-6, ent=0.866] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.13it/s, pg=-0.0579, ret=7.11e-5, glen=60.2, tlen=220, kl=0.03, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.13it/s, pg=0.0965, ret=-0.000172, glen=61, tlen=221, kl=0.0284, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=0.0965, ret=-0.000172, glen=61, tlen=221, kl=0.0284, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.15it/s, pg=0.144, ret=-0.000136, glen=60.2, tlen=221, kl=0.0287, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.16it/s, pg=0.144, ret=-0.000136, glen=60.2, tlen=221, kl=0.0287, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.16it/s, pg=0.0374, ret=6.59e-5, glen=60.6, tlen=221, kl=0.0353, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.16it/s, pg=0.0374, ret=6.59e-5, glen=60.6, tlen=221, kl=0.0353, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.16it/s, pg=0.00928, ret=-2.12e-5, glen=60.4, tlen=220, kl=0.0365, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:29,  1.17it/s, pg=0.00928, ret=-2.12e-5, glen=60.4, tlen=220, kl=0.0365, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:29,  1.17it/s, pg=-0.0728, ret=0.000101, glen=60.5, tlen=221, kl=0.0536, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.17it/s, pg=-0.0728, ret=0.000101, glen=60.5, tlen=221, kl=0.0536, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.17it/s, pg=-0.0468, ret=6.17e-5, glen=61.2, tlen=222, kl=0.0402, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:29,  1.11it/s, pg=-0.0468, ret=6.17e-5, glen=61.2, tlen=222, kl=0.0402, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:29,  1.11it/s, pg=0.0881, ret=-0.000166, glen=58.4, tlen=218, kl=0.0463, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:28,  1.12it/s, pg=0.0881, ret=-0.000166, glen=58.4, tlen=218, kl=0.0463, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:28,  1.12it/s, pg=-0.043, ret=5.72e-5, glen=59.5, tlen=220, kl=0.042, act_lr=1e-6, ent=0.911]   Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:27,  1.14it/s, pg=-0.043, ret=5.72e-5, glen=59.5, tlen=220, kl=0.042, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:27,  1.14it/s, pg=0.274, ret=-0.000298, glen=61.5, tlen=222, kl=0.0384, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.15it/s, pg=0.274, ret=-0.000298, glen=61.5, tlen=222, kl=0.0384, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:15<00:26,  1.15it/s, pg=-0.0598, ret=8.02e-5, glen=59.4, tlen=219, kl=0.059, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.15it/s, pg=-0.0598, ret=8.02e-5, glen=59.4, tlen=219, kl=0.059, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.15it/s, pg=0.0407, ret=-5.2e-5, glen=57.8, tlen=218, kl=0.078, act_lr=1e-6, ent=0.887] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=0.0407, ret=-5.2e-5, glen=57.8, tlen=218, kl=0.078, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=0.0109, ret=-1.57e-6, glen=62.2, tlen=222, kl=0.0369, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.16it/s, pg=0.0109, ret=-1.57e-6, glen=62.2, tlen=222, kl=0.0369, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.16it/s, pg=-0.0938, ret=0.00012, glen=60, tlen=220, kl=0.0323, act_lr=1e-6, ent=0.914]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0938, ret=0.00012, glen=60, tlen=220, kl=0.0323, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0467, ret=6.11e-5, glen=59.8, tlen=220, kl=0.0287, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.15it/s, pg=-0.0467, ret=6.11e-5, glen=59.8, tlen=220, kl=0.0287, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.15it/s, pg=-0.0615, ret=7.52e-5, glen=60.5, tlen=221, kl=0.0385, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.15it/s, pg=-0.0615, ret=7.52e-5, glen=60.5, tlen=221, kl=0.0385, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.15it/s, pg=0.0181, ret=-4.05e-5, glen=60.6, tlen=221, kl=0.0283, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=0.0181, ret=-4.05e-5, glen=60.6, tlen=221, kl=0.0283, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:21<00:19,  1.16it/s, pg=-0.0105, ret=-3.09e-5, glen=60, tlen=221, kl=0.0547, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.16it/s, pg=-0.0105, ret=-3.09e-5, glen=60, tlen=221, kl=0.0547, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.16it/s, pg=0.118, ret=-0.000198, glen=59.9, tlen=220, kl=0.0499, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=0.118, ret=-0.000198, glen=59.9, tlen=220, kl=0.0499, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=0.164, ret=-0.000179, glen=61.3, tlen=221, kl=0.0667, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.164, ret=-0.000179, glen=61.3, tlen=221, kl=0.0667, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.027, ret=-5.71e-5, glen=62, tlen=223, kl=0.0243, act_lr=1e-6, ent=0.919]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.027, ret=-5.71e-5, glen=62, tlen=223, kl=0.0243, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.00128, ret=-7.72e-6, glen=60.5, tlen=221, kl=0.0276, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.00128, ret=-7.72e-6, glen=60.5, tlen=221, kl=0.0276, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=0.0426, ret=-3.33e-5, glen=61.1, tlen=222, kl=0.0434, act_lr=1e-6, ent=0.9]    Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:17,  1.01s/it, pg=0.0426, ret=-3.33e-5, glen=61.1, tlen=222, kl=0.0434, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:17,  1.01s/it, pg=-0.0652, ret=8.76e-5, glen=59.8, tlen=220, kl=0.0638, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:15,  1.04it/s, pg=-0.0652, ret=8.76e-5, glen=59.8, tlen=220, kl=0.0638, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:15,  1.04it/s, pg=-0.0641, ret=8.39e-5, glen=61.6, tlen=222, kl=0.0296, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.08it/s, pg=-0.0641, ret=8.39e-5, glen=61.6, tlen=222, kl=0.0296, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.08it/s, pg=-0.084, ret=0.000114, glen=61.3, tlen=222, kl=0.0443, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=-0.084, ret=0.000114, glen=61.3, tlen=222, kl=0.0443, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:29<00:12,  1.11it/s, pg=-0.0787, ret=9.98e-5, glen=60.9, tlen=221, kl=0.0418, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.0787, ret=9.98e-5, glen=60.9, tlen=221, kl=0.0418, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:30<00:11,  1.13it/s, pg=0.0668, ret=-6.84e-5, glen=60.4, tlen=220, kl=0.0531, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.12it/s, pg=0.0668, ret=-6.84e-5, glen=60.4, tlen=220, kl=0.0531, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:31<00:10,  1.12it/s, pg=0.000244, ret=-2.89e-5, glen=60.4, tlen=221, kl=0.0309, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.13it/s, pg=0.000244, ret=-2.89e-5, glen=60.4, tlen=221, kl=0.0309, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.13it/s, pg=0.0261, ret=-5.54e-5, glen=62.6, tlen=223, kl=0.03, act_lr=1e-6, ent=0.895]    Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.14it/s, pg=0.0261, ret=-5.54e-5, glen=62.6, tlen=223, kl=0.03, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.14it/s, pg=-0.0468, ret=6.6e-5, glen=60.6, tlen=221, kl=0.0574, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.15it/s, pg=-0.0468, ret=6.6e-5, glen=60.6, tlen=221, kl=0.0574, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.15it/s, pg=0.116, ret=-0.000152, glen=61.3, tlen=222, kl=0.0276, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.16it/s, pg=0.116, ret=-0.000152, glen=61.3, tlen=222, kl=0.0276, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.16it/s, pg=0.00128, ret=1.15e-6, glen=61.2, tlen=222, kl=0.038, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:06,  1.16it/s, pg=0.00128, ret=1.15e-6, glen=61.2, tlen=222, kl=0.038, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:35<00:06,  1.16it/s, pg=-0.0851, ret=0.000113, glen=59.9, tlen=221, kl=0.0271, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0851, ret=0.000113, glen=59.9, tlen=221, kl=0.0271, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:36<00:05,  1.17it/s, pg=-0.0443, ret=5.79e-5, glen=58.8, tlen=219, kl=0.0389, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0443, ret=5.79e-5, glen=58.8, tlen=219, kl=0.0389, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.161, ret=-0.000167, glen=60.3, tlen=221, kl=0.0369, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.161, ret=-0.000167, glen=60.3, tlen=221, kl=0.0369, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0692, ret=9.49e-5, glen=62.6, tlen=223, kl=0.0351, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0692, ret=9.49e-5, glen=62.6, tlen=223, kl=0.0351, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0588, ret=8e-5, glen=63.2, tlen=224, kl=0.026, act_lr=1e-6, ent=0.895]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0588, ret=8e-5, glen=63.2, tlen=224, kl=0.026, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.067, ret=8.49e-5, glen=60.7, tlen=221, kl=0.0342, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.067, ret=8.49e-5, glen=60.7, tlen=221, kl=0.0342, act_lr=1e-6, ent=0.889]
2025-07-24 21:15:30.190 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.50s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=-0.0626, ret=8.2e-5, glen=59.5, tlen=219, kl=0.0247, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.11it/s, pg=-0.0626, ret=8.2e-5, glen=59.5, tlen=219, kl=0.0247, act_lr=1e-6, ent=0.907]
2025-07-24 21:15:30.924 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.59s
2025-07-24 21:15:33.123 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.20s
2025-07-24 21:15:33.473 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.90s
2025-07-24 21:15:33.480 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00139617919921875, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9051765320093735, 'kl': 0.038503895635190216, 'response_length': 60.61914518605108, 'total_length': 221.00272734268853, 'teacher_total_length': 232.97696221393088, 'return': 2.539302089565393e-07, 'policy_update_steps': 1.0}

Episode [10/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [22:23<01:51, 111.02s/it][A2025-07-24 21:15:33.486 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<02:36,  1.08it/s, est. speed input: 198.26 toks/s, output: 39.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:01<00:01, 65.27it/s, est. speed input: 5231.95 toks/s, output: 1524.93 toks/s]Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:01<00:00, 100.14it/s, est. speed input: 6984.60 toks/s, output: 2161.56 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/172 [00:02<00:00, 80.71it/s, est. speed input: 10289.67 toks/s, output: 3677.43 toks/s]
2025-07-24 21:15:37.843 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 317.8719,strategyqa_test/accuracy: 0.5284,eval_accuracy: 0.5284
2025-07-24 21:15:38.092 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:15:56.505 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:15:56.685 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:15:56.686 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 18.59s
2025-07-24 21:15:58.059 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 59.4578,std_num_tokens: 12.6482,avg_correct_num_tokens: 59.4519,std_correct_num_tokens: 12.6323,avg_incorrect_num_tokens: 61.1333,std_incorrect_num_tokens: 16.4798
2025-07-24 21:15:58.342 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.66s
2025-07-24 21:15:59.551 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.20s
2025-07-24 21:16:12.228 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 95
2025-07-24 21:16:12.229 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.68s
2025-07-24 21:16:13.177 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.49s
2025-07-24 21:16:13.178 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.0770036529651597e-05, avg_kl: 0.06775416324013157, avg_response_length: 59.478455955103826, avg_orm_score: 0.0, avg_custom_rewards: 2.0770036529651597e-05
2025-07-24 21:16:13.232 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter129_replay_buffer.jsonl
2025-07-24 21:16:13.880 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.65s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:02<00:00, 88.72it/s, est. speed input: 10368.71 toks/s, output: 3683.73 toks/s][32m [repeated 56x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:03<00:00, 48.27it/s, est. speed input: 8750.99 toks/s, output: 3268.83 toks/s][32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:01<?, ?it/s, pg=-0.0393, ret=4.78e-5, glen=60.2, tlen=220, kl=0.0363, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:01<00:23,  1.01s/it, pg=-0.0393, ret=4.78e-5, glen=60.2, tlen=220, kl=0.0363, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:01<00:23,  1.01s/it, pg=-0.0549, ret=7.05e-5, glen=59.4, tlen=219, kl=0.0524, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:01<00:20,  1.08it/s, pg=-0.0549, ret=7.05e-5, glen=59.4, tlen=219, kl=0.0524, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:02<00:20,  1.08it/s, pg=-0.0717, ret=8.87e-5, glen=60.5, tlen=221, kl=0.0326, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:02<00:18,  1.12it/s, pg=-0.0717, ret=8.87e-5, glen=60.5, tlen=221, kl=0.0326, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:03<00:18,  1.12it/s, pg=0.0165, ret=-2.87e-5, glen=59.2, tlen=219, kl=0.212, act_lr=1e-6, ent=0.897] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:03<00:18,  1.11it/s, pg=0.0165, ret=-2.87e-5, glen=59.2, tlen=219, kl=0.212, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:04<00:18,  1.11it/s, pg=0.0453, ret=-4.39e-5, glen=57.7, tlen=218, kl=0.0578, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:04<00:17,  1.11it/s, pg=0.0453, ret=-4.39e-5, glen=57.7, tlen=218, kl=0.0578, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:05<00:17,  1.11it/s, pg=-0.0565, ret=7.01e-5, glen=59.1, tlen=219, kl=0.0709, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:05<00:15,  1.13it/s, pg=-0.0565, ret=7.01e-5, glen=59.1, tlen=219, kl=0.0709, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:06<00:15,  1.13it/s, pg=-0.0535, ret=6.91e-5, glen=59, tlen=219, kl=0.0702, act_lr=1e-6, ent=0.905]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:06<00:14,  1.14it/s, pg=-0.0535, ret=6.91e-5, glen=59, tlen=219, kl=0.0702, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:07<00:14,  1.14it/s, pg=0.137, ret=-0.000293, glen=60.5, tlen=221, kl=0.0336, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:13,  1.15it/s, pg=0.137, ret=-0.000293, glen=60.5, tlen=221, kl=0.0336, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:13,  1.15it/s, pg=0.0341, ret=-5.66e-5, glen=58.6, tlen=219, kl=0.0377, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:07<00:12,  1.16it/s, pg=0.0341, ret=-5.66e-5, glen=58.6, tlen=219, kl=0.0377, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:08<00:12,  1.16it/s, pg=0.148, ret=-0.000168, glen=57.7, tlen=218, kl=0.0331, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:08<00:12,  1.16it/s, pg=0.148, ret=-0.000168, glen=57.7, tlen=218, kl=0.0331, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:09<00:12,  1.16it/s, pg=-0.0657, ret=8.23e-5, glen=59, tlen=219, kl=0.0345, act_lr=1e-6, ent=0.909]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:09<00:11,  1.17it/s, pg=-0.0657, ret=8.23e-5, glen=59, tlen=219, kl=0.0345, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:10<00:11,  1.17it/s, pg=-0.0633, ret=8.41e-5, glen=60, tlen=220, kl=0.0418, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:10<00:10,  1.17it/s, pg=-0.0633, ret=8.41e-5, glen=60, tlen=220, kl=0.0418, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:11<00:10,  1.17it/s, pg=0.168, ret=-0.00017, glen=58.7, tlen=219, kl=0.0878, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:11<00:09,  1.15it/s, pg=0.168, ret=-0.00017, glen=58.7, tlen=219, kl=0.0878, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:12<00:09,  1.15it/s, pg=-0.0576, ret=7.49e-5, glen=59, tlen=219, kl=0.0284, act_lr=1e-6, ent=0.881] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:12<00:08,  1.15it/s, pg=-0.0576, ret=7.49e-5, glen=59, tlen=219, kl=0.0284, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:13<00:08,  1.15it/s, pg=-0.0782, ret=9.76e-5, glen=58.4, tlen=218, kl=0.417, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.14it/s, pg=-0.0782, ret=9.76e-5, glen=58.4, tlen=218, kl=0.417, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:14<00:07,  1.14it/s, pg=-0.0561, ret=7.42e-5, glen=59.6, tlen=220, kl=0.0276, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:14<00:06,  1.15it/s, pg=-0.0561, ret=7.42e-5, glen=59.6, tlen=220, kl=0.0276, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:14<00:06,  1.15it/s, pg=-0.0688, ret=8.71e-5, glen=61.2, tlen=221, kl=0.0502, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:14<00:06,  1.15it/s, pg=-0.0688, ret=8.71e-5, glen=61.2, tlen=221, kl=0.0502, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:15<00:06,  1.15it/s, pg=0.163, ret=-0.000179, glen=61.2, tlen=221, kl=0.028, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:15<00:05,  1.16it/s, pg=0.163, ret=-0.000179, glen=61.2, tlen=221, kl=0.028, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:16<00:05,  1.16it/s, pg=0.00256, ret=2.41e-5, glen=60.3, tlen=220, kl=0.0635, act_lr=1e-6, ent=0.87]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:16<00:04,  1.16it/s, pg=0.00256, ret=2.41e-5, glen=60.3, tlen=220, kl=0.0635, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:17<00:04,  1.16it/s, pg=-0.0672, ret=8.01e-5, glen=60.7, tlen=221, kl=0.0508, act_lr=1e-6, ent=0.867]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:17<00:03,  1.17it/s, pg=-0.0672, ret=8.01e-5, glen=60.7, tlen=221, kl=0.0508, act_lr=1e-6, ent=0.867]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:18<00:03,  1.17it/s, pg=-0.0606, ret=7.52e-5, glen=60.6, tlen=221, kl=0.0474, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:18<00:02,  1.17it/s, pg=-0.0606, ret=7.52e-5, glen=60.6, tlen=221, kl=0.0474, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:19<00:02,  1.17it/s, pg=-0.0516, ret=6.17e-5, glen=59.9, tlen=220, kl=0.041, act_lr=1e-6, ent=0.892] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:19<00:01,  1.17it/s, pg=-0.0516, ret=6.17e-5, glen=59.9, tlen=220, kl=0.041, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:20<00:01,  1.17it/s, pg=0.149, ret=-0.000146, glen=57.4, tlen=217, kl=0.0302, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.17it/s, pg=0.149, ret=-0.000146, glen=57.4, tlen=217, kl=0.0302, act_lr=1e-6, ent=0.88]
2025-07-24 21:16:35.038 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.96s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.17it/s, pg=-0.0482, ret=5.96e-5, glen=59.8, tlen=220, kl=0.0346, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.10it/s, pg=-0.0482, ret=5.96e-5, glen=59.8, tlen=220, kl=0.0346, act_lr=1e-6, ent=0.898]
2025-07-24 21:16:35.847 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 21:16:38.468 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.62s
2025-07-24 21:16:38.820 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 24.86s
2025-07-24 21:16:38.823 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0012644131978352864, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8952868928511938, 'kl': 0.06746800616383553, 'response_length': 59.48712968826294, 'total_length': 219.5884017944336, 'teacher_total_length': 231.54748026529947, 'return': 2.525247812930805e-06, 'policy_update_steps': 1.0}

Episode [10/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [23:28<00:00, 97.19s/it] [A[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,898] [INFO] [utils.py:782:see_memory_usage] MA 7.33 GB         Max_MA 7.33 GB         CA 8.37 GB         Max_CA 8 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,898] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.63 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee79dc3f0b0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,899] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,900] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.02025-07-24 21:16:48.037 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:54:58,187] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 20:53:09,272] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:56:48,785] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 20:58:40,005] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:00:37,808] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:02:29,304] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:04:22,875] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:06:15,562] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:08:07,134] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:11:49,346] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:13:39,251] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:15:30,179] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:16:35,031] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:45,657] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:45,852] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 3729, num_elems = 19.55B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,356] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,356] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,363] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,364] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,734] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,734] [INFO] [utils.py:782:see_memory_usage] MA 8.06 GB         Max_MA 13.02 GB         CA 9.1 GB         Max_CA 45 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:47,734] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.44 GB, percent = 21.5%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [11/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [10/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [23:38<00:00, 109.09s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,031] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,031] [INFO] [utils.py:782:see_memory_usage] MA 8.06 GB         Max_MA 8.06 GB         CA 9.1 GB         Max_CA 9 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,031] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.44 GB, percent = 21.5%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   amp_enabled .................. Falsehuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   amp_params ................... False
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 21:16:48.350 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   1%|          | 1/172 [00:00<02:36,  1.09it/s, est. speed input: 196.73 toks/s, output: 39.34 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:02<00:00, 92.36it/s, est. speed input: 10610.71 toks/s, output: 3769.04 toks/s]
2025-07-24 21:16:52.735 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 313.0015,strategyqa_test/accuracy: 0.5444,eval_accuracy: 0.5444
2025-07-24 21:16:53.025 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:17:28.343 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:17:28.528 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:17:28.528 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.50s
2025-07-24 21:17:30.334 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0012,avg_pass_at_n: 1.0000,avg_num_tokens: 60.3372,std_num_tokens: 12.5636,avg_correct_num_tokens: 60.3300,std_correct_num_tokens: 12.5576,avg_incorrect_num_tokens: 62.4286,std_incorrect_num_tokens: 14.0546
2025-07-24 21:17:30.755 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.23s
2025-07-24 21:17:33.439 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.68s
2025-07-24 21:17:57.594 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 21:17:57.594 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.15s
2025-07-24 21:17:58.843 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 21:17:58.844 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -6.40747788525954e-06, avg_kl: 0.0, avg_response_length: 60.352780163943116, avg_orm_score: 0.0, avg_custom_rewards: -6.40747788525954e-06
2025-07-24 21:17:58.879 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter130_replay_buffer.jsonl
2025-07-24 21:18:00.087 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:02<00:00, 82.35it/s, est. speed input: 10189.81 toks/s, output: 3631.38 toks/s][32m [repeated 56x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 47.64it/s, est. speed input: 8657.31 toks/s, output: 3094.62 toks/s] [32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.185, ret=-0.000127, glen=58.8, tlen=219, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:48,  1.07s/it, pg=0.185, ret=-0.000127, glen=58.8, tlen=219, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:48,  1.07s/it, pg=0.245, ret=-0.000296, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.879]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:42,  1.04it/s, pg=0.245, ret=-0.000296, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:42,  1.04it/s, pg=0.0229, ret=-5.03e-5, glen=59.9, tlen=219, kl=0, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.08it/s, pg=0.0229, ret=-5.03e-5, glen=59.9, tlen=219, kl=0, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.08it/s, pg=-0.0518, ret=6.37e-5, glen=60.8, tlen=220, kl=0, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.11it/s, pg=-0.0518, ret=6.37e-5, glen=60.8, tlen=220, kl=0, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.11it/s, pg=-0.1, ret=0.000118, glen=61.3, tlen=221, kl=0, act_lr=1e-6, ent=0.898]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.11it/s, pg=-0.1, ret=0.000118, glen=61.3, tlen=221, kl=0, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.11it/s, pg=-0.0454, ret=5.79e-5, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=-0.0454, ret=5.79e-5, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=0.0341, ret=-5.02e-5, glen=60.1, tlen=219, kl=0, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=0.0341, ret=-5.02e-5, glen=60.1, tlen=219, kl=0, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=-0.044, ret=5.21e-5, glen=59.5, tlen=219, kl=0, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=-0.044, ret=5.21e-5, glen=59.5, tlen=219, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.15it/s, pg=-0.071, ret=9.84e-5, glen=60.7, tlen=220, kl=0, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=-0.071, ret=9.84e-5, glen=60.7, tlen=220, kl=0, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=-0.095, ret=0.000117, glen=59.1, tlen=219, kl=0, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.16it/s, pg=-0.095, ret=0.000117, glen=59.1, tlen=219, kl=0, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.16it/s, pg=0.114, ret=-0.000178, glen=61, tlen=221, kl=0, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:31,  1.13it/s, pg=0.114, ret=-0.000178, glen=61, tlen=221, kl=0, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:31,  1.13it/s, pg=-0.046, ret=5.34e-5, glen=59.2, tlen=219, kl=0, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.14it/s, pg=-0.046, ret=5.34e-5, glen=59.2, tlen=219, kl=0, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.14it/s, pg=-0.0533, ret=6.61e-5, glen=59.7, tlen=219, kl=0, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:29,  1.12it/s, pg=-0.0533, ret=6.61e-5, glen=59.7, tlen=219, kl=0, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:29,  1.12it/s, pg=-0.0107, ret=1.46e-5, glen=62, tlen=222, kl=0, act_lr=1e-6, ent=0.929]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:28,  1.13it/s, pg=-0.0107, ret=1.46e-5, glen=62, tlen=222, kl=0, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:28,  1.13it/s, pg=-0.0277, ret=3.36e-5, glen=58.3, tlen=218, kl=0, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:27,  1.15it/s, pg=-0.0277, ret=3.36e-5, glen=58.3, tlen=218, kl=0, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:27,  1.15it/s, pg=0.132, ret=-0.000176, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=0.132, ret=-0.000176, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:15<00:25,  1.16it/s, pg=-0.0292, ret=3.15e-5, glen=62.1, tlen=222, kl=0, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.16it/s, pg=-0.0292, ret=3.15e-5, glen=62.1, tlen=222, kl=0, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.16it/s, pg=0.14, ret=-0.000168, glen=60.4, tlen=220, kl=0, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.17it/s, pg=0.14, ret=-0.000168, glen=60.4, tlen=220, kl=0, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.17it/s, pg=-0.0436, ret=5.59e-5, glen=60.2, tlen=220, kl=0, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0436, ret=5.59e-5, glen=60.2, tlen=220, kl=0, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0762, ret=9.76e-5, glen=59.1, tlen=219, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0762, ret=9.76e-5, glen=59.1, tlen=219, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.0114, ret=-5.74e-5, glen=59.3, tlen=219, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.0114, ret=-5.74e-5, glen=59.3, tlen=219, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.037, ret=-5.44e-5, glen=60.1, tlen=220, kl=0, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.037, ret=-5.44e-5, glen=60.1, tlen=220, kl=0, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.17it/s, pg=-0.0388, ret=5.71e-5, glen=61.4, tlen=221, kl=0, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0388, ret=5.71e-5, glen=61.4, tlen=221, kl=0, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0726, ret=9.53e-5, glen=61, tlen=221, kl=0, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0726, ret=9.53e-5, glen=61, tlen=221, kl=0, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0515, ret=6.5e-5, glen=60.3, tlen=220, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=-0.0515, ret=6.5e-5, glen=60.3, tlen=220, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=-0.108, ret=0.000119, glen=61.5, tlen=221, kl=0, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.108, ret=0.000119, glen=61.5, tlen=221, kl=0, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=0.0737, ret=-0.000126, glen=62.1, tlen=222, kl=0, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=0.0737, ret=-0.000126, glen=62.1, tlen=222, kl=0, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0584, ret=7.52e-5, glen=61.3, tlen=221, kl=0, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0584, ret=7.52e-5, glen=61.3, tlen=221, kl=0, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=0.0859, ret=-0.000168, glen=60.6, tlen=221, kl=0, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=0.0859, ret=-0.000168, glen=60.6, tlen=221, kl=0, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0821, ret=0.000118, glen=62, tlen=222, kl=0, act_lr=1e-6, ent=0.953]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0821, ret=0.000118, glen=62, tlen=222, kl=0, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0344, ret=4.24e-5, glen=60.7, tlen=220, kl=0, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0344, ret=4.24e-5, glen=60.7, tlen=220, kl=0, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=0.202, ret=-0.000296, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.202, ret=-0.000296, glen=59.8, tlen=220, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0522, ret=6.37e-5, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0522, ret=6.37e-5, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0566, ret=7.04e-5, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=0.894]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=-0.0566, ret=7.04e-5, glen=59, tlen=219, kl=0, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=-0.0615, ret=7.62e-5, glen=59.1, tlen=218, kl=0, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0615, ret=7.62e-5, glen=59.1, tlen=218, kl=0, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0413, ret=4.76e-5, glen=59.7, tlen=219, kl=0, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0413, ret=4.76e-5, glen=59.7, tlen=219, kl=0, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=0.0512, ret=-4.66e-5, glen=59.2, tlen=219, kl=0, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=0.0512, ret=-4.66e-5, glen=59.2, tlen=219, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=-0.0503, ret=5.59e-5, glen=60.7, tlen=220, kl=0, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0503, ret=5.59e-5, glen=60.7, tlen=220, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0276, ret=3.47e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0276, ret=3.47e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0507, ret=5.61e-5, glen=62, tlen=222, kl=0, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0507, ret=5.61e-5, glen=62, tlen=222, kl=0, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0446, ret=5.94e-5, glen=59.7, tlen=219, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0446, ret=5.94e-5, glen=59.7, tlen=219, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0014, ret=-1.41e-6, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0014, ret=-1.41e-6, glen=59.9, tlen=220, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0874, ret=-6.84e-5, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0874, ret=-6.84e-5, glen=59.6, tlen=220, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.0307, ret=-2.34e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.0307, ret=-2.34e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.0344, ret=4.17e-5, glen=59.9, tlen=219, kl=0, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0344, ret=4.17e-5, glen=59.9, tlen=219, kl=0, act_lr=1e-6, ent=0.931]
2025-07-24 21:18:40.410 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.15s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=0.0647, ret=-3.96e-5, glen=61.9, tlen=222, kl=0, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=0.0647, ret=-3.96e-5, glen=61.9, tlen=222, kl=0, act_lr=1e-6, ent=0.882]
2025-07-24 21:18:41.438 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.96s
2025-07-24 21:18:44.015 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 21:18:44.377 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.24s
2025-07-24 21:18:44.383 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0008757218070652174, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9171083271503448, 'kl': 0.0, 'response_length': 60.34260260540506, 'total_length': 220.1595409227454, 'teacher_total_length': 232.08356376316237, 'return': 2.3622163533455814e-07, 'policy_update_steps': 1.0}
Episode [11/20]:   8%|‚ñä         | 1/13 [01:56<23:16, 116.35s/it]2025-07-24 21:18:44.429 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:19:19.810 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:19:19.995 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:19:19.996 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.57s
2025-07-24 21:19:21.780 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 60.0497,std_num_tokens: 12.7950,avg_correct_num_tokens: 60.0429,std_correct_num_tokens: 12.7982,avg_incorrect_num_tokens: 62.1111,std_incorrect_num_tokens: 11.6152
2025-07-24 21:19:22.205 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.21s
2025-07-24 21:19:24.706 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.50s
2025-07-24 21:19:48.242 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 21:19:48.242 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.53s
2025-07-24 21:19:49.803 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.06s
2025-07-24 21:19:49.804 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.5356820038325347e-05, avg_kl: 0.0016594352302970467, avg_response_length: 60.06580107552664, avg_orm_score: 0.0, avg_custom_rewards: 1.5356820038325347e-05
2025-07-24 21:19:49.839 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter131_replay_buffer.jsonl
2025-07-24 21:19:51.109 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.27s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.0828, ret=-4.87e-5, glen=59.9, tlen=221, kl=0.00158, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.00s/it, pg=0.0828, ret=-4.87e-5, glen=59.9, tlen=221, kl=0.00158, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.00s/it, pg=0.024, ret=-6.13e-5, glen=61.6, tlen=222, kl=0.00162, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=0.024, ret=-6.13e-5, glen=61.6, tlen=222, kl=0.00162, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0439, ret=5.21e-5, glen=60.6, tlen=221, kl=0.00149, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.09it/s, pg=-0.0439, ret=5.21e-5, glen=60.6, tlen=221, kl=0.00149, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.09it/s, pg=-0.0729, ret=8.68e-5, glen=60.6, tlen=221, kl=0.00186, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:38,  1.08it/s, pg=-0.0729, ret=8.68e-5, glen=60.6, tlen=221, kl=0.00186, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:38,  1.08it/s, pg=0.029, ret=-3.91e-5, glen=59.6, tlen=220, kl=0.00181, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:37,  1.10it/s, pg=0.029, ret=-3.91e-5, glen=59.6, tlen=220, kl=0.00181, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:37,  1.10it/s, pg=-0.0657, ret=8.04e-5, glen=58.4, tlen=219, kl=0.00155, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.12it/s, pg=-0.0657, ret=8.04e-5, glen=58.4, tlen=219, kl=0.00155, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.12it/s, pg=-0.0725, ret=9.56e-5, glen=60.3, tlen=221, kl=0.00149, act_lr=1e-6, ent=0.872]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=-0.0725, ret=9.56e-5, glen=60.3, tlen=221, kl=0.00149, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=0.159, ret=-0.00027, glen=60.2, tlen=221, kl=0.00146, act_lr=1e-6, ent=0.882] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=0.159, ret=-0.00027, glen=60.2, tlen=221, kl=0.00146, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.15it/s, pg=-0.0607, ret=7.26e-5, glen=58.6, tlen=219, kl=0.00158, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=-0.0607, ret=7.26e-5, glen=58.6, tlen=219, kl=0.00158, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=-0.0498, ret=6.67e-5, glen=60.5, tlen=221, kl=0.00155, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.16it/s, pg=-0.0498, ret=6.67e-5, glen=60.5, tlen=221, kl=0.00155, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.16it/s, pg=-0.06, ret=7.33e-5, glen=59.1, tlen=220, kl=0.00167, act_lr=1e-6, ent=0.898]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.16it/s, pg=-0.06, ret=7.33e-5, glen=59.1, tlen=220, kl=0.00167, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.16it/s, pg=-0.0759, ret=9.6e-5, glen=59.3, tlen=220, kl=0.00139, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.17it/s, pg=-0.0759, ret=9.6e-5, glen=59.3, tlen=220, kl=0.00139, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.17it/s, pg=0.145, ret=-0.000167, glen=59.5, tlen=220, kl=0.00172, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=0.145, ret=-0.000167, glen=59.5, tlen=220, kl=0.00172, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=-0.0439, ret=5.21e-5, glen=60.8, tlen=221, kl=0.00173, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=-0.0439, ret=5.21e-5, glen=60.8, tlen=221, kl=0.00173, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.0463, ret=5.93e-5, glen=59.9, tlen=221, kl=0.00173, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0463, ret=5.93e-5, glen=59.9, tlen=221, kl=0.00173, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.0292, ret=-4.44e-5, glen=60.7, tlen=221, kl=0.00148, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=0.0292, ret=-4.44e-5, glen=60.7, tlen=221, kl=0.00148, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0563, ret=6.86e-5, glen=59.7, tlen=220, kl=0.00173, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0563, ret=6.86e-5, glen=59.7, tlen=220, kl=0.00173, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.0383, ret=-6.03e-5, glen=61.2, tlen=221, kl=0.00155, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.0383, ret=-6.03e-5, glen=61.2, tlen=221, kl=0.00155, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=0.00854, ret=-3.19e-5, glen=60.2, tlen=221, kl=0.00217, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.17it/s, pg=0.00854, ret=-3.19e-5, glen=60.2, tlen=221, kl=0.00217, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.17it/s, pg=-0.0604, ret=7.04e-5, glen=59.3, tlen=219, kl=0.0017, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.15it/s, pg=-0.0604, ret=7.04e-5, glen=59.3, tlen=219, kl=0.0017, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.15it/s, pg=0.0581, ret=-4.65e-5, glen=58.8, tlen=219, kl=0.00161, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.15it/s, pg=0.0581, ret=-4.65e-5, glen=58.8, tlen=219, kl=0.00161, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.15it/s, pg=-0.0461, ret=5.72e-5, glen=59.4, tlen=220, kl=0.00162, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.16it/s, pg=-0.0461, ret=5.72e-5, glen=59.4, tlen=220, kl=0.00162, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.16it/s, pg=-0.0485, ret=6.04e-5, glen=60.8, tlen=221, kl=0.00163, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0485, ret=6.04e-5, glen=60.8, tlen=221, kl=0.00163, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0542, ret=7.05e-5, glen=59.4, tlen=220, kl=0.00164, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0542, ret=7.05e-5, glen=59.4, tlen=220, kl=0.00164, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0747, ret=9.26e-5, glen=60.3, tlen=221, kl=0.0021, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.15it/s, pg=-0.0747, ret=9.26e-5, glen=60.3, tlen=221, kl=0.0021, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.15it/s, pg=-0.0784, ret=0.000109, glen=61.3, tlen=222, kl=0.00155, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.15it/s, pg=-0.0784, ret=0.000109, glen=61.3, tlen=222, kl=0.00155, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.15it/s, pg=0.0496, ret=-3.24e-5, glen=60.4, tlen=221, kl=0.00156, act_lr=1e-6, ent=0.886] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=0.0496, ret=-3.24e-5, glen=60.4, tlen=221, kl=0.00156, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=-0.0497, ret=6.5e-5, glen=60.4, tlen=221, kl=0.00166, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0497, ret=6.5e-5, glen=60.4, tlen=221, kl=0.00166, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=0.12, ret=-0.000176, glen=61.2, tlen=222, kl=0.00164, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=0.12, ret=-0.000176, glen=61.2, tlen=222, kl=0.00164, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0632, ret=7.52e-5, glen=59.4, tlen=220, kl=0.0016, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0632, ret=7.52e-5, glen=59.4, tlen=220, kl=0.0016, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=0.0753, ret=-5.27e-5, glen=60.6, tlen=221, kl=0.00142, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.0753, ret=-5.27e-5, glen=60.6, tlen=221, kl=0.00142, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.224, ret=-0.000303, glen=59.1, tlen=220, kl=0.00153, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=0.224, ret=-0.000303, glen=59.1, tlen=220, kl=0.00153, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.0596, ret=-5.79e-5, glen=59.8, tlen=220, kl=0.00167, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.0596, ret=-5.79e-5, glen=59.8, tlen=220, kl=0.00167, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.044, ret=5.52e-5, glen=62.4, tlen=223, kl=0.00159, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.044, ret=5.52e-5, glen=62.4, tlen=223, kl=0.00159, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=-0.065, ret=7.91e-5, glen=60.2, tlen=221, kl=0.00167, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.065, ret=7.91e-5, glen=60.2, tlen=221, kl=0.00167, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0583, ret=6.84e-5, glen=59.7, tlen=220, kl=0.00181, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0583, ret=6.84e-5, glen=59.7, tlen=220, kl=0.00181, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0557, ret=6.75e-5, glen=60.5, tlen=221, kl=0.00176, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0557, ret=6.75e-5, glen=60.5, tlen=221, kl=0.00176, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=0.0332, ret=-5.4e-5, glen=59.6, tlen=220, kl=0.00191, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0332, ret=-5.4e-5, glen=59.6, tlen=220, kl=0.00191, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0776, ret=-6.11e-5, glen=59.2, tlen=219, kl=0.00159, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.0776, ret=-6.11e-5, glen=59.2, tlen=219, kl=0.00159, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.142, ret=-0.00017, glen=61.3, tlen=222, kl=0.00181, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.142, ret=-0.00017, glen=61.3, tlen=222, kl=0.00181, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0607, ret=7.79e-5, glen=60.3, tlen=221, kl=0.00159, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.14it/s, pg=-0.0607, ret=7.79e-5, glen=60.3, tlen=221, kl=0.00159, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.14it/s, pg=0.0483, ret=-4.82e-5, glen=60.5, tlen=221, kl=0.00148, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.15it/s, pg=0.0483, ret=-4.82e-5, glen=60.5, tlen=221, kl=0.00148, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.15it/s, pg=0.0154, ret=-3.31e-5, glen=58.6, tlen=219, kl=0.0016, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.16it/s, pg=0.0154, ret=-3.31e-5, glen=58.6, tlen=219, kl=0.0016, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.16it/s, pg=-0.0543, ret=6.56e-5, glen=60.4, tlen=220, kl=0.00147, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.16it/s, pg=-0.0543, ret=6.56e-5, glen=60.4, tlen=220, kl=0.00147, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.16it/s, pg=0.0549, ret=-4.54e-5, glen=62.3, tlen=223, kl=0.00163, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.0549, ret=-4.54e-5, glen=62.3, tlen=223, kl=0.00163, act_lr=1e-6, ent=0.892]
2025-07-24 21:20:31.355 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.08s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=-0.0455, ret=6.03e-5, glen=57.8, tlen=218, kl=0.00229, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=-0.0455, ret=6.03e-5, glen=57.8, tlen=218, kl=0.00229, act_lr=1e-6, ent=0.915]
2025-07-24 21:20:32.015 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 21:20:34.156 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.14s
2025-07-24 21:20:34.490 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.33s
2025-07-24 21:20:34.497 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007009091584578804, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.902674665917521, 'kl': 0.0016588128131368885, 'response_length': 60.07088304602581, 'total_length': 220.53486235245416, 'teacher_total_length': 232.58133597995925, 'return': 1.617963077834524e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  15%|‚ñà‚ñå        | 2/13 [03:46<20:39, 112.68s/it]2025-07-24 21:20:34.542 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:21:09.333 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:21:09.510 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:21:09.511 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.97s
2025-07-24 21:21:11.367 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 59.6167,std_num_tokens: 12.2805,avg_correct_num_tokens: 59.6154,std_correct_num_tokens: 12.2790,avg_incorrect_num_tokens: 60.0385,std_incorrect_num_tokens: 12.7384
2025-07-24 21:21:11.801 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.29s
2025-07-24 21:21:14.282 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.48s
2025-07-24 21:21:37.798 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 21:21:37.799 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.52s
2025-07-24 21:21:39.017 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 21:21:39.018 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.71774120714802e-05, avg_kl: 0.0021784017374227335, avg_response_length: 59.653890756460335, avg_orm_score: 0.0, avg_custom_rewards: 1.71774120714802e-05
2025-07-24 21:21:39.044 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter132_replay_buffer.jsonl
2025-07-24 21:21:40.254 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.21s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=-0.063, ret=7.47e-5, glen=59.5, tlen=220, kl=0.00219, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.00it/s, pg=-0.063, ret=7.47e-5, glen=59.5, tlen=220, kl=0.00219, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.00it/s, pg=-0.0666, ret=8.02e-5, glen=60.2, tlen=221, kl=0.00215, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0666, ret=8.02e-5, glen=60.2, tlen=221, kl=0.00215, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0268, ret=3.82e-5, glen=57.8, tlen=219, kl=0.00225, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.10it/s, pg=-0.0268, ret=3.82e-5, glen=57.8, tlen=219, kl=0.00225, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.10it/s, pg=0.0461, ret=-6e-5, glen=60, tlen=220, kl=0.0019, act_lr=1e-6, ent=0.955]      Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:38,  1.10it/s, pg=0.0461, ret=-6e-5, glen=60, tlen=220, kl=0.0019, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:38,  1.10it/s, pg=-0.0712, ret=9.26e-5, glen=60.1, tlen=221, kl=0.00223, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:37,  1.08it/s, pg=-0.0712, ret=9.26e-5, glen=60.1, tlen=221, kl=0.00223, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:37,  1.08it/s, pg=-0.0586, ret=7.33e-5, glen=60.6, tlen=221, kl=0.00228, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:36,  1.11it/s, pg=-0.0586, ret=7.33e-5, glen=60.6, tlen=221, kl=0.00228, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:36,  1.11it/s, pg=0.00116, ret=-2.89e-5, glen=59.9, tlen=220, kl=0.00191, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.13it/s, pg=0.00116, ret=-2.89e-5, glen=59.9, tlen=220, kl=0.00191, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.13it/s, pg=-0.0848, ret=0.000102, glen=59.7, tlen=221, kl=0.00214, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.14it/s, pg=-0.0848, ret=0.000102, glen=59.7, tlen=221, kl=0.00214, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.14it/s, pg=-0.0418, ret=5.48e-5, glen=60.1, tlen=221, kl=0.00218, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=-0.0418, ret=5.48e-5, glen=60.1, tlen=221, kl=0.00218, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=-0.0427, ret=5.31e-5, glen=60.2, tlen=221, kl=0.00208, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.16it/s, pg=-0.0427, ret=5.31e-5, glen=60.2, tlen=221, kl=0.00208, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.16it/s, pg=-0.0418, ret=5.54e-5, glen=59.8, tlen=220, kl=0.00207, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=-0.0418, ret=5.54e-5, glen=59.8, tlen=220, kl=0.00207, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0473, ret=5.59e-5, glen=60.7, tlen=221, kl=0.00212, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=-0.0473, ret=5.59e-5, glen=60.7, tlen=221, kl=0.00212, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=-0.0663, ret=7.61e-5, glen=58.6, tlen=219, kl=0.00243, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0663, ret=7.61e-5, glen=58.6, tlen=219, kl=0.00243, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0538, ret=6.89e-5, glen=59.4, tlen=220, kl=0.00236, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0538, ret=6.89e-5, glen=59.4, tlen=220, kl=0.00236, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.237, ret=-0.00029, glen=57.6, tlen=218, kl=0.00234, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.16it/s, pg=0.237, ret=-0.00029, glen=57.6, tlen=218, kl=0.00234, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.16it/s, pg=0.0506, ret=-6.71e-5, glen=58.4, tlen=219, kl=0.00217, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=0.0506, ret=-6.71e-5, glen=58.4, tlen=219, kl=0.00217, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=0.0726, ret=-5.79e-5, glen=59.8, tlen=221, kl=0.00196, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=0.0726, ret=-5.79e-5, glen=59.8, tlen=221, kl=0.00196, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.047, ret=5.16e-5, glen=57.7, tlen=218, kl=0.00218, act_lr=1e-6, ent=0.932] 
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.047, ret=5.16e-5, glen=57.7, tlen=218, kl=0.00218, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0547, ret=9.84e-5, glen=59.7, tlen=221, kl=0.00224, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0547, ret=9.84e-5, glen=59.7, tlen=221, kl=0.00224, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0519, ret=6.36e-5, glen=61.5, tlen=222, kl=0.00206, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0519, ret=6.36e-5, glen=61.5, tlen=222, kl=0.00206, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.062, ret=7.43e-5, glen=58.9, tlen=220, kl=0.00212, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.15it/s, pg=-0.062, ret=7.43e-5, glen=58.9, tlen=220, kl=0.00212, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.15it/s, pg=-0.0435, ret=5.4e-5, glen=60.3, tlen=221, kl=0.00225, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.16it/s, pg=-0.0435, ret=5.4e-5, glen=60.3, tlen=221, kl=0.00225, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.16it/s, pg=-0.0844, ret=9.84e-5, glen=60.1, tlen=221, kl=0.00242, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=-0.0844, ret=9.84e-5, glen=60.1, tlen=221, kl=0.00242, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=0.123, ret=-0.00015, glen=60.8, tlen=222, kl=0.00224, act_lr=1e-6, ent=0.937] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=0.123, ret=-0.00015, glen=60.8, tlen=222, kl=0.00224, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=0.106, ret=-0.000168, glen=60.7, tlen=221, kl=0.00213, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.17it/s, pg=0.106, ret=-0.000168, glen=60.7, tlen=221, kl=0.00213, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.17it/s, pg=-0.0563, ret=6.29e-5, glen=57.3, tlen=218, kl=0.00195, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.0563, ret=6.29e-5, glen=57.3, tlen=218, kl=0.00195, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0615, ret=7.58e-5, glen=60.4, tlen=220, kl=0.00235, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0615, ret=7.58e-5, glen=60.4, tlen=220, kl=0.00235, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=0.133, ret=-0.000194, glen=59.2, tlen=220, kl=0.00199, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=0.133, ret=-0.000194, glen=59.2, tlen=220, kl=0.00199, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0584, ret=7.22e-5, glen=59, tlen=220, kl=0.00221, act_lr=1e-6, ent=0.944]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0584, ret=7.22e-5, glen=59, tlen=220, kl=0.00221, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.0155, ret=-5.25e-5, glen=59.3, tlen=219, kl=0.00225, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.0155, ret=-5.25e-5, glen=59.3, tlen=219, kl=0.00225, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0439, ret=6.19e-5, glen=61, tlen=222, kl=0.0022, act_lr=1e-6, ent=0.986]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0439, ret=6.19e-5, glen=61, tlen=222, kl=0.0022, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=0.0531, ret=-6.21e-5, glen=59.4, tlen=220, kl=0.002, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=0.0531, ret=-6.21e-5, glen=59.4, tlen=220, kl=0.002, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=-0.0279, ret=3.95e-5, glen=58.1, tlen=218, kl=0.00241, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.14it/s, pg=-0.0279, ret=3.95e-5, glen=58.1, tlen=218, kl=0.00241, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.14it/s, pg=0.0708, ret=-5.93e-5, glen=58.8, tlen=220, kl=0.00208, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=0.0708, ret=-5.93e-5, glen=58.8, tlen=220, kl=0.00208, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=0.23, ret=-0.00028, glen=60.6, tlen=221, kl=0.00249, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.23, ret=-0.00028, glen=60.6, tlen=221, kl=0.00249, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=0.0289, ret=-4.82e-5, glen=60.3, tlen=221, kl=0.00223, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=0.0289, ret=-4.82e-5, glen=60.3, tlen=221, kl=0.00223, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0592, ret=7.14e-5, glen=59.8, tlen=221, kl=0.00214, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0592, ret=7.14e-5, glen=59.8, tlen=221, kl=0.00214, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.0398, ret=5.16e-5, glen=59.2, tlen=220, kl=0.00228, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0398, ret=5.16e-5, glen=59.2, tlen=220, kl=0.00228, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0752, ret=8.42e-5, glen=59.3, tlen=220, kl=0.00235, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0752, ret=8.42e-5, glen=59.3, tlen=220, kl=0.00235, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.0901, ret=-5.99e-5, glen=61, tlen=222, kl=0.00211, act_lr=1e-6, ent=0.934]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.0901, ret=-5.99e-5, glen=61, tlen=222, kl=0.00211, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0524, ret=-3.09e-5, glen=60.2, tlen=221, kl=0.00199, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0524, ret=-3.09e-5, glen=60.2, tlen=221, kl=0.00199, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0813, ret=-8.27e-5, glen=59.9, tlen=221, kl=0.00201, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0813, ret=-8.27e-5, glen=59.9, tlen=221, kl=0.00201, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0228, ret=-1.03e-5, glen=59.3, tlen=220, kl=0.0023, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0228, ret=-1.03e-5, glen=59.3, tlen=220, kl=0.0023, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0896, ret=9.75e-5, glen=59.6, tlen=220, kl=0.00238, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0896, ret=9.75e-5, glen=59.6, tlen=220, kl=0.00238, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=0.0516, ret=-6.04e-5, glen=59.4, tlen=220, kl=0.00206, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=0.0516, ret=-6.04e-5, glen=59.4, tlen=220, kl=0.00206, act_lr=1e-6, ent=0.943]
2025-07-24 21:22:20.502 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=0.0639, ret=-2.87e-5, glen=61, tlen=222, kl=0.00202, act_lr=1e-6, ent=0.933]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=0.0639, ret=-2.87e-5, glen=61, tlen=222, kl=0.00202, act_lr=1e-6, ent=0.933]
2025-07-24 21:22:21.350 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 21:22:23.931 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 21:22:24.277 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.96s
2025-07-24 21:22:24.283 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0007529880689538044, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9428507219190183, 'kl': 0.0021781092104704485, 'response_length': 59.652580178302266, 'total_length': 220.3270907194718, 'teacher_total_length': 232.28310162088147, 'return': 2.008585199845307e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:36<18:33, 111.36s/it]2025-07-24 21:22:24.332 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:22:59.287 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:22:59.467 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:22:59.467 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.14s
2025-07-24 21:23:01.178 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 59.2821,std_num_tokens: 12.7239,avg_correct_num_tokens: 59.2758,std_correct_num_tokens: 12.7205,avg_incorrect_num_tokens: 62.2941,std_incorrect_num_tokens: 13.9443
2025-07-24 21:23:01.595 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.13s
2025-07-24 21:23:04.336 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.74s
2025-07-24 21:23:27.858 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 181
2025-07-24 21:23:27.858 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.52s
2025-07-24 21:23:29.101 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:23:29.101 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.2624776955611633e-05, avg_kl: 0.003877587081319061, avg_response_length: 59.312180408456705, avg_orm_score: 0.0, avg_custom_rewards: 1.2624776955611633e-05
2025-07-24 21:23:29.128 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter133_replay_buffer.jsonl
2025-07-24 21:23:30.404 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.28s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0365, ret=3.43e-5, glen=59.2, tlen=219, kl=0.00386, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.0365, ret=3.43e-5, glen=59.2, tlen=219, kl=0.00386, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.034, ret=3.36e-5, glen=58.1, tlen=218, kl=0.00372, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.034, ret=3.36e-5, glen=58.1, tlen=218, kl=0.00372, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=0.083, ret=-7.07e-5, glen=58, tlen=218, kl=0.00385, act_lr=1e-6, ent=0.951]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=0.083, ret=-7.07e-5, glen=58, tlen=218, kl=0.00385, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0426, ret=4.15e-5, glen=60.3, tlen=221, kl=0.00399, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.11it/s, pg=-0.0426, ret=4.15e-5, glen=60.3, tlen=221, kl=0.00399, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.11it/s, pg=-0.0577, ret=5.57e-5, glen=59.6, tlen=220, kl=0.00445, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=-0.0577, ret=5.57e-5, glen=59.6, tlen=220, kl=0.00445, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=-0.0435, ret=3.88e-5, glen=56.9, tlen=216, kl=0.00411, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=-0.0435, ret=3.88e-5, glen=56.9, tlen=216, kl=0.00411, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=-0.0628, ret=5.79e-5, glen=60.3, tlen=221, kl=0.00392, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=-0.0628, ret=5.79e-5, glen=60.3, tlen=221, kl=0.00392, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=-0.0342, ret=3.03e-5, glen=58.6, tlen=218, kl=0.00359, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.15it/s, pg=-0.0342, ret=3.03e-5, glen=58.6, tlen=218, kl=0.00359, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.15it/s, pg=-0.0469, ret=4.37e-5, glen=57.5, tlen=217, kl=0.00415, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.16it/s, pg=-0.0469, ret=4.37e-5, glen=57.5, tlen=217, kl=0.00415, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=0.0309, ret=-6.36e-5, glen=60.3, tlen=220, kl=0.00362, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.16it/s, pg=0.0309, ret=-6.36e-5, glen=60.3, tlen=220, kl=0.00362, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.16it/s, pg=-0.0429, ret=4.56e-5, glen=59.6, tlen=220, kl=0.00397, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.14it/s, pg=-0.0429, ret=4.56e-5, glen=59.6, tlen=220, kl=0.00397, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.14it/s, pg=-0.0404, ret=4.35e-5, glen=62.2, tlen=222, kl=0.00406, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=-0.0404, ret=4.35e-5, glen=62.2, tlen=222, kl=0.00406, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=-0.0409, ret=4.44e-5, glen=61.2, tlen=222, kl=0.00352, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0409, ret=4.44e-5, glen=61.2, tlen=222, kl=0.00352, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0438, ret=4.09e-5, glen=60, tlen=220, kl=0.00376, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0438, ret=4.09e-5, glen=60, tlen=220, kl=0.00376, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.0255, ret=-5.11e-5, glen=57.8, tlen=218, kl=0.0041, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.0255, ret=-5.11e-5, glen=57.8, tlen=218, kl=0.0041, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.17it/s, pg=-0.0341, ret=3.4e-5, glen=58.6, tlen=219, kl=0.00408, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.14it/s, pg=-0.0341, ret=3.4e-5, glen=58.6, tlen=219, kl=0.00408, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.14it/s, pg=-0.0312, ret=2.68e-5, glen=60.3, tlen=220, kl=0.00354, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:25,  1.15it/s, pg=-0.0312, ret=2.68e-5, glen=60.3, tlen=220, kl=0.00354, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.15it/s, pg=-0.047, ret=4.81e-5, glen=60.4, tlen=221, kl=0.00358, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=-0.047, ret=4.81e-5, glen=60.4, tlen=221, kl=0.00358, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=0.191, ret=-0.000197, glen=60.4, tlen=220, kl=0.00349, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.16it/s, pg=0.191, ret=-0.000197, glen=60.4, tlen=220, kl=0.00349, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.16it/s, pg=0.0628, ret=-7.06e-5, glen=58.6, tlen=219, kl=0.00375, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.16it/s, pg=0.0628, ret=-7.06e-5, glen=58.6, tlen=219, kl=0.00375, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.16it/s, pg=0.0895, ret=-5.98e-5, glen=58.5, tlen=218, kl=0.00408, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.16it/s, pg=0.0895, ret=-5.98e-5, glen=58.5, tlen=218, kl=0.00408, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.16it/s, pg=0.115, ret=-5.74e-5, glen=59.4, tlen=219, kl=0.00388, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.115, ret=-5.74e-5, glen=59.4, tlen=219, kl=0.00388, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.17it/s, pg=0.0737, ret=-7.5e-5, glen=58.7, tlen=219, kl=0.0042, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=0.0737, ret=-7.5e-5, glen=58.7, tlen=219, kl=0.0042, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=-0.0587, ret=6.37e-5, glen=60.1, tlen=221, kl=0.00357, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0587, ret=6.37e-5, glen=60.1, tlen=221, kl=0.00357, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0582, ret=5.46e-5, glen=58.1, tlen=218, kl=0.00385, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=-0.0582, ret=5.46e-5, glen=58.1, tlen=218, kl=0.00385, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=-0.0526, ret=5.02e-5, glen=60.2, tlen=220, kl=0.00423, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.0526, ret=5.02e-5, glen=60.2, tlen=220, kl=0.00423, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0421, ret=4.02e-5, glen=60.2, tlen=220, kl=0.00433, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0421, ret=4.02e-5, glen=60.2, tlen=220, kl=0.00433, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0417, ret=3.75e-5, glen=58.1, tlen=218, kl=0.00368, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0417, ret=3.75e-5, glen=58.1, tlen=218, kl=0.00368, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0227, ret=2.27e-5, glen=58.8, tlen=219, kl=0.00391, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0227, ret=2.27e-5, glen=58.8, tlen=219, kl=0.00391, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0302, ret=2.51e-5, glen=59, tlen=219, kl=0.0039, act_lr=1e-6, ent=0.921]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0302, ret=2.51e-5, glen=59, tlen=219, kl=0.0039, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.028, ret=2.68e-5, glen=59.9, tlen=220, kl=0.00356, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.028, ret=2.68e-5, glen=59.9, tlen=220, kl=0.00356, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0641, ret=6.22e-5, glen=58.6, tlen=218, kl=0.00409, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.13it/s, pg=-0.0641, ret=6.22e-5, glen=58.6, tlen=218, kl=0.00409, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=-0.0359, ret=3.61e-5, glen=59.3, tlen=219, kl=0.00388, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0359, ret=3.61e-5, glen=59.3, tlen=219, kl=0.00388, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=0.163, ret=-0.000173, glen=60.3, tlen=221, kl=0.00364, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=0.163, ret=-0.000173, glen=60.3, tlen=221, kl=0.00364, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=0.0342, ret=-6.65e-5, glen=59.9, tlen=220, kl=0.00368, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.0342, ret=-6.65e-5, glen=59.9, tlen=220, kl=0.00368, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0347, ret=3.28e-5, glen=60.8, tlen=221, kl=0.00378, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0347, ret=3.28e-5, glen=60.8, tlen=221, kl=0.00378, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0367, ret=3.68e-5, glen=60.9, tlen=221, kl=0.00348, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0367, ret=3.68e-5, glen=60.9, tlen=221, kl=0.00348, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=0.0954, ret=-8.14e-5, glen=58.8, tlen=219, kl=0.00389, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0954, ret=-8.14e-5, glen=58.8, tlen=219, kl=0.00389, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.118, ret=-7.31e-5, glen=58.1, tlen=218, kl=0.00397, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.118, ret=-7.31e-5, glen=58.1, tlen=218, kl=0.00397, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0403, ret=3.98e-5, glen=58.4, tlen=218, kl=0.00414, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0403, ret=3.98e-5, glen=58.4, tlen=218, kl=0.00414, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0886, ret=-6.54e-5, glen=58.7, tlen=218, kl=0.00377, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0886, ret=-6.54e-5, glen=58.7, tlen=218, kl=0.00377, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.216, ret=-0.00019, glen=58, tlen=218, kl=0.0042, act_lr=1e-6, ent=0.913]    Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.216, ret=-0.00019, glen=58, tlen=218, kl=0.0042, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0745, ret=7.3e-5, glen=56.9, tlen=217, kl=0.00395, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0745, ret=7.3e-5, glen=56.9, tlen=217, kl=0.00395, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0345, ret=3.09e-5, glen=60.5, tlen=220, kl=0.00444, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0345, ret=3.09e-5, glen=60.5, tlen=220, kl=0.00444, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.17it/s, pg=-0.0489, ret=4.55e-5, glen=59.3, tlen=219, kl=0.00353, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0489, ret=4.55e-5, glen=59.3, tlen=219, kl=0.00353, act_lr=1e-6, ent=0.939]
2025-07-24 21:24:10.663 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.05s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0486, ret=5.35e-5, glen=60.9, tlen=221, kl=0.00348, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0486, ret=5.35e-5, glen=60.9, tlen=221, kl=0.00348, act_lr=1e-6, ent=0.959]
2025-07-24 21:24:11.342 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 21:24:13.630 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-24 21:24:13.966 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.48s
2025-07-24 21:24:13.972 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -9.789674178413723e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.935903030893077, 'kl': 0.0038735762886379075, 'response_length': 59.308159455009125, 'total_length': 219.33850860595703, 'teacher_total_length': 231.30621337890625, 'return': 1.2059002113781627e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [07:25<16:36, 110.70s/it]2025-07-24 21:24:14.015 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:24:54.396 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:24:54.581 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:24:54.582 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 40.57s
2025-07-24 21:24:56.547 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 59.8992,std_num_tokens: 15.9574,avg_correct_num_tokens: 59.9000,std_correct_num_tokens: 15.9617,avg_incorrect_num_tokens: 59.4286,std_incorrect_num_tokens: 13.1894
2025-07-24 21:24:56.984 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.40s
2025-07-24 21:24:59.473 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.49s
2025-07-24 21:25:23.002 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 21:25:23.002 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.53s
2025-07-24 21:25:24.254 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 21:25:24.254 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -3.1399791663676335e-06, avg_kl: 0.005796327695741758, avg_response_length: 59.923188010414876, avg_orm_score: 0.0, avg_custom_rewards: -3.1399791663676335e-06
2025-07-24 21:25:24.282 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter134_replay_buffer.jsonl
2025-07-24 21:25:25.523 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.24s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0395, ret=3.47e-5, glen=60, tlen=221, kl=0.00576, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0395, ret=3.47e-5, glen=60, tlen=221, kl=0.00576, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0386, ret=3.25e-5, glen=59.6, tlen=220, kl=0.00516, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.0386, ret=3.25e-5, glen=59.6, tlen=220, kl=0.00516, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0403, ret=3.88e-5, glen=59.9, tlen=220, kl=0.00896, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.0403, ret=3.88e-5, glen=59.9, tlen=220, kl=0.00896, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0345, ret=2.7e-5, glen=59.2, tlen=220, kl=0.00665, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:36,  1.14it/s, pg=-0.0345, ret=2.7e-5, glen=59.2, tlen=220, kl=0.00665, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:36,  1.14it/s, pg=-0.0269, ret=2.46e-5, glen=59.1, tlen=219, kl=0.00629, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:35,  1.15it/s, pg=-0.0269, ret=2.46e-5, glen=59.1, tlen=219, kl=0.00629, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:35,  1.15it/s, pg=-0.0413, ret=3.47e-5, glen=60.6, tlen=221, kl=0.0056, act_lr=1e-6, ent=0.952] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.14it/s, pg=-0.0413, ret=3.47e-5, glen=60.6, tlen=221, kl=0.0056, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.14it/s, pg=-0.0207, ret=2.3e-5, glen=59.6, tlen=220, kl=0.00525, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.15it/s, pg=-0.0207, ret=2.3e-5, glen=59.6, tlen=220, kl=0.00525, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.15it/s, pg=-0.0263, ret=2.51e-5, glen=60.8, tlen=221, kl=0.0051, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0263, ret=2.51e-5, glen=60.8, tlen=221, kl=0.0051, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0648, ret=4.95e-5, glen=59.2, tlen=220, kl=0.00531, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.17it/s, pg=-0.0648, ret=4.95e-5, glen=59.2, tlen=220, kl=0.00531, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.17it/s, pg=-0.0425, ret=3.85e-5, glen=59.2, tlen=220, kl=0.00531, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=-0.0425, ret=3.85e-5, glen=59.2, tlen=220, kl=0.00531, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=-0.0182, ret=1.9e-5, glen=59.9, tlen=220, kl=0.006, act_lr=1e-6, ent=0.967]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=-0.0182, ret=1.9e-5, glen=59.9, tlen=220, kl=0.006, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=0.0797, ret=-8.18e-5, glen=59.3, tlen=220, kl=0.00539, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=0.0797, ret=-8.18e-5, glen=59.3, tlen=220, kl=0.00539, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=0.222, ret=-0.000212, glen=60.1, tlen=221, kl=0.00603, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=0.222, ret=-0.000212, glen=60.1, tlen=221, kl=0.00603, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0332, ret=2.88e-5, glen=59, tlen=219, kl=0.00524, act_lr=1e-6, ent=0.962]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=-0.0332, ret=2.88e-5, glen=59, tlen=219, kl=0.00524, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.0595, ret=5.73e-5, glen=61.1, tlen=222, kl=0.00581, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0595, ret=5.73e-5, glen=61.1, tlen=222, kl=0.00581, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.034, ret=3.3e-5, glen=59.9, tlen=221, kl=0.0058, act_lr=1e-6, ent=0.936]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.034, ret=3.3e-5, glen=59.9, tlen=221, kl=0.0058, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0591, ret=4.46e-5, glen=60, tlen=221, kl=0.00524, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0591, ret=4.46e-5, glen=60, tlen=221, kl=0.00524, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.13, ret=-9.08e-5, glen=58.9, tlen=219, kl=0.00642, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.13, ret=-9.08e-5, glen=58.9, tlen=219, kl=0.00642, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0379, ret=3.86e-5, glen=59.5, tlen=220, kl=0.00525, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.17it/s, pg=-0.0379, ret=3.86e-5, glen=59.5, tlen=220, kl=0.00525, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.17it/s, pg=0.116, ret=-7.91e-5, glen=59.8, tlen=220, kl=0.00571, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=0.116, ret=-7.91e-5, glen=59.8, tlen=220, kl=0.00571, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0629, ret=5.61e-5, glen=61.4, tlen=223, kl=0.00564, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=-0.0629, ret=5.61e-5, glen=61.4, tlen=223, kl=0.00564, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=-0.0253, ret=2.53e-5, glen=60.9, tlen=221, kl=0.00616, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0253, ret=2.53e-5, glen=60.9, tlen=221, kl=0.00616, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.0454, ret=-7.63e-5, glen=60.7, tlen=222, kl=0.00719, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.18it/s, pg=0.0454, ret=-7.63e-5, glen=60.7, tlen=222, kl=0.00719, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=-0.0462, ret=4.52e-5, glen=60.6, tlen=221, kl=0.00591, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0462, ret=4.52e-5, glen=60.6, tlen=221, kl=0.00591, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0335, ret=2.89e-5, glen=59.9, tlen=221, kl=0.00537, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.15it/s, pg=-0.0335, ret=2.89e-5, glen=59.9, tlen=221, kl=0.00537, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.15it/s, pg=-0.0344, ret=3.17e-5, glen=57.8, tlen=218, kl=0.00541, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.16it/s, pg=-0.0344, ret=3.17e-5, glen=57.8, tlen=218, kl=0.00541, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.16it/s, pg=-0.0301, ret=2.7e-5, glen=60.8, tlen=221, kl=0.00481, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=-0.0301, ret=2.7e-5, glen=60.8, tlen=221, kl=0.00481, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=-0.0565, ret=4.24e-5, glen=59.7, tlen=221, kl=0.00523, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0565, ret=4.24e-5, glen=59.7, tlen=221, kl=0.00523, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.054, ret=4.8e-5, glen=59.4, tlen=220, kl=0.00625, act_lr=1e-6, ent=0.968]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.054, ret=4.8e-5, glen=59.4, tlen=220, kl=0.00625, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.08, ret=7.09e-5, glen=59.6, tlen=220, kl=0.0052, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.08, ret=7.09e-5, glen=59.6, tlen=220, kl=0.0052, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=0.146, ret=-9.07e-5, glen=60.7, tlen=221, kl=0.00609, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.146, ret=-9.07e-5, glen=60.7, tlen=221, kl=0.00609, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0401, ret=3.5e-5, glen=60.4, tlen=221, kl=0.00608, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=-0.0401, ret=3.5e-5, glen=60.4, tlen=221, kl=0.00608, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.217, ret=-0.000206, glen=60.6, tlen=221, kl=0.00569, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.217, ret=-0.000206, glen=60.6, tlen=221, kl=0.00569, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0124, ret=1.36e-5, glen=60.3, tlen=221, kl=0.00523, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0124, ret=1.36e-5, glen=60.3, tlen=221, kl=0.00523, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=-0.0746, ret=6.31e-5, glen=58, tlen=218, kl=0.00642, act_lr=1e-6, ent=0.946]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0746, ret=6.31e-5, glen=58, tlen=218, kl=0.00642, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0281, ret=2.7e-5, glen=60.4, tlen=221, kl=0.0059, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0281, ret=2.7e-5, glen=60.4, tlen=221, kl=0.0059, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=0.0716, ret=-8.68e-5, glen=59.8, tlen=220, kl=0.00556, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=0.0716, ret=-8.68e-5, glen=59.8, tlen=220, kl=0.00556, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0231, ret=2.49e-5, glen=59.4, tlen=221, kl=0.00602, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:32<00:06,  1.17it/s, pg=-0.0231, ret=2.49e-5, glen=59.4, tlen=221, kl=0.00602, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0285, ret=2.7e-5, glen=59.5, tlen=220, kl=0.00604, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0285, ret=2.7e-5, glen=59.5, tlen=220, kl=0.00604, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0315, ret=3.09e-5, glen=60, tlen=220, kl=0.00523, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0315, ret=3.09e-5, glen=60, tlen=220, kl=0.00523, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.055, ret=4.93e-5, glen=59.6, tlen=220, kl=0.00584, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.055, ret=4.93e-5, glen=59.6, tlen=220, kl=0.00584, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.162, ret=-0.000199, glen=59.4, tlen=220, kl=0.00562, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.162, ret=-0.000199, glen=59.4, tlen=220, kl=0.00562, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0938, ret=-7.82e-5, glen=58.5, tlen=219, kl=0.00619, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0938, ret=-7.82e-5, glen=58.5, tlen=219, kl=0.00619, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.0875, ret=-5.97e-5, glen=60.4, tlen=221, kl=0.00625, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.0875, ret=-5.97e-5, glen=60.4, tlen=221, kl=0.00625, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0221, ret=2.74e-5, glen=65.6, tlen=226, kl=0.00526, act_lr=1e-6, ent=1.1]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.17it/s, pg=-0.0221, ret=2.74e-5, glen=65.6, tlen=226, kl=0.00526, act_lr=1e-6, ent=1.1]
2025-07-24 21:26:05.572 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.87s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.041, ret=3.88e-5, glen=57.4, tlen=217, kl=0.00587, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.041, ret=3.88e-5, glen=57.4, tlen=217, kl=0.00587, act_lr=1e-6, ent=0.908]
2025-07-24 21:26:06.438 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 21:26:09.053 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.61s
2025-07-24 21:26:09.397 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.82s
2025-07-24 21:26:09.403 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005455846371858016, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.951106598843699, 'kl': 0.00579833984375, 'response_length': 59.8980304883874, 'total_length': 220.45063483196756, 'teacher_total_length': 232.43938711415166, 'return': 2.7257022378287966e-08, 'policy_update_steps': 1.0}
Episode [11/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [09:21<14:59, 112.41s/it]2025-07-24 21:26:09.447 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:26:44.980 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:26:45.159 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:26:45.160 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.71s
2025-07-24 21:26:46.826 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 60.7590,std_num_tokens: 12.2912,avg_correct_num_tokens: 60.7504,std_correct_num_tokens: 12.2927,avg_incorrect_num_tokens: 65.1875,std_incorrect_num_tokens: 10.6197
2025-07-24 21:26:47.246 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.09s
2025-07-24 21:26:49.941 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.69s
2025-07-24 21:27:13.508 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:27:13.508 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.57s
2025-07-24 21:27:14.759 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:27:14.759 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -7.615777596454722e-06, avg_kl: 0.008542253671448088, avg_response_length: 60.78168731439309, avg_orm_score: 0.0, avg_custom_rewards: -7.615777596454722e-06
2025-07-24 21:27:14.788 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter135_replay_buffer.jsonl
2025-07-24 21:27:16.079 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.29s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0392, ret=3.47e-5, glen=60.5, tlen=221, kl=0.00777, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0392, ret=3.47e-5, glen=60.5, tlen=221, kl=0.00777, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0397, ret=4e-5, glen=59.8, tlen=220, kl=0.0105, act_lr=1e-6, ent=0.963]    Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0397, ret=4e-5, glen=59.8, tlen=220, kl=0.0105, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=0.0992, ret=-9.02e-5, glen=61.3, tlen=222, kl=0.00848, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=0.0992, ret=-9.02e-5, glen=61.3, tlen=222, kl=0.00848, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0324, ret=3.09e-5, glen=61.5, tlen=222, kl=0.00829, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.11it/s, pg=-0.0324, ret=3.09e-5, glen=61.5, tlen=222, kl=0.00829, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.11it/s, pg=-0.0497, ret=4.66e-5, glen=60.5, tlen=221, kl=0.00735, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=-0.0497, ret=4.66e-5, glen=60.5, tlen=221, kl=0.00735, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=-0.0528, ret=5.02e-5, glen=60.8, tlen=221, kl=0.00928, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.12it/s, pg=-0.0528, ret=5.02e-5, glen=60.8, tlen=221, kl=0.00928, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.12it/s, pg=-0.0505, ret=4.9e-5, glen=58.8, tlen=219, kl=0.00736, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=-0.0505, ret=4.9e-5, glen=58.8, tlen=219, kl=0.00736, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=-0.0576, ret=5.59e-5, glen=60.7, tlen=221, kl=0.00826, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=-0.0576, ret=5.59e-5, glen=60.7, tlen=221, kl=0.00826, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=-0.0414, ret=3.85e-5, glen=61.6, tlen=222, kl=0.00755, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.16it/s, pg=-0.0414, ret=3.85e-5, glen=61.6, tlen=222, kl=0.00755, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=0.0752, ret=-8.14e-5, glen=59.9, tlen=220, kl=0.00716, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.16it/s, pg=0.0752, ret=-8.14e-5, glen=59.9, tlen=220, kl=0.00716, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.16it/s, pg=-0.0466, ret=4.44e-5, glen=59.7, tlen=220, kl=0.00695, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:29,  1.17it/s, pg=-0.0466, ret=4.44e-5, glen=59.7, tlen=220, kl=0.00695, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:29,  1.17it/s, pg=0.114, ret=-7.87e-5, glen=61.8, tlen=222, kl=0.00875, act_lr=1e-6, ent=0.953] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.17it/s, pg=0.114, ret=-7.87e-5, glen=61.8, tlen=222, kl=0.00875, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.17it/s, pg=-0.0272, ret=2.7e-5, glen=59.9, tlen=220, kl=0.0101, act_lr=1e-6, ent=0.952] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=-0.0272, ret=2.7e-5, glen=59.9, tlen=220, kl=0.0101, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=-0.0499, ret=4.62e-5, glen=61.1, tlen=221, kl=0.00909, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.18it/s, pg=-0.0499, ret=4.62e-5, glen=61.1, tlen=221, kl=0.00909, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.18it/s, pg=-0.0544, ret=5.21e-5, glen=60.9, tlen=221, kl=0.0075, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.18it/s, pg=-0.0544, ret=5.21e-5, glen=60.9, tlen=221, kl=0.0075, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.18it/s, pg=-0.0349, ret=3.91e-5, glen=61.7, tlen=223, kl=0.00712, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.18it/s, pg=-0.0349, ret=3.91e-5, glen=61.7, tlen=223, kl=0.00712, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.18it/s, pg=-0.0516, ret=5.08e-5, glen=60.7, tlen=221, kl=0.0104, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.18it/s, pg=-0.0516, ret=5.08e-5, glen=60.7, tlen=221, kl=0.0104, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.18it/s, pg=-0.0385, ret=3.34e-5, glen=60.5, tlen=221, kl=0.00899, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.18it/s, pg=-0.0385, ret=3.34e-5, glen=60.5, tlen=221, kl=0.00899, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.18it/s, pg=0.21, ret=-0.000187, glen=61.2, tlen=222, kl=0.0078, act_lr=1e-6, ent=0.927]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.18it/s, pg=0.21, ret=-0.000187, glen=61.2, tlen=222, kl=0.0078, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.18it/s, pg=-0.047, ret=4.23e-5, glen=60.1, tlen=221, kl=0.0091, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.18it/s, pg=-0.047, ret=4.23e-5, glen=60.1, tlen=221, kl=0.0091, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.18it/s, pg=-0.0503, ret=4.82e-5, glen=60.3, tlen=220, kl=0.00777, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=-0.0503, ret=4.82e-5, glen=60.3, tlen=220, kl=0.00777, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=-0.0488, ret=4.68e-5, glen=60.1, tlen=221, kl=0.00715, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:18<00:20,  1.18it/s, pg=-0.0488, ret=4.68e-5, glen=60.1, tlen=221, kl=0.00715, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=-0.0607, ret=5.65e-5, glen=60.7, tlen=221, kl=0.00745, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.18it/s, pg=-0.0607, ret=5.65e-5, glen=60.7, tlen=221, kl=0.00745, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=0.207, ret=-0.000194, glen=58.8, tlen=219, kl=0.00964, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=0.207, ret=-0.000194, glen=58.8, tlen=219, kl=0.00964, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=-0.0533, ret=4.73e-5, glen=61, tlen=222, kl=0.0101, act_lr=1e-6, ent=0.956]   Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=-0.0533, ret=4.73e-5, glen=61, tlen=222, kl=0.0101, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=-0.0342, ret=3.58e-5, glen=61.6, tlen=222, kl=0.00861, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:16,  1.18it/s, pg=-0.0342, ret=3.58e-5, glen=61.6, tlen=222, kl=0.00861, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:16,  1.18it/s, pg=-0.0312, ret=2.92e-5, glen=61.6, tlen=222, kl=0.00774, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.18it/s, pg=-0.0312, ret=2.92e-5, glen=61.6, tlen=222, kl=0.00774, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.18it/s, pg=0.189, ret=-0.000189, glen=62.4, tlen=223, kl=0.00689, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.18it/s, pg=0.189, ret=-0.000189, glen=62.4, tlen=223, kl=0.00689, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.18it/s, pg=0.0691, ret=-6.53e-5, glen=62.6, tlen=224, kl=0.00883, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=0.0691, ret=-6.53e-5, glen=62.6, tlen=224, kl=0.00883, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0444, ret=4.24e-5, glen=60.4, tlen=221, kl=0.0117, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0444, ret=4.24e-5, glen=60.4, tlen=221, kl=0.0117, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.0417, ret=-8.17e-5, glen=62.1, tlen=223, kl=0.00999, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:26<00:13,  1.12it/s, pg=0.0417, ret=-8.17e-5, glen=62.1, tlen=223, kl=0.00999, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.251, ret=-0.000222, glen=60.6, tlen=221, kl=0.00721, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=0.251, ret=-0.000222, glen=60.6, tlen=221, kl=0.00721, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0446, ret=4.12e-5, glen=61.8, tlen=222, kl=0.00696, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0446, ret=4.12e-5, glen=61.8, tlen=222, kl=0.00696, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.03, ret=2.7e-5, glen=60.3, tlen=221, kl=0.00763, act_lr=1e-6, ent=0.956]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.03, ret=2.7e-5, glen=60.3, tlen=221, kl=0.00763, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=-0.0549, ret=5.14e-5, glen=60.2, tlen=220, kl=0.0115, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0549, ret=5.14e-5, glen=60.2, tlen=220, kl=0.0115, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0434, ret=4.01e-5, glen=58.5, tlen=219, kl=0.00908, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0434, ret=4.01e-5, glen=58.5, tlen=219, kl=0.00908, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0515, ret=4.85e-5, glen=60.4, tlen=221, kl=0.00847, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0515, ret=4.85e-5, glen=60.4, tlen=221, kl=0.00847, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=0.106, ret=-8.49e-5, glen=59.4, tlen=220, kl=0.00768, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:32<00:06,  1.17it/s, pg=0.106, ret=-8.49e-5, glen=59.4, tlen=220, kl=0.00768, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0457, ret=4.25e-5, glen=61, tlen=222, kl=0.00858, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0457, ret=4.25e-5, glen=61, tlen=222, kl=0.00858, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0342, ret=2.94e-5, glen=61.5, tlen=222, kl=0.00758, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0342, ret=2.94e-5, glen=61.5, tlen=222, kl=0.00758, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.035, ret=3.41e-5, glen=59.7, tlen=220, kl=0.00861, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.035, ret=3.41e-5, glen=59.7, tlen=220, kl=0.00861, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.0615, ret=-7.47e-5, glen=61.1, tlen=222, kl=0.0102, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=0.0615, ret=-7.47e-5, glen=61.1, tlen=222, kl=0.0102, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.068, ret=-7.66e-5, glen=61.8, tlen=222, kl=0.0109, act_lr=1e-6, ent=0.964] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=0.068, ret=-7.66e-5, glen=61.8, tlen=222, kl=0.0109, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=-0.0406, ret=3.93e-5, glen=61.7, tlen=223, kl=0.00793, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:37<00:01,  1.18it/s, pg=-0.0406, ret=3.93e-5, glen=61.7, tlen=223, kl=0.00793, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0548, ret=5.04e-5, glen=60.8, tlen=221, kl=0.00832, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.18it/s, pg=-0.0548, ret=5.04e-5, glen=60.8, tlen=221, kl=0.00832, act_lr=1e-6, ent=0.932]
2025-07-24 21:27:56.007 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.75s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0411, ret=3.75e-5, glen=62.7, tlen=223, kl=0.00816, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0411, ret=3.75e-5, glen=62.7, tlen=223, kl=0.00816, act_lr=1e-6, ent=0.948]
2025-07-24 21:27:56.687 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 21:27:59.063 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.37s
2025-07-24 21:27:59.397 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.26s
2025-07-24 21:27:59.403 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00046394182288128394, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9437062649623208, 'kl': 0.008533477783203125, 'response_length': 60.78504802869714, 'total_length': 221.31237859311312, 'teacher_total_length': 233.20397219450578, 'return': 6.165369679523713e-08, 'policy_update_steps': 1.0}
Episode [11/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [11:11<13:01, 111.59s/it]2025-07-24 21:27:59.449 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:28:43.443 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:28:43.624 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:28:43.624 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 44.18s
2025-07-24 21:28:45.536 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0004,avg_pass_at_n: 1.0000,avg_num_tokens: 63.4293,std_num_tokens: 16.3222,avg_correct_num_tokens: 63.4169,std_correct_num_tokens: 16.3168,avg_incorrect_num_tokens: 68.5000,std_incorrect_num_tokens: 17.6989
2025-07-24 21:28:45.972 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.35s
2025-07-24 21:28:48.566 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.59s
2025-07-24 21:29:12.736 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 21:29:12.737 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.17s
2025-07-24 21:29:13.977 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:29:13.978 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 9.404608942064884e-06, avg_kl: 0.015188681112753379, avg_response_length: 63.44939408173432, avg_orm_score: 0.0, avg_custom_rewards: 9.404608942064884e-06
2025-07-24 21:29:14.009 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter136_replay_buffer.jsonl
2025-07-24 21:29:15.292 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.29s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:01<?, ?it/s, pg=-0.0492, ret=5.09e-5, glen=64.5, tlen=225, kl=0.0117, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.02s/it, pg=-0.0492, ret=5.09e-5, glen=64.5, tlen=225, kl=0.0117, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.02s/it, pg=-0.0612, ret=6.85e-5, glen=64, tlen=224, kl=0.0161, act_lr=1e-6, ent=0.955]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.08it/s, pg=-0.0612, ret=6.85e-5, glen=64, tlen=224, kl=0.0161, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.08it/s, pg=-0.0452, ret=4.87e-5, glen=63.3, tlen=224, kl=0.0154, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:39,  1.12it/s, pg=-0.0452, ret=4.87e-5, glen=63.3, tlen=224, kl=0.0154, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:39,  1.12it/s, pg=0.304, ret=-0.00032, glen=64, tlen=224, kl=0.0155, act_lr=1e-6, ent=0.965]   Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:37,  1.14it/s, pg=0.304, ret=-0.00032, glen=64, tlen=224, kl=0.0155, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:37,  1.14it/s, pg=0.0463, ret=-7.55e-5, glen=63.9, tlen=224, kl=0.00985, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:36,  1.15it/s, pg=0.0463, ret=-7.55e-5, glen=63.9, tlen=224, kl=0.00985, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:36,  1.15it/s, pg=-0.0636, ret=6.61e-5, glen=63.9, tlen=224, kl=0.0161, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:35,  1.16it/s, pg=-0.0636, ret=6.61e-5, glen=63.9, tlen=224, kl=0.0161, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:35,  1.16it/s, pg=-0.0457, ret=4.64e-5, glen=64, tlen=224, kl=0.0145, act_lr=1e-6, ent=1]      Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:34,  1.17it/s, pg=-0.0457, ret=4.64e-5, glen=64, tlen=224, kl=0.0145, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:34,  1.17it/s, pg=-0.0406, ret=3.98e-5, glen=63.3, tlen=223, kl=0.0197, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:06<00:33,  1.17it/s, pg=-0.0406, ret=3.98e-5, glen=63.3, tlen=223, kl=0.0197, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.17it/s, pg=0.0233, ret=-5.25e-5, glen=64.9, tlen=225, kl=0.0108, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:07<00:32,  1.17it/s, pg=0.0233, ret=-5.25e-5, glen=64.9, tlen=225, kl=0.0108, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:32,  1.17it/s, pg=-0.0422, ret=4.74e-5, glen=63.2, tlen=224, kl=0.0138, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:31,  1.17it/s, pg=-0.0422, ret=4.74e-5, glen=63.2, tlen=224, kl=0.0138, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:31,  1.17it/s, pg=-0.0764, ret=8.36e-5, glen=62.6, tlen=223, kl=0.0116, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:31,  1.15it/s, pg=-0.0764, ret=8.36e-5, glen=62.6, tlen=223, kl=0.0116, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:31,  1.15it/s, pg=0.0518, ret=-6.83e-5, glen=63, tlen=223, kl=0.0111, act_lr=1e-6, ent=0.994]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:30,  1.15it/s, pg=0.0518, ret=-6.83e-5, glen=63, tlen=223, kl=0.0111, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:30,  1.15it/s, pg=0.0837, ret=-6.21e-5, glen=63.2, tlen=224, kl=0.0322, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.15it/s, pg=0.0837, ret=-6.21e-5, glen=63.2, tlen=224, kl=0.0322, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.15it/s, pg=-0.0399, ret=4.07e-5, glen=62.9, tlen=223, kl=0.0105, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:28,  1.15it/s, pg=-0.0399, ret=4.07e-5, glen=62.9, tlen=223, kl=0.0105, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:28,  1.15it/s, pg=-0.0582, ret=6.03e-5, glen=62.4, tlen=223, kl=0.0109, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.0582, ret=6.03e-5, glen=62.4, tlen=223, kl=0.0109, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.06, ret=6.77e-5, glen=64.2, tlen=225, kl=0.011, act_lr=1e-6, ent=0.976]   Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:13<00:26,  1.17it/s, pg=-0.06, ret=6.77e-5, glen=64.2, tlen=225, kl=0.011, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.17it/s, pg=-0.0473, ret=5.18e-5, glen=64.1, tlen=224, kl=0.0111, act_lr=1e-6, ent=0.993]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:25,  1.17it/s, pg=-0.0473, ret=5.18e-5, glen=64.1, tlen=224, kl=0.0111, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:25,  1.17it/s, pg=0.0428, ret=-7.43e-5, glen=65.7, tlen=226, kl=0.0137, act_lr=1e-6, ent=1.22] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:24,  1.17it/s, pg=0.0428, ret=-7.43e-5, glen=65.7, tlen=226, kl=0.0137, act_lr=1e-6, ent=1.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:24,  1.17it/s, pg=-0.0336, ret=3.39e-5, glen=63.8, tlen=224, kl=0.0107, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:23,  1.17it/s, pg=-0.0336, ret=3.39e-5, glen=63.8, tlen=224, kl=0.0107, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:23,  1.17it/s, pg=0.136, ret=-7.5e-5, glen=63.7, tlen=224, kl=0.0331, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.17it/s, pg=0.136, ret=-7.5e-5, glen=63.7, tlen=224, kl=0.0331, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.17it/s, pg=-0.0476, ret=6.6e-5, glen=61.8, tlen=222, kl=0.0134, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.17it/s, pg=-0.0476, ret=6.6e-5, glen=61.8, tlen=222, kl=0.0134, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.17it/s, pg=-0.0404, ret=4.3e-5, glen=61.5, tlen=222, kl=0.016, act_lr=1e-6, ent=0.984]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.0404, ret=4.3e-5, glen=61.5, tlen=222, kl=0.016, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=0.12, ret=-9.17e-5, glen=62.9, tlen=223, kl=0.0252, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:19<00:20,  1.17it/s, pg=0.12, ret=-9.17e-5, glen=62.9, tlen=223, kl=0.0252, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=0.0995, ret=-5.73e-5, glen=64, tlen=224, kl=0.0116, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.17it/s, pg=0.0995, ret=-5.73e-5, glen=64, tlen=224, kl=0.0116, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.17it/s, pg=-0.0356, ret=3.99e-5, glen=62.8, tlen=223, kl=0.0123, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.17it/s, pg=-0.0356, ret=3.99e-5, glen=62.8, tlen=223, kl=0.0123, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.17it/s, pg=-0.0683, ret=6.76e-5, glen=63.4, tlen=224, kl=0.0182, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:17,  1.18it/s, pg=-0.0683, ret=6.76e-5, glen=63.4, tlen=224, kl=0.0182, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:17,  1.18it/s, pg=0.137, ret=-0.000101, glen=63.7, tlen=224, kl=0.0172, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:17,  1.18it/s, pg=0.137, ret=-0.000101, glen=63.7, tlen=224, kl=0.0172, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:17,  1.18it/s, pg=-0.0658, ret=7.19e-5, glen=64.5, tlen=224, kl=0.0132, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.18it/s, pg=-0.0658, ret=7.19e-5, glen=64.5, tlen=224, kl=0.0132, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.18it/s, pg=-0.0501, ret=4.61e-5, glen=63.8, tlen=224, kl=0.0123, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:16,  1.07it/s, pg=-0.0501, ret=4.61e-5, glen=63.8, tlen=224, kl=0.0123, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:16,  1.07it/s, pg=-0.0756, ret=8.38e-5, glen=63.3, tlen=224, kl=0.0116, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.10it/s, pg=-0.0756, ret=8.38e-5, glen=63.3, tlen=224, kl=0.0116, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.10it/s, pg=0.0544, ret=-7.96e-5, glen=63.3, tlen=224, kl=0.0172, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:26<00:14,  1.12it/s, pg=0.0544, ret=-7.96e-5, glen=63.3, tlen=224, kl=0.0172, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.12it/s, pg=-0.079, ret=7.96e-5, glen=63.6, tlen=224, kl=0.0101, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:27<00:13,  1.14it/s, pg=-0.079, ret=7.96e-5, glen=63.6, tlen=224, kl=0.0101, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=-0.0427, ret=4.38e-5, glen=62.9, tlen=223, kl=0.0439, act_lr=1e-6, ent=1]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.15it/s, pg=-0.0427, ret=4.38e-5, glen=62.9, tlen=223, kl=0.0439, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.15it/s, pg=-0.0381, ret=4.22e-5, glen=64.9, tlen=225, kl=0.0148, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.16it/s, pg=-0.0381, ret=4.22e-5, glen=64.9, tlen=225, kl=0.0148, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.16it/s, pg=-0.0333, ret=3.09e-5, glen=62.3, tlen=222, kl=0.0152, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.16it/s, pg=-0.0333, ret=3.09e-5, glen=62.3, tlen=222, kl=0.0152, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.16it/s, pg=0.0968, ret=-0.000168, glen=63.6, tlen=224, kl=0.0121, act_lr=1e-6, ent=0.98]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.17it/s, pg=0.0968, ret=-0.000168, glen=63.6, tlen=224, kl=0.0121, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.17it/s, pg=-0.0295, ret=3.26e-5, glen=63.6, tlen=224, kl=0.0432, act_lr=1e-6, ent=0.993]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=-0.0295, ret=3.26e-5, glen=63.6, tlen=224, kl=0.0432, act_lr=1e-6, ent=0.993]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=-0.0578, ret=5.61e-5, glen=64.5, tlen=225, kl=0.0138, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:32<00:07,  1.17it/s, pg=-0.0578, ret=5.61e-5, glen=64.5, tlen=225, kl=0.0138, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=0.0938, ret=-6.79e-5, glen=63, tlen=223, kl=0.0094, act_lr=1e-6, ent=0.982]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:33<00:06,  1.17it/s, pg=0.0938, ret=-6.79e-5, glen=63, tlen=223, kl=0.0094, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=0.0547, ret=-7.87e-5, glen=64.6, tlen=224, kl=0.0202, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:05,  1.18it/s, pg=0.0547, ret=-7.87e-5, glen=64.6, tlen=224, kl=0.0202, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:05,  1.18it/s, pg=0.0701, ret=-7.76e-5, glen=62.9, tlen=223, kl=0.0124, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.18it/s, pg=0.0701, ret=-7.76e-5, glen=62.9, tlen=223, kl=0.0124, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.18it/s, pg=0.047, ret=-5.16e-5, glen=63, tlen=223, kl=0.0135, act_lr=1e-6, ent=0.968]   Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.18it/s, pg=0.047, ret=-5.16e-5, glen=63, tlen=223, kl=0.0135, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.18it/s, pg=-0.0795, ret=8.3e-5, glen=63.1, tlen=223, kl=0.0115, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.16it/s, pg=-0.0795, ret=8.3e-5, glen=63.1, tlen=223, kl=0.0115, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.16it/s, pg=-0.0558, ret=5.93e-5, glen=63.1, tlen=223, kl=0.00983, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.16it/s, pg=-0.0558, ret=5.93e-5, glen=63.1, tlen=223, kl=0.00983, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.16it/s, pg=-0.0358, ret=3.39e-5, glen=63.2, tlen=223, kl=0.0181, act_lr=1e-6, ent=0.967] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:38<00:01,  1.16it/s, pg=-0.0358, ret=3.39e-5, glen=63.2, tlen=223, kl=0.0181, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.16it/s, pg=0.0339, ret=-5.89e-5, glen=62.5, tlen=223, kl=0.0169, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:39<00:00,  1.17it/s, pg=0.0339, ret=-5.89e-5, glen=62.5, tlen=223, kl=0.0169, act_lr=1e-6, ent=0.942]
2025-07-24 21:29:56.176 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.70s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.17it/s, pg=-0.0388, ret=3.95e-5, glen=62.8, tlen=223, kl=0.0113, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.13it/s, pg=-0.0388, ret=3.95e-5, glen=62.8, tlen=223, kl=0.0113, act_lr=1e-6, ent=0.976]
2025-07-24 21:29:57.022 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 21:29:59.610 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 21:29:59.942 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.58s
2025-07-24 21:29:59.951 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0008885809715757978, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9768030998554635, 'kl': 0.015828071756565823, 'response_length': 63.46847444899539, 'total_length': 223.795034530315, 'teacher_total_length': 235.82213446434508, 'return': 1.3852840446758063e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [13:11<11:27, 114.52s/it]2025-07-24 21:30:00.006 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:30:36.030 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:30:36.201 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 21:30:36.202 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.20s
2025-07-24 21:30:37.885 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 61.8431,std_num_tokens: 12.2090,avg_correct_num_tokens: 61.8349,std_correct_num_tokens: 12.2011,avg_incorrect_num_tokens: 67.0000,std_incorrect_num_tokens: 15.6205
2025-07-24 21:30:38.277 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.08s
2025-07-24 21:30:40.841 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.56s
2025-07-24 21:31:04.411 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:31:04.412 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.57s
2025-07-24 21:31:05.863 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 21:31:05.863 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.826403685036253e-06, avg_kl: 0.0184112715590847, avg_response_length: 61.85977231218515, avg_orm_score: 0.0, avg_custom_rewards: 3.826403685036253e-06
2025-07-24 21:31:05.895 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter137_replay_buffer.jsonl
2025-07-24 21:31:07.221 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.33s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.036, ret=-6.61e-5, glen=62.4, tlen=223, kl=0.0375, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=0.036, ret=-6.61e-5, glen=62.4, tlen=223, kl=0.0375, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0359, ret=3.09e-5, glen=61.3, tlen=221, kl=0.0153, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.0359, ret=3.09e-5, glen=61.3, tlen=221, kl=0.0153, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0324, ret=2.89e-5, glen=61.5, tlen=222, kl=0.023, act_lr=1e-6, ent=0.958] Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.08it/s, pg=-0.0324, ret=2.89e-5, glen=61.5, tlen=222, kl=0.023, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.08it/s, pg=-0.0463, ret=3.86e-5, glen=61.5, tlen=221, kl=0.0134, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.11it/s, pg=-0.0463, ret=3.86e-5, glen=61.5, tlen=221, kl=0.0134, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.11it/s, pg=0.0945, ret=-7.86e-5, glen=61.8, tlen=222, kl=0.0283, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.14it/s, pg=0.0945, ret=-7.86e-5, glen=61.8, tlen=222, kl=0.0283, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.14it/s, pg=0.0636, ret=-9.08e-5, glen=64.4, tlen=225, kl=0.0134, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.15it/s, pg=0.0636, ret=-9.08e-5, glen=64.4, tlen=225, kl=0.0134, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.15it/s, pg=-0.0284, ret=2.7e-5, glen=60.7, tlen=221, kl=0.0168, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=-0.0284, ret=2.7e-5, glen=60.7, tlen=221, kl=0.0168, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=-0.0517, ret=4.41e-5, glen=63.4, tlen=224, kl=0.0146, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0517, ret=4.41e-5, glen=63.4, tlen=224, kl=0.0146, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0391, ret=3.47e-5, glen=61.2, tlen=221, kl=0.0233, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.17it/s, pg=-0.0391, ret=3.47e-5, glen=61.2, tlen=221, kl=0.0233, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.17it/s, pg=-0.034, ret=2.61e-5, glen=62.9, tlen=223, kl=0.0228, act_lr=1e-6, ent=0.926] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=-0.034, ret=2.61e-5, glen=62.9, tlen=223, kl=0.0228, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=-0.0521, ret=4.27e-5, glen=62.4, tlen=223, kl=0.031, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=-0.0521, ret=4.27e-5, glen=62.4, tlen=223, kl=0.031, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0315, ret=3.14e-5, glen=60.2, tlen=221, kl=0.0156, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=-0.0315, ret=3.14e-5, glen=60.2, tlen=221, kl=0.0156, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=-0.041, ret=3.41e-5, glen=60.5, tlen=221, kl=0.0193, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.041, ret=3.41e-5, glen=60.5, tlen=221, kl=0.0193, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=0.139, ret=-9.22e-5, glen=61.7, tlen=222, kl=0.0173, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=0.139, ret=-9.22e-5, glen=61.7, tlen=222, kl=0.0173, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=-0.0478, ret=4.1e-5, glen=62.4, tlen=223, kl=0.0184, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0478, ret=4.1e-5, glen=62.4, tlen=223, kl=0.0184, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.0627, ret=-7.72e-5, glen=61.7, tlen=222, kl=0.0232, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=0.0627, ret=-7.72e-5, glen=61.7, tlen=222, kl=0.0232, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=0.142, ret=-7.82e-5, glen=62.7, tlen=223, kl=0.0168, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=0.142, ret=-7.82e-5, glen=62.7, tlen=223, kl=0.0168, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0482, ret=4.32e-5, glen=62.9, tlen=223, kl=0.0165, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0482, ret=4.32e-5, glen=62.9, tlen=223, kl=0.0165, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0419, ret=3.54e-5, glen=62.4, tlen=223, kl=0.0178, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0419, ret=3.54e-5, glen=62.4, tlen=223, kl=0.0178, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0264, ret=2.33e-5, glen=61.5, tlen=222, kl=0.0226, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0264, ret=2.33e-5, glen=61.5, tlen=222, kl=0.0226, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.143, ret=-7.91e-5, glen=61.5, tlen=222, kl=0.0208, act_lr=1e-6, ent=0.952] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.143, ret=-7.91e-5, glen=61.5, tlen=222, kl=0.0208, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=-0.0471, ret=3.88e-5, glen=61.9, tlen=223, kl=0.015, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=-0.0471, ret=3.88e-5, glen=61.9, tlen=223, kl=0.015, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=-0.0241, ret=1.96e-5, glen=62.1, tlen=222, kl=0.0172, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.18it/s, pg=-0.0241, ret=1.96e-5, glen=62.1, tlen=222, kl=0.0172, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=-0.0407, ret=3.15e-5, glen=62.8, tlen=223, kl=0.0192, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=-0.0407, ret=3.15e-5, glen=62.8, tlen=223, kl=0.0192, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=-0.026, ret=2.14e-5, glen=62.2, tlen=223, kl=0.0161, act_lr=1e-6, ent=0.952] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=-0.026, ret=2.14e-5, glen=62.2, tlen=223, kl=0.0161, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=-0.0379, ret=3.28e-5, glen=60, tlen=220, kl=0.0165, act_lr=1e-6, ent=0.943] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:16,  1.18it/s, pg=-0.0379, ret=3.28e-5, glen=60, tlen=220, kl=0.0165, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:16,  1.18it/s, pg=-0.0402, ret=3.52e-5, glen=61.8, tlen=222, kl=0.0187, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.18it/s, pg=-0.0402, ret=3.52e-5, glen=61.8, tlen=222, kl=0.0187, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.18it/s, pg=0.0627, ret=-9.14e-5, glen=59.9, tlen=220, kl=0.0184, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.18it/s, pg=0.0627, ret=-9.14e-5, glen=59.9, tlen=220, kl=0.0184, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.18it/s, pg=-0.0386, ret=3.17e-5, glen=61.7, tlen=222, kl=0.0166, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0386, ret=3.17e-5, glen=61.7, tlen=222, kl=0.0166, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.139, ret=-9.65e-5, glen=61.4, tlen=221, kl=0.0179, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.07it/s, pg=0.139, ret=-9.65e-5, glen=61.4, tlen=221, kl=0.0179, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.07it/s, pg=-0.0497, ret=4.11e-5, glen=61.7, tlen=222, kl=0.0192, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.10it/s, pg=-0.0497, ret=4.11e-5, glen=61.7, tlen=222, kl=0.0192, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.10it/s, pg=-0.0416, ret=3.47e-5, glen=60.7, tlen=221, kl=0.0294, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.10it/s, pg=-0.0416, ret=3.47e-5, glen=60.7, tlen=221, kl=0.0294, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.10it/s, pg=-0.0542, ret=4.63e-5, glen=59.9, tlen=220, kl=0.0187, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.12it/s, pg=-0.0542, ret=4.63e-5, glen=59.9, tlen=220, kl=0.0187, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.12it/s, pg=0.132, ret=-9.43e-5, glen=62.8, tlen=223, kl=0.019, act_lr=1e-6, ent=0.934]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=0.132, ret=-9.43e-5, glen=62.8, tlen=223, kl=0.019, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=0.0781, ret=-8.46e-5, glen=61.6, tlen=222, kl=0.0138, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=0.0781, ret=-8.46e-5, glen=61.6, tlen=222, kl=0.0138, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=-0.038, ret=3.12e-5, glen=62, tlen=222, kl=0.0145, act_lr=1e-6, ent=0.935]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.13it/s, pg=-0.038, ret=3.12e-5, glen=62, tlen=222, kl=0.0145, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.13it/s, pg=0.104, ret=-9.64e-5, glen=62.9, tlen=223, kl=0.0135, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.14it/s, pg=0.104, ret=-9.64e-5, glen=62.9, tlen=223, kl=0.0135, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.14it/s, pg=-0.0349, ret=3.1e-5, glen=62.2, tlen=222, kl=0.0144, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.15it/s, pg=-0.0349, ret=3.1e-5, glen=62.2, tlen=222, kl=0.0144, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.15it/s, pg=-0.041, ret=3.4e-5, glen=63.4, tlen=224, kl=0.0155, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:06,  1.15it/s, pg=-0.041, ret=3.4e-5, glen=63.4, tlen=224, kl=0.0155, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:06,  1.15it/s, pg=0.119, ret=-9e-5, glen=61.4, tlen=222, kl=0.0162, act_lr=1e-6, ent=0.93]   Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.16it/s, pg=0.119, ret=-9e-5, glen=61.4, tlen=222, kl=0.0162, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.16it/s, pg=-0.0555, ret=4.63e-5, glen=61.2, tlen=221, kl=0.0138, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.16it/s, pg=-0.0555, ret=4.63e-5, glen=61.2, tlen=221, kl=0.0138, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.16it/s, pg=-0.0455, ret=4.16e-5, glen=61.6, tlen=222, kl=0.0124, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0455, ret=4.16e-5, glen=61.6, tlen=222, kl=0.0124, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0568, ret=4.82e-5, glen=61.5, tlen=222, kl=0.019, act_lr=1e-6, ent=0.969] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0568, ret=4.82e-5, glen=61.5, tlen=222, kl=0.019, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0214, ret=1.75e-5, glen=62.1, tlen=222, kl=0.0145, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.15it/s, pg=-0.0214, ret=1.75e-5, glen=62.1, tlen=222, kl=0.0145, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.15it/s, pg=-0.0281, ret=2.59e-5, glen=62.7, tlen=223, kl=0.015, act_lr=1e-6, ent=0.969] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.15it/s, pg=-0.0281, ret=2.59e-5, glen=62.7, tlen=223, kl=0.015, act_lr=1e-6, ent=0.969]
2025-07-24 21:31:47.577 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.15it/s, pg=-0.0475, ret=4.07e-5, glen=63.2, tlen=223, kl=0.0135, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=-0.0475, ret=4.07e-5, glen=63.2, tlen=223, kl=0.0135, act_lr=1e-6, ent=0.97]
2025-07-24 21:31:48.253 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 21:31:50.465 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.21s
2025-07-24 21:31:50.802 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.52s
2025-07-24 21:31:50.878 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00023286238960597825, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9537835756073827, 'kl': 0.01839115312727897, 'response_length': 61.87029216600501, 'total_length': 222.10766468877378, 'teacher_total_length': 234.06029410984206, 'return': 3.45368552307659e-07, 'policy_update_steps': 1.0}
Episode [11/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [15:02<09:26, 113.37s/it]2025-07-24 21:31:50.910 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:32:27.260 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:32:27.455 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 21:32:27.455 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.55s
2025-07-24 21:32:29.608 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0002,avg_pass_at_n: 1.0000,avg_num_tokens: 62.9713,std_num_tokens: 12.5120,avg_correct_num_tokens: 62.9691,std_correct_num_tokens: 12.5138,avg_incorrect_num_tokens: 64.0588,std_incorrect_num_tokens: 11.5426
2025-07-24 21:32:30.063 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.61s
2025-07-24 21:32:32.722 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.66s
2025-07-24 21:32:56.936 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 21:32:56.936 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.21s
2025-07-24 21:32:58.291 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.88s
2025-07-24 21:32:58.291 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.1619717209927134e-05, avg_kl: 0.026558211043074324, avg_response_length: 62.9938578425227, avg_orm_score: 0.0, avg_custom_rewards: 1.1619717209927134e-05
2025-07-24 21:32:58.324 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter138_replay_buffer.jsonl
2025-07-24 21:32:59.645 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.32s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s, pg=-0.0642, ret=6.46e-5, glen=64.5, tlen=225, kl=0.0248, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:00<00:45,  1.01it/s, pg=-0.0642, ret=6.46e-5, glen=64.5, tlen=225, kl=0.0248, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:45,  1.01it/s, pg=-0.042, ret=4.19e-5, glen=63.2, tlen=224, kl=0.0218, act_lr=1e-6, ent=0.973] Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.09it/s, pg=-0.042, ret=4.19e-5, glen=63.2, tlen=224, kl=0.0218, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.09it/s, pg=-0.0296, ret=3.17e-5, glen=62.6, tlen=224, kl=0.0193, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:40,  1.09it/s, pg=-0.0296, ret=3.17e-5, glen=62.6, tlen=224, kl=0.0193, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:40,  1.09it/s, pg=-0.015, ret=1.61e-5, glen=63.5, tlen=224, kl=0.0213, act_lr=1e-6, ent=1.01]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:39,  1.10it/s, pg=-0.015, ret=1.61e-5, glen=63.5, tlen=224, kl=0.0213, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:39,  1.10it/s, pg=0.0882, ret=-7.99e-5, glen=64.6, tlen=225, kl=0.0681, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:39,  1.06it/s, pg=0.0882, ret=-7.99e-5, glen=64.6, tlen=225, kl=0.0681, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:39,  1.06it/s, pg=0.0815, ret=-7.88e-5, glen=63.7, tlen=224, kl=0.0281, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:37,  1.09it/s, pg=0.0815, ret=-7.88e-5, glen=63.7, tlen=224, kl=0.0281, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:37,  1.09it/s, pg=-0.0602, ret=6.25e-5, glen=63.8, tlen=225, kl=0.0188, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:35,  1.12it/s, pg=-0.0602, ret=6.25e-5, glen=63.8, tlen=225, kl=0.0188, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:07<00:35,  1.12it/s, pg=-0.0434, ret=3.94e-5, glen=62.4, tlen=223, kl=0.0346, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:34,  1.14it/s, pg=-0.0434, ret=3.94e-5, glen=62.4, tlen=223, kl=0.0346, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:08<00:34,  1.14it/s, pg=0.0351, ret=-5.8e-5, glen=63.5, tlen=224, kl=0.0358, act_lr=1e-6, ent=0.972] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.15it/s, pg=0.0351, ret=-5.8e-5, glen=63.5, tlen=224, kl=0.0358, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.15it/s, pg=0.118, ret=-7.69e-5, glen=63.6, tlen=225, kl=0.0212, act_lr=1e-6, ent=0.98] Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:32,  1.15it/s, pg=0.118, ret=-7.69e-5, glen=63.6, tlen=225, kl=0.0212, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:32,  1.15it/s, pg=-0.0379, ret=3.81e-5, glen=64.2, tlen=225, kl=0.0283, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:31,  1.16it/s, pg=-0.0379, ret=3.81e-5, glen=64.2, tlen=225, kl=0.0283, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:31,  1.16it/s, pg=-0.0751, ret=7.16e-5, glen=62.3, tlen=223, kl=0.0267, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:30,  1.16it/s, pg=-0.0751, ret=7.16e-5, glen=62.3, tlen=223, kl=0.0267, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:30,  1.16it/s, pg=-0.0448, ret=4.84e-5, glen=63, tlen=224, kl=0.0308, act_lr=1e-6, ent=0.991] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.17it/s, pg=-0.0448, ret=4.84e-5, glen=63, tlen=224, kl=0.0308, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.17it/s, pg=0.0735, ret=-9.72e-5, glen=61.6, tlen=222, kl=0.0327, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:28,  1.17it/s, pg=0.0735, ret=-9.72e-5, glen=61.6, tlen=222, kl=0.0327, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:28,  1.17it/s, pg=-0.0424, ret=4.08e-5, glen=61.9, tlen=223, kl=0.0245, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.17it/s, pg=-0.0424, ret=4.08e-5, glen=61.9, tlen=223, kl=0.0245, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:14<00:27,  1.17it/s, pg=-0.0353, ret=3.61e-5, glen=62.6, tlen=223, kl=0.0296, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.17it/s, pg=-0.0353, ret=3.61e-5, glen=62.6, tlen=223, kl=0.0296, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.17it/s, pg=-0.0172, ret=2e-5, glen=64.4, tlen=225, kl=0.0319, act_lr=1e-6, ent=1.01]    Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:26,  1.15it/s, pg=-0.0172, ret=2e-5, glen=64.4, tlen=225, kl=0.0319, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:26,  1.15it/s, pg=0.114, ret=-8.34e-5, glen=63.6, tlen=224, kl=0.022, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:25,  1.16it/s, pg=0.114, ret=-8.34e-5, glen=63.6, tlen=224, kl=0.022, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:25,  1.16it/s, pg=0.1, ret=-7.91e-5, glen=61.7, tlen=222, kl=0.0252, act_lr=1e-6, ent=0.973] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:24,  1.16it/s, pg=0.1, ret=-7.91e-5, glen=61.7, tlen=222, kl=0.0252, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:24,  1.16it/s, pg=-0.0336, ret=3.53e-5, glen=62.6, tlen=223, kl=0.0359, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.17it/s, pg=-0.0336, ret=3.53e-5, glen=62.6, tlen=223, kl=0.0359, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.17it/s, pg=-0.0388, ret=3.94e-5, glen=61.6, tlen=223, kl=0.0261, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.17it/s, pg=-0.0388, ret=3.94e-5, glen=61.6, tlen=223, kl=0.0261, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.17it/s, pg=-0.0362, ret=3.61e-5, glen=63.1, tlen=224, kl=0.0282, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.0362, ret=3.61e-5, glen=63.1, tlen=224, kl=0.0282, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:20<00:21,  1.17it/s, pg=0.0408, ret=-5.2e-5, glen=62, tlen=223, kl=0.0229, act_lr=1e-6, ent=0.935]   Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=0.0408, ret=-5.2e-5, glen=62, tlen=223, kl=0.0229, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=-0.034, ret=3.96e-5, glen=62.5, tlen=223, kl=0.0228, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.17it/s, pg=-0.034, ret=3.96e-5, glen=62.5, tlen=223, kl=0.0228, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.17it/s, pg=-0.063, ret=6.04e-5, glen=62.5, tlen=222, kl=0.0186, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.17it/s, pg=-0.063, ret=6.04e-5, glen=62.5, tlen=222, kl=0.0186, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.17it/s, pg=0.0468, ret=-7.83e-5, glen=63.8, tlen=224, kl=0.0191, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:17,  1.17it/s, pg=0.0468, ret=-7.83e-5, glen=63.8, tlen=224, kl=0.0191, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:17,  1.17it/s, pg=0.105, ret=-8.27e-5, glen=63.5, tlen=224, kl=0.0238, act_lr=1e-6, ent=0.97]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:17,  1.17it/s, pg=0.105, ret=-8.27e-5, glen=63.5, tlen=224, kl=0.0238, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:17,  1.17it/s, pg=-0.0268, ret=3.15e-5, glen=62.7, tlen=223, kl=0.0208, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.17it/s, pg=-0.0268, ret=3.15e-5, glen=62.7, tlen=223, kl=0.0208, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.17it/s, pg=0.063, ret=-7.63e-5, glen=62.1, tlen=223, kl=0.0312, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:16,  1.07it/s, pg=0.063, ret=-7.63e-5, glen=62.1, tlen=223, kl=0.0312, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:16,  1.07it/s, pg=-0.076, ret=0.000115, glen=64.6, tlen=225, kl=0.0272, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.10it/s, pg=-0.076, ret=0.000115, glen=64.6, tlen=225, kl=0.0272, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:27<00:15,  1.10it/s, pg=0.096, ret=-8.03e-5, glen=62.2, tlen=223, kl=0.0312, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.12it/s, pg=0.096, ret=-8.03e-5, glen=62.2, tlen=223, kl=0.0312, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:28<00:14,  1.12it/s, pg=0.0461, ret=-6.56e-5, glen=62.2, tlen=222, kl=0.029, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=0.0461, ret=-6.56e-5, glen=62.2, tlen=222, kl=0.029, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=-0.0515, ret=5.02e-5, glen=62.3, tlen=223, kl=0.0218, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.12it/s, pg=-0.0515, ret=5.02e-5, glen=62.3, tlen=223, kl=0.0218, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.12it/s, pg=0.0819, ret=-6.76e-5, glen=63.4, tlen=224, kl=0.0244, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.14it/s, pg=0.0819, ret=-6.76e-5, glen=63.4, tlen=224, kl=0.0244, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.14it/s, pg=-0.0543, ret=5.33e-5, glen=62.8, tlen=223, kl=0.0311, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.15it/s, pg=-0.0543, ret=5.33e-5, glen=62.8, tlen=223, kl=0.0311, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.15it/s, pg=-0.0387, ret=4e-5, glen=62.9, tlen=224, kl=0.0209, act_lr=1e-6, ent=0.969]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.16it/s, pg=-0.0387, ret=4e-5, glen=62.9, tlen=224, kl=0.0209, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.16it/s, pg=-0.032, ret=3.15e-5, glen=63.2, tlen=224, kl=0.0195, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.16it/s, pg=-0.032, ret=3.15e-5, glen=63.2, tlen=224, kl=0.0195, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:33<00:08,  1.16it/s, pg=-0.0369, ret=3.83e-5, glen=63.4, tlen=224, kl=0.0235, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=-0.0369, ret=3.83e-5, glen=63.4, tlen=224, kl=0.0235, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:34<00:07,  1.17it/s, pg=0.0801, ret=-7.67e-5, glen=61.9, tlen=222, kl=0.0272, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=0.0801, ret=-7.67e-5, glen=61.9, tlen=222, kl=0.0272, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=-0.0482, ret=4.61e-5, glen=63.1, tlen=224, kl=0.024, act_lr=1e-6, ent=0.981] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:05,  1.17it/s, pg=-0.0482, ret=4.61e-5, glen=63.1, tlen=224, kl=0.024, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:05,  1.17it/s, pg=0.0445, ret=-7.07e-5, glen=61.9, tlen=223, kl=0.0255, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.17it/s, pg=0.0445, ret=-7.07e-5, glen=61.9, tlen=223, kl=0.0255, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.17it/s, pg=-0.0365, ret=3.75e-5, glen=63.9, tlen=225, kl=0.024, act_lr=1e-6, ent=0.964] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.18it/s, pg=-0.0365, ret=3.75e-5, glen=63.9, tlen=225, kl=0.024, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.18it/s, pg=-0.0514, ret=4.81e-5, glen=63.1, tlen=224, kl=0.0192, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.18it/s, pg=-0.0514, ret=4.81e-5, glen=63.1, tlen=224, kl=0.0192, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.18it/s, pg=-0.0508, ret=5.28e-5, glen=63.2, tlen=224, kl=0.0245, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.18it/s, pg=-0.0508, ret=5.28e-5, glen=63.2, tlen=224, kl=0.0245, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:39<00:02,  1.18it/s, pg=-0.0406, ret=3.7e-5, glen=62.9, tlen=224, kl=0.0184, act_lr=1e-6, ent=0.991] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.18it/s, pg=-0.0406, ret=3.7e-5, glen=62.9, tlen=224, kl=0.0184, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.18it/s, pg=0.0542, ret=-7.99e-5, glen=64.1, tlen=225, kl=0.0223, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:39<00:00,  1.18it/s, pg=0.0542, ret=-7.99e-5, glen=64.1, tlen=225, kl=0.0223, act_lr=1e-6, ent=0.967]
2025-07-24 21:33:40.780 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.93s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.18it/s, pg=-0.0577, ret=5.7e-5, glen=62.4, tlen=223, kl=0.0387, act_lr=1e-6, ent=0.978] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.13it/s, pg=-0.0577, ret=5.7e-5, glen=62.4, tlen=223, kl=0.0387, act_lr=1e-6, ent=0.978]
2025-07-24 21:33:41.622 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 21:33:44.182 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 21:33:44.521 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.79s
2025-07-24 21:33:44.527 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0009710027816447806, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9754297568442973, 'kl': 0.026538280730551863, 'response_length': 62.9980977646848, 'total_length': 223.63188301248752, 'teacher_total_length': 235.6428170711436, 'return': 1.6317448790551739e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [16:56<07:33, 113.46s/it]2025-07-24 21:33:44.574 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:34:21.073 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:34:21.247 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 21:34:21.248 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.67s
2025-07-24 21:34:23.089 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0004,avg_pass_at_n: 1.0000,avg_num_tokens: 62.7729,std_num_tokens: 12.5168,avg_correct_num_tokens: 62.7740,std_correct_num_tokens: 12.5138,avg_incorrect_num_tokens: 62.0000,std_incorrect_num_tokens: 14.5290
2025-07-24 21:34:23.368 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.12s
2025-07-24 21:34:25.975 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.60s
2025-07-24 21:34:50.201 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 21:34:50.202 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.22s
2025-07-24 21:34:51.540 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.89s
2025-07-24 21:34:51.540 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.1452926933211653e-05, avg_kl: 0.02808639938766892, avg_response_length: 62.798306604333824, avg_orm_score: 0.0, avg_custom_rewards: 1.1452926933211653e-05
2025-07-24 21:34:51.569 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter139_replay_buffer.jsonl
2025-07-24 21:34:52.848 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.28s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:01<?, ?it/s, pg=-0.0415, ret=3.67e-5, glen=64.4, tlen=226, kl=0.0226, act_lr=1e-6, ent=0.991]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:47,  1.03s/it, pg=-0.0415, ret=3.67e-5, glen=64.4, tlen=226, kl=0.0226, act_lr=1e-6, ent=0.991]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:47,  1.03s/it, pg=-0.0417, ret=3.03e-5, glen=63.6, tlen=225, kl=0.0219, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.08it/s, pg=-0.0417, ret=3.03e-5, glen=63.6, tlen=225, kl=0.0219, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.08it/s, pg=-0.0162, ret=1.56e-5, glen=62.8, tlen=224, kl=0.0224, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:40,  1.09it/s, pg=-0.0162, ret=1.56e-5, glen=62.8, tlen=224, kl=0.0224, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:40,  1.09it/s, pg=-0.0355, ret=3.1e-5, glen=61.9, tlen=223, kl=0.0186, act_lr=1e-6, ent=0.981] Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:38,  1.12it/s, pg=-0.0355, ret=3.1e-5, glen=61.9, tlen=223, kl=0.0186, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:38,  1.12it/s, pg=-0.0274, ret=2.51e-5, glen=61.5, tlen=222, kl=0.0242, act_lr=1e-6, ent=1.02]Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:37,  1.11it/s, pg=-0.0274, ret=2.51e-5, glen=61.5, tlen=222, kl=0.0242, act_lr=1e-6, ent=1.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:37,  1.11it/s, pg=0.0672, ret=-8.21e-5, glen=62.4, tlen=223, kl=0.0256, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:36,  1.13it/s, pg=0.0672, ret=-8.21e-5, glen=62.4, tlen=223, kl=0.0256, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:36,  1.13it/s, pg=-0.0455, ret=3.74e-5, glen=62.7, tlen=223, kl=0.0467, act_lr=1e-6, ent=1]    Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:35,  1.14it/s, pg=-0.0455, ret=3.74e-5, glen=62.7, tlen=223, kl=0.0467, act_lr=1e-6, ent=1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:07<00:35,  1.14it/s, pg=-0.0439, ret=3.55e-5, glen=61.3, tlen=222, kl=0.0425, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.15it/s, pg=-0.0439, ret=3.55e-5, glen=61.3, tlen=222, kl=0.0425, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:33,  1.15it/s, pg=-0.0434, ret=3.37e-5, glen=62.8, tlen=224, kl=0.0315, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:07<00:32,  1.16it/s, pg=-0.0434, ret=3.37e-5, glen=62.8, tlen=224, kl=0.0315, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:32,  1.16it/s, pg=0.0902, ret=-9.02e-5, glen=61.3, tlen=222, kl=0.0217, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:31,  1.16it/s, pg=0.0902, ret=-9.02e-5, glen=61.3, tlen=222, kl=0.0217, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:31,  1.16it/s, pg=-0.0338, ret=2.82e-5, glen=63.9, tlen=225, kl=0.0238, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:30,  1.16it/s, pg=-0.0338, ret=2.82e-5, glen=63.9, tlen=225, kl=0.0238, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:30,  1.16it/s, pg=0.184, ret=-0.000106, glen=61.7, tlen=222, kl=0.0236, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:29,  1.17it/s, pg=0.184, ret=-0.000106, glen=61.7, tlen=222, kl=0.0236, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:29,  1.17it/s, pg=-0.0375, ret=2.96e-5, glen=62.9, tlen=224, kl=0.0291, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.15it/s, pg=-0.0375, ret=2.96e-5, glen=62.9, tlen=224, kl=0.0291, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.15it/s, pg=-0.034, ret=2.72e-5, glen=61.3, tlen=222, kl=0.0245, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:28,  1.15it/s, pg=-0.034, ret=2.72e-5, glen=61.3, tlen=222, kl=0.0245, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:28,  1.15it/s, pg=-0.0162, ret=1.4e-5, glen=63.2, tlen=224, kl=0.0227, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.0162, ret=1.4e-5, glen=63.2, tlen=224, kl=0.0227, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:14<00:27,  1.16it/s, pg=-0.0526, ret=4.65e-5, glen=64.2, tlen=225, kl=0.0233, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:27,  1.13it/s, pg=-0.0526, ret=4.65e-5, glen=64.2, tlen=225, kl=0.0233, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:27,  1.13it/s, pg=0.106, ret=-9.68e-5, glen=63.8, tlen=225, kl=0.0211, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:26,  1.14it/s, pg=0.106, ret=-9.68e-5, glen=63.8, tlen=225, kl=0.0211, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:26,  1.14it/s, pg=-0.0576, ret=4.45e-5, glen=64.2, tlen=225, kl=0.031, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:25,  1.15it/s, pg=-0.0576, ret=4.45e-5, glen=64.2, tlen=225, kl=0.031, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:25,  1.15it/s, pg=0.164, ret=-8.33e-5, glen=60.7, tlen=221, kl=0.0482, act_lr=1e-6, ent=0.999]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:24,  1.15it/s, pg=0.164, ret=-8.33e-5, glen=60.7, tlen=221, kl=0.0482, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:24,  1.15it/s, pg=-0.0289, ret=2.6e-5, glen=62.5, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.16it/s, pg=-0.0289, ret=2.6e-5, glen=62.5, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.16it/s, pg=-0.0527, ret=4.09e-5, glen=62.1, tlen=223, kl=0.0353, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.16it/s, pg=-0.0527, ret=4.09e-5, glen=62.1, tlen=223, kl=0.0353, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.16it/s, pg=-0.0333, ret=2.56e-5, glen=62.9, tlen=224, kl=0.0232, act_lr=1e-6, ent=0.99] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.0333, ret=2.56e-5, glen=62.9, tlen=224, kl=0.0232, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:20<00:21,  1.17it/s, pg=-0.0241, ret=2e-5, glen=63.1, tlen=224, kl=0.0367, act_lr=1e-6, ent=0.978]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.15it/s, pg=-0.0241, ret=2e-5, glen=63.1, tlen=224, kl=0.0367, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.15it/s, pg=-0.0414, ret=3.43e-5, glen=64, tlen=225, kl=0.0259, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.15it/s, pg=-0.0414, ret=3.43e-5, glen=64, tlen=225, kl=0.0259, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.15it/s, pg=-0.0394, ret=3.55e-5, glen=63.3, tlen=224, kl=0.0235, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.16it/s, pg=-0.0394, ret=3.55e-5, glen=63.3, tlen=224, kl=0.0235, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.16it/s, pg=-0.0358, ret=3.18e-5, glen=62.5, tlen=224, kl=0.0255, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:18,  1.16it/s, pg=-0.0358, ret=3.18e-5, glen=62.5, tlen=224, kl=0.0255, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:18,  1.16it/s, pg=-0.0142, ret=1.4e-5, glen=63.6, tlen=224, kl=0.0273, act_lr=1e-6, ent=0.988] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:17,  1.17it/s, pg=-0.0142, ret=1.4e-5, glen=63.6, tlen=224, kl=0.0273, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:17,  1.17it/s, pg=-0.0412, ret=3.1e-5, glen=62.2, tlen=223, kl=0.0606, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.17it/s, pg=-0.0412, ret=3.1e-5, glen=62.2, tlen=223, kl=0.0606, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.17it/s, pg=-0.0307, ret=2.62e-5, glen=63.5, tlen=225, kl=0.0226, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:16,  1.07it/s, pg=-0.0307, ret=2.62e-5, glen=63.5, tlen=225, kl=0.0226, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:16,  1.07it/s, pg=0.0884, ret=-9.43e-5, glen=63.3, tlen=224, kl=0.0228, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.10it/s, pg=0.0884, ret=-9.43e-5, glen=63.3, tlen=224, kl=0.0228, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:27<00:15,  1.10it/s, pg=-0.0265, ret=2.16e-5, glen=62.5, tlen=223, kl=0.0224, act_lr=1e-6, ent=1.01] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.12it/s, pg=-0.0265, ret=2.16e-5, glen=62.5, tlen=223, kl=0.0224, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:28<00:14,  1.12it/s, pg=-0.0266, ret=2.02e-5, glen=64.4, tlen=226, kl=0.0261, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=-0.0266, ret=2.02e-5, glen=64.4, tlen=226, kl=0.0261, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.14it/s, pg=-0.0521, ret=4.22e-5, glen=63.9, tlen=225, kl=0.0244, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.15it/s, pg=-0.0521, ret=4.22e-5, glen=63.9, tlen=225, kl=0.0244, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.15it/s, pg=-0.0417, ret=3.53e-5, glen=62.4, tlen=223, kl=0.0496, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.15it/s, pg=-0.0417, ret=3.53e-5, glen=62.4, tlen=223, kl=0.0496, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.15it/s, pg=-0.0362, ret=2.72e-5, glen=62.5, tlen=223, kl=0.0257, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.16it/s, pg=-0.0362, ret=2.72e-5, glen=62.5, tlen=223, kl=0.0257, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.16it/s, pg=-0.0381, ret=3.03e-5, glen=63.5, tlen=225, kl=0.0211, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.16it/s, pg=-0.0381, ret=3.03e-5, glen=63.5, tlen=225, kl=0.0211, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.16it/s, pg=0.115, ret=-9.24e-5, glen=62.4, tlen=223, kl=0.0251, act_lr=1e-6, ent=0.948] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=0.115, ret=-9.24e-5, glen=62.4, tlen=223, kl=0.0251, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:33<00:08,  1.17it/s, pg=-0.0406, ret=3.17e-5, glen=63.9, tlen=225, kl=0.0198, act_lr=1e-6, ent=0.99]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=-0.0406, ret=3.17e-5, glen=63.9, tlen=225, kl=0.0198, act_lr=1e-6, ent=0.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:34<00:07,  1.17it/s, pg=0.27, ret=-0.000218, glen=61, tlen=222, kl=0.0298, act_lr=1e-6, ent=0.983]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=0.27, ret=-0.000218, glen=61, tlen=222, kl=0.0298, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=-0.0386, ret=2.97e-5, glen=62.7, tlen=223, kl=0.021, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:05,  1.17it/s, pg=-0.0386, ret=2.97e-5, glen=62.7, tlen=223, kl=0.021, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:05,  1.17it/s, pg=-0.0344, ret=2.93e-5, glen=62.1, tlen=223, kl=0.0282, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.17it/s, pg=-0.0344, ret=2.93e-5, glen=62.1, tlen=223, kl=0.0282, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.17it/s, pg=-0.0565, ret=4.71e-5, glen=61.9, tlen=222, kl=0.0242, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.17it/s, pg=-0.0565, ret=4.71e-5, glen=61.9, tlen=222, kl=0.0242, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.17it/s, pg=0.164, ret=-0.000101, glen=62.7, tlen=223, kl=0.028, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.17it/s, pg=0.164, ret=-0.000101, glen=62.7, tlen=223, kl=0.028, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.17it/s, pg=-0.0341, ret=2.76e-5, glen=61.9, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.994]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.17it/s, pg=-0.0341, ret=2.76e-5, glen=61.9, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.994]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:39<00:02,  1.17it/s, pg=-0.0427, ret=3.43e-5, glen=63.4, tlen=225, kl=0.0217, act_lr=1e-6, ent=0.992]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.17it/s, pg=-0.0427, ret=3.43e-5, glen=63.4, tlen=225, kl=0.0217, act_lr=1e-6, ent=0.992]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:40<00:01,  1.17it/s, pg=0.107, ret=-0.000108, glen=62.6, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.17it/s, pg=0.107, ret=-0.000108, glen=62.6, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.974]
2025-07-24 21:35:34.002 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.96s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.17it/s, pg=-0.0288, ret=2.36e-5, glen=62.5, tlen=223, kl=0.0607, act_lr=1e-6, ent=0.986]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.12it/s, pg=-0.0288, ret=2.36e-5, glen=62.5, tlen=223, kl=0.0607, act_lr=1e-6, ent=0.986]
2025-07-24 21:35:34.829 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 21:35:37.379 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.55s
2025-07-24 21:35:37.732 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.82s
2025-07-24 21:35:37.738 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0001949553794049202, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.976565957069397, 'kl': 0.02814467409823803, 'response_length': 62.76204510952564, 'total_length': 223.55278826774435, 'teacher_total_length': 235.6280209155793, 'return': 1.0388251966821584e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [18:49<05:40, 113.38s/it]2025-07-24 21:35:37.745 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   1%|          | 1/172 [00:00<02:41,  1.06it/s, est. speed input: 187.63 toks/s, output: 39.22 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/172 [00:02<00:00, 120.92it/s, est. speed input: 7612.28 toks/s, output: 2564.67 toks/s]Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 107/172 [00:02<00:00, 122.45it/s, est. speed input: 8359.32 toks/s, output: 2912.35 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:02<00:00, 84.74it/s, est. speed input: 9945.51 toks/s, output: 3685.76 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 38.80it/s, est. speed input: 8384.77 toks/s, output: 3171.30 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 46.14it/s, est. speed input: 8384.77 toks/s, output: 3171.30 toks/s]
2025-07-24 21:35:42.330 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 326.8705,strategyqa_test/accuracy: 0.4891,eval_accuracy: 0.4891
2025-07-24 21:35:42.621 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:36:19.042 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:36:19.226 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:36:19.227 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.61s
2025-07-24 21:36:21.284 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 62.6594,std_num_tokens: 12.5330,avg_correct_num_tokens: 62.6521,std_correct_num_tokens: 12.5347,avg_incorrect_num_tokens: 64.8889,std_incorrect_num_tokens: 11.8019
2025-07-24 21:36:21.743 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.52s
2025-07-24 21:36:24.361 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.62s
2025-07-24 21:36:48.419 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 185
2025-07-24 21:36:48.420 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.06s
2025-07-24 21:36:49.614 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:36:49.614 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.0755902621895075e-06, avg_kl: 0.037283242715371624, avg_response_length: 62.67987314172693, avg_orm_score: 0.0, avg_custom_rewards: 1.0755902621895075e-06
2025-07-24 21:36:49.644 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter140_replay_buffer.jsonl
2025-07-24 21:36:50.972 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.33s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/172 [00:03<00:00, 81.95it/s, est. speed input: 9240.70 toks/s, output: 3402.90 toks/s][32m [repeated 55x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/172 [00:02<00:00, 81.34it/s, est. speed input: 9673.94 toks/s, output: 3585.20 toks/s]Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:02<00:00, 84.25it/s, est. speed input: 9951.52 toks/s, output: 3692.08 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:03<00:00, 50.03it/s, est. speed input: 9069.96 toks/s, output: 3387.08 toks/s][32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/47 [00:01<?, ?it/s, pg=-0.0507, ret=6.2e-5, glen=62.6, tlen=223, kl=0.0228, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.01s/it, pg=-0.0507, ret=6.2e-5, glen=62.6, tlen=223, kl=0.0228, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/47 [00:01<00:46,  1.01s/it, pg=-0.0439, ret=5.5e-5, glen=62, tlen=223, kl=0.0225, act_lr=1e-6, ent=0.975]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:01<00:41,  1.08it/s, pg=-0.0439, ret=5.5e-5, glen=62, tlen=223, kl=0.0225, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/47 [00:02<00:41,  1.08it/s, pg=0.000671, ret=-1.9e-5, glen=62.9, tlen=223, kl=0.0373, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:02<00:39,  1.12it/s, pg=0.000671, ret=-1.9e-5, glen=62.9, tlen=223, kl=0.0373, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/47 [00:03<00:39,  1.12it/s, pg=0.0223, ret=-4.84e-5, glen=62.3, tlen=223, kl=0.0248, act_lr=1e-6, ent=0.97]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:03<00:37,  1.14it/s, pg=0.0223, ret=-4.84e-5, glen=62.3, tlen=223, kl=0.0248, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/47 [00:04<00:37,  1.14it/s, pg=-0.0707, ret=8.62e-5, glen=64.1, tlen=225, kl=0.0246, act_lr=1e-6, ent=0.995]Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:04<00:37,  1.13it/s, pg=-0.0707, ret=8.62e-5, glen=64.1, tlen=225, kl=0.0246, act_lr=1e-6, ent=0.995]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/47 [00:05<00:37,  1.13it/s, pg=0.0127, ret=-2.73e-5, glen=61.7, tlen=223, kl=0.079, act_lr=1e-6, ent=0.997] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:05<00:37,  1.10it/s, pg=0.0127, ret=-2.73e-5, glen=61.7, tlen=223, kl=0.079, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/47 [00:06<00:37,  1.10it/s, pg=-0.0365, ret=4.5e-5, glen=62.7, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:06<00:35,  1.12it/s, pg=-0.0365, ret=4.5e-5, glen=62.7, tlen=223, kl=0.0222, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/47 [00:07<00:35,  1.12it/s, pg=0.0433, ret=-4.04e-5, glen=62.6, tlen=223, kl=0.0304, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:07<00:34,  1.14it/s, pg=0.0433, ret=-4.04e-5, glen=62.6, tlen=223, kl=0.0304, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/47 [00:08<00:34,  1.14it/s, pg=0.045, ret=-5.45e-5, glen=64.2, tlen=225, kl=0.0274, act_lr=1e-6, ent=0.976] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.15it/s, pg=0.045, ret=-5.45e-5, glen=64.2, tlen=225, kl=0.0274, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/47 [00:08<00:33,  1.15it/s, pg=-0.0466, ret=5.59e-5, glen=62.7, tlen=223, kl=0.0258, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:08<00:32,  1.15it/s, pg=-0.0466, ret=5.59e-5, glen=62.7, tlen=223, kl=0.0258, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 10/47 [00:09<00:32,  1.15it/s, pg=0.0543, ret=-6.32e-5, glen=64.2, tlen=225, kl=0.0225, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:09<00:31,  1.16it/s, pg=0.0543, ret=-6.32e-5, glen=64.2, tlen=225, kl=0.0225, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/47 [00:10<00:31,  1.16it/s, pg=0.0685, ret=-7.55e-5, glen=62.6, tlen=224, kl=0.0387, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:10<00:30,  1.16it/s, pg=0.0685, ret=-7.55e-5, glen=62.6, tlen=224, kl=0.0387, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/47 [00:11<00:30,  1.16it/s, pg=0.156, ret=-0.000176, glen=62, tlen=222, kl=0.0266, act_lr=1e-6, ent=0.999]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:11<00:29,  1.17it/s, pg=0.156, ret=-0.000176, glen=62, tlen=222, kl=0.0266, act_lr=1e-6, ent=0.999]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/47 [00:12<00:29,  1.17it/s, pg=0.0417, ret=-7.33e-5, glen=63.9, tlen=225, kl=0.0369, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:12<00:28,  1.15it/s, pg=0.0417, ret=-7.33e-5, glen=63.9, tlen=225, kl=0.0369, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 14/47 [00:13<00:28,  1.15it/s, pg=-0.0535, ret=6.24e-5, glen=62.6, tlen=224, kl=0.023, act_lr=1e-6, ent=0.986] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:13<00:27,  1.16it/s, pg=-0.0535, ret=6.24e-5, glen=62.6, tlen=224, kl=0.023, act_lr=1e-6, ent=0.986]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 15/47 [00:14<00:27,  1.16it/s, pg=0.032, ret=-3.06e-5, glen=62.4, tlen=223, kl=0.0224, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.16it/s, pg=0.032, ret=-3.06e-5, glen=62.4, tlen=223, kl=0.0224, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 16/47 [00:14<00:26,  1.16it/s, pg=0.145, ret=-0.000166, glen=62.7, tlen=224, kl=0.0265, act_lr=1e-6, ent=1.01]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:14<00:25,  1.17it/s, pg=0.145, ret=-0.000166, glen=62.7, tlen=224, kl=0.0265, act_lr=1e-6, ent=1.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:15<00:25,  1.17it/s, pg=-0.0521, ret=5.85e-5, glen=60.9, tlen=222, kl=0.036, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:15<00:24,  1.17it/s, pg=-0.0521, ret=5.85e-5, glen=60.9, tlen=222, kl=0.036, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/47 [00:16<00:24,  1.17it/s, pg=0.191, ret=-0.000268, glen=64.7, tlen=225, kl=0.0226, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:16<00:23,  1.17it/s, pg=0.191, ret=-0.000268, glen=64.7, tlen=225, kl=0.0226, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 19/47 [00:17<00:23,  1.17it/s, pg=-0.0749, ret=9.48e-5, glen=63.8, tlen=225, kl=0.023, act_lr=1e-6, ent=0.958] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:17<00:23,  1.17it/s, pg=-0.0749, ret=9.48e-5, glen=63.8, tlen=225, kl=0.023, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/47 [00:18<00:23,  1.17it/s, pg=0.0732, ret=-6.55e-5, glen=61.3, tlen=222, kl=0.0269, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:18<00:22,  1.17it/s, pg=0.0732, ret=-6.55e-5, glen=61.3, tlen=222, kl=0.0269, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/47 [00:19<00:22,  1.17it/s, pg=-0.0618, ret=8.62e-5, glen=63.2, tlen=224, kl=0.0314, act_lr=1e-6, ent=0.987]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.0618, ret=8.62e-5, glen=63.2, tlen=224, kl=0.0314, act_lr=1e-6, ent=0.987]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 22/47 [00:19<00:21,  1.17it/s, pg=-0.083, ret=0.000104, glen=62.4, tlen=223, kl=0.0324, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:19<00:20,  1.17it/s, pg=-0.083, ret=0.000104, glen=62.4, tlen=223, kl=0.0324, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 23/47 [00:20<00:20,  1.17it/s, pg=-0.0568, ret=7.37e-5, glen=61.4, tlen=222, kl=0.0276, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:20<00:19,  1.17it/s, pg=-0.0568, ret=7.37e-5, glen=61.4, tlen=222, kl=0.0276, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/47 [00:21<00:19,  1.17it/s, pg=-0.0477, ret=5.86e-5, glen=62.2, tlen=223, kl=0.0817, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:21<00:18,  1.17it/s, pg=-0.0477, ret=5.86e-5, glen=62.2, tlen=223, kl=0.0817, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 25/47 [00:22<00:18,  1.17it/s, pg=-0.0285, ret=3.47e-5, glen=61.3, tlen=222, kl=0.0685, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:22<00:18,  1.14it/s, pg=-0.0285, ret=3.47e-5, glen=61.3, tlen=222, kl=0.0685, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 26/47 [00:23<00:18,  1.14it/s, pg=-0.0831, ret=9.68e-5, glen=63.3, tlen=224, kl=0.0227, act_lr=1e-6, ent=0.998]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:23<00:17,  1.15it/s, pg=-0.0831, ret=9.68e-5, glen=63.3, tlen=224, kl=0.0227, act_lr=1e-6, ent=0.998]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/47 [00:24<00:17,  1.15it/s, pg=-0.0473, ret=5.85e-5, glen=62.2, tlen=223, kl=0.0236, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:24<00:16,  1.16it/s, pg=-0.0473, ret=5.85e-5, glen=62.2, tlen=223, kl=0.0236, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 28/47 [00:25<00:16,  1.16it/s, pg=0.0266, ret=-4.83e-5, glen=63.1, tlen=224, kl=0.02, act_lr=1e-6, ent=0.977]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:25<00:17,  1.06it/s, pg=0.0266, ret=-4.83e-5, glen=63.1, tlen=224, kl=0.02, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 29/47 [00:26<00:17,  1.06it/s, pg=0.0439, ret=-4.2e-5, glen=63.7, tlen=225, kl=0.0492, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:26<00:15,  1.09it/s, pg=0.0439, ret=-4.2e-5, glen=63.7, tlen=225, kl=0.0492, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 30/47 [00:27<00:15,  1.09it/s, pg=0.0148, ret=-2.48e-5, glen=63.4, tlen=224, kl=0.0393, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:27<00:14,  1.12it/s, pg=0.0148, ret=-2.48e-5, glen=63.4, tlen=224, kl=0.0393, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 31/47 [00:28<00:14,  1.12it/s, pg=-0.0941, ret=0.000113, glen=63.8, tlen=225, kl=0.205, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.13it/s, pg=-0.0941, ret=0.000113, glen=63.8, tlen=225, kl=0.205, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 32/47 [00:28<00:13,  1.13it/s, pg=-0.0408, ret=5.47e-5, glen=64.3, tlen=225, kl=0.0239, act_lr=1e-6, ent=0.968]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:28<00:12,  1.15it/s, pg=-0.0408, ret=5.47e-5, glen=64.3, tlen=225, kl=0.0239, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 33/47 [00:29<00:12,  1.15it/s, pg=-0.037, ret=4.29e-5, glen=62.1, tlen=223, kl=0.0213, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:29<00:11,  1.15it/s, pg=-0.037, ret=4.29e-5, glen=62.1, tlen=223, kl=0.0213, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 34/47 [00:30<00:11,  1.15it/s, pg=-0.0486, ret=6.06e-5, glen=60.2, tlen=221, kl=0.0811, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:30<00:10,  1.16it/s, pg=-0.0486, ret=6.06e-5, glen=60.2, tlen=221, kl=0.0811, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 35/47 [00:31<00:10,  1.16it/s, pg=-0.0639, ret=8.11e-5, glen=61.9, tlen=222, kl=0.0299, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:31<00:09,  1.17it/s, pg=-0.0639, ret=8.11e-5, glen=61.9, tlen=222, kl=0.0299, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 36/47 [00:32<00:09,  1.17it/s, pg=-0.0707, ret=9.1e-5, glen=63.2, tlen=224, kl=0.0242, act_lr=1e-6, ent=0.984] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:32<00:08,  1.17it/s, pg=-0.0707, ret=9.1e-5, glen=63.2, tlen=224, kl=0.0242, act_lr=1e-6, ent=0.984]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 37/47 [00:33<00:08,  1.17it/s, pg=-0.0759, ret=9.26e-5, glen=62.8, tlen=224, kl=0.0242, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=-0.0759, ret=9.26e-5, glen=62.8, tlen=224, kl=0.0242, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 38/47 [00:33<00:07,  1.17it/s, pg=0.0287, ret=-6.64e-5, glen=62.1, tlen=223, kl=0.0317, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:33<00:06,  1.17it/s, pg=0.0287, ret=-6.64e-5, glen=62.1, tlen=223, kl=0.0317, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 39/47 [00:34<00:06,  1.17it/s, pg=0.249, ret=-0.000258, glen=63.3, tlen=224, kl=0.0432, act_lr=1e-6, ent=0.998]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:34<00:05,  1.17it/s, pg=0.249, ret=-0.000258, glen=63.3, tlen=224, kl=0.0432, act_lr=1e-6, ent=0.998]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 40/47 [00:35<00:05,  1.17it/s, pg=-0.0365, ret=4.6e-5, glen=62, tlen=223, kl=0.0298, act_lr=1e-6, ent=0.957]   Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:35<00:05,  1.17it/s, pg=-0.0365, ret=4.6e-5, glen=62, tlen=223, kl=0.0298, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 41/47 [00:36<00:05,  1.17it/s, pg=0.0427, ret=-4.76e-5, glen=62.4, tlen=223, kl=0.0606, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:36<00:04,  1.17it/s, pg=0.0427, ret=-4.76e-5, glen=62.4, tlen=223, kl=0.0606, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 42/47 [00:37<00:04,  1.17it/s, pg=0.154, ret=-0.000186, glen=64.1, tlen=225, kl=0.0233, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:37<00:03,  1.17it/s, pg=0.154, ret=-0.000186, glen=64.1, tlen=225, kl=0.0233, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 43/47 [00:38<00:03,  1.17it/s, pg=-0.0376, ret=5.09e-5, glen=62.1, tlen=223, kl=0.024, act_lr=1e-6, ent=0.969] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:38<00:02,  1.17it/s, pg=-0.0376, ret=5.09e-5, glen=62.1, tlen=223, kl=0.024, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 44/47 [00:39<00:02,  1.17it/s, pg=-0.049, ret=6.29e-5, glen=62.2, tlen=223, kl=0.0594, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.17it/s, pg=-0.049, ret=6.29e-5, glen=62.2, tlen=223, kl=0.0594, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 45/47 [00:39<00:01,  1.17it/s, pg=-0.0363, ret=4.31e-5, glen=63, tlen=223, kl=0.0242, act_lr=1e-6, ent=1]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:39<00:00,  1.17it/s, pg=-0.0363, ret=4.31e-5, glen=63, tlen=223, kl=0.0242, act_lr=1e-6, ent=1]
2025-07-24 21:37:32.031 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.87s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.17it/s, pg=0.0467, ret=-5.24e-5, glen=62.2, tlen=222, kl=0.0229, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 46/47 [00:40<00:00,  1.13it/s, pg=0.0467, ret=-5.24e-5, glen=62.2, tlen=222, kl=0.0229, act_lr=1e-6, ent=0.976]
2025-07-24 21:37:32.718 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-24 21:37:34.718 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.00s
2025-07-24 21:37:35.052 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.01s
2025-07-24 21:37:35.058 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0013883570407299285, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9742627473587685, 'kl': 0.037164241709607714, 'response_length': 62.690059174882606, 'total_length': 223.427298687874, 'teacher_total_length': 235.38878615359042, 'return': -1.3240270608877564e-06, 'policy_update_steps': 1.0}
Episode [11/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [20:47<03:49, 114.59s/it]2025-07-24 21:37:35.105 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:38:11.304 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:38:11.483 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:38:11.483 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.38s
2025-07-24 21:38:13.262 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 62.0302,std_num_tokens: 12.2464,avg_correct_num_tokens: 62.0151,std_correct_num_tokens: 12.2331,avg_incorrect_num_tokens: 67.3913,std_incorrect_num_tokens: 15.3955
2025-07-24 21:38:13.690 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.21s
2025-07-24 21:38:16.553 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.86s
2025-07-24 21:38:40.158 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:38:40.158 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.60s
2025-07-24 21:38:41.391 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 21:38:41.391 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.171243955310353e-05, avg_kl: 0.03436212954313859, avg_response_length: 61.987459037614904, avg_orm_score: 0.0, avg_custom_rewards: 1.171243955310353e-05
2025-07-24 21:38:41.424 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter141_replay_buffer.jsonl
2025-07-24 21:38:42.710 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.29s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0768, ret=9.18e-5, glen=64.6, tlen=225, kl=0.0385, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0768, ret=9.18e-5, glen=64.6, tlen=225, kl=0.0385, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0625, ret=7.08e-5, glen=61.5, tlen=222, kl=0.0815, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0625, ret=7.08e-5, glen=61.5, tlen=222, kl=0.0815, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0692, ret=7.82e-5, glen=64.2, tlen=224, kl=0.0229, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.13it/s, pg=-0.0692, ret=7.82e-5, glen=64.2, tlen=224, kl=0.0229, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.13it/s, pg=-0.032, ret=3.67e-5, glen=58.3, tlen=218, kl=0.0556, act_lr=1e-6, ent=0.983] Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=-0.032, ret=3.67e-5, glen=58.3, tlen=218, kl=0.0556, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=0.0446, ret=-9.07e-5, glen=61.8, tlen=222, kl=0.0218, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.11it/s, pg=0.0446, ret=-9.07e-5, glen=61.8, tlen=222, kl=0.0218, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.11it/s, pg=0.0552, ret=-7.46e-5, glen=62.2, tlen=222, kl=0.0421, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.11it/s, pg=0.0552, ret=-7.46e-5, glen=62.2, tlen=222, kl=0.0421, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.11it/s, pg=0.203, ret=-0.000172, glen=63.2, tlen=224, kl=0.024, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:35,  1.11it/s, pg=0.203, ret=-0.000172, glen=63.2, tlen=224, kl=0.024, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:35,  1.11it/s, pg=-0.0757, ret=8.59e-5, glen=62, tlen=222, kl=0.0297, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=-0.0757, ret=8.59e-5, glen=62, tlen=222, kl=0.0297, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.13it/s, pg=-0.0496, ret=5.65e-5, glen=61.8, tlen=222, kl=0.024, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=-0.0496, ret=5.65e-5, glen=61.8, tlen=222, kl=0.024, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=-0.0342, ret=3.47e-5, glen=60.9, tlen=221, kl=0.0458, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.13it/s, pg=-0.0342, ret=3.47e-5, glen=60.9, tlen=221, kl=0.0458, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.13it/s, pg=0.0363, ret=-6.11e-5, glen=62.3, tlen=222, kl=0.0183, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.14it/s, pg=0.0363, ret=-6.11e-5, glen=62.3, tlen=222, kl=0.0183, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.14it/s, pg=0.0952, ret=-6.71e-5, glen=63, tlen=223, kl=0.0284, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=0.0952, ret=-6.71e-5, glen=63, tlen=223, kl=0.0284, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=0.057, ret=-7.59e-5, glen=62.5, tlen=222, kl=0.0258, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=0.057, ret=-7.59e-5, glen=62.5, tlen=222, kl=0.0258, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.049, ret=5.53e-5, glen=62.4, tlen=223, kl=0.0375, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.049, ret=5.53e-5, glen=62.4, tlen=223, kl=0.0375, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=-0.0589, ret=6.75e-5, glen=61.8, tlen=222, kl=0.0299, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0589, ret=6.75e-5, glen=61.8, tlen=222, kl=0.0299, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.17it/s, pg=-0.0847, ret=9.5e-5, glen=62.8, tlen=223, kl=0.0302, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0847, ret=9.5e-5, glen=62.8, tlen=223, kl=0.0302, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0736, ret=8.76e-5, glen=62.8, tlen=223, kl=0.0193, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0736, ret=8.76e-5, glen=62.8, tlen=223, kl=0.0193, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0385, ret=4.32e-5, glen=62.2, tlen=222, kl=0.0259, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0385, ret=4.32e-5, glen=62.2, tlen=222, kl=0.0259, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=0.0857, ret=-6.17e-5, glen=60.9, tlen=221, kl=0.0283, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=0.0857, ret=-6.17e-5, glen=60.9, tlen=221, kl=0.0283, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0442, ret=4.91e-5, glen=62.5, tlen=222, kl=0.0222, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0442, ret=4.91e-5, glen=62.5, tlen=222, kl=0.0222, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.022, ret=-5.79e-5, glen=60.4, tlen=221, kl=0.0308, act_lr=1e-6, ent=0.949] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=0.022, ret=-5.79e-5, glen=60.4, tlen=221, kl=0.0308, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.0424, ret=-6.52e-5, glen=61.7, tlen=221, kl=0.0235, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.16it/s, pg=0.0424, ret=-6.52e-5, glen=61.7, tlen=221, kl=0.0235, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.16it/s, pg=-0.0568, ret=6.51e-5, glen=62.8, tlen=223, kl=0.0292, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:20,  1.13it/s, pg=-0.0568, ret=6.51e-5, glen=62.8, tlen=223, kl=0.0292, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:20,  1.13it/s, pg=-0.0512, ret=5.45e-5, glen=61.1, tlen=221, kl=0.0223, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:19,  1.15it/s, pg=-0.0512, ret=5.45e-5, glen=61.1, tlen=221, kl=0.0223, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:19,  1.15it/s, pg=0.17, ret=-0.000155, glen=64.6, tlen=224, kl=0.0218, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=0.17, ret=-0.000155, glen=64.6, tlen=224, kl=0.0218, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=-0.0562, ret=6.25e-5, glen=62.7, tlen=223, kl=0.0331, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.16it/s, pg=-0.0562, ret=6.25e-5, glen=62.7, tlen=223, kl=0.0331, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.16it/s, pg=-0.0663, ret=7.59e-5, glen=61.8, tlen=222, kl=0.0266, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=-0.0663, ret=7.59e-5, glen=61.8, tlen=222, kl=0.0266, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=-0.0621, ret=6.67e-5, glen=61.9, tlen=222, kl=0.0269, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0621, ret=6.67e-5, glen=61.9, tlen=222, kl=0.0269, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0471, ret=5.61e-5, glen=63.4, tlen=224, kl=0.0386, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0471, ret=5.61e-5, glen=63.4, tlen=224, kl=0.0386, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=0.0536, ret=-5.21e-5, glen=62.2, tlen=222, kl=0.027, act_lr=1e-6, ent=0.943] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.0536, ret=-5.21e-5, glen=62.2, tlen=222, kl=0.027, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0232, ret=2.28e-5, glen=60, tlen=220, kl=0.0288, act_lr=1e-6, ent=0.945] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0232, ret=2.28e-5, glen=60, tlen=220, kl=0.0288, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=0.0618, ret=-5.4e-5, glen=62, tlen=222, kl=0.0238, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=0.0618, ret=-5.4e-5, glen=62, tlen=222, kl=0.0238, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.11it/s, pg=-0.049, ret=5.51e-5, glen=61.1, tlen=222, kl=0.0212, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.13it/s, pg=-0.049, ret=5.51e-5, glen=61.1, tlen=222, kl=0.0212, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.0637, ret=7.64e-5, glen=61.3, tlen=221, kl=0.18, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.0637, ret=7.64e-5, glen=61.3, tlen=221, kl=0.18, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=0.171, ret=-0.000165, glen=63.1, tlen=223, kl=0.0281, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=0.171, ret=-0.000165, glen=63.1, tlen=223, kl=0.0281, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=0.0419, ret=-6.08e-5, glen=62.4, tlen=223, kl=0.0254, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=0.0419, ret=-6.08e-5, glen=62.4, tlen=223, kl=0.0254, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0695, ret=7.88e-5, glen=60.9, tlen=221, kl=0.0782, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0695, ret=7.88e-5, glen=60.9, tlen=221, kl=0.0782, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=0.0672, ret=-8.34e-5, glen=62.7, tlen=223, kl=0.0249, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0672, ret=-8.34e-5, glen=62.7, tlen=223, kl=0.0249, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0469, ret=5.02e-5, glen=61.7, tlen=222, kl=0.0232, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0469, ret=5.02e-5, glen=61.7, tlen=222, kl=0.0232, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.0714, ret=-7.68e-5, glen=61.2, tlen=221, kl=0.0196, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.0714, ret=-7.68e-5, glen=61.2, tlen=221, kl=0.0196, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=0.0278, ret=-4.73e-5, glen=62.5, tlen=222, kl=0.0268, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=0.0278, ret=-4.73e-5, glen=62.5, tlen=222, kl=0.0268, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0547, ret=6.17e-5, glen=60, tlen=220, kl=0.0275, act_lr=1e-6, ent=0.924] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0547, ret=6.17e-5, glen=60, tlen=220, kl=0.0275, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0391, ret=-6.59e-5, glen=60.8, tlen=221, kl=0.0241, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0391, ret=-6.59e-5, glen=60.8, tlen=221, kl=0.0241, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.058, ret=6.79e-5, glen=61.2, tlen=222, kl=0.0255, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.058, ret=6.79e-5, glen=61.2, tlen=222, kl=0.0255, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=0.0558, ret=-6.83e-5, glen=62.8, tlen=223, kl=0.0544, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=0.0558, ret=-6.83e-5, glen=62.8, tlen=223, kl=0.0544, act_lr=1e-6, ent=0.946]
2025-07-24 21:39:23.025 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.18it/s, pg=0.0517, ret=-6.6e-5, glen=61.2, tlen=221, kl=0.0373, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=0.0517, ret=-6.6e-5, glen=61.2, tlen=221, kl=0.0373, act_lr=1e-6, ent=0.95]
2025-07-24 21:39:23.854 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:39:26.465 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.61s
2025-07-24 21:39:26.795 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.02s
2025-07-24 21:39:26.801 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -3.060050632642663e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9439102968443995, 'kl': 0.03436212954313859, 'response_length': 61.98745876809825, 'total_length': 222.03896597157353, 'teacher_total_length': 234.09487351127294, 'return': 5.25670143163196e-07, 'policy_update_steps': 1.0}
Episode [11/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [22:38<01:53, 113.72s/it]2025-07-24 21:39:26.807 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   1%|          | 1/172 [00:00<02:07,  1.34it/s, est. speed input: 234.01 toks/s, output: 37.44 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   1%|          | 2/172 [00:00<01:10,  2.40it/s, est. speed input: 380.74 toks/s, output: 68.64 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:02<00:00, 99.08it/s, est. speed input: 7324.18 toks/s, output: 2390.43 toks/s]Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:02<00:00, 125.52it/s, est. speed input: 8904.72 toks/s, output: 3036.18 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/172 [00:02<00:00, 86.84it/s, est. speed input: 10316.68 toks/s, output: 3734.22 toks/s]
2025-07-24 21:39:31.606 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 326.5721,strategyqa_test/accuracy: 0.5459,eval_accuracy: 0.5459
2025-07-24 21:39:31.864 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:39:50.243 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:39:50.422 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:39:50.423 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 18.56s
2025-07-24 21:39:51.491 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0146,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 60.4781,std_num_tokens: 11.9978,avg_correct_num_tokens: 60.4762,std_correct_num_tokens: 11.9969,avg_incorrect_num_tokens: 62.5000,std_incorrect_num_tokens: 12.7574
2025-07-24 21:39:51.740 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.32s
2025-07-24 21:39:53.177 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.44s
2025-07-24 21:40:05.840 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 96
2025-07-24 21:40:05.840 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.66s
2025-07-24 21:40:06.937 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.64s
2025-07-24 21:40:06.938 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.4717800089177521e-05, avg_kl: 0.05355707804361979, avg_response_length: 60.49447317918142, avg_orm_score: 0.0, avg_custom_rewards: 1.4717800089177521e-05
2025-07-24 21:40:06.963 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter142_replay_buffer.jsonl
2025-07-24 21:40:07.629 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.67s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:02<00:00, 73.63it/s, est. speed input: 9215.84 toks/s, output: 3303.41 toks/s][32m [repeated 52x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:04<00:00, 42.59it/s, est. speed input: 7720.41 toks/s, output: 2871.42 toks/s][32m [repeated 9x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, pg=-0.0287, ret=1.76e-5, glen=60.8, tlen=221, kl=0.0266, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:00<00:22,  1.03it/s, pg=-0.0287, ret=1.76e-5, glen=60.8, tlen=221, kl=0.0266, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:01<00:22,  1.03it/s, pg=-0.0135, ret=9.65e-6, glen=61.2, tlen=221, kl=0.055, act_lr=1e-6, ent=0.975] Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:01<00:19,  1.10it/s, pg=-0.0135, ret=9.65e-6, glen=61.2, tlen=221, kl=0.055, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:02<00:19,  1.10it/s, pg=-0.0475, ret=7.12e-5, glen=59.7, tlen=220, kl=0.362, act_lr=1e-6, ent=0.988]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:02<00:18,  1.14it/s, pg=-0.0475, ret=7.12e-5, glen=59.7, tlen=220, kl=0.362, act_lr=1e-6, ent=0.988]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:03<00:18,  1.14it/s, pg=0.11, ret=-0.000104, glen=59.3, tlen=219, kl=0.034, act_lr=1e-6, ent=0.978] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:03<00:17,  1.15it/s, pg=0.11, ret=-0.000104, glen=59.3, tlen=219, kl=0.034, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:04<00:17,  1.15it/s, pg=-0.0337, ret=2.27e-5, glen=59.6, tlen=219, kl=0.03, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:04<00:16,  1.16it/s, pg=-0.0337, ret=2.27e-5, glen=59.6, tlen=219, kl=0.03, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:05<00:16,  1.16it/s, pg=-0.0428, ret=2.89e-5, glen=60.7, tlen=221, kl=0.0366, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:05<00:15,  1.16it/s, pg=-0.0428, ret=2.89e-5, glen=60.7, tlen=221, kl=0.0366, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:06<00:15,  1.16it/s, pg=-0.0161, ret=1.16e-5, glen=60.3, tlen=221, kl=0.035, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:06<00:15,  1.13it/s, pg=-0.0161, ret=1.16e-5, glen=60.3, tlen=221, kl=0.035, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:07<00:15,  1.13it/s, pg=-0.0171, ret=1.16e-5, glen=60.2, tlen=220, kl=0.0669, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:14,  1.14it/s, pg=-0.0171, ret=1.16e-5, glen=60.2, tlen=220, kl=0.0669, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:14,  1.14it/s, pg=-0.0316, ret=2.08e-5, glen=60, tlen=220, kl=0.0391, act_lr=1e-6, ent=0.973]  Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:07<00:13,  1.15it/s, pg=-0.0316, ret=2.08e-5, glen=60, tlen=220, kl=0.0391, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:08<00:13,  1.15it/s, pg=-0.0275, ret=1.78e-5, glen=61.3, tlen=221, kl=0.0506, act_lr=1e-6, ent=0.989]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:08<00:12,  1.14it/s, pg=-0.0275, ret=1.78e-5, glen=61.3, tlen=221, kl=0.0506, act_lr=1e-6, ent=0.989]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:09<00:12,  1.14it/s, pg=-0.0474, ret=3.09e-5, glen=60.5, tlen=221, kl=0.0358, act_lr=1e-6, ent=0.98] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:09<00:11,  1.15it/s, pg=-0.0474, ret=3.09e-5, glen=60.5, tlen=221, kl=0.0358, act_lr=1e-6, ent=0.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:10<00:11,  1.15it/s, pg=-0.0287, ret=1.74e-5, glen=61.2, tlen=221, kl=0.0533, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:10<00:10,  1.16it/s, pg=-0.0287, ret=1.74e-5, glen=61.2, tlen=221, kl=0.0533, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:11<00:10,  1.16it/s, pg=-0.0412, ret=2.68e-5, glen=59.6, tlen=220, kl=0.0509, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:11<00:09,  1.16it/s, pg=-0.0412, ret=2.68e-5, glen=59.6, tlen=220, kl=0.0509, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:12<00:09,  1.16it/s, pg=0.169, ret=-9.84e-5, glen=59.9, tlen=220, kl=0.0261, act_lr=1e-6, ent=0.969] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:12<00:08,  1.17it/s, pg=0.169, ret=-9.84e-5, glen=59.9, tlen=220, kl=0.0261, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:13<00:08,  1.17it/s, pg=-0.0385, ret=2.68e-5, glen=60.4, tlen=221, kl=0.0324, act_lr=1e-6, ent=0.997]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.15it/s, pg=-0.0385, ret=2.68e-5, glen=60.4, tlen=221, kl=0.0324, act_lr=1e-6, ent=0.997]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.15it/s, pg=-0.041, ret=2.74e-5, glen=62.4, tlen=223, kl=0.0441, act_lr=1e-6, ent=0.961] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:13<00:06,  1.16it/s, pg=-0.041, ret=2.74e-5, glen=62.4, tlen=223, kl=0.0441, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:14<00:06,  1.16it/s, pg=-0.0178, ret=1.18e-5, glen=61.8, tlen=222, kl=0.0299, act_lr=1e-6, ent=0.978]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:14<00:06,  1.16it/s, pg=-0.0178, ret=1.18e-5, glen=61.8, tlen=222, kl=0.0299, act_lr=1e-6, ent=0.978]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:15<00:06,  1.16it/s, pg=-0.0307, ret=1.96e-5, glen=60.4, tlen=221, kl=0.0349, act_lr=1e-6, ent=0.985]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:15<00:05,  1.16it/s, pg=-0.0307, ret=1.96e-5, glen=60.4, tlen=221, kl=0.0349, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:16<00:05,  1.16it/s, pg=0.218, ret=-0.000112, glen=60.6, tlen=221, kl=0.0492, act_lr=1e-6, ent=0.977]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:16<00:04,  1.17it/s, pg=0.218, ret=-0.000112, glen=60.6, tlen=221, kl=0.0492, act_lr=1e-6, ent=0.977]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:17<00:04,  1.17it/s, pg=-0.0312, ret=2.12e-5, glen=61.5, tlen=221, kl=0.0276, act_lr=1e-6, ent=0.974]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:17<00:03,  1.17it/s, pg=-0.0312, ret=2.12e-5, glen=61.5, tlen=221, kl=0.0276, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:18<00:03,  1.17it/s, pg=0.119, ret=-9.45e-5, glen=60.4, tlen=221, kl=0.0271, act_lr=1e-6, ent=0.985] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:18<00:02,  1.17it/s, pg=0.119, ret=-9.45e-5, glen=60.4, tlen=221, kl=0.0271, act_lr=1e-6, ent=0.985]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:19<00:02,  1.17it/s, pg=-0.0326, ret=2.31e-5, glen=60.4, tlen=220, kl=0.0493, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:19<00:01,  1.17it/s, pg=-0.0326, ret=2.31e-5, glen=60.4, tlen=220, kl=0.0493, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:19<00:01,  1.17it/s, pg=-0.0233, ret=1.56e-5, glen=59.8, tlen=220, kl=0.0305, act_lr=1e-6, ent=0.97]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:19<00:00,  1.18it/s, pg=-0.0233, ret=1.56e-5, glen=59.8, tlen=220, kl=0.0305, act_lr=1e-6, ent=0.97]
2025-07-24 21:40:28.671 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.18it/s, pg=-0.0337, ret=2.31e-5, glen=60.1, tlen=221, kl=0.0586, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.11it/s, pg=-0.0337, ret=2.31e-5, glen=60.1, tlen=221, kl=0.0586, act_lr=1e-6, ent=0.939]
2025-07-24 21:40:29.347 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 21:40:31.529 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.18s
2025-07-24 21:40:31.878 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 24.17s
2025-07-24 21:40:31.882 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0003681182861328125, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9735826055208842, 'kl': 0.05355707804361979, 'response_length': 60.494473139444985, 'total_length': 220.62471071879068, 'teacher_total_length': 232.6488577524821, 'return': 1.942123162734788e-06, 'policy_update_steps': 1.0}
Episode [11/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [23:43<00:00, 98.99s/it] 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,031] [INFO] [utils.py:782:see_memory_usage] MA 8.06 GB         Max_MA 8.06 GB         CA 9.1 GB         Max_CA 9 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,031] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.44 GB, percent = 21.5%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,032] [INFO] [config.py:1003:print]   amp_params ................... False2025-07-24 21:40:40.987 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee79d69ec00>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,033] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:16:48,034] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:18:40,403] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 21:16:47,308] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:20:31,348] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:22:20,495] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:24:10,656] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:26:05,565] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:31:47,562] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:33:40,773] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:37:32,024] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:39:23,018] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1436499)[0m [2025-07-24 21:40:38,563] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:38,768] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 4068, num_elems = 21.33B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,271] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,271] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,278] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,280] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,664] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,665] [INFO] [utils.py:782:see_memory_usage] MA 8.79 GB         Max_MA 13.75 GB         CA 9.83 GB         Max_CA 48 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,665] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.74 GB, percent = 22.0%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [12/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [11/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [23:52<00:00, 110.23s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,980] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,981] [INFO] [utils.py:782:see_memory_usage] MA 8.79 GB         Max_MA 8.79 GB         CA 9.83 GB         Max_CA 10 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,981] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.74 GB, percent = 22.0%huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee79d07c1d0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,983] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,984] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 21:40:41.301 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:41:17.648 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:41:17.833 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:41:17.833 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.53s
2025-07-24 21:41:19.654 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 62.2573,std_num_tokens: 12.7511,avg_correct_num_tokens: 62.2631,std_correct_num_tokens: 12.7553,avg_incorrect_num_tokens: 58.3333,std_incorrect_num_tokens: 8.5570
2025-07-24 21:41:20.063 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.23s
2025-07-24 21:41:22.610 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.55s
2025-07-24 21:41:46.721 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:41:46.721 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.11s
2025-07-24 21:41:48.055 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.88s
2025-07-24 21:41:48.055 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 4.845684325909647e-06, avg_kl: 0.0, avg_response_length: 62.275732537974484, avg_orm_score: 0.0, avg_custom_rewards: 4.845684325909647e-06
2025-07-24 21:41:48.090 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter143_replay_buffer.jsonl
2025-07-24 21:41:49.367 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.28s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0305, ret=2.74e-5, glen=62.1, tlen=223, kl=0, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:46,  1.03s/it, pg=-0.0305, ret=2.74e-5, glen=62.1, tlen=223, kl=0, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:46,  1.03s/it, pg=-0.0275, ret=2.35e-5, glen=61.4, tlen=222, kl=0, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.0275, ret=2.35e-5, glen=61.4, tlen=222, kl=0, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.032, ret=2.77e-5, glen=62.2, tlen=223, kl=0, act_lr=1e-6, ent=0.95]  Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.032, ret=2.77e-5, glen=62.2, tlen=223, kl=0, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0409, ret=3.59e-5, glen=62.1, tlen=223, kl=0, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:36,  1.14it/s, pg=-0.0409, ret=3.59e-5, glen=62.1, tlen=223, kl=0, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:36,  1.14it/s, pg=-0.0231, ret=2.12e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.94] Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=-0.0231, ret=2.12e-5, glen=60.8, tlen=221, kl=0, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=-0.0268, ret=2.4e-5, glen=63.4, tlen=224, kl=0, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.14it/s, pg=-0.0268, ret=2.4e-5, glen=63.4, tlen=224, kl=0, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.14it/s, pg=-0.0293, ret=2.74e-5, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.15it/s, pg=-0.0293, ret=2.74e-5, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.15it/s, pg=-0.0327, ret=2.75e-5, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0327, ret=2.75e-5, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=0.0816, ret=-9.35e-5, glen=61.9, tlen=223, kl=0, act_lr=1e-6, ent=0.95] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.16it/s, pg=0.0816, ret=-9.35e-5, glen=61.9, tlen=223, kl=0, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=-0.0443, ret=3.67e-5, glen=60.7, tlen=221, kl=0, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.17it/s, pg=-0.0443, ret=3.67e-5, glen=60.7, tlen=221, kl=0, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.17it/s, pg=-0.0419, ret=3.31e-5, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=-0.0419, ret=3.31e-5, glen=61.3, tlen=222, kl=0, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0344, ret=3.14e-5, glen=62, tlen=223, kl=0, act_lr=1e-6, ent=0.946]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=-0.0344, ret=3.14e-5, glen=62, tlen=223, kl=0, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=-0.0332, ret=2.76e-5, glen=63.1, tlen=224, kl=0, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0332, ret=2.76e-5, glen=63.1, tlen=224, kl=0, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=0.0764, ret=-8.01e-5, glen=62.3, tlen=223, kl=0, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=0.0764, ret=-8.01e-5, glen=62.3, tlen=223, kl=0, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.0517, ret=4.21e-5, glen=62.5, tlen=224, kl=0, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0517, ret=4.21e-5, glen=62.5, tlen=224, kl=0, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0226, ret=2.17e-5, glen=62.4, tlen=223, kl=0, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0226, ret=2.17e-5, glen=62.4, tlen=223, kl=0, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=0.0958, ret=-9.2e-5, glen=63.3, tlen=224, kl=0, act_lr=1e-6, ent=0.963] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=0.0958, ret=-9.2e-5, glen=63.3, tlen=224, kl=0, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0199, ret=1.99e-5, glen=63.3, tlen=224, kl=0, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0199, ret=1.99e-5, glen=63.3, tlen=224, kl=0, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0309, ret=2.76e-5, glen=61.7, tlen=223, kl=0, act_lr=1e-6, ent=0.983]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.17it/s, pg=-0.0309, ret=2.76e-5, glen=61.7, tlen=223, kl=0, act_lr=1e-6, ent=0.983]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.17it/s, pg=-0.0377, ret=3.49e-5, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.18it/s, pg=-0.0377, ret=3.49e-5, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.18it/s, pg=-0.04, ret=3.63e-5, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=0.968]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=-0.04, ret=3.63e-5, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.18it/s, pg=0.11, ret=-8.86e-5, glen=61.5, tlen=222, kl=0, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=0.11, ret=-8.86e-5, glen=61.5, tlen=222, kl=0, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=-0.0286, ret=2.58e-5, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.18it/s, pg=-0.0286, ret=2.58e-5, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=-0.0272, ret=2.51e-5, glen=61.6, tlen=222, kl=0, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=-0.0272, ret=2.51e-5, glen=61.6, tlen=222, kl=0, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=-0.0476, ret=3.83e-5, glen=62.9, tlen=224, kl=0, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=-0.0476, ret=3.83e-5, glen=62.9, tlen=224, kl=0, act_lr=1e-6, ent=0.969]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=0.103, ret=-0.000107, glen=62.9, tlen=224, kl=0, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:16,  1.18it/s, pg=0.103, ret=-0.000107, glen=62.9, tlen=224, kl=0, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:16,  1.18it/s, pg=-0.0465, ret=4.1e-5, glen=61.7, tlen=222, kl=0, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.18it/s, pg=-0.0465, ret=4.1e-5, glen=61.7, tlen=222, kl=0, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.18it/s, pg=0.252, ret=-0.000217, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.18it/s, pg=0.252, ret=-0.000217, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.18it/s, pg=-0.035, ret=3.18e-5, glen=63.4, tlen=224, kl=0, act_lr=1e-6, ent=0.968] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.035, ret=3.18e-5, glen=63.4, tlen=224, kl=0, act_lr=1e-6, ent=0.968]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0383, ret=3.55e-5, glen=63.4, tlen=224, kl=0, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0383, ret=3.55e-5, glen=63.4, tlen=224, kl=0, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=0.105, ret=-8.81e-5, glen=62.4, tlen=223, kl=0, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:26<00:13,  1.12it/s, pg=0.105, ret=-8.81e-5, glen=62.4, tlen=223, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0404, ret=3.7e-5, glen=61.8, tlen=223, kl=0, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=-0.0404, ret=3.7e-5, glen=61.8, tlen=223, kl=0, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.129, ret=-7.58e-5, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.129, ret=-7.58e-5, glen=60.9, tlen=221, kl=0, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0283, ret=2.61e-5, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0283, ret=2.61e-5, glen=62.8, tlen=224, kl=0, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=-0.0286, ret=2.58e-5, glen=63.9, tlen=224, kl=0, act_lr=1e-6, ent=0.96] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0286, ret=2.58e-5, glen=63.9, tlen=224, kl=0, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0459, ret=4.12e-5, glen=62.3, tlen=223, kl=0, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0459, ret=4.12e-5, glen=62.3, tlen=223, kl=0, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0447, ret=3.74e-5, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0447, ret=3.74e-5, glen=62.5, tlen=223, kl=0, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0416, ret=3.8e-5, glen=62.9, tlen=224, kl=0, act_lr=1e-6, ent=0.941] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:32<00:06,  1.17it/s, pg=-0.0416, ret=3.8e-5, glen=62.9, tlen=224, kl=0, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.275, ret=-0.000217, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.275, ret=-0.000217, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0248, ret=2.39e-5, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0248, ret=2.39e-5, glen=61.1, tlen=222, kl=0, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0443, ret=3.6e-5, glen=62.3, tlen=223, kl=0, act_lr=1e-6, ent=0.96]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.18it/s, pg=-0.0443, ret=3.6e-5, glen=62.3, tlen=223, kl=0, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.18it/s, pg=-0.0397, ret=3.51e-5, glen=61, tlen=222, kl=0, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0397, ret=3.51e-5, glen=61, tlen=222, kl=0, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0552, ret=4.85e-5, glen=64.9, tlen=226, kl=0, act_lr=1e-6, ent=0.979]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=-0.0552, ret=4.85e-5, glen=64.9, tlen=226, kl=0, act_lr=1e-6, ent=0.979]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=0.0712, ret=-6.71e-5, glen=61.7, tlen=222, kl=0, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:37<00:01,  1.18it/s, pg=0.0712, ret=-6.71e-5, glen=61.7, tlen=222, kl=0, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0396, ret=3.56e-5, glen=62.6, tlen=223, kl=0, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.18it/s, pg=-0.0396, ret=3.56e-5, glen=62.6, tlen=223, kl=0, act_lr=1e-6, ent=0.965]
2025-07-24 21:42:29.347 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.80s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0336, ret=2.82e-5, glen=63.6, tlen=224, kl=0, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0336, ret=2.82e-5, glen=63.6, tlen=224, kl=0, act_lr=1e-6, ent=0.953]
2025-07-24 21:42:30.175 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:42:32.716 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 21:42:33.124 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.70s
2025-07-24 21:42:33.147 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00020358873450237772, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9483062028884888, 'kl': 0.0, 'response_length': 62.275732537974484, 'total_length': 223.09012835958728, 'teacher_total_length': 235.11884705916694, 'return': 2.1875581771145453e-07, 'policy_update_steps': 1.0}

Episode [12/20]:   8%|‚ñä         | 1/13 [01:52<22:25, 112.16s/it][A2025-07-24 21:42:33.195 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:43:09.756 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:43:09.931 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:43:09.932 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.74s
2025-07-24 21:43:11.760 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0000,avg_pass_at_n: 1.0000,avg_num_tokens: 62.7656,std_num_tokens: 12.1708,avg_correct_num_tokens: 62.7627,std_correct_num_tokens: 12.1673,avg_incorrect_num_tokens: 64.7500,std_incorrect_num_tokens: 14.2017
2025-07-24 21:43:12.230 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.30s
2025-07-24 21:43:15.037 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.81s
2025-07-24 21:43:38.645 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:43:38.645 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.61s
2025-07-24 21:43:39.970 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.91s
2025-07-24 21:43:39.971 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 8.316688447097398e-07, avg_kl: 0.0013907059379245925, avg_response_length: 62.78872139557548, avg_orm_score: 0.0, avg_custom_rewards: 8.316688447097398e-07
2025-07-24 21:43:40.008 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter144_replay_buffer.jsonl
2025-07-24 21:43:41.361 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=0.102, ret=-8.09e-5, glen=62.6, tlen=222, kl=0.00171, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:46,  1.03s/it, pg=0.102, ret=-8.09e-5, glen=62.6, tlen=222, kl=0.00171, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:46,  1.03s/it, pg=-0.0601, ret=5.21e-5, glen=63.9, tlen=224, kl=0.00154, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:41,  1.07it/s, pg=-0.0601, ret=5.21e-5, glen=63.9, tlen=224, kl=0.00154, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:41,  1.07it/s, pg=-0.0512, ret=4.36e-5, glen=62.8, tlen=223, kl=0.00132, act_lr=1e-6, ent=0.981]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.0512, ret=4.36e-5, glen=62.8, tlen=223, kl=0.00132, act_lr=1e-6, ent=0.981]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0449, ret=3.55e-5, glen=62.1, tlen=222, kl=0.00121, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:36,  1.14it/s, pg=-0.0449, ret=3.55e-5, glen=62.1, tlen=222, kl=0.00121, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:36,  1.14it/s, pg=-0.0727, ret=6.05e-5, glen=65.1, tlen=226, kl=0.00129, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:35,  1.15it/s, pg=-0.0727, ret=6.05e-5, glen=65.1, tlen=226, kl=0.00129, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:35,  1.15it/s, pg=0.334, ret=-0.000212, glen=62.8, tlen=223, kl=0.00128, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.16it/s, pg=0.334, ret=-0.000212, glen=62.8, tlen=223, kl=0.00128, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.16it/s, pg=-0.00744, ret=6.05e-6, glen=64.2, tlen=224, kl=0.00115, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=-0.00744, ret=6.05e-6, glen=64.2, tlen=224, kl=0.00115, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=-0.0128, ret=9.65e-6, glen=62.7, tlen=223, kl=0.00145, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.14it/s, pg=-0.0128, ret=9.65e-6, glen=62.7, tlen=223, kl=0.00145, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.14it/s, pg=-0.0409, ret=3.12e-5, glen=62.6, tlen=223, kl=0.0016, act_lr=1e-6, ent=0.959] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:32,  1.15it/s, pg=-0.0409, ret=3.12e-5, glen=62.6, tlen=223, kl=0.0016, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=-0.0102, ret=9.82e-6, glen=63.1, tlen=223, kl=0.00133, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=-0.0102, ret=9.82e-6, glen=63.1, tlen=223, kl=0.00133, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=0.108, ret=-0.000114, glen=62.3, tlen=222, kl=0.00144, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=0.108, ret=-0.000114, glen=62.3, tlen=222, kl=0.00144, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0501, ret=3.28e-5, glen=61.8, tlen=222, kl=0.00152, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=-0.0501, ret=3.28e-5, glen=61.8, tlen=222, kl=0.00152, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=-0.0111, ret=1.21e-5, glen=62.8, tlen=223, kl=0.00135, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0111, ret=1.21e-5, glen=62.8, tlen=223, kl=0.00135, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0263, ret=2.59e-5, glen=63, tlen=223, kl=0.00138, act_lr=1e-6, ent=0.951]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0263, ret=2.59e-5, glen=63, tlen=223, kl=0.00138, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=-0.0278, ret=2.7e-5, glen=62, tlen=222, kl=0.00127, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:27,  1.14it/s, pg=-0.0278, ret=2.7e-5, glen=62, tlen=222, kl=0.00127, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:27,  1.14it/s, pg=0.102, ret=-0.000103, glen=64, tlen=224, kl=0.00137, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.15it/s, pg=0.102, ret=-0.000103, glen=64, tlen=224, kl=0.00137, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.15it/s, pg=-0.0518, ret=3.94e-5, glen=62.8, tlen=223, kl=0.00138, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:25,  1.13it/s, pg=-0.0518, ret=3.94e-5, glen=62.8, tlen=223, kl=0.00138, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.13it/s, pg=-0.0302, ret=2.31e-5, glen=62.1, tlen=222, kl=0.00126, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.14it/s, pg=-0.0302, ret=2.31e-5, glen=62.1, tlen=222, kl=0.00126, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.14it/s, pg=-0.0455, ret=3.8e-5, glen=63.6, tlen=224, kl=0.00115, act_lr=1e-6, ent=0.971] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.15it/s, pg=-0.0455, ret=3.8e-5, glen=63.6, tlen=224, kl=0.00115, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.15it/s, pg=-0.0246, ret=2.18e-5, glen=63.8, tlen=224, kl=0.0014, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.15it/s, pg=-0.0246, ret=2.18e-5, glen=63.8, tlen=224, kl=0.0014, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.15it/s, pg=-0.0295, ret=2.4e-5, glen=65, tlen=225, kl=0.00133, act_lr=1e-6, ent=0.959]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.16it/s, pg=-0.0295, ret=2.4e-5, glen=65, tlen=225, kl=0.00133, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.16it/s, pg=-0.0275, ret=2.51e-5, glen=63.1, tlen=223, kl=0.0014, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:21,  1.14it/s, pg=-0.0275, ret=2.51e-5, glen=63.1, tlen=223, kl=0.0014, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:21,  1.14it/s, pg=-0.029, ret=2.32e-5, glen=62.5, tlen=223, kl=0.00179, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:20,  1.15it/s, pg=-0.029, ret=2.32e-5, glen=62.5, tlen=223, kl=0.00179, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:20,  1.15it/s, pg=-0.0536, ret=4.74e-5, glen=63.5, tlen=224, kl=0.00124, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:19,  1.16it/s, pg=-0.0536, ret=4.74e-5, glen=63.5, tlen=224, kl=0.00124, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:19,  1.16it/s, pg=-0.0251, ret=2.4e-5, glen=63, tlen=223, kl=0.00135, act_lr=1e-6, ent=0.94]    Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.0251, ret=2.4e-5, glen=63, tlen=223, kl=0.00135, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=0.239, ret=-0.000182, glen=62.6, tlen=222, kl=0.00146, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.239, ret=-0.000182, glen=62.6, tlen=222, kl=0.00146, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0798, ret=5.71e-5, glen=63.2, tlen=223, kl=0.00134, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0798, ret=5.71e-5, glen=63.2, tlen=223, kl=0.00134, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0519, ret=4.2e-5, glen=62.3, tlen=222, kl=0.00145, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0519, ret=4.2e-5, glen=62.3, tlen=222, kl=0.00145, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.00869, ret=8.07e-6, glen=61.9, tlen=223, kl=0.00134, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.00869, ret=8.07e-6, glen=61.9, tlen=223, kl=0.00134, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.00787, ret=7.89e-6, glen=61.6, tlen=222, kl=0.00155, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.00787, ret=7.89e-6, glen=61.6, tlen=222, kl=0.00155, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=0.0934, ret=-0.00011, glen=62.1, tlen=222, kl=0.00135, act_lr=1e-6, ent=0.937] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.0934, ret=-0.00011, glen=62.1, tlen=222, kl=0.00135, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.12it/s, pg=-0.0354, ret=2.72e-5, glen=62.3, tlen=222, kl=0.00134, act_lr=1e-6, ent=0.973]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0354, ret=2.72e-5, glen=62.3, tlen=222, kl=0.00134, act_lr=1e-6, ent=0.973]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.069, ret=-6.07e-5, glen=61.6, tlen=222, kl=0.00132, act_lr=1e-6, ent=0.939] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.069, ret=-6.07e-5, glen=61.6, tlen=222, kl=0.00132, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0347, ret=2.8e-5, glen=64.3, tlen=225, kl=0.00122, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0347, ret=2.8e-5, glen=64.3, tlen=225, kl=0.00122, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=0.107, ret=-8.27e-5, glen=63.6, tlen=224, kl=0.00135, act_lr=1e-6, ent=0.971]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.107, ret=-8.27e-5, glen=63.6, tlen=224, kl=0.00135, act_lr=1e-6, ent=0.971]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=0.131, ret=-7.64e-5, glen=62.5, tlen=222, kl=0.00187, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=0.131, ret=-7.64e-5, glen=62.5, tlen=222, kl=0.00187, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0834, ret=6.68e-5, glen=61.7, tlen=222, kl=0.00158, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0834, ret=6.68e-5, glen=61.7, tlen=222, kl=0.00158, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=0.0964, ret=-0.000111, glen=63.5, tlen=223, kl=0.00152, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.0964, ret=-0.000111, glen=63.5, tlen=223, kl=0.00152, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0534, ret=4.19e-5, glen=60.4, tlen=220, kl=0.00131, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0534, ret=4.19e-5, glen=60.4, tlen=220, kl=0.00131, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0329, ret=2.95e-5, glen=63.3, tlen=224, kl=0.00124, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0329, ret=2.95e-5, glen=63.3, tlen=224, kl=0.00124, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0593, ret=4.91e-5, glen=62.1, tlen=222, kl=0.00135, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0593, ret=4.91e-5, glen=62.1, tlen=222, kl=0.00135, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0163, ret=1.16e-5, glen=62.7, tlen=223, kl=0.00121, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.18it/s, pg=-0.0163, ret=1.16e-5, glen=62.7, tlen=223, kl=0.00121, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.18it/s, pg=-0.0374, ret=2.52e-5, glen=62.2, tlen=223, kl=0.00137, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=-0.0374, ret=2.52e-5, glen=62.2, tlen=223, kl=0.00137, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.18it/s, pg=-0.0355, ret=3.09e-5, glen=61.6, tlen=222, kl=0.00144, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0355, ret=3.09e-5, glen=61.6, tlen=222, kl=0.00144, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=-0.0671, ret=6.34e-5, glen=62.7, tlen=223, kl=0.00144, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0671, ret=6.34e-5, glen=62.7, tlen=223, kl=0.00144, act_lr=1e-6, ent=0.952]
2025-07-24 21:44:21.651 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0363, ret=3.35e-5, glen=62.9, tlen=223, kl=0.00136, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0363, ret=3.35e-5, glen=62.9, tlen=223, kl=0.00136, act_lr=1e-6, ent=0.923]
2025-07-24 21:44:22.501 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 21:44:25.067 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 21:44:25.407 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.94s
2025-07-24 21:44:25.415 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00021014006241508153, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9474312844483749, 'kl': 0.0013907059379245925, 'response_length': 62.788720835810125, 'total_length': 222.88935288139012, 'teacher_total_length': 234.89991528054944, 'return': 3.7747964676742886e-08, 'policy_update_steps': 1.0}

Episode [12/20]:  15%|‚ñà‚ñå        | 2/13 [03:44<20:34, 112.22s/it][A2025-07-24 21:44:25.462 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:45:01.288 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:45:01.474 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 21:45:01.474 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.01s
2025-07-24 21:45:03.338 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 60.9852,std_num_tokens: 11.9975,avg_correct_num_tokens: 60.9784,std_correct_num_tokens: 11.9937,avg_incorrect_num_tokens: 67.2222,std_incorrect_num_tokens: 13.6445
2025-07-24 21:45:03.648 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.17s
2025-07-24 21:45:06.576 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.93s
2025-07-24 21:45:30.262 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:45:30.262 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.69s
2025-07-24 21:45:31.514 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.82s
2025-07-24 21:45:31.515 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.3954529300372784e-06, avg_kl: 0.002461209323236851, avg_response_length: 60.99380115863404, avg_orm_score: 0.0, avg_custom_rewards: 3.3954529300372784e-06
2025-07-24 21:45:31.549 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter145_replay_buffer.jsonl
2025-07-24 21:45:32.802 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0411, ret=2.67e-5, glen=61.2, tlen=221, kl=0.00307, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.0411, ret=2.67e-5, glen=61.2, tlen=221, kl=0.00307, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.0336, ret=2.31e-5, glen=60.8, tlen=221, kl=0.00282, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.0336, ret=2.31e-5, glen=60.8, tlen=221, kl=0.00282, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.028, ret=1.91e-5, glen=61.2, tlen=221, kl=0.00233, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.028, ret=1.91e-5, glen=61.2, tlen=221, kl=0.00233, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.034, ret=2.31e-5, glen=60.9, tlen=221, kl=0.00211, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=-0.034, ret=2.31e-5, glen=60.9, tlen=221, kl=0.00211, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=-0.0168, ret=9.65e-6, glen=60.8, tlen=220, kl=0.00185, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:35,  1.14it/s, pg=-0.0168, ret=9.65e-6, glen=60.8, tlen=220, kl=0.00185, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:35,  1.14it/s, pg=-0.0308, ret=2.47e-5, glen=59.8, tlen=220, kl=0.00346, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:34,  1.15it/s, pg=-0.0308, ret=2.47e-5, glen=59.8, tlen=220, kl=0.00346, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:34,  1.15it/s, pg=0.112, ret=-0.000101, glen=60.7, tlen=221, kl=0.00284, act_lr=1e-6, ent=0.97] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.16it/s, pg=0.112, ret=-0.000101, glen=60.7, tlen=221, kl=0.00284, act_lr=1e-6, ent=0.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.16it/s, pg=-0.0549, ret=3.86e-5, glen=59.7, tlen=220, kl=0.0024, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0549, ret=3.86e-5, glen=59.7, tlen=220, kl=0.0024, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0373, ret=2.54e-5, glen=61.4, tlen=221, kl=0.00249, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.17it/s, pg=-0.0373, ret=2.54e-5, glen=61.4, tlen=221, kl=0.00249, act_lr=1e-6, ent=0.982]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.17it/s, pg=-0.0191, ret=1e-5, glen=60.4, tlen=220, kl=0.00209, act_lr=1e-6, ent=0.935]   Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.17it/s, pg=-0.0191, ret=1e-5, glen=60.4, tlen=220, kl=0.00209, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.17it/s, pg=-0.0629, ret=4.63e-5, glen=59.5, tlen=220, kl=0.00208, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.14it/s, pg=-0.0629, ret=4.63e-5, glen=59.5, tlen=220, kl=0.00208, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.14it/s, pg=-0.0395, ret=2.92e-5, glen=60.3, tlen=220, kl=0.00266, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.15it/s, pg=-0.0395, ret=2.92e-5, glen=60.3, tlen=220, kl=0.00266, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.15it/s, pg=-0.0318, ret=2.35e-5, glen=62.2, tlen=222, kl=0.00244, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0318, ret=2.35e-5, glen=62.2, tlen=222, kl=0.00244, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=0.131, ret=-0.000106, glen=62.3, tlen=222, kl=0.00237, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=0.131, ret=-0.000106, glen=62.3, tlen=222, kl=0.00237, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.0855, ret=-6.93e-5, glen=59.4, tlen=219, kl=0.00318, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.0855, ret=-6.93e-5, glen=59.4, tlen=219, kl=0.00318, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0201, ret=1.35e-5, glen=61.3, tlen=221, kl=0.00211, act_lr=1e-6, ent=0.963]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0201, ret=1.35e-5, glen=61.3, tlen=221, kl=0.00211, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0456, ret=3.09e-5, glen=61.4, tlen=221, kl=0.00217, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0456, ret=3.09e-5, glen=61.4, tlen=221, kl=0.00217, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.0415, ret=3.09e-5, glen=60.9, tlen=221, kl=0.00351, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.0415, ret=3.09e-5, glen=60.9, tlen=221, kl=0.00351, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0431, ret=2.89e-5, glen=62.6, tlen=222, kl=0.00208, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0431, ret=2.89e-5, glen=62.6, tlen=222, kl=0.00208, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0241, ret=1.52e-5, glen=61.4, tlen=221, kl=0.00204, act_lr=1e-6, ent=0.975]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0241, ret=1.52e-5, glen=61.4, tlen=221, kl=0.00204, act_lr=1e-6, ent=0.975]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0181, ret=1.16e-5, glen=60, tlen=220, kl=0.00264, act_lr=1e-6, ent=0.947]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.17it/s, pg=-0.0181, ret=1.16e-5, glen=60, tlen=220, kl=0.00264, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.17it/s, pg=0.189, ret=-0.000102, glen=60.4, tlen=221, kl=0.00209, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=0.189, ret=-0.000102, glen=60.4, tlen=221, kl=0.00209, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.17it/s, pg=-0.0252, ret=1.74e-5, glen=60.5, tlen=221, kl=0.00234, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.17it/s, pg=-0.0252, ret=1.74e-5, glen=60.5, tlen=221, kl=0.00234, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0321, ret=2.12e-5, glen=60.7, tlen=220, kl=0.0025, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0321, ret=2.12e-5, glen=60.7, tlen=220, kl=0.0025, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.048, ret=3.47e-5, glen=60.9, tlen=221, kl=0.00254, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=-0.048, ret=3.47e-5, glen=60.9, tlen=221, kl=0.00254, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=0.143, ret=-0.000103, glen=58.6, tlen=219, kl=0.00217, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=0.143, ret=-0.000103, glen=58.6, tlen=219, kl=0.00217, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0413, ret=2.89e-5, glen=61.5, tlen=221, kl=0.00346, act_lr=1e-6, ent=0.961]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0413, ret=2.89e-5, glen=61.5, tlen=221, kl=0.00346, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0223, ret=1.54e-5, glen=60.8, tlen=221, kl=0.00212, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0223, ret=1.54e-5, glen=60.8, tlen=221, kl=0.00212, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0332, ret=1.93e-5, glen=61.7, tlen=222, kl=0.00231, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.06it/s, pg=-0.0332, ret=1.93e-5, glen=61.7, tlen=222, kl=0.00231, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.06it/s, pg=-0.016, ret=9.65e-6, glen=62.5, tlen=222, kl=0.0022, act_lr=1e-6, ent=0.966]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.09it/s, pg=-0.016, ret=9.65e-6, glen=62.5, tlen=222, kl=0.0022, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.09it/s, pg=0.152, ret=-0.000106, glen=59.4, tlen=219, kl=0.00248, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:26<00:13,  1.12it/s, pg=0.152, ret=-0.000106, glen=59.4, tlen=219, kl=0.00248, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0259, ret=1.74e-5, glen=60.1, tlen=220, kl=0.00237, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.13it/s, pg=-0.0259, ret=1.74e-5, glen=60.1, tlen=220, kl=0.00237, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=0.172, ret=-7.61e-5, glen=61.5, tlen=221, kl=0.00196, act_lr=1e-6, ent=0.966] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.172, ret=-7.61e-5, glen=61.5, tlen=221, kl=0.00196, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.021, ret=1.35e-5, glen=60.6, tlen=220, kl=0.00278, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=-0.021, ret=1.35e-5, glen=60.6, tlen=220, kl=0.00278, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=-0.0114, ret=7.72e-6, glen=60.2, tlen=220, kl=0.00204, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0114, ret=7.72e-6, glen=60.2, tlen=220, kl=0.00204, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=0.0956, ret=-9.64e-5, glen=61.6, tlen=222, kl=0.0025, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=0.0956, ret=-9.64e-5, glen=61.6, tlen=222, kl=0.0025, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0349, ret=2.33e-5, glen=62.8, tlen=223, kl=0.0045, act_lr=1e-6, ent=0.95]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0349, ret=2.33e-5, glen=62.8, tlen=223, kl=0.0045, act_lr=1e-6, ent=0.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0383, ret=2.51e-5, glen=62.1, tlen=222, kl=0.00229, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:32<00:06,  1.17it/s, pg=-0.0383, ret=2.51e-5, glen=62.1, tlen=222, kl=0.00229, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0461, ret=2.99e-5, glen=63.2, tlen=223, kl=0.00221, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0461, ret=2.99e-5, glen=63.2, tlen=223, kl=0.00221, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.023, ret=1.58e-5, glen=61, tlen=221, kl=0.0022, act_lr=1e-6, ent=0.959]    Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.023, ret=1.58e-5, glen=61, tlen=221, kl=0.0022, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0567, ret=4.28e-5, glen=61.6, tlen=221, kl=0.0026, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0567, ret=4.28e-5, glen=61.6, tlen=221, kl=0.0026, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0402, ret=2.7e-5, glen=61.4, tlen=221, kl=0.00269, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0402, ret=2.7e-5, glen=61.4, tlen=221, kl=0.00269, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.0482, ret=3.42e-5, glen=60.7, tlen=220, kl=0.00224, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0482, ret=3.42e-5, glen=60.7, tlen=220, kl=0.00224, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0228, ret=1.58e-5, glen=61.2, tlen=221, kl=0.00235, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0228, ret=1.58e-5, glen=61.2, tlen=221, kl=0.00235, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.166, ret=-8.1e-5, glen=61.5, tlen=221, kl=0.00206, act_lr=1e-6, ent=0.94]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.17it/s, pg=0.166, ret=-8.1e-5, glen=61.5, tlen=221, kl=0.00206, act_lr=1e-6, ent=0.94]
2025-07-24 21:46:12.907 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.93s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0421, ret=3.08e-5, glen=59.8, tlen=220, kl=0.002, act_lr=1e-6, ent=0.982]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0421, ret=3.08e-5, glen=59.8, tlen=220, kl=0.002, act_lr=1e-6, ent=0.982]
2025-07-24 21:46:13.749 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:46:16.334 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 21:46:16.664 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.81s
2025-07-24 21:46:16.670 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00011327992314877717, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9518429960893549, 'kl': 0.0024723799332328463, 'response_length': 60.969378181125805, 'total_length': 220.8939630259638, 'teacher_total_length': 232.9205444999363, 'return': 4.324721845952571e-07, 'policy_update_steps': 1.0}

Episode [12/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:35<18:37, 111.78s/it][A2025-07-24 21:46:16.721 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:46:52.414 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:46:52.594 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:46:52.594 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.87s
2025-07-24 21:46:54.601 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0144,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 61.8621,std_num_tokens: 12.1867,avg_correct_num_tokens: 61.8502,std_correct_num_tokens: 12.1507,avg_incorrect_num_tokens: 71.6000,std_incorrect_num_tokens: 27.7424
2025-07-24 21:46:55.018 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.42s
2025-07-24 21:46:57.583 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.56s
2025-07-24 21:47:21.169 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:47:21.169 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.59s
2025-07-24 21:47:22.359 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 21:47:22.360 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -9.201641978258317e-07, avg_kl: 0.0026318492785177595, avg_response_length: 61.87479158828819, avg_orm_score: 0.0, avg_custom_rewards: -9.201641978258317e-07
2025-07-24 21:47:22.389 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter146_replay_buffer.jsonl
2025-07-24 21:47:23.644 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.042, ret=2.89e-5, glen=61.1, tlen=221, kl=0.00253, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.042, ret=2.89e-5, glen=61.1, tlen=221, kl=0.00253, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.0304, ret=2.12e-5, glen=60.4, tlen=220, kl=0.00261, act_lr=1e-6, ent=0.96]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.0304, ret=2.12e-5, glen=60.4, tlen=220, kl=0.00261, act_lr=1e-6, ent=0.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0411, ret=3.16e-5, glen=62.3, tlen=222, kl=0.0028, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.0411, ret=3.16e-5, glen=62.3, tlen=222, kl=0.0028, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0277, ret=1.96e-5, glen=63.4, tlen=224, kl=0.00253, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=-0.0277, ret=1.96e-5, glen=63.4, tlen=224, kl=0.00253, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=0.0913, ret=-9.26e-5, glen=60.9, tlen=221, kl=0.00245, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.12it/s, pg=0.0913, ret=-9.26e-5, glen=60.9, tlen=221, kl=0.00245, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.12it/s, pg=-0.0566, ret=4.31e-5, glen=61.6, tlen=222, kl=0.0025, act_lr=1e-6, ent=0.942] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=-0.0566, ret=4.31e-5, glen=61.6, tlen=222, kl=0.0025, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=-0.0216, ret=1.37e-5, glen=62.3, tlen=223, kl=0.00274, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.15it/s, pg=-0.0216, ret=1.37e-5, glen=62.3, tlen=223, kl=0.00274, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.15it/s, pg=-0.0451, ret=3.32e-5, glen=62.3, tlen=223, kl=0.00366, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=-0.0451, ret=3.32e-5, glen=62.3, tlen=223, kl=0.00366, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.13it/s, pg=-0.0285, ret=1.97e-5, glen=62.5, tlen=223, kl=0.00239, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:32,  1.14it/s, pg=-0.0285, ret=1.97e-5, glen=62.5, tlen=223, kl=0.00239, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.14it/s, pg=-0.0468, ret=3.48e-5, glen=61.1, tlen=221, kl=0.0037, act_lr=1e-6, ent=0.962] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.15it/s, pg=-0.0468, ret=3.48e-5, glen=61.1, tlen=221, kl=0.0037, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.15it/s, pg=-0.0522, ret=3.9e-5, glen=62.1, tlen=223, kl=0.0023, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.14it/s, pg=-0.0522, ret=3.9e-5, glen=62.1, tlen=223, kl=0.0023, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.14it/s, pg=-0.0314, ret=1.93e-5, glen=61.4, tlen=221, kl=0.00263, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.14it/s, pg=-0.0314, ret=1.93e-5, glen=61.4, tlen=221, kl=0.00263, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.14it/s, pg=-0.0507, ret=3.96e-5, glen=62.2, tlen=223, kl=0.00246, act_lr=1e-6, ent=0.967]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.15it/s, pg=-0.0507, ret=3.96e-5, glen=62.2, tlen=223, kl=0.00246, act_lr=1e-6, ent=0.967]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.15it/s, pg=-0.0246, ret=1.74e-5, glen=62, tlen=222, kl=0.00288, act_lr=1e-6, ent=0.955]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0246, ret=1.74e-5, glen=62, tlen=222, kl=0.00288, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.132, ret=-9.07e-5, glen=61.6, tlen=222, kl=0.00232, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.132, ret=-9.07e-5, glen=61.6, tlen=222, kl=0.00232, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.17it/s, pg=0.326, ret=-0.000114, glen=62.9, tlen=223, kl=0.00234, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=0.326, ret=-0.000114, glen=62.9, tlen=223, kl=0.00234, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0296, ret=1.93e-5, glen=61.6, tlen=222, kl=0.00257, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:25,  1.16it/s, pg=-0.0296, ret=1.93e-5, glen=61.6, tlen=222, kl=0.00257, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:25,  1.16it/s, pg=-0.0314, ret=2.34e-5, glen=61.2, tlen=222, kl=0.00257, act_lr=1e-6, ent=0.965]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=-0.0314, ret=2.34e-5, glen=61.2, tlen=222, kl=0.00257, act_lr=1e-6, ent=0.965]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=-0.0388, ret=2.76e-5, glen=61.4, tlen=221, kl=0.00262, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0388, ret=2.76e-5, glen=61.4, tlen=221, kl=0.00262, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0115, ret=5.79e-6, glen=62.2, tlen=222, kl=0.00251, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0115, ret=5.79e-6, glen=62.2, tlen=222, kl=0.00251, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=0.156, ret=-9.22e-5, glen=61.9, tlen=222, kl=0.00412, act_lr=1e-6, ent=0.943] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=0.156, ret=-9.22e-5, glen=61.9, tlen=222, kl=0.00412, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.18it/s, pg=-0.0382, ret=2.89e-5, glen=61.8, tlen=222, kl=0.00298, act_lr=1e-6, ent=0.964]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=-0.0382, ret=2.89e-5, glen=61.8, tlen=222, kl=0.00298, act_lr=1e-6, ent=0.964]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=-0.0468, ret=3.61e-5, glen=62.8, tlen=224, kl=0.00261, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:19,  1.18it/s, pg=-0.0468, ret=3.61e-5, glen=62.8, tlen=224, kl=0.00261, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=-0.00919, ret=3.86e-6, glen=61.6, tlen=221, kl=0.00255, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=-0.00919, ret=3.86e-6, glen=61.6, tlen=221, kl=0.00255, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=0.0565, ret=-7.65e-5, glen=62.1, tlen=222, kl=0.00227, act_lr=1e-6, ent=0.961] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.18it/s, pg=0.0565, ret=-7.65e-5, glen=62.1, tlen=222, kl=0.00227, act_lr=1e-6, ent=0.961]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.18it/s, pg=-0.0526, ret=3.67e-5, glen=61.6, tlen=222, kl=0.00229, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:16,  1.18it/s, pg=-0.0526, ret=3.67e-5, glen=61.6, tlen=222, kl=0.00229, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:16,  1.18it/s, pg=-0.0409, ret=2.97e-5, glen=64.5, tlen=225, kl=0.00264, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=-0.0409, ret=2.97e-5, glen=64.5, tlen=225, kl=0.00264, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=-0.0273, ret=1.76e-5, glen=61.8, tlen=222, kl=0.0024, act_lr=1e-6, ent=0.937] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.16it/s, pg=-0.0273, ret=1.76e-5, glen=61.8, tlen=222, kl=0.0024, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.16it/s, pg=-0.0413, ret=2.7e-5, glen=60.5, tlen=221, kl=0.00267, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.06it/s, pg=-0.0413, ret=2.7e-5, glen=60.5, tlen=221, kl=0.00267, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.06it/s, pg=-0.026, ret=1.96e-5, glen=62.1, tlen=222, kl=0.00261, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.026, ret=1.96e-5, glen=62.1, tlen=222, kl=0.00261, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0355, ret=2.52e-5, glen=61.5, tlen=222, kl=0.00266, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0355, ret=2.52e-5, glen=61.5, tlen=222, kl=0.00266, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0248, ret=1.54e-5, glen=62.2, tlen=222, kl=0.0023, act_lr=1e-6, ent=0.937] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.13it/s, pg=-0.0248, ret=1.54e-5, glen=62.2, tlen=222, kl=0.0023, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.13it/s, pg=0.07, ret=-8.49e-5, glen=60.6, tlen=221, kl=0.00263, act_lr=1e-6, ent=0.94]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.07, ret=-8.49e-5, glen=60.6, tlen=221, kl=0.00263, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0392, ret=2.93e-5, glen=61.1, tlen=221, kl=0.00262, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=-0.0392, ret=2.93e-5, glen=61.1, tlen=221, kl=0.00262, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=0.147, ret=-9.45e-5, glen=62.3, tlen=223, kl=0.00321, act_lr=1e-6, ent=0.947] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.147, ret=-9.45e-5, glen=62.3, tlen=223, kl=0.00321, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0546, ret=4.18e-5, glen=63.8, tlen=224, kl=0.00223, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0546, ret=4.18e-5, glen=63.8, tlen=224, kl=0.00223, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0583, ret=4.49e-5, glen=61.8, tlen=222, kl=0.00269, act_lr=1e-6, ent=0.942]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0583, ret=4.49e-5, glen=61.8, tlen=222, kl=0.00269, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=-0.0422, ret=3.35e-5, glen=61.3, tlen=221, kl=0.00257, act_lr=1e-6, ent=0.972]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0422, ret=3.35e-5, glen=61.3, tlen=221, kl=0.00257, act_lr=1e-6, ent=0.972]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0315, ret=2.17e-5, glen=61.5, tlen=221, kl=0.00256, act_lr=1e-6, ent=0.976]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0315, ret=2.17e-5, glen=61.5, tlen=221, kl=0.00256, act_lr=1e-6, ent=0.976]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.0937, ret=-0.0001, glen=61.8, tlen=222, kl=0.00245, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.0937, ret=-0.0001, glen=61.8, tlen=222, kl=0.00245, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0202, ret=1.38e-5, glen=60.3, tlen=220, kl=0.00226, act_lr=1e-6, ent=0.962]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0202, ret=1.38e-5, glen=60.3, tlen=220, kl=0.00226, act_lr=1e-6, ent=0.962]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0302, ret=2.12e-5, glen=61.8, tlen=222, kl=0.00274, act_lr=1e-6, ent=0.957]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.18it/s, pg=-0.0302, ret=2.12e-5, glen=61.8, tlen=222, kl=0.00274, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.18it/s, pg=0.0993, ret=-0.0001, glen=62.7, tlen=223, kl=0.00246, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.18it/s, pg=0.0993, ret=-0.0001, glen=62.7, tlen=223, kl=0.00246, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.18it/s, pg=-0.0353, ret=2.32e-5, glen=61.9, tlen=222, kl=0.00245, act_lr=1e-6, ent=0.966]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.15it/s, pg=-0.0353, ret=2.32e-5, glen=61.9, tlen=222, kl=0.00245, act_lr=1e-6, ent=0.966]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.15it/s, pg=-0.0412, ret=2.96e-5, glen=62.6, tlen=223, kl=0.00257, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.16it/s, pg=-0.0412, ret=2.96e-5, glen=62.6, tlen=223, kl=0.00257, act_lr=1e-6, ent=0.921]
2025-07-24 21:48:03.845 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.03s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.16it/s, pg=0.117, ret=-9.12e-5, glen=62.1, tlen=223, kl=0.00234, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=0.117, ret=-9.12e-5, glen=62.1, tlen=223, kl=0.00234, act_lr=1e-6, ent=0.938]
2025-07-24 21:48:04.694 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 21:48:07.241 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.55s
2025-07-24 21:48:07.584 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.89s
2025-07-24 21:48:07.590 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0003650499426800272, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9453665963981462, 'kl': 0.002630399621051291, 'response_length': 61.881632182909094, 'total_length': 222.0616057022758, 'teacher_total_length': 234.103375642196, 'return': 1.4243774469081393e-09, 'policy_update_steps': 1.0}

Episode [12/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [07:26<16:42, 111.44s/it][A2025-07-24 21:48:07.635 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:48:43.498 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:48:43.680 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:48:43.681 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 36.05s
2025-07-24 21:48:45.408 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0149,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 61.7786,std_num_tokens: 12.3120,avg_correct_num_tokens: 61.7763,std_correct_num_tokens: 12.3124,avg_incorrect_num_tokens: 64.8333,std_incorrect_num_tokens: 11.3639
2025-07-24 21:48:45.820 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.14s
2025-07-24 21:48:48.673 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.85s
2025-07-24 21:49:12.341 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 184
2025-07-24 21:49:12.341 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.67s
2025-07-24 21:49:13.588 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 21:49:13.588 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -8.385710395447424e-07, avg_kl: 0.0047948256782863454, avg_response_length: 61.77430024354354, avg_orm_score: 0.0, avg_custom_rewards: -8.385710395447424e-07
2025-07-24 21:49:13.617 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter147_replay_buffer.jsonl
2025-07-24 21:49:14.918 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.30s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0285, ret=1.77e-5, glen=61.7, tlen=222, kl=0.00406, act_lr=1e-6, ent=0.946]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.0285, ret=1.77e-5, glen=61.7, tlen=222, kl=0.00406, act_lr=1e-6, ent=0.946]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.01s/it, pg=-0.032, ret=1.75e-5, glen=61.3, tlen=222, kl=0.00463, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.032, ret=1.75e-5, glen=61.3, tlen=222, kl=0.00463, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0225, ret=1.37e-5, glen=61.3, tlen=222, kl=0.00431, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.12it/s, pg=-0.0225, ret=1.37e-5, glen=61.3, tlen=222, kl=0.00431, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.12it/s, pg=-0.0121, ret=6.05e-6, glen=62.1, tlen=223, kl=0.00466, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:36,  1.14it/s, pg=-0.0121, ret=6.05e-6, glen=62.1, tlen=223, kl=0.00466, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:36,  1.14it/s, pg=-0.042, ret=2.51e-5, glen=60.9, tlen=221, kl=0.00401, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.12it/s, pg=-0.042, ret=2.51e-5, glen=60.9, tlen=221, kl=0.00401, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.12it/s, pg=-0.0216, ret=1.16e-5, glen=60.2, tlen=220, kl=0.0046, act_lr=1e-6, ent=0.998]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.14it/s, pg=-0.0216, ret=1.16e-5, glen=60.2, tlen=220, kl=0.0046, act_lr=1e-6, ent=0.998]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.14it/s, pg=-0.0304, ret=1.72e-5, glen=60.2, tlen=221, kl=0.00449, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.12it/s, pg=-0.0304, ret=1.72e-5, glen=60.2, tlen=221, kl=0.00449, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.12it/s, pg=-0.0265, ret=1.37e-5, glen=62.4, tlen=223, kl=0.00452, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:34,  1.11it/s, pg=-0.0265, ret=1.37e-5, glen=62.4, tlen=223, kl=0.00452, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:34,  1.11it/s, pg=-0.0396, ret=2.5e-5, glen=61.1, tlen=222, kl=0.00424, act_lr=1e-6, ent=0.923] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:33,  1.12it/s, pg=-0.0396, ret=2.5e-5, glen=61.1, tlen=222, kl=0.00424, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:33,  1.12it/s, pg=-0.0308, ret=1.99e-5, glen=61.7, tlen=222, kl=0.00394, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=-0.0308, ret=1.99e-5, glen=61.7, tlen=222, kl=0.00394, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=0.158, ret=-9.65e-5, glen=60.6, tlen=221, kl=0.00557, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=0.158, ret=-9.65e-5, glen=60.6, tlen=221, kl=0.00557, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0337, ret=1.93e-5, glen=59.8, tlen=220, kl=0.00476, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=-0.0337, ret=1.93e-5, glen=59.8, tlen=220, kl=0.00476, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=-0.0449, ret=2.58e-5, glen=62.4, tlen=224, kl=0.00422, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=-0.0449, ret=2.58e-5, glen=62.4, tlen=224, kl=0.00422, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0471, ret=2.9e-5, glen=62.3, tlen=223, kl=0.00463, act_lr=1e-6, ent=0.974] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.16it/s, pg=-0.0471, ret=2.9e-5, glen=62.3, tlen=223, kl=0.00463, act_lr=1e-6, ent=0.974]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.16it/s, pg=0.143, ret=-0.000121, glen=62.6, tlen=223, kl=0.00437, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=0.143, ret=-0.000121, glen=62.6, tlen=223, kl=0.00437, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.17it/s, pg=-0.0265, ret=1.76e-5, glen=62, tlen=222, kl=0.00447, act_lr=1e-6, ent=0.948]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0265, ret=1.76e-5, glen=62, tlen=222, kl=0.00447, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.00626, ret=1.93e-6, glen=61.6, tlen=222, kl=0.00475, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.00626, ret=1.93e-6, glen=61.6, tlen=222, kl=0.00475, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=-0.00672, ret=4.04e-6, glen=61.4, tlen=222, kl=0.00387, act_lr=1e-6, ent=0.955]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=-0.00672, ret=4.04e-6, glen=61.4, tlen=222, kl=0.00387, act_lr=1e-6, ent=0.955]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0179, ret=9.73e-6, glen=62.5, tlen=223, kl=0.00437, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0179, ret=9.73e-6, glen=62.5, tlen=223, kl=0.00437, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.038, ret=1.94e-5, glen=61.7, tlen=222, kl=0.00489, act_lr=1e-6, ent=0.935] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.038, ret=1.94e-5, glen=61.7, tlen=222, kl=0.00489, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0259, ret=1.54e-5, glen=61.6, tlen=222, kl=0.005, act_lr=1e-6, ent=0.931] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.15it/s, pg=-0.0259, ret=1.54e-5, glen=61.6, tlen=222, kl=0.005, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.15it/s, pg=-0.0224, ret=1.35e-5, glen=60.6, tlen=221, kl=0.005, act_lr=1e-6, ent=0.951]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.16it/s, pg=-0.0224, ret=1.35e-5, glen=60.6, tlen=221, kl=0.005, act_lr=1e-6, ent=0.951]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.16it/s, pg=-0.0423, ret=2.71e-5, glen=61.7, tlen=222, kl=0.00589, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=-0.0423, ret=2.71e-5, glen=61.7, tlen=222, kl=0.00589, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.16it/s, pg=-0.0338, ret=1.93e-5, glen=61.4, tlen=222, kl=0.00465, act_lr=1e-6, ent=0.952]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0338, ret=1.93e-5, glen=61.4, tlen=222, kl=0.00465, act_lr=1e-6, ent=0.952]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0287, ret=1.56e-5, glen=62.5, tlen=223, kl=0.00499, act_lr=1e-6, ent=0.959]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=-0.0287, ret=1.56e-5, glen=62.5, tlen=223, kl=0.00499, act_lr=1e-6, ent=0.959]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=-0.035, ret=2.17e-5, glen=60.6, tlen=221, kl=0.0055, act_lr=1e-6, ent=0.932]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.035, ret=2.17e-5, glen=60.6, tlen=221, kl=0.0055, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.011, ret=5.79e-6, glen=61.7, tlen=222, kl=0.00533, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.011, ret=5.79e-6, glen=61.7, tlen=222, kl=0.00533, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0314, ret=1.97e-5, glen=62.8, tlen=223, kl=0.00325, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0314, ret=1.97e-5, glen=62.8, tlen=223, kl=0.00325, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0163, ret=9.82e-6, glen=61.8, tlen=222, kl=0.00473, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0163, ret=9.82e-6, glen=61.8, tlen=222, kl=0.00473, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0385, ret=2.39e-5, glen=62.7, tlen=223, kl=0.00983, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0385, ret=2.39e-5, glen=62.7, tlen=223, kl=0.00983, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0212, ret=1.18e-5, glen=62.2, tlen=223, kl=0.00578, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0212, ret=1.18e-5, glen=62.2, tlen=223, kl=0.00578, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=0.252, ret=-0.000109, glen=63.1, tlen=224, kl=0.0044, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=0.252, ret=-0.000109, glen=63.1, tlen=224, kl=0.0044, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=-0.0222, ret=1.37e-5, glen=62.2, tlen=223, kl=0.00373, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=-0.0222, ret=1.37e-5, glen=62.2, tlen=223, kl=0.00373, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0231, ret=1.21e-5, glen=64.3, tlen=225, kl=0.00772, act_lr=1e-6, ent=0.939]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0231, ret=1.21e-5, glen=64.3, tlen=225, kl=0.00772, act_lr=1e-6, ent=0.939]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=0.203, ret=-0.000113, glen=63.5, tlen=224, kl=0.00393, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.203, ret=-0.000113, glen=63.5, tlen=224, kl=0.00393, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0409, ret=2.53e-5, glen=61.1, tlen=222, kl=0.00538, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0409, ret=2.53e-5, glen=61.1, tlen=222, kl=0.00538, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.0463, ret=2.7e-5, glen=60.9, tlen=221, kl=0.00642, act_lr=1e-6, ent=0.957] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0463, ret=2.7e-5, glen=60.9, tlen=221, kl=0.00642, act_lr=1e-6, ent=0.957]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.17it/s, pg=-0.0264, ret=1.55e-5, glen=61.2, tlen=222, kl=0.00391, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0264, ret=1.55e-5, glen=61.2, tlen=222, kl=0.00391, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=0.196, ret=-0.000106, glen=61.1, tlen=222, kl=0.00492, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=0.196, ret=-0.000106, glen=61.1, tlen=222, kl=0.00492, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.157, ret=-0.0001, glen=62.1, tlen=222, kl=0.00422, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.157, ret=-0.0001, glen=62.1, tlen=222, kl=0.00422, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0214, ret=1.18e-5, glen=63.1, tlen=224, kl=0.00438, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0214, ret=1.18e-5, glen=63.1, tlen=224, kl=0.00438, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.019, ret=1.18e-5, glen=61, tlen=222, kl=0.00507, act_lr=1e-6, ent=0.963]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.019, ret=1.18e-5, glen=61, tlen=222, kl=0.00507, act_lr=1e-6, ent=0.963]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=-0.034, ret=1.97e-5, glen=62.8, tlen=223, kl=0.00429, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.034, ret=1.97e-5, glen=62.8, tlen=223, kl=0.00429, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=-0.0126, ret=7.8e-6, glen=62.5, tlen=223, kl=0.00407, act_lr=1e-6, ent=0.954]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=-0.0126, ret=7.8e-6, glen=62.5, tlen=223, kl=0.00407, act_lr=1e-6, ent=0.954]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=-0.0235, ret=1.18e-5, glen=62.1, tlen=223, kl=0.00492, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0235, ret=1.18e-5, glen=62.1, tlen=223, kl=0.00492, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.18it/s, pg=-0.0192, ret=9.91e-6, glen=61.2, tlen=222, kl=0.00389, act_lr=1e-6, ent=0.969]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.0192, ret=9.91e-6, glen=61.2, tlen=222, kl=0.00389, act_lr=1e-6, ent=0.969]
2025-07-24 21:49:55.077 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.99s
2025-07-24 21:49:55.895 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 21:49:58.497 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 21:49:58.843 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.87s
2025-07-24 21:49:58.849 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00014159990393597147, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9408894168293994, 'kl': 0.0047948256782863454, 'response_length': 61.774300367935844, 'total_length': 222.2809942494268, 'teacher_total_length': 234.26762290622878, 'return': -3.726474618736614e-08, 'policy_update_steps': 1.0}

Episode [12/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [09:17<14:51, 111.38s/it][A2025-07-24 21:49:58.895 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:50:34.250 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:50:34.425 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 21:50:34.426 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.53s
2025-07-24 21:50:36.439 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 60.9500,std_num_tokens: 12.1636,avg_correct_num_tokens: 60.9468,std_correct_num_tokens: 12.1661,avg_incorrect_num_tokens: 64.1250,std_incorrect_num_tokens: 8.6377
2025-07-24 21:50:36.853 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.43s
2025-07-24 21:50:39.444 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.59s
2025-07-24 21:51:03.119 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 183
2025-07-24 21:51:03.120 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.67s
2025-07-24 21:51:04.419 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.86s
2025-07-24 21:51:04.419 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.1641204372821933e-06, avg_kl: 0.012296249306267076, avg_response_length: 60.95234288283385, avg_orm_score: 0.0, avg_custom_rewards: 2.1641204372821933e-06
2025-07-24 21:51:04.446 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter148_replay_buffer.jsonl
2025-07-24 21:51:05.684 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.24s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.044, ret=2.74e-5, glen=61.3, tlen=221, kl=0.00952, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.044, ret=2.74e-5, glen=61.3, tlen=221, kl=0.00952, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.02s/it, pg=-0.027, ret=1.93e-5, glen=60.7, tlen=220, kl=0.00983, act_lr=1e-6, ent=0.953]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.08it/s, pg=-0.027, ret=1.93e-5, glen=60.7, tlen=220, kl=0.00983, act_lr=1e-6, ent=0.953]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.08it/s, pg=-0.0435, ret=2.89e-5, glen=60.1, tlen=220, kl=0.0101, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.11it/s, pg=-0.0435, ret=2.89e-5, glen=60.1, tlen=220, kl=0.0101, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.11it/s, pg=-0.043, ret=2.57e-5, glen=62.4, tlen=222, kl=0.00996, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.13it/s, pg=-0.043, ret=2.57e-5, glen=62.4, tlen=222, kl=0.00996, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.13it/s, pg=-0.0277, ret=1.93e-5, glen=60.1, tlen=220, kl=0.00915, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.13it/s, pg=-0.0277, ret=1.93e-5, glen=60.1, tlen=220, kl=0.00915, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.13it/s, pg=-0.0524, ret=3.64e-5, glen=60.9, tlen=221, kl=0.011, act_lr=1e-6, ent=0.935]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.14it/s, pg=-0.0524, ret=3.64e-5, glen=60.9, tlen=221, kl=0.011, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.14it/s, pg=-0.029, ret=1.93e-5, glen=60.5, tlen=220, kl=0.0099, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:33,  1.15it/s, pg=-0.029, ret=1.93e-5, glen=60.5, tlen=220, kl=0.0099, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:33,  1.15it/s, pg=-0.0228, ret=1.53e-5, glen=60.1, tlen=220, kl=0.0995, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.14it/s, pg=-0.0228, ret=1.53e-5, glen=60.1, tlen=220, kl=0.0995, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:08<00:33,  1.14it/s, pg=-0.0394, ret=2.64e-5, glen=59.6, tlen=220, kl=0.00856, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.12it/s, pg=-0.0394, ret=2.64e-5, glen=59.6, tlen=220, kl=0.00856, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.12it/s, pg=-0.0202, ret=1.35e-5, glen=60.6, tlen=220, kl=0.0133, act_lr=1e-6, ent=0.956]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=-0.0202, ret=1.35e-5, glen=60.6, tlen=220, kl=0.0133, act_lr=1e-6, ent=0.956]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=0.146, ret=-0.000104, glen=59.6, tlen=220, kl=0.00817, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:30,  1.15it/s, pg=0.146, ret=-0.000104, glen=59.6, tlen=220, kl=0.00817, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:30,  1.15it/s, pg=-0.0353, ret=2.31e-5, glen=60.8, tlen=221, kl=0.00903, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.16it/s, pg=-0.0353, ret=2.31e-5, glen=60.8, tlen=221, kl=0.00903, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.16it/s, pg=0.137, ret=-9.85e-5, glen=61.3, tlen=221, kl=0.00964, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.16it/s, pg=0.137, ret=-9.85e-5, glen=61.3, tlen=221, kl=0.00964, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.16it/s, pg=-0.0341, ret=3.45e-5, glen=60.4, tlen=220, kl=0.0088, act_lr=1e-6, ent=0.949]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=-0.0341, ret=3.45e-5, glen=60.4, tlen=220, kl=0.0088, act_lr=1e-6, ent=0.949]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.016, ret=9.65e-6, glen=60, tlen=220, kl=0.0113, act_lr=1e-6, ent=0.902]   Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:27,  1.14it/s, pg=-0.016, ret=9.65e-6, glen=60, tlen=220, kl=0.0113, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:27,  1.14it/s, pg=-0.0188, ret=1.35e-5, glen=60.7, tlen=221, kl=0.0154, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.15it/s, pg=-0.0188, ret=1.35e-5, glen=60.7, tlen=221, kl=0.0154, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:26,  1.15it/s, pg=0.18, ret=-0.000106, glen=60.1, tlen=220, kl=0.00903, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.16it/s, pg=0.18, ret=-0.000106, glen=60.1, tlen=220, kl=0.00903, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.16it/s, pg=-0.0467, ret=3.1e-5, glen=61.3, tlen=222, kl=0.00923, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:24,  1.16it/s, pg=-0.0467, ret=3.1e-5, glen=61.3, tlen=222, kl=0.00923, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:24,  1.16it/s, pg=-0.0351, ret=2.37e-5, glen=63, tlen=223, kl=0.00941, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0351, ret=2.37e-5, glen=63, tlen=223, kl=0.00941, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0317, ret=2.46e-5, glen=59.5, tlen=219, kl=0.0116, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.15it/s, pg=-0.0317, ret=2.46e-5, glen=59.5, tlen=219, kl=0.0116, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.15it/s, pg=-0.0369, ret=2.38e-5, glen=61.7, tlen=222, kl=0.00893, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.16it/s, pg=-0.0369, ret=2.38e-5, glen=61.7, tlen=222, kl=0.00893, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.16it/s, pg=0.192, ret=-0.000104, glen=60.5, tlen=220, kl=0.00739, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.16it/s, pg=0.192, ret=-0.000104, glen=60.5, tlen=220, kl=0.00739, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.16it/s, pg=-0.0419, ret=2.92e-5, glen=60.9, tlen=221, kl=0.0193, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0419, ret=2.92e-5, glen=60.9, tlen=221, kl=0.0193, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.17it/s, pg=-0.0299, ret=2.11e-5, glen=60.4, tlen=220, kl=0.0106, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.17it/s, pg=-0.0299, ret=2.11e-5, glen=60.4, tlen=220, kl=0.0106, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.17it/s, pg=-0.0228, ret=1.35e-5, glen=61.5, tlen=221, kl=0.0124, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:17,  1.17it/s, pg=-0.0228, ret=1.35e-5, glen=61.5, tlen=221, kl=0.0124, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:17,  1.17it/s, pg=-0.0215, ret=1.55e-5, glen=61.1, tlen=221, kl=0.00645, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.0215, ret=1.55e-5, glen=61.1, tlen=221, kl=0.00645, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0205, ret=1.35e-5, glen=60.9, tlen=221, kl=0.00939, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.18it/s, pg=-0.0205, ret=1.35e-5, glen=60.9, tlen=221, kl=0.00939, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.18it/s, pg=0.109, ret=-0.000106, glen=62.3, tlen=222, kl=0.00896, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=0.109, ret=-0.000106, glen=62.3, tlen=222, kl=0.00896, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0208, ret=1.35e-5, glen=61.4, tlen=221, kl=0.00923, act_lr=1e-6, ent=0.941]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0208, ret=1.35e-5, glen=61.4, tlen=221, kl=0.00923, act_lr=1e-6, ent=0.941]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.0323, ret=2.35e-5, glen=61.8, tlen=222, kl=0.0116, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.0323, ret=2.35e-5, glen=61.8, tlen=222, kl=0.0116, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.053, ret=3.47e-5, glen=60.8, tlen=221, kl=0.0115, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.11it/s, pg=-0.053, ret=3.47e-5, glen=60.8, tlen=221, kl=0.0115, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.11it/s, pg=0.164, ret=-0.000108, glen=62.5, tlen=223, kl=0.00796, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.12it/s, pg=0.164, ret=-0.000108, glen=62.5, tlen=223, kl=0.00796, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.12it/s, pg=-0.0374, ret=2.7e-5, glen=60.3, tlen=221, kl=0.00819, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.13it/s, pg=-0.0374, ret=2.7e-5, glen=60.3, tlen=221, kl=0.00819, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.0338, ret=2.3e-5, glen=60, tlen=220, kl=0.0124, act_lr=1e-6, ent=0.912]   Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.14it/s, pg=-0.0338, ret=2.3e-5, glen=60, tlen=220, kl=0.0124, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.14it/s, pg=-0.033, ret=2.15e-5, glen=61.2, tlen=221, kl=0.00896, act_lr=1e-6, ent=0.9]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.15it/s, pg=-0.033, ret=2.15e-5, glen=61.2, tlen=221, kl=0.00896, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.15it/s, pg=-0.0341, ret=2.53e-5, glen=62, tlen=222, kl=0.0102, act_lr=1e-6, ent=0.938]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0341, ret=2.53e-5, glen=62, tlen=222, kl=0.0102, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0413, ret=2.7e-5, glen=61.3, tlen=221, kl=0.0108, act_lr=1e-6, ent=0.93]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0413, ret=2.7e-5, glen=61.3, tlen=221, kl=0.0108, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.0351, ret=2.53e-5, glen=61.7, tlen=222, kl=0.00749, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0351, ret=2.53e-5, glen=61.7, tlen=222, kl=0.00749, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.00925, ret=5.79e-6, glen=60.7, tlen=220, kl=0.0105, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.00925, ret=5.79e-6, glen=60.7, tlen=220, kl=0.0105, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0285, ret=1.93e-5, glen=60.1, tlen=220, kl=0.00822, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0285, ret=1.93e-5, glen=60.1, tlen=220, kl=0.00822, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0278, ret=1.94e-5, glen=62.5, tlen=223, kl=0.0134, act_lr=1e-6, ent=0.934] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0278, ret=1.94e-5, glen=62.5, tlen=223, kl=0.0134, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=0.163, ret=-9.79e-5, glen=60.7, tlen=221, kl=0.0115, act_lr=1e-6, ent=0.936] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.18it/s, pg=0.163, ret=-9.79e-5, glen=60.7, tlen=221, kl=0.0115, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.18it/s, pg=-0.0313, ret=2.12e-5, glen=60.8, tlen=221, kl=0.0135, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.15it/s, pg=-0.0313, ret=2.12e-5, glen=60.8, tlen=221, kl=0.0135, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.15it/s, pg=0.125, ret=-0.000104, glen=61.5, tlen=222, kl=0.0101, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.16it/s, pg=0.125, ret=-0.000104, glen=61.5, tlen=222, kl=0.0101, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.16it/s, pg=-0.0435, ret=2.89e-5, glen=61.1, tlen=221, kl=0.0104, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.16it/s, pg=-0.0435, ret=2.89e-5, glen=61.1, tlen=221, kl=0.0104, act_lr=1e-6, ent=0.927]
2025-07-24 21:51:45.986 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.13s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.16it/s, pg=-0.0348, ret=2.51e-5, glen=60.9, tlen=221, kl=0.0133, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=-0.0348, ret=2.51e-5, glen=60.9, tlen=221, kl=0.0133, act_lr=1e-6, ent=0.931]
2025-07-24 21:51:46.821 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 21:51:49.355 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-24 21:51:49.688 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.95s
2025-07-24 21:51:49.694 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00042193868885869563, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9253082003282465, 'kl': 0.012285481328549593, 'response_length': 60.94922032563583, 'total_length': 220.89617356010106, 'teacher_total_length': 232.99065929910412, 'return': 4.437958780371169e-07, 'policy_update_steps': 1.0}

Episode [12/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [11:08<12:58, 111.20s/it][A2025-07-24 21:51:49.740 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:52:24.821 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:52:24.998 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:52:24.998 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 35.26s
2025-07-24 21:52:26.785 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 59.7048,std_num_tokens: 11.8170,avg_correct_num_tokens: 59.6982,std_correct_num_tokens: 11.8089,avg_incorrect_num_tokens: 63.5714,std_incorrect_num_tokens: 15.3517
2025-07-24 21:52:27.242 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.24s
2025-07-24 21:52:30.136 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.89s
2025-07-24 21:52:53.799 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 21:52:53.799 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.66s
2025-07-24 21:52:55.035 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.83s
2025-07-24 21:52:55.036 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 8.667842548975086e-06, avg_kl: 0.022611177884615384, avg_response_length: 59.70170014769166, avg_orm_score: 0.0, avg_custom_rewards: 8.667842548975086e-06
2025-07-24 21:52:55.065 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter149_replay_buffer.jsonl
2025-07-24 21:52:56.330 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.27s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s, pg=0.0901, ret=-8.83e-5, glen=60.3, tlen=221, kl=0.0202, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:00<00:44,  1.00it/s, pg=0.0901, ret=-8.83e-5, glen=60.3, tlen=221, kl=0.0202, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:44,  1.00it/s, pg=-0.0515, ret=4.4e-5, glen=59.4, tlen=220, kl=0.0223, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0515, ret=4.4e-5, glen=59.4, tlen=220, kl=0.0223, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0648, ret=8.01e-5, glen=57.3, tlen=217, kl=0.0239, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:39,  1.10it/s, pg=-0.0648, ret=8.01e-5, glen=57.3, tlen=217, kl=0.0239, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:39,  1.10it/s, pg=-0.0388, ret=3.22e-5, glen=58.5, tlen=219, kl=0.0249, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.12it/s, pg=-0.0388, ret=3.22e-5, glen=58.5, tlen=219, kl=0.0249, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.12it/s, pg=-0.0295, ret=2.31e-5, glen=59.6, tlen=220, kl=0.0228, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.14it/s, pg=-0.0295, ret=2.31e-5, glen=59.6, tlen=220, kl=0.0228, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.14it/s, pg=-0.0284, ret=2.68e-5, glen=57.9, tlen=218, kl=0.0177, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.12it/s, pg=-0.0284, ret=2.68e-5, glen=57.9, tlen=218, kl=0.0177, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.12it/s, pg=0.0789, ret=-7.91e-5, glen=60.5, tlen=221, kl=0.0215, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.14it/s, pg=0.0789, ret=-7.91e-5, glen=60.5, tlen=221, kl=0.0215, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.14it/s, pg=0.215, ret=-0.000194, glen=58.9, tlen=219, kl=0.0246, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=0.215, ret=-0.000194, glen=58.9, tlen=219, kl=0.0246, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:33,  1.15it/s, pg=-0.024, ret=1.93e-5, glen=59.9, tlen=220, kl=0.0172, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:32,  1.15it/s, pg=-0.024, ret=1.93e-5, glen=59.9, tlen=220, kl=0.0172, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:32,  1.15it/s, pg=0.146, ret=-8.74e-5, glen=59.8, tlen=220, kl=0.0225, act_lr=1e-6, ent=0.937]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:31,  1.14it/s, pg=0.146, ret=-8.74e-5, glen=59.8, tlen=220, kl=0.0225, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:31,  1.14it/s, pg=-0.0535, ret=3.76e-5, glen=60.2, tlen=220, kl=0.0274, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:31,  1.12it/s, pg=-0.0535, ret=3.76e-5, glen=60.2, tlen=220, kl=0.0274, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:31,  1.12it/s, pg=-0.0586, ret=5.64e-5, glen=58.7, tlen=219, kl=0.0246, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:30,  1.11it/s, pg=-0.0586, ret=5.64e-5, glen=58.7, tlen=219, kl=0.0246, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:30,  1.11it/s, pg=-0.05, ret=3.86e-5, glen=59.9, tlen=220, kl=0.019, act_lr=1e-6, ent=0.937]   Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:29,  1.13it/s, pg=-0.05, ret=3.86e-5, glen=59.9, tlen=220, kl=0.019, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:29,  1.13it/s, pg=-0.058, ret=4.42e-5, glen=60.4, tlen=221, kl=0.0221, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:28,  1.14it/s, pg=-0.058, ret=4.42e-5, glen=60.4, tlen=221, kl=0.0221, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:28,  1.14it/s, pg=0.103, ret=-7.79e-5, glen=58.9, tlen=219, kl=0.0249, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.15it/s, pg=0.103, ret=-7.79e-5, glen=58.9, tlen=219, kl=0.0249, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:14<00:26,  1.15it/s, pg=0.11, ret=-0.00016, glen=61.1, tlen=222, kl=0.017, act_lr=1e-6, ent=0.944]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=0.11, ret=-0.00016, glen=61.1, tlen=222, kl=0.017, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.16it/s, pg=-0.0529, ret=4.68e-5, glen=59.7, tlen=220, kl=0.0195, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0529, ret=4.68e-5, glen=59.7, tlen=220, kl=0.0195, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.133, ret=-9.46e-5, glen=59.8, tlen=220, kl=0.0176, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.133, ret=-9.46e-5, glen=59.8, tlen=220, kl=0.0176, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.0717, ret=6.03e-5, glen=60.7, tlen=222, kl=0.0258, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:23,  1.17it/s, pg=-0.0717, ret=6.03e-5, glen=60.7, tlen=222, kl=0.0258, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:23,  1.17it/s, pg=-0.0292, ret=2.87e-5, glen=59.5, tlen=219, kl=0.0305, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.17it/s, pg=-0.0292, ret=2.87e-5, glen=59.5, tlen=219, kl=0.0305, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.17it/s, pg=-0.0315, ret=2.7e-5, glen=60.5, tlen=221, kl=0.0166, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=-0.0315, ret=2.7e-5, glen=60.5, tlen=221, kl=0.0166, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.18it/s, pg=0.319, ret=-0.000219, glen=61.1, tlen=222, kl=0.0152, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:20,  1.18it/s, pg=0.319, ret=-0.000219, glen=61.1, tlen=222, kl=0.0152, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:20<00:20,  1.18it/s, pg=-0.0308, ret=2.89e-5, glen=59.1, tlen=220, kl=0.0213, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=-0.0308, ret=2.89e-5, glen=59.1, tlen=220, kl=0.0213, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:19,  1.18it/s, pg=0.106, ret=-9.45e-5, glen=60, tlen=220, kl=0.0306, act_lr=1e-6, ent=0.9]     Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:18,  1.18it/s, pg=0.106, ret=-9.45e-5, glen=60, tlen=220, kl=0.0306, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:18,  1.18it/s, pg=-0.034, ret=3.26e-5, glen=58.7, tlen=219, kl=0.0191, act_lr=1e-6, ent=0.958]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.034, ret=3.26e-5, glen=58.7, tlen=219, kl=0.0191, act_lr=1e-6, ent=0.958]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=-0.0317, ret=2.89e-5, glen=59.8, tlen=221, kl=0.0199, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.16it/s, pg=-0.0317, ret=2.89e-5, glen=59.8, tlen=221, kl=0.0199, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.16it/s, pg=-0.0452, ret=4.24e-5, glen=60.7, tlen=221, kl=0.0248, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.16it/s, pg=-0.0452, ret=4.24e-5, glen=60.7, tlen=221, kl=0.0248, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.16it/s, pg=-0.0428, ret=3.86e-5, glen=59.5, tlen=220, kl=0.0277, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0428, ret=3.86e-5, glen=59.5, tlen=220, kl=0.0277, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0284, ret=2.47e-5, glen=58.6, tlen=219, kl=0.028, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:16,  1.01it/s, pg=-0.0284, ret=2.47e-5, glen=58.6, tlen=219, kl=0.028, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:16,  1.01it/s, pg=-0.0419, ret=3.28e-5, glen=59.2, tlen=219, kl=0.0493, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:15,  1.06it/s, pg=-0.0419, ret=3.28e-5, glen=59.2, tlen=219, kl=0.0493, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:15,  1.06it/s, pg=-0.037, ret=3.25e-5, glen=60.2, tlen=220, kl=0.0236, act_lr=1e-6, ent=0.907] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.09it/s, pg=-0.037, ret=3.25e-5, glen=60.2, tlen=220, kl=0.0236, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:28<00:13,  1.09it/s, pg=-0.0531, ret=4.82e-5, glen=60.3, tlen=221, kl=0.0167, act_lr=1e-6, ent=0.948]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.12it/s, pg=-0.0531, ret=4.82e-5, glen=60.3, tlen=221, kl=0.0167, act_lr=1e-6, ent=0.948]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:29<00:12,  1.12it/s, pg=-0.0336, ret=3.04e-5, glen=59.4, tlen=220, kl=0.0175, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.0336, ret=3.04e-5, glen=59.4, tlen=220, kl=0.0175, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.13it/s, pg=-0.0343, ret=2.86e-5, glen=59.3, tlen=219, kl=0.0248, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.15it/s, pg=-0.0343, ret=2.86e-5, glen=59.3, tlen=219, kl=0.0248, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.15it/s, pg=-0.0569, ret=4.77e-5, glen=59.2, tlen=219, kl=0.0162, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=-0.0569, ret=4.77e-5, glen=59.2, tlen=219, kl=0.0162, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0359, ret=2.89e-5, glen=59.8, tlen=220, kl=0.0177, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.16it/s, pg=-0.0359, ret=2.89e-5, glen=59.8, tlen=220, kl=0.0177, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.16it/s, pg=-0.0316, ret=2.48e-5, glen=59.3, tlen=219, kl=0.0244, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.16it/s, pg=-0.0316, ret=2.48e-5, glen=59.3, tlen=219, kl=0.0244, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:33<00:07,  1.16it/s, pg=-0.0342, ret=2.68e-5, glen=59.6, tlen=220, kl=0.0229, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0342, ret=2.68e-5, glen=59.6, tlen=220, kl=0.0229, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:34<00:06,  1.17it/s, pg=-0.0414, ret=3.67e-5, glen=61.4, tlen=221, kl=0.0255, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0414, ret=3.67e-5, glen=61.4, tlen=221, kl=0.0255, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=-0.0707, ret=6.17e-5, glen=60.1, tlen=220, kl=0.0259, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=-0.0707, ret=6.17e-5, glen=60.1, tlen=220, kl=0.0259, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0506, ret=4.3e-5, glen=58.9, tlen=220, kl=0.0224, act_lr=1e-6, ent=0.904] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0506, ret=4.3e-5, glen=58.9, tlen=220, kl=0.0224, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.0297, ret=2.73e-5, glen=60.5, tlen=221, kl=0.0181, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.17it/s, pg=-0.0297, ret=2.73e-5, glen=60.5, tlen=221, kl=0.0181, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.17it/s, pg=0.0331, ret=-6.94e-5, glen=60.2, tlen=221, kl=0.024, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=0.0331, ret=-6.94e-5, glen=60.2, tlen=221, kl=0.024, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.115, ret=-9.64e-5, glen=60.8, tlen=221, kl=0.0179, act_lr=1e-6, ent=0.93] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.18it/s, pg=0.115, ret=-9.64e-5, glen=60.8, tlen=221, kl=0.0179, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:39<00:01,  1.18it/s, pg=-0.0285, ret=2.88e-5, glen=59.6, tlen=220, kl=0.0211, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.0285, ret=2.88e-5, glen=59.6, tlen=220, kl=0.0211, act_lr=1e-6, ent=0.922]
2025-07-24 21:53:36.694 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 40.19s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.17it/s, pg=-0.0593, ret=5.02e-5, glen=59.4, tlen=220, kl=0.022, act_lr=1e-6, ent=0.932] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:40<00:00,  1.12it/s, pg=-0.0593, ret=5.02e-5, glen=59.4, tlen=220, kl=0.022, act_lr=1e-6, ent=0.932]
2025-07-24 21:53:37.536 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:53:40.125 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 21:53:40.477 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 44.09s
2025-07-24 21:53:40.483 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.000998538473378057, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9219549816587697, 'kl': 0.022589642068614132, 'response_length': 59.69817385466202, 'total_length': 220.0712482618249, 'teacher_total_length': 232.14094211744225, 'return': 1.0699853626884641e-06, 'policy_update_steps': 1.0}

Episode [12/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [12:59<11:06, 111.06s/it][A2025-07-24 21:53:46.757 | INFO     | orz.ppo.trainer:train:183 - Successfully save model weights, training continue.
2025-07-24 21:53:46.759 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<02:33,  1.11it/s, est. speed input: 202.75 toks/s, output: 38.99 toks/s]Processed prompts:   1%|          | 2/172 [00:00<01:12,  2.33it/s, est. speed input: 363.59 toks/s, output: 74.12 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   1%|          | 1/172 [00:00<02:44,  1.04it/s, est. speed input: 186.36 toks/s, output: 39.56 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:02<00:00, 89.79it/s, est. speed input: 10417.31 toks/s, output: 3660.47 toks/s]
2025-07-24 21:53:51.402 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 313.3275,strategyqa_test/accuracy: 0.5269,eval_accuracy: 0.5269
2025-07-24 21:53:51.686 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:54:26.188 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:54:26.375 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 21:54:26.375 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.69s
2025-07-24 21:54:28.445 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0000,avg_pass_at_n: 1.0000,avg_num_tokens: 59.0243,std_num_tokens: 11.7371,avg_correct_num_tokens: 59.0188,std_correct_num_tokens: 11.7313,avg_incorrect_num_tokens: 66.5000,std_incorrect_num_tokens: 16.3580
2025-07-24 21:54:28.846 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.47s
2025-07-24 21:54:31.415 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.57s
2025-07-24 21:54:55.091 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 182
2025-07-24 21:54:55.092 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.67s
2025-07-24 21:54:56.368 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.86s
2025-07-24 21:54:56.368 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.2254951378473868e-07, avg_kl: 0.03295697222699176, avg_response_length: 59.049642017909456, avg_orm_score: 0.0, avg_custom_rewards: 1.2254951378473868e-07
2025-07-24 21:54:56.405 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter150_replay_buffer.jsonl
2025-07-24 21:54:57.658 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.25s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  27%|‚ñà‚ñà‚ñã       | 47/172 [00:01<00:02, 54.95it/s, est. speed input: 5015.54 toks/s, output: 1483.15 toks/s]Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/172 [00:01<00:01, 85.04it/s, est. speed input: 6732.35 toks/s, output: 2101.94 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/172 [00:02<00:00, 82.26it/s, est. speed input: 10255.97 toks/s, output: 3654.84 toks/s][32m [repeated 50x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 44.88it/s, est. speed input: 8130.24 toks/s, output: 2996.55 toks/s] [32m [repeated 10x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/46 [00:01<?, ?it/s, pg=-0.0331, ret=1.93e-5, glen=60.7, tlen=222, kl=0.0377, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.00s/it, pg=-0.0331, ret=1.93e-5, glen=60.7, tlen=222, kl=0.0377, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/46 [00:01<00:45,  1.00s/it, pg=-0.0412, ret=2.29e-5, glen=58.1, tlen=219, kl=0.0308, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:01<00:40,  1.09it/s, pg=-0.0412, ret=2.29e-5, glen=58.1, tlen=219, kl=0.0308, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/46 [00:02<00:40,  1.09it/s, pg=-0.0228, ret=1.16e-5, glen=58.3, tlen=219, kl=0.0321, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:02<00:38,  1.10it/s, pg=-0.0228, ret=1.16e-5, glen=58.3, tlen=219, kl=0.0321, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/46 [00:03<00:38,  1.10it/s, pg=-0.024, ret=1.53e-5, glen=58.8, tlen=220, kl=0.0295, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:03<00:37,  1.13it/s, pg=-0.024, ret=1.53e-5, glen=58.8, tlen=220, kl=0.0295, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 4/46 [00:04<00:37,  1.13it/s, pg=-0.0189, ret=9.48e-6, glen=57.8, tlen=218, kl=0.032, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:04<00:36,  1.11it/s, pg=-0.0189, ret=9.48e-6, glen=57.8, tlen=218, kl=0.032, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/46 [00:05<00:36,  1.11it/s, pg=-0.0307, ret=1.74e-5, glen=59, tlen=220, kl=0.0263, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:05<00:35,  1.13it/s, pg=-0.0307, ret=1.74e-5, glen=59, tlen=220, kl=0.0263, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/46 [00:06<00:35,  1.13it/s, pg=-0.0182, ret=9.65e-6, glen=59.1, tlen=220, kl=0.0358, act_lr=1e-6, ent=0.945]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:06<00:34,  1.15it/s, pg=-0.0182, ret=9.65e-6, glen=59.1, tlen=220, kl=0.0358, act_lr=1e-6, ent=0.945]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 7/46 [00:07<00:34,  1.15it/s, pg=-0.0496, ret=2.68e-5, glen=57.8, tlen=218, kl=0.0303, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0496, ret=2.68e-5, glen=57.8, tlen=218, kl=0.0303, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/46 [00:07<00:32,  1.16it/s, pg=-0.0434, ret=2.3e-5, glen=58.6, tlen=219, kl=0.034, act_lr=1e-6, ent=0.937]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:07<00:31,  1.16it/s, pg=-0.0434, ret=2.3e-5, glen=58.6, tlen=219, kl=0.034, act_lr=1e-6, ent=0.937]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 9/46 [00:08<00:31,  1.16it/s, pg=-0.00846, ret=3.78e-6, glen=57.4, tlen=218, kl=0.0332, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:08<00:30,  1.17it/s, pg=-0.00846, ret=3.78e-6, glen=57.4, tlen=218, kl=0.0332, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/46 [00:09<00:30,  1.17it/s, pg=-0.0227, ret=1.31e-5, glen=57.6, tlen=218, kl=0.0329, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:09<00:29,  1.17it/s, pg=-0.0227, ret=1.31e-5, glen=57.6, tlen=218, kl=0.0329, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/46 [00:10<00:29,  1.17it/s, pg=-0.0329, ret=1.74e-5, glen=59.8, tlen=221, kl=0.0326, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:10<00:29,  1.17it/s, pg=-0.0329, ret=1.74e-5, glen=59.8, tlen=221, kl=0.0326, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 12/46 [00:11<00:29,  1.17it/s, pg=-0.0176, ret=9.4e-6, glen=58.7, tlen=219, kl=0.0264, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:11<00:28,  1.17it/s, pg=-0.0176, ret=9.4e-6, glen=58.7, tlen=219, kl=0.0264, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 13/46 [00:12<00:28,  1.17it/s, pg=-0.0398, ret=2.51e-5, glen=59.8, tlen=221, kl=0.0379, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:12<00:27,  1.17it/s, pg=-0.0398, ret=2.51e-5, glen=59.8, tlen=221, kl=0.0379, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 14/46 [00:13<00:27,  1.17it/s, pg=-0.0232, ret=1.34e-5, glen=58.3, tlen=219, kl=0.0367, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0232, ret=1.34e-5, glen=58.3, tlen=219, kl=0.0367, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/46 [00:13<00:26,  1.17it/s, pg=-0.0366, ret=1.9e-5, glen=59.4, tlen=220, kl=0.0316, act_lr=1e-6, ent=0.929] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:13<00:25,  1.17it/s, pg=-0.0366, ret=1.9e-5, glen=59.4, tlen=220, kl=0.0316, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 16/46 [00:14<00:25,  1.17it/s, pg=-0.0316, ret=1.72e-5, glen=59.7, tlen=221, kl=0.0246, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:14<00:24,  1.17it/s, pg=-0.0316, ret=1.72e-5, glen=59.7, tlen=221, kl=0.0246, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 17/46 [00:15<00:24,  1.17it/s, pg=0.157, ret=-0.000112, glen=58.5, tlen=219, kl=0.0314, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:15<00:23,  1.17it/s, pg=0.157, ret=-0.000112, glen=58.5, tlen=219, kl=0.0314, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 18/46 [00:16<00:23,  1.17it/s, pg=-0.032, ret=1.91e-5, glen=59.2, tlen=220, kl=0.0336, act_lr=1e-6, ent=0.903] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:16<00:22,  1.17it/s, pg=-0.032, ret=1.91e-5, glen=59.2, tlen=220, kl=0.0336, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/46 [00:17<00:22,  1.17it/s, pg=0.208, ret=-0.000108, glen=59.4, tlen=220, kl=0.0312, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:17<00:22,  1.18it/s, pg=0.208, ret=-0.000108, glen=59.4, tlen=220, kl=0.0312, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 20/46 [00:18<00:22,  1.18it/s, pg=-0.0201, ret=1.15e-5, glen=59.5, tlen=220, kl=0.0258, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:18<00:21,  1.18it/s, pg=-0.0201, ret=1.15e-5, glen=59.5, tlen=220, kl=0.0258, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 21/46 [00:19<00:21,  1.18it/s, pg=-0.0282, ret=1.74e-5, glen=59.7, tlen=220, kl=0.0268, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:21,  1.14it/s, pg=-0.0282, ret=1.74e-5, glen=59.7, tlen=220, kl=0.0268, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/46 [00:19<00:21,  1.14it/s, pg=-0.0372, ret=1.92e-5, glen=58.1, tlen=219, kl=0.0419, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:19<00:20,  1.15it/s, pg=-0.0372, ret=1.92e-5, glen=58.1, tlen=219, kl=0.0419, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/46 [00:20<00:20,  1.15it/s, pg=-0.0406, ret=2.48e-5, glen=58.8, tlen=219, kl=0.0325, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:20<00:19,  1.16it/s, pg=-0.0406, ret=2.48e-5, glen=58.8, tlen=219, kl=0.0325, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 24/46 [00:21<00:19,  1.16it/s, pg=-0.0317, ret=1.69e-5, glen=59.2, tlen=220, kl=0.0285, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:21<00:18,  1.16it/s, pg=-0.0317, ret=1.69e-5, glen=59.2, tlen=220, kl=0.0285, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 25/46 [00:22<00:18,  1.16it/s, pg=-0.0234, ret=1.14e-5, glen=59.6, tlen=220, kl=0.0489, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:22<00:17,  1.17it/s, pg=-0.0234, ret=1.14e-5, glen=59.6, tlen=220, kl=0.0489, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 26/46 [00:23<00:17,  1.17it/s, pg=-0.0239, ret=1.33e-5, glen=59.4, tlen=221, kl=0.0287, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:23<00:16,  1.17it/s, pg=-0.0239, ret=1.33e-5, glen=59.4, tlen=221, kl=0.0287, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 27/46 [00:24<00:16,  1.17it/s, pg=-0.0298, ret=1.77e-5, glen=60.9, tlen=222, kl=0.0295, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:24<00:15,  1.17it/s, pg=-0.0298, ret=1.77e-5, glen=60.9, tlen=222, kl=0.0295, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 28/46 [00:25<00:15,  1.17it/s, pg=-0.0386, ret=2.12e-5, glen=59.2, tlen=220, kl=0.0342, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:25<00:15,  1.07it/s, pg=-0.0386, ret=2.12e-5, glen=59.2, tlen=220, kl=0.0342, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 29/46 [00:26<00:15,  1.07it/s, pg=-0.044, ret=2.48e-5, glen=59.8, tlen=221, kl=0.0316, act_lr=1e-6, ent=0.938] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:26<00:14,  1.10it/s, pg=-0.044, ret=2.48e-5, glen=59.8, tlen=221, kl=0.0316, act_lr=1e-6, ent=0.938]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 30/46 [00:27<00:14,  1.10it/s, pg=-0.0254, ret=1.37e-5, glen=59.3, tlen=220, kl=0.0287, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0254, ret=1.37e-5, glen=59.3, tlen=220, kl=0.0287, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 31/46 [00:27<00:13,  1.12it/s, pg=-0.0177, ret=9.65e-6, glen=60.2, tlen=221, kl=0.0352, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:27<00:12,  1.14it/s, pg=-0.0177, ret=9.65e-6, glen=60.2, tlen=221, kl=0.0352, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 32/46 [00:28<00:12,  1.14it/s, pg=0.212, ret=-0.000112, glen=60.1, tlen=221, kl=0.0405, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:28<00:11,  1.15it/s, pg=0.212, ret=-0.000112, glen=60.1, tlen=221, kl=0.0405, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 33/46 [00:29<00:11,  1.15it/s, pg=-0.0359, ret=2.12e-5, glen=58.5, tlen=219, kl=0.0343, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:29<00:10,  1.16it/s, pg=-0.0359, ret=2.12e-5, glen=58.5, tlen=219, kl=0.0343, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 34/46 [00:30<00:10,  1.16it/s, pg=0.456, ret=-0.000222, glen=58.8, tlen=219, kl=0.0324, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:30<00:09,  1.16it/s, pg=0.456, ret=-0.000222, glen=58.8, tlen=219, kl=0.0324, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 35/46 [00:31<00:09,  1.16it/s, pg=-0.0109, ret=5.7e-6, glen=58.9, tlen=219, kl=0.0275, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:31<00:08,  1.17it/s, pg=-0.0109, ret=5.7e-6, glen=58.9, tlen=219, kl=0.0275, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 36/46 [00:32<00:08,  1.17it/s, pg=-0.021, ret=1.35e-5, glen=60.9, tlen=222, kl=0.0302, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.021, ret=1.35e-5, glen=60.9, tlen=222, kl=0.0302, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 37/46 [00:32<00:07,  1.17it/s, pg=-0.0121, ret=5.79e-6, glen=59.3, tlen=220, kl=0.0309, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:32<00:06,  1.17it/s, pg=-0.0121, ret=5.79e-6, glen=59.3, tlen=220, kl=0.0309, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 38/46 [00:33<00:06,  1.17it/s, pg=-0.0214, ret=1.13e-5, glen=56.9, tlen=218, kl=0.0479, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:33<00:05,  1.17it/s, pg=-0.0214, ret=1.13e-5, glen=56.9, tlen=218, kl=0.0479, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/46 [00:34<00:05,  1.17it/s, pg=0.179, ret=-9.85e-5, glen=59.1, tlen=220, kl=0.039, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:34<00:05,  1.17it/s, pg=0.179, ret=-9.85e-5, glen=59.1, tlen=220, kl=0.039, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 40/46 [00:35<00:05,  1.17it/s, pg=-0.0453, ret=2.51e-5, glen=59.2, tlen=220, kl=0.0516, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:35<00:04,  1.17it/s, pg=-0.0453, ret=2.51e-5, glen=59.2, tlen=220, kl=0.0516, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 41/46 [00:36<00:04,  1.17it/s, pg=-0.018, ret=9.65e-6, glen=59.2, tlen=220, kl=0.0308, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:36<00:03,  1.18it/s, pg=-0.018, ret=9.65e-6, glen=59.2, tlen=220, kl=0.0308, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 42/46 [00:37<00:03,  1.18it/s, pg=-0.0266, ret=1.54e-5, glen=58.4, tlen=219, kl=0.026, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:37<00:02,  1.17it/s, pg=-0.0266, ret=1.54e-5, glen=58.4, tlen=219, kl=0.026, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 43/46 [00:38<00:02,  1.17it/s, pg=0.164, ret=-0.000105, glen=56.5, tlen=217, kl=0.0332, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=0.164, ret=-0.000105, glen=56.5, tlen=217, kl=0.0332, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 44/46 [00:38<00:01,  1.17it/s, pg=-0.0335, ret=1.9e-5, glen=60.3, tlen=221, kl=0.0301, act_lr=1e-6, ent=0.921] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:38<00:00,  1.17it/s, pg=-0.0335, ret=1.9e-5, glen=60.3, tlen=221, kl=0.0301, act_lr=1e-6, ent=0.921]
2025-07-24 21:55:37.726 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.17it/s, pg=-0.037, ret=2.1e-5, glen=60, tlen=221, kl=0.0304, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 45/46 [00:39<00:00,  1.13it/s, pg=-0.037, ret=2.1e-5, glen=60, tlen=221, kl=0.0304, act_lr=1e-6, ent=0.905]
2025-07-24 21:55:38.571 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:55:41.173 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 21:55:41.509 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.80s
2025-07-24 21:55:41.515 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.004918637483016304, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9134601743324943, 'kl': 0.03299281908118207, 'response_length': 59.039771453194, 'total_length': 219.78905785602072, 'teacher_total_length': 231.80715245785922, 'return': -2.5116879966014456e-06, 'policy_update_steps': 1.0}

Episode [12/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [15:00<09:31, 114.24s/it][A2025-07-24 21:55:41.563 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:56:15.177 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:56:15.356 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:56:15.357 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.79s
2025-07-24 21:56:17.052 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0007,avg_pass_at_n: 1.0000,avg_num_tokens: 57.0791,std_num_tokens: 11.5835,avg_correct_num_tokens: 57.0784,std_correct_num_tokens: 11.5838,avg_incorrect_num_tokens: 58.5000,std_incorrect_num_tokens: 10.7121
2025-07-24 21:56:17.470 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.11s
2025-07-24 21:56:20.323 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.85s
2025-07-24 21:56:43.313 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 180
2025-07-24 21:56:43.314 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 22.99s
2025-07-24 21:56:44.555 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 21:56:44.555 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 3.4041117436976897e-06, avg_kl: 0.07113172743055556, avg_response_length: 57.09149409400092, avg_orm_score: 0.0, avg_custom_rewards: 3.4041117436976897e-06
2025-07-24 21:56:44.585 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter151_replay_buffer.jsonl
2025-07-24 21:56:45.838 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=-0.0179, ret=9.23e-6, glen=56.5, tlen=218, kl=0.0429, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.01it/s, pg=-0.0179, ret=9.23e-6, glen=56.5, tlen=218, kl=0.0429, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.01it/s, pg=0.24, ret=-0.000109, glen=56.6, tlen=217, kl=0.0515, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=0.24, ret=-0.000109, glen=56.6, tlen=217, kl=0.0515, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.0153, ret=7.38e-6, glen=55.6, tlen=216, kl=0.0555, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.10it/s, pg=-0.0153, ret=7.38e-6, glen=55.6, tlen=216, kl=0.0555, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.10it/s, pg=-0.0304, ret=1.48e-5, glen=57.4, tlen=218, kl=0.0573, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.0304, ret=1.48e-5, glen=57.4, tlen=218, kl=0.0573, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=-0.0141, ret=7.38e-6, glen=56.1, tlen=217, kl=0.199, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.14it/s, pg=-0.0141, ret=7.38e-6, glen=56.1, tlen=217, kl=0.199, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.14it/s, pg=-0.0197, ret=7.55e-6, glen=57.6, tlen=218, kl=0.0587, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=-0.0197, ret=7.55e-6, glen=57.6, tlen=218, kl=0.0587, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=-0.00455, ret=1.93e-6, glen=57.3, tlen=218, kl=0.0651, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:32,  1.16it/s, pg=-0.00455, ret=1.93e-6, glen=57.3, tlen=218, kl=0.0651, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:32,  1.16it/s, pg=-0.0107, ret=5.54e-6, glen=57.4, tlen=218, kl=0.0511, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0107, ret=5.54e-6, glen=57.4, tlen=218, kl=0.0511, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0318, ret=1.53e-5, glen=57.9, tlen=219, kl=0.0495, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:30,  1.17it/s, pg=-0.0318, ret=1.53e-5, glen=57.9, tlen=219, kl=0.0495, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:30,  1.17it/s, pg=-0.0255, ret=1.49e-5, glen=56.5, tlen=217, kl=0.0554, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.14it/s, pg=-0.0255, ret=1.49e-5, glen=56.5, tlen=217, kl=0.0554, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.14it/s, pg=-0.0232, ret=1.12e-5, glen=57.2, tlen=218, kl=0.0569, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.15it/s, pg=-0.0232, ret=1.12e-5, glen=57.2, tlen=218, kl=0.0569, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.15it/s, pg=-0.0305, ret=1.52e-5, glen=56.8, tlen=217, kl=0.051, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=-0.0305, ret=1.52e-5, glen=56.8, tlen=217, kl=0.051, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=-0.015, ret=5.54e-6, glen=56.8, tlen=217, kl=0.0547, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=-0.015, ret=5.54e-6, glen=56.8, tlen=217, kl=0.0547, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=-0.0267, ret=1.16e-5, glen=58.6, tlen=219, kl=0.0651, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=-0.0267, ret=1.16e-5, glen=58.6, tlen=219, kl=0.0651, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=-0.0443, ret=2.06e-5, glen=57.5, tlen=218, kl=0.0537, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0443, ret=2.06e-5, glen=57.5, tlen=218, kl=0.0537, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.032, ret=1.49e-5, glen=57, tlen=217, kl=0.0429, act_lr=1e-6, ent=0.897]   Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.16it/s, pg=-0.032, ret=1.49e-5, glen=57, tlen=217, kl=0.0429, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.16it/s, pg=0.000197, ret=0, glen=56.9, tlen=217, kl=0.049, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.16it/s, pg=0.000197, ret=0, glen=56.9, tlen=217, kl=0.049, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.16it/s, pg=-0.0333, ret=1.88e-5, glen=57.8, tlen=219, kl=0.0457, act_lr=1e-6, ent=0.882]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=-0.0333, ret=1.88e-5, glen=57.8, tlen=219, kl=0.0457, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=0.166, ret=-0.000103, glen=56.5, tlen=217, kl=0.047, act_lr=1e-6, ent=0.898] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=0.166, ret=-0.000103, glen=56.5, tlen=217, kl=0.047, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0315, ret=1.49e-5, glen=56, tlen=216, kl=0.0552, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0315, ret=1.49e-5, glen=56, tlen=216, kl=0.0552, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.00825, ret=3.69e-6, glen=56, tlen=216, kl=0.0506, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.00825, ret=3.69e-6, glen=56, tlen=216, kl=0.0506, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.02, ret=7.38e-6, glen=57.5, tlen=218, kl=0.0515, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.02, ret=7.38e-6, glen=57.5, tlen=218, kl=0.0515, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0253, ret=1.11e-5, glen=56.5, tlen=217, kl=0.0423, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=-0.0253, ret=1.11e-5, glen=56.5, tlen=217, kl=0.0423, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.017, ret=7.63e-6, glen=57.6, tlen=218, kl=0.0511, act_lr=1e-6, ent=0.9]   Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.017, ret=7.63e-6, glen=57.6, tlen=218, kl=0.0511, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.013, ret=7.38e-6, glen=57.2, tlen=217, kl=0.0473, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.18it/s, pg=-0.013, ret=7.38e-6, glen=57.2, tlen=217, kl=0.0473, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.18it/s, pg=-0.0335, ret=1.52e-5, glen=58.2, tlen=219, kl=0.0683, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.18it/s, pg=-0.0335, ret=1.52e-5, glen=58.2, tlen=219, kl=0.0683, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.18it/s, pg=-0.0125, ret=5.54e-6, glen=56.1, tlen=217, kl=0.546, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.18it/s, pg=-0.0125, ret=5.54e-6, glen=56.1, tlen=217, kl=0.546, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.18it/s, pg=-0.0117, ret=5.54e-6, glen=56.4, tlen=217, kl=0.0566, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.18it/s, pg=-0.0117, ret=5.54e-6, glen=56.4, tlen=217, kl=0.0566, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.18it/s, pg=0.000197, ret=0, glen=56.5, tlen=217, kl=0.0587, act_lr=1e-6, ent=0.872]     Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=0.000197, ret=0, glen=56.5, tlen=217, kl=0.0587, act_lr=1e-6, ent=0.872]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0329, ret=1.7e-5, glen=58.4, tlen=219, kl=0.0562, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0329, ret=1.7e-5, glen=58.4, tlen=219, kl=0.0562, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.05, ret=2.29e-5, glen=57.3, tlen=217, kl=0.0479, act_lr=1e-6, ent=0.87]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:26<00:12,  1.12it/s, pg=-0.05, ret=2.29e-5, glen=57.3, tlen=217, kl=0.0479, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0189, ret=9.4e-6, glen=57.3, tlen=218, kl=0.0464, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.0189, ret=9.4e-6, glen=57.3, tlen=218, kl=0.0464, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0335, ret=1.66e-5, glen=56.1, tlen=217, kl=0.0522, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0335, ret=1.66e-5, glen=56.1, tlen=217, kl=0.0522, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0268, ret=1.5e-5, glen=57.2, tlen=218, kl=0.0636, act_lr=1e-6, ent=0.893] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0268, ret=1.5e-5, glen=57.2, tlen=218, kl=0.0636, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.02, ret=7.47e-6, glen=56.8, tlen=217, kl=0.0534, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.02, ret=7.47e-6, glen=56.8, tlen=217, kl=0.0534, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0313, ret=1.5e-5, glen=57.5, tlen=218, kl=0.0496, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0313, ret=1.5e-5, glen=57.5, tlen=218, kl=0.0496, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=-0.0302, ret=1.29e-5, glen=56.7, tlen=217, kl=0.0471, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0302, ret=1.29e-5, glen=56.7, tlen=217, kl=0.0471, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0187, ret=9.4e-6, glen=57.9, tlen=219, kl=0.0658, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=-0.0187, ret=9.4e-6, glen=57.9, tlen=219, kl=0.0658, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0107, ret=5.54e-6, glen=56.4, tlen=216, kl=0.204, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0107, ret=5.54e-6, glen=56.4, tlen=216, kl=0.204, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0425, ret=1.88e-5, glen=57.9, tlen=219, kl=0.0529, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.15it/s, pg=-0.0425, ret=1.88e-5, glen=57.9, tlen=219, kl=0.0529, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.15it/s, pg=0.23, ret=-0.000113, glen=58.6, tlen=219, kl=0.0489, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.15it/s, pg=0.23, ret=-0.000113, glen=58.6, tlen=219, kl=0.0489, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.15it/s, pg=-0.00889, ret=3.86e-6, glen=58.3, tlen=219, kl=0.0573, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.16it/s, pg=-0.00889, ret=3.86e-6, glen=58.3, tlen=219, kl=0.0573, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.16it/s, pg=-0.0262, ret=1.3e-5, glen=57.3, tlen=218, kl=0.0739, act_lr=1e-6, ent=0.892]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.16it/s, pg=-0.0262, ret=1.3e-5, glen=57.3, tlen=218, kl=0.0739, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.16it/s, pg=-0.0145, ret=7.55e-6, glen=56.8, tlen=217, kl=0.0527, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0145, ret=7.55e-6, glen=56.8, tlen=217, kl=0.0527, act_lr=1e-6, ent=0.894]
2025-07-24 21:57:25.082 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.06s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=0.282, ret=-0.000103, glen=56.7, tlen=217, kl=0.0496, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=0.282, ret=-0.000103, glen=56.7, tlen=217, kl=0.0496, act_lr=1e-6, ent=0.893]
2025-07-24 21:57:25.755 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 21:57:28.043 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-24 21:57:28.376 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.48s
2025-07-24 21:57:28.382 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 8.993678622775608e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9013384487893846, 'kl': 0.07113172743055556, 'response_length': 57.09149432712131, 'total_length': 217.60349731445314, 'teacher_total_length': 229.61487969292534, 'return': 1.4981200769802348e-07, 'policy_update_steps': 1.0}

Episode [12/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [16:47<07:27, 111.93s/it][A2025-07-24 21:57:28.429 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 21:58:01.813 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 21:58:01.993 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 21:58:01.994 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.56s
2025-07-24 21:58:03.681 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 56.0586,std_num_tokens: 11.8385,avg_correct_num_tokens: 56.0652,std_correct_num_tokens: 11.8428,avg_incorrect_num_tokens: 52.8824,std_incorrect_num_tokens: 9.0156
2025-07-24 21:58:04.106 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.11s
2025-07-24 21:58:06.616 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.51s
2025-07-24 21:58:29.693 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 21:58:29.694 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.08s
2025-07-24 21:58:30.902 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 21:58:30.902 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 7.209172290255571e-06, avg_kl: 0.07232427330656424, avg_response_length: 56.07443791394793, avg_orm_score: 0.0, avg_custom_rewards: 7.209172290255571e-06
2025-07-24 21:58:30.943 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter152_replay_buffer.jsonl
2025-07-24 21:58:32.120 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=-0.06, ret=5.86e-5, glen=57.2, tlen=218, kl=0.0638, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.01it/s, pg=-0.06, ret=5.86e-5, glen=57.2, tlen=218, kl=0.0638, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.01it/s, pg=0.0495, ret=-5.39e-5, glen=58.3, tlen=219, kl=0.0673, act_lr=1e-6, ent=0.879]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=0.0495, ret=-5.39e-5, glen=58.3, tlen=219, kl=0.0673, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=0.272, ret=-0.000316, glen=55.2, tlen=215, kl=0.0726, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.12it/s, pg=0.272, ret=-0.000316, glen=55.2, tlen=215, kl=0.0726, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.12it/s, pg=0.102, ret=-7.57e-5, glen=54.7, tlen=215, kl=0.0622, act_lr=1e-6, ent=0.875] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:35,  1.14it/s, pg=0.102, ret=-7.57e-5, glen=54.7, tlen=215, kl=0.0622, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:35,  1.14it/s, pg=-0.0389, ret=4.46e-5, glen=56.5, tlen=217, kl=0.0679, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:34,  1.15it/s, pg=-0.0389, ret=4.46e-5, glen=56.5, tlen=217, kl=0.0679, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:34,  1.15it/s, pg=0.0619, ret=-4.27e-5, glen=55.9, tlen=216, kl=0.0522, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.12it/s, pg=0.0619, ret=-4.27e-5, glen=55.9, tlen=216, kl=0.0522, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.12it/s, pg=-0.0391, ret=4.02e-5, glen=56, tlen=216, kl=0.0668, act_lr=1e-6, ent=0.913]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.13it/s, pg=-0.0391, ret=4.02e-5, glen=56, tlen=216, kl=0.0668, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.13it/s, pg=-0.0456, ret=4.41e-5, glen=54.5, tlen=215, kl=0.0892, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.0456, ret=4.41e-5, glen=54.5, tlen=215, kl=0.0892, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.0323, ret=3.34e-5, glen=56.4, tlen=217, kl=0.0818, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.15it/s, pg=-0.0323, ret=3.34e-5, glen=56.4, tlen=217, kl=0.0818, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=0.107, ret=-6.53e-5, glen=56.6, tlen=217, kl=0.0635, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=0.107, ret=-6.53e-5, glen=56.6, tlen=217, kl=0.0635, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=-0.0287, ret=2.61e-5, glen=55.9, tlen=217, kl=0.0759, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.17it/s, pg=-0.0287, ret=2.61e-5, glen=55.9, tlen=217, kl=0.0759, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.17it/s, pg=-0.0305, ret=3.27e-5, glen=54.3, tlen=215, kl=0.0771, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.0305, ret=3.27e-5, glen=54.3, tlen=215, kl=0.0771, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=-0.0744, ret=7.35e-5, glen=56.7, tlen=217, kl=0.0507, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.0744, ret=7.35e-5, glen=56.7, tlen=217, kl=0.0507, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0923, ret=-9.23e-5, glen=56.1, tlen=217, kl=0.08, act_lr=1e-6, ent=0.893]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0923, ret=-9.23e-5, glen=56.1, tlen=217, kl=0.08, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.0413, ret=4.12e-5, glen=55.7, tlen=216, kl=0.0611, act_lr=1e-6, ent=0.873]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0413, ret=4.12e-5, glen=55.7, tlen=216, kl=0.0611, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.057, ret=5.63e-5, glen=57.7, tlen=218, kl=0.108, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=-0.057, ret=5.63e-5, glen=57.7, tlen=218, kl=0.108, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0832, ret=7.51e-5, glen=55.9, tlen=216, kl=0.0684, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.0832, ret=7.51e-5, glen=55.9, tlen=216, kl=0.0684, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0379, ret=3.69e-5, glen=55.2, tlen=216, kl=0.0851, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:22,  1.17it/s, pg=-0.0379, ret=3.69e-5, glen=55.2, tlen=216, kl=0.0851, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:22,  1.17it/s, pg=-0.0285, ret=3.35e-5, glen=56.3, tlen=217, kl=0.0906, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.18it/s, pg=-0.0285, ret=3.35e-5, glen=56.3, tlen=217, kl=0.0906, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.18it/s, pg=-0.0743, ret=7.74e-5, glen=55.4, tlen=216, kl=0.0889, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.18it/s, pg=-0.0743, ret=7.74e-5, glen=55.4, tlen=216, kl=0.0889, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.18it/s, pg=0.0855, ret=-8.76e-5, glen=57.6, tlen=218, kl=0.105, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.18it/s, pg=0.0855, ret=-8.76e-5, glen=57.6, tlen=218, kl=0.105, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.18it/s, pg=0.0874, ret=-6.46e-5, glen=56.2, tlen=217, kl=0.0545, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.15it/s, pg=0.0874, ret=-6.46e-5, glen=56.2, tlen=217, kl=0.0545, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.15it/s, pg=-0.0214, ret=2.02e-5, glen=56, tlen=217, kl=0.059, act_lr=1e-6, ent=0.897]   Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:19,  1.16it/s, pg=-0.0214, ret=2.02e-5, glen=56, tlen=217, kl=0.059, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.16it/s, pg=-0.0459, ret=4.48e-5, glen=56.1, tlen=217, kl=0.0643, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.16it/s, pg=-0.0459, ret=4.48e-5, glen=56.1, tlen=217, kl=0.0643, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.16it/s, pg=-0.0528, ret=5.18e-5, glen=58.6, tlen=219, kl=0.0932, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.16it/s, pg=-0.0528, ret=5.18e-5, glen=58.6, tlen=219, kl=0.0932, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.16it/s, pg=-0.0487, ret=5.35e-5, glen=54.9, tlen=216, kl=0.078, act_lr=1e-6, ent=0.88]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0487, ret=5.35e-5, glen=54.9, tlen=216, kl=0.078, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.0327, ret=3.53e-5, glen=56.7, tlen=217, kl=0.0574, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0327, ret=3.53e-5, glen=56.7, tlen=217, kl=0.0574, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0678, ret=6.38e-5, glen=55.3, tlen=216, kl=0.0632, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0678, ret=6.38e-5, glen=55.3, tlen=216, kl=0.0632, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.071, ret=-9.97e-5, glen=55.4, tlen=216, kl=0.0635, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.07it/s, pg=0.071, ret=-9.97e-5, glen=55.4, tlen=216, kl=0.0635, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.07it/s, pg=-0.0441, ret=4.28e-5, glen=55.4, tlen=216, kl=0.0657, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0441, ret=4.28e-5, glen=55.4, tlen=216, kl=0.0657, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0615, ret=5.86e-5, glen=55.2, tlen=215, kl=0.0784, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0615, ret=5.86e-5, glen=55.2, tlen=215, kl=0.0784, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.0944, ret=-8.48e-5, glen=56.4, tlen=217, kl=0.059, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=0.0944, ret=-8.48e-5, glen=56.4, tlen=217, kl=0.059, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0752, ret=7.27e-5, glen=56.5, tlen=217, kl=0.088, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0752, ret=7.27e-5, glen=56.5, tlen=217, kl=0.088, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0394, ret=4.27e-5, glen=57.5, tlen=218, kl=0.0586, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0394, ret=4.27e-5, glen=57.5, tlen=218, kl=0.0586, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=0.2, ret=-0.000186, glen=54.2, tlen=215, kl=0.0675, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=0.2, ret=-0.000186, glen=54.2, tlen=215, kl=0.0675, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.0809, ret=-9.21e-5, glen=57.4, tlen=218, kl=0.0722, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=0.0809, ret=-9.21e-5, glen=57.4, tlen=218, kl=0.0722, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=-0.0312, ret=2.95e-5, glen=55.6, tlen=216, kl=0.057, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0312, ret=2.95e-5, glen=55.6, tlen=216, kl=0.057, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0857, ret=-8.15e-5, glen=53.5, tlen=214, kl=0.0711, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=0.0857, ret=-8.15e-5, glen=53.5, tlen=214, kl=0.0711, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0758, ret=-7.7e-5, glen=56.3, tlen=217, kl=0.0793, act_lr=1e-6, ent=0.894] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=0.0758, ret=-7.7e-5, glen=56.3, tlen=217, kl=0.0793, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0451, ret=4.69e-5, glen=57.5, tlen=218, kl=0.0895, act_lr=1e-6, ent=0.9] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0451, ret=4.69e-5, glen=57.5, tlen=218, kl=0.0895, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.038, ret=3.58e-5, glen=56.9, tlen=218, kl=0.0643, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.038, ret=3.58e-5, glen=56.9, tlen=218, kl=0.0643, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0436, ret=4.65e-5, glen=57, tlen=218, kl=0.0698, act_lr=1e-6, ent=0.909] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0436, ret=4.65e-5, glen=57, tlen=218, kl=0.0698, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=0.0631, ret=-7.7e-5, glen=56.7, tlen=217, kl=0.068, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=0.0631, ret=-7.7e-5, glen=56.7, tlen=217, kl=0.068, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0561, ret=4.93e-5, glen=54.1, tlen=214, kl=0.0826, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0561, ret=4.93e-5, glen=54.1, tlen=214, kl=0.0826, act_lr=1e-6, ent=0.886]
2025-07-24 21:59:11.350 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.01s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0416, ret=4.25e-5, glen=55.7, tlen=216, kl=0.0671, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.0416, ret=4.25e-5, glen=55.7, tlen=216, kl=0.0671, act_lr=1e-6, ent=0.912]
2025-07-24 21:59:12.185 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-24 21:59:14.784 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 21:59:15.118 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.90s
2025-07-24 21:59:15.125 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.002481163872612847, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8978787978490194, 'kl': 0.07221272786458334, 'response_length': 56.078613705105255, 'total_length': 216.62062208387587, 'teacher_total_length': 228.64159647623697, 'return': -1.9037076627783891e-06, 'policy_update_steps': 1.0}

Episode [12/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [18:34<05:30, 110.33s/it][A2025-07-24 21:59:15.170 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:01:26.854 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:01:27.041 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 22:01:27.041 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 131.87s
2025-07-24 22:01:28.789 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0011,avg_pass_at_n: 1.0000,avg_num_tokens: 56.3507,std_num_tokens: 88.5105,avg_correct_num_tokens: 55.3703,std_correct_num_tokens: 11.4063,avg_incorrect_num_tokens: 629.0714,std_incorrect_num_tokens: 2044.3833
2025-07-24 22:01:29.265 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.22s
2025-07-24 22:01:32.069 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.80s
2025-07-24 22:01:55.127 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 180
2025-07-24 22:01:55.127 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.06s
2025-07-24 22:01:56.313 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.79s
2025-07-24 22:01:56.313 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0004043830034788698, avg_kl: 0.07983356051974827, avg_response_length: 59.74987239837647, avg_orm_score: 0.0, avg_custom_rewards: -0.0004043830034788698
2025-07-24 22:01:56.342 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter153_replay_buffer.jsonl
2025-07-24 22:01:57.511 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.136, ret=2.55e-5, glen=54.3, tlen=215, kl=0.0756, act_lr=1e-6, ent=0.871]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.136, ret=2.55e-5, glen=54.3, tlen=215, kl=0.0756, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.01s/it, pg=-0.141, ret=3.89e-5, glen=56.6, tlen=217, kl=0.0863, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.141, ret=3.89e-5, glen=56.6, tlen=217, kl=0.0863, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.14, ret=3.69e-5, glen=55.7, tlen=216, kl=0.0714, act_lr=1e-6, ent=0.928] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.10it/s, pg=-0.14, ret=3.69e-5, glen=55.7, tlen=216, kl=0.0714, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.10it/s, pg=-0.0984, ret=-8.33e-5, glen=54.7, tlen=215, kl=0.059, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:37,  1.10it/s, pg=-0.0984, ret=-8.33e-5, glen=54.7, tlen=215, kl=0.059, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:37,  1.10it/s, pg=-0.0623, ret=-9.78e-5, glen=54.8, tlen=216, kl=0.0761, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.13it/s, pg=-0.0623, ret=-9.78e-5, glen=54.8, tlen=216, kl=0.0761, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.13it/s, pg=-0.139, ret=3.73e-5, glen=56, tlen=217, kl=0.0657, act_lr=1e-6, ent=0.879]    Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.14it/s, pg=-0.139, ret=3.73e-5, glen=56, tlen=217, kl=0.0657, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.14it/s, pg=-0.139, ret=3.5e-5, glen=54.3, tlen=215, kl=0.0649, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.15it/s, pg=-0.139, ret=3.5e-5, glen=54.3, tlen=215, kl=0.0649, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.15it/s, pg=-0.141, ret=4.31e-5, glen=55.2, tlen=216, kl=0.0673, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.141, ret=4.31e-5, glen=55.2, tlen=216, kl=0.0673, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.129, ret=1.11e-5, glen=54.5, tlen=215, kl=0.075, act_lr=1e-6, ent=0.882] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:30,  1.16it/s, pg=-0.129, ret=1.11e-5, glen=54.5, tlen=215, kl=0.075, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:30,  1.16it/s, pg=-0.141, ret=4.71e-5, glen=55.6, tlen=217, kl=0.0767, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=-0.141, ret=4.71e-5, glen=55.6, tlen=217, kl=0.0767, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=-0.146, ret=5.54e-5, glen=56.6, tlen=218, kl=0.0607, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.17it/s, pg=-0.146, ret=5.54e-5, glen=56.6, tlen=218, kl=0.0607, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.17it/s, pg=-0.0926, ret=-7.75e-5, glen=56, tlen=217, kl=0.092, act_lr=1e-6, ent=0.904] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.0926, ret=-7.75e-5, glen=56, tlen=217, kl=0.092, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=-0.136, ret=2.91e-5, glen=54.3, tlen=215, kl=0.0702, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=-0.136, ret=2.91e-5, glen=54.3, tlen=215, kl=0.0702, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=-0.144, ret=5.17e-5, glen=55.3, tlen=216, kl=0.188, act_lr=1e-6, ent=0.894] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=-0.144, ret=5.17e-5, glen=55.3, tlen=216, kl=0.188, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=-0.144, ret=5.11e-5, glen=55, tlen=216, kl=0.0684, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.13it/s, pg=-0.144, ret=5.11e-5, glen=55, tlen=216, kl=0.0684, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:26,  1.13it/s, pg=-0.0981, ret=-8.49e-5, glen=55.8, tlen=217, kl=0.0796, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=-0.0981, ret=-8.49e-5, glen=55.8, tlen=217, kl=0.0796, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=-0.136, ret=2.83e-5, glen=56.6, tlen=218, kl=0.0966, act_lr=1e-6, ent=0.894] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.15it/s, pg=-0.136, ret=2.83e-5, glen=56.6, tlen=218, kl=0.0966, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.15it/s, pg=-0.136, ret=2.97e-5, glen=56.4, tlen=217, kl=0.0645, act_lr=1e-6, ent=0.91] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.16it/s, pg=-0.136, ret=2.97e-5, glen=56.4, tlen=217, kl=0.0645, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.16it/s, pg=-0.139, ret=3.69e-5, glen=55.2, tlen=216, kl=0.0779, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.16it/s, pg=-0.139, ret=3.69e-5, glen=55.2, tlen=216, kl=0.0779, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.16it/s, pg=-0.141, ret=4.25e-5, glen=55.8, tlen=217, kl=0.0737, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=-0.141, ret=4.25e-5, glen=55.8, tlen=217, kl=0.0737, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=-0.0991, ret=-7.94e-5, glen=55.7, tlen=216, kl=0.0613, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.0991, ret=-7.94e-5, glen=55.7, tlen=216, kl=0.0613, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.14, ret=3.87e-5, glen=53.8, tlen=214, kl=0.0803, act_lr=1e-6, ent=0.908]   Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.14, ret=3.87e-5, glen=53.8, tlen=214, kl=0.0803, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.17it/s, pg=-0.0781, ret=-8.25e-5, glen=53.8, tlen=215, kl=0.102, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0781, ret=-8.25e-5, glen=53.8, tlen=215, kl=0.102, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.141, ret=4.3e-5, glen=56.2, tlen=217, kl=0.078, act_lr=1e-6, ent=0.902]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.15it/s, pg=-0.141, ret=4.3e-5, glen=56.2, tlen=217, kl=0.078, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.15it/s, pg=-0.142, ret=4.25e-5, glen=55.8, tlen=217, kl=0.123, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.16it/s, pg=-0.142, ret=4.25e-5, glen=55.8, tlen=217, kl=0.123, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.16it/s, pg=-0.138, ret=3.28e-5, glen=55.3, tlen=216, kl=0.102, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.16it/s, pg=-0.138, ret=3.28e-5, glen=55.3, tlen=216, kl=0.102, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.16it/s, pg=-0.143, ret=4.55e-5, glen=56.1, tlen=217, kl=0.0884, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.143, ret=4.55e-5, glen=56.1, tlen=217, kl=0.0884, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.138, ret=3.18e-5, glen=55.5, tlen=216, kl=0.0624, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.138, ret=3.18e-5, glen=55.5, tlen=216, kl=0.0624, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0927, ret=-9.6e-5, glen=54.6, tlen=215, kl=0.0759, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0927, ret=-9.6e-5, glen=54.6, tlen=215, kl=0.0759, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.136, ret=2.77e-5, glen=54.5, tlen=215, kl=0.0794, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.136, ret=2.77e-5, glen=54.5, tlen=215, kl=0.0794, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0874, ret=-7.3e-5, glen=54.6, tlen=216, kl=0.066, act_lr=1e-6, ent=0.866]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0874, ret=-7.3e-5, glen=54.6, tlen=216, kl=0.066, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.144, ret=4.62e-5, glen=55.1, tlen=216, kl=0.0745, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.144, ret=4.62e-5, glen=55.1, tlen=216, kl=0.0745, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.14, ret=4.13e-5, glen=56, tlen=217, kl=0.0621, act_lr=1e-6, ent=0.9]     Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.14, ret=4.13e-5, glen=56, tlen=217, kl=0.0621, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.115, ret=-6.09e-5, glen=55.3, tlen=216, kl=0.122, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.13it/s, pg=-0.115, ret=-6.09e-5, glen=55.3, tlen=216, kl=0.122, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.13it/s, pg=-0.141, ret=3.88e-5, glen=56.2, tlen=217, kl=0.0765, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:09,  1.11it/s, pg=-0.141, ret=3.88e-5, glen=56.2, tlen=217, kl=0.0765, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:09,  1.11it/s, pg=-0.108, ret=-7.7e-5, glen=55.1, tlen=216, kl=0.0893, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.13it/s, pg=-0.108, ret=-7.7e-5, glen=55.1, tlen=216, kl=0.0893, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.13it/s, pg=-0.135, ret=2.4e-5, glen=54.5, tlen=215, kl=0.0641, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:07,  1.14it/s, pg=-0.135, ret=2.4e-5, glen=54.5, tlen=215, kl=0.0641, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:07,  1.14it/s, pg=-0.142, ret=4.43e-5, glen=54.9, tlen=216, kl=0.069, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:06,  1.15it/s, pg=-0.142, ret=4.43e-5, glen=54.9, tlen=216, kl=0.069, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:06,  1.15it/s, pg=-0.137, ret=3.18e-5, glen=55.6, tlen=216, kl=0.0738, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.16it/s, pg=-0.137, ret=3.18e-5, glen=55.6, tlen=216, kl=0.0738, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.16it/s, pg=-0.136, ret=2.92e-5, glen=55.8, tlen=217, kl=0.0889, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.16it/s, pg=-0.136, ret=2.92e-5, glen=55.8, tlen=217, kl=0.0889, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.16it/s, pg=-0.141, ret=4.6e-5, glen=55.9, tlen=217, kl=0.071, act_lr=1e-6, ent=0.917]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.141, ret=4.6e-5, glen=55.9, tlen=217, kl=0.071, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0839, ret=-8.31e-5, glen=56.1, tlen=217, kl=0.0618, act_lr=1e-6, ent=0.869]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0839, ret=-8.31e-5, glen=56.1, tlen=217, kl=0.0618, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.103, ret=-8.01e-5, glen=54.8, tlen=215, kl=0.0721, act_lr=1e-6, ent=0.9]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.103, ret=-8.01e-5, glen=54.8, tlen=215, kl=0.0721, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=1.59, ret=-0.00235, glen=253, tlen=414, kl=0.0621, act_lr=1e-6, ent=0.67]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=1.59, ret=-0.00235, glen=253, tlen=414, kl=0.0621, act_lr=1e-6, ent=0.67]
2025-07-24 22:02:37.148 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.41s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.0986, ret=-8.12e-5, glen=55.8, tlen=216, kl=0.0981, act_lr=1e-6, ent=0.879]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0986, ret=-8.12e-5, glen=55.8, tlen=216, kl=0.0981, act_lr=1e-6, ent=0.879]
2025-07-24 22:02:37.988 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 22:02:40.606 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.62s
2025-07-24 22:02:40.954 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 43.34s
2025-07-24 22:02:40.960 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.08772549099392361, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.893039345741272, 'kl': 0.07983356051974827, 'response_length': 59.749872504340274, 'total_length': 220.5383558485243, 'teacher_total_length': 233.12391493055554, 'return': -4.987933233577577e-05, 'policy_update_steps': 1.0}

Episode [12/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [21:59<04:39, 139.56s/it][A2025-07-24 22:02:41.006 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:03:13.761 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:03:13.948 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 22:03:13.949 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.94s
2025-07-24 22:03:15.897 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0006,avg_pass_at_n: 1.0000,avg_num_tokens: 54.9044,std_num_tokens: 11.5229,avg_correct_num_tokens: 54.9055,std_correct_num_tokens: 11.5245,avg_incorrect_num_tokens: 54.0909,std_incorrect_num_tokens: 10.2643
2025-07-24 22:03:16.314 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.37s
2025-07-24 22:03:18.824 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.51s
2025-07-24 22:03:41.911 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 22:03:41.912 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.09s
2025-07-24 22:03:43.203 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.86s
2025-07-24 22:03:43.203 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -7.878558636967385e-06, avg_kl: 0.09308547116397473, avg_response_length: 54.93130973215853, avg_orm_score: 0.0, avg_custom_rewards: -7.878558636967385e-06
2025-07-24 22:03:43.231 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter154_replay_buffer.jsonl
2025-07-24 22:03:44.395 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0387, ret=2.95e-5, glen=54.6, tlen=215, kl=0.0681, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.0387, ret=2.95e-5, glen=54.6, tlen=215, kl=0.0681, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.0127, ret=1.11e-5, glen=55.4, tlen=216, kl=0.103, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.08it/s, pg=-0.0127, ret=1.11e-5, glen=55.4, tlen=216, kl=0.103, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.08it/s, pg=-0.0338, ret=2.56e-5, glen=54.5, tlen=215, kl=0.0861, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.09it/s, pg=-0.0338, ret=2.56e-5, glen=54.5, tlen=215, kl=0.0861, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.09it/s, pg=-0.0228, ret=1.81e-5, glen=54.3, tlen=214, kl=0.0798, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.0228, ret=1.81e-5, glen=54.3, tlen=214, kl=0.0798, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=0.146, ret=-8.26e-5, glen=57.1, tlen=217, kl=0.116, act_lr=1e-6, ent=0.895]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.11it/s, pg=0.146, ret=-8.26e-5, glen=57.1, tlen=217, kl=0.116, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.11it/s, pg=0.0996, ret=-8.97e-5, glen=54.1, tlen=214, kl=0.0796, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.13it/s, pg=0.0996, ret=-8.97e-5, glen=54.1, tlen=214, kl=0.0796, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.13it/s, pg=0.284, ret=-0.000183, glen=55.4, tlen=216, kl=0.0761, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.15it/s, pg=0.284, ret=-0.000183, glen=55.4, tlen=216, kl=0.0761, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.15it/s, pg=-0.0505, ret=3.86e-5, glen=55.5, tlen=216, kl=0.0823, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.0505, ret=3.86e-5, glen=55.5, tlen=216, kl=0.0823, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.039, ret=3.13e-5, glen=54.5, tlen=214, kl=0.0826, act_lr=1e-6, ent=0.904] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.16it/s, pg=-0.039, ret=3.13e-5, glen=54.5, tlen=214, kl=0.0826, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.16it/s, pg=-0.0641, ret=5.28e-5, glen=53.6, tlen=214, kl=0.0848, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=-0.0641, ret=5.28e-5, glen=53.6, tlen=214, kl=0.0848, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=-0.0267, ret=2.51e-5, glen=53.6, tlen=213, kl=0.0875, act_lr=1e-6, ent=0.869]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.17it/s, pg=-0.0267, ret=2.51e-5, glen=53.6, tlen=213, kl=0.0875, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.17it/s, pg=0.126, ret=-0.000104, glen=54.2, tlen=214, kl=0.122, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=0.126, ret=-0.000104, glen=54.2, tlen=214, kl=0.122, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=0.0928, ret=-9.54e-5, glen=56.8, tlen=217, kl=0.0803, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=0.0928, ret=-9.54e-5, glen=56.8, tlen=217, kl=0.0803, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=-0.032, ret=2.77e-5, glen=55.5, tlen=216, kl=0.0807, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.032, ret=2.77e-5, glen=55.5, tlen=216, kl=0.0807, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=-0.0358, ret=2.55e-5, glen=54.2, tlen=214, kl=0.0951, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0358, ret=2.55e-5, glen=54.2, tlen=214, kl=0.0951, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0316, ret=2.75e-5, glen=53.9, tlen=214, kl=0.0916, act_lr=1e-6, ent=0.871]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=-0.0316, ret=2.75e-5, glen=53.9, tlen=214, kl=0.0916, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0391, ret=2.95e-5, glen=55.4, tlen=215, kl=0.0917, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.0391, ret=2.95e-5, glen=55.4, tlen=215, kl=0.0917, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.047, ret=3.97e-5, glen=53.6, tlen=213, kl=0.108, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=-0.047, ret=3.97e-5, glen=53.6, tlen=213, kl=0.108, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=-0.0257, ret=2.56e-5, glen=55.5, tlen=215, kl=0.0763, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.0257, ret=2.56e-5, glen=55.5, tlen=215, kl=0.0763, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=0.149, ret=-8.65e-5, glen=55.4, tlen=216, kl=0.082, act_lr=1e-6, ent=0.89]   Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=0.149, ret=-8.65e-5, glen=55.4, tlen=216, kl=0.082, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.022, ret=1.81e-5, glen=54.3, tlen=214, kl=0.0957, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=-0.022, ret=1.81e-5, glen=54.3, tlen=214, kl=0.0957, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.0308, ret=2.58e-5, glen=55.4, tlen=216, kl=0.126, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0308, ret=2.58e-5, glen=55.4, tlen=216, kl=0.126, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0295, ret=2.4e-5, glen=54.8, tlen=215, kl=0.0962, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=-0.0295, ret=2.4e-5, glen=54.8, tlen=215, kl=0.0962, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0274, ret=2.15e-5, glen=55.5, tlen=216, kl=0.131, act_lr=1e-6, ent=0.885]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.0274, ret=2.15e-5, glen=55.5, tlen=216, kl=0.131, act_lr=1e-6, ent=0.885]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=0.113, ret=-9.06e-5, glen=55.3, tlen=215, kl=0.132, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.15it/s, pg=0.113, ret=-9.06e-5, glen=55.3, tlen=215, kl=0.132, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.15it/s, pg=-0.0582, ret=4.23e-5, glen=54.6, tlen=215, kl=0.126, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.15it/s, pg=-0.0582, ret=4.23e-5, glen=54.6, tlen=215, kl=0.126, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.15it/s, pg=-0.0269, ret=2.22e-5, glen=55.6, tlen=216, kl=0.0852, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.16it/s, pg=-0.0269, ret=2.22e-5, glen=55.6, tlen=216, kl=0.0852, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.16it/s, pg=-0.0124, ret=1.45e-5, glen=53.5, tlen=213, kl=0.0922, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.16it/s, pg=-0.0124, ret=1.45e-5, glen=53.5, tlen=213, kl=0.0922, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.16it/s, pg=-0.0407, ret=2.77e-5, glen=56.3, tlen=216, kl=0.0912, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.06it/s, pg=-0.0407, ret=2.77e-5, glen=56.3, tlen=216, kl=0.0912, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.06it/s, pg=0.108, ret=-8.49e-5, glen=55.9, tlen=216, kl=0.0861, act_lr=1e-6, ent=0.927] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.09it/s, pg=0.108, ret=-8.49e-5, glen=55.9, tlen=216, kl=0.0861, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.09it/s, pg=-0.0263, ret=1.66e-5, glen=55, tlen=215, kl=0.0972, act_lr=1e-6, ent=0.882] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0263, ret=1.66e-5, glen=55, tlen=215, kl=0.0972, act_lr=1e-6, ent=0.882]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0815, ret=5.91e-5, glen=56.1, tlen=216, kl=0.0933, act_lr=1e-6, ent=0.935]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.13it/s, pg=-0.0815, ret=5.91e-5, glen=56.1, tlen=216, kl=0.0933, act_lr=1e-6, ent=0.935]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=0.133, ret=-9.07e-5, glen=55.8, tlen=216, kl=0.0676, act_lr=1e-6, ent=0.89]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=0.133, ret=-9.07e-5, glen=55.8, tlen=216, kl=0.0676, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0503, ret=3.51e-5, glen=55, tlen=215, kl=0.0865, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0503, ret=3.51e-5, glen=55, tlen=215, kl=0.0865, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0427, ret=2.95e-5, glen=54.7, tlen=214, kl=0.115, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0427, ret=2.95e-5, glen=54.7, tlen=214, kl=0.115, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0455, ret=3.48e-5, glen=55.5, tlen=216, kl=0.0677, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.0455, ret=3.48e-5, glen=55.5, tlen=216, kl=0.0677, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0363, ret=2.58e-5, glen=55.3, tlen=215, kl=0.0724, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0363, ret=2.58e-5, glen=55.3, tlen=215, kl=0.0724, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0284, ret=2.58e-5, glen=55.5, tlen=216, kl=0.0773, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0284, ret=2.58e-5, glen=55.5, tlen=216, kl=0.0773, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0881, ret=-9.43e-5, glen=54.5, tlen=215, kl=0.0974, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=0.0881, ret=-9.43e-5, glen=54.5, tlen=215, kl=0.0974, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0111, ret=1.11e-5, glen=54.1, tlen=214, kl=0.0754, act_lr=1e-6, ent=0.873]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0111, ret=1.11e-5, glen=54.1, tlen=214, kl=0.0754, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0277, ret=2.22e-5, glen=55.1, tlen=215, kl=0.0914, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0277, ret=2.22e-5, glen=55.1, tlen=215, kl=0.0914, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0637, ret=4.18e-5, glen=54.2, tlen=214, kl=0.0691, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.0637, ret=4.18e-5, glen=54.2, tlen=214, kl=0.0691, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0253, ret=2.22e-5, glen=56.1, tlen=216, kl=0.0973, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0253, ret=2.22e-5, glen=56.1, tlen=216, kl=0.0973, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=0.0662, ret=-8.49e-5, glen=55.1, tlen=215, kl=0.0948, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=0.0662, ret=-8.49e-5, glen=55.1, tlen=215, kl=0.0948, act_lr=1e-6, ent=0.89]
2025-07-24 22:04:23.641 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.08s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=-0.0551, ret=3.9e-5, glen=53.2, tlen=213, kl=0.149, act_lr=1e-6, ent=0.895] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.0551, ret=3.9e-5, glen=53.2, tlen=213, kl=0.149, act_lr=1e-6, ent=0.895]
2025-07-24 22:04:24.412 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.70s
2025-07-24 22:04:26.855 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.44s
2025-07-24 22:04:27.188 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.74s
2025-07-24 22:04:27.194 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.003651852077907986, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8959077954292297, 'kl': 0.09303860134548611, 'response_length': 54.95832689073351, 'total_length': 215.07577209472657, 'teacher_total_length': 226.98521592881946, 'return': -2.6572462755009636e-06, 'policy_update_steps': 1.0}

Episode [12/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [23:46<02:09, 129.42s/it][A2025-07-24 22:04:27.201 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<02:18,  1.23it/s, est. speed input: 225.18 toks/s, output: 38.14 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  30%|‚ñà‚ñà‚ñà       | 52/172 [00:01<00:01, 67.18it/s, est. speed input: 5668.28 toks/s, output: 1500.45 toks/s]Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/172 [00:01<00:01, 89.11it/s, est. speed input: 6983.09 toks/s, output: 1951.63 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:02<00:00, 94.83it/s, est. speed input: 11059.39 toks/s, output: 3652.92 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 29.32it/s, est. speed input: 8524.62 toks/s, output: 2866.41 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 47.05it/s, est. speed input: 8524.62 toks/s, output: 2866.41 toks/s]
2025-07-24 22:04:31.701 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 293.0102,strategyqa_test/accuracy: 0.5240,eval_accuracy: 0.5240
2025-07-24 22:04:31.974 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:04:49.256 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:04:49.435 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 22:04:49.436 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 17.46s
2025-07-24 22:04:50.546 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0000,avg_pass_at_n: 1.0000,avg_num_tokens: 55.9345,std_num_tokens: 12.1611,avg_correct_num_tokens: 55.9365,std_correct_num_tokens: 12.1609,avg_incorrect_num_tokens: 53.7500,std_incorrect_num_tokens: 12.2347
2025-07-24 22:04:50.839 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.40s
2025-07-24 22:04:52.232 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.39s
2025-07-24 22:05:04.980 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 94
2025-07-24 22:05:04.981 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 12.75s
2025-07-24 22:05:05.988 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.57s
2025-07-24 22:05:05.988 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.2169034775108733e-05, avg_kl: 0.11685505319148937, avg_response_length: 55.949572218225356, avg_orm_score: 0.0, avg_custom_rewards: 1.2169034775108733e-05
2025-07-24 22:05:06.054 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter155_replay_buffer.jsonl
2025-07-24 22:05:06.708 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.66s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:02<00:00, 81.87it/s, est. speed input: 10690.16 toks/s, output: 3485.33 toks/s][32m [repeated 52x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 57.27it/s, est. speed input: 10407.28 toks/s, output: 3499.05 toks/s][32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, pg=-0.0206, ret=1.48e-5, glen=55.8, tlen=216, kl=0.0997, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:00<00:21,  1.05it/s, pg=-0.0206, ret=1.48e-5, glen=55.8, tlen=216, kl=0.0997, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/24 [00:01<00:21,  1.05it/s, pg=-0.0238, ret=1.69e-5, glen=56.8, tlen=218, kl=0.0995, act_lr=1e-6, ent=0.934]Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:01<00:19,  1.11it/s, pg=-0.0238, ret=1.69e-5, glen=56.8, tlen=218, kl=0.0995, act_lr=1e-6, ent=0.934]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 2/24 [00:02<00:19,  1.11it/s, pg=-0.0472, ret=3.02e-5, glen=56.4, tlen=217, kl=0.136, act_lr=1e-6, ent=0.911] Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:02<00:18,  1.11it/s, pg=-0.0472, ret=3.02e-5, glen=56.4, tlen=217, kl=0.136, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 3/24 [00:03<00:18,  1.11it/s, pg=-0.0354, ret=2.35e-5, glen=54.4, tlen=215, kl=0.129, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:03<00:17,  1.13it/s, pg=-0.0354, ret=2.35e-5, glen=54.4, tlen=215, kl=0.129, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 4/24 [00:04<00:17,  1.13it/s, pg=-0.0302, ret=2.02e-5, glen=55.3, tlen=216, kl=0.114, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:04<00:17,  1.10it/s, pg=-0.0302, ret=2.02e-5, glen=55.3, tlen=216, kl=0.114, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 5/24 [00:05<00:17,  1.10it/s, pg=-0.0388, ret=2.63e-5, glen=55.4, tlen=216, kl=0.101, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:05<00:16,  1.12it/s, pg=-0.0388, ret=2.63e-5, glen=55.4, tlen=216, kl=0.101, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 6/24 [00:06<00:16,  1.12it/s, pg=-0.033, ret=2.13e-5, glen=57.5, tlen=218, kl=0.094, act_lr=1e-6, ent=0.915] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:06<00:14,  1.14it/s, pg=-0.033, ret=2.13e-5, glen=57.5, tlen=218, kl=0.094, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 7/24 [00:07<00:14,  1.14it/s, pg=0.202, ret=-9e-5, glen=55.5, tlen=217, kl=0.12, act_lr=1e-6, ent=0.893]    Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:13,  1.15it/s, pg=0.202, ret=-9e-5, glen=55.5, tlen=217, kl=0.12, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 8/24 [00:07<00:13,  1.15it/s, pg=-0.0395, ret=2.58e-5, glen=55.3, tlen=216, kl=0.128, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:07<00:12,  1.16it/s, pg=-0.0395, ret=2.58e-5, glen=55.3, tlen=216, kl=0.128, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 9/24 [00:08<00:12,  1.16it/s, pg=-0.0352, ret=2.6e-5, glen=56, tlen=216, kl=0.0916, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:08<00:12,  1.16it/s, pg=-0.0352, ret=2.6e-5, glen=56, tlen=216, kl=0.0916, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 10/24 [00:09<00:12,  1.16it/s, pg=-0.0272, ret=1.86e-5, glen=56.9, tlen=218, kl=0.133, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:09<00:11,  1.17it/s, pg=-0.0272, ret=1.86e-5, glen=56.9, tlen=218, kl=0.133, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 11/24 [00:10<00:11,  1.17it/s, pg=-0.0237, ret=1.69e-5, glen=55.5, tlen=216, kl=0.116, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:10<00:10,  1.17it/s, pg=-0.0237, ret=1.69e-5, glen=55.5, tlen=216, kl=0.116, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 12/24 [00:11<00:10,  1.17it/s, pg=-0.0265, ret=1.9e-5, glen=56.4, tlen=217, kl=0.131, act_lr=1e-6, ent=0.912] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:11<00:09,  1.17it/s, pg=-0.0265, ret=1.9e-5, glen=56.4, tlen=217, kl=0.131, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 13/24 [00:12<00:09,  1.17it/s, pg=-0.0315, ret=2.03e-5, glen=56.2, tlen=217, kl=0.115, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:12<00:08,  1.17it/s, pg=-0.0315, ret=2.03e-5, glen=56.2, tlen=217, kl=0.115, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14/24 [00:13<00:08,  1.17it/s, pg=-0.0197, ret=1.48e-5, glen=56, tlen=216, kl=0.146, act_lr=1e-6, ent=0.922]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.17it/s, pg=-0.0197, ret=1.48e-5, glen=56, tlen=216, kl=0.146, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 15/24 [00:13<00:07,  1.17it/s, pg=0.132, ret=-9.64e-5, glen=55.1, tlen=216, kl=0.161, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:13<00:06,  1.17it/s, pg=0.132, ret=-9.64e-5, glen=55.1, tlen=216, kl=0.161, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 16/24 [00:14<00:06,  1.17it/s, pg=-0.0294, ret=2.06e-5, glen=57.1, tlen=218, kl=0.108, act_lr=1e-6, ent=0.933]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:14<00:05,  1.17it/s, pg=-0.0294, ret=2.06e-5, glen=57.1, tlen=218, kl=0.108, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 17/24 [00:15<00:05,  1.17it/s, pg=-0.0307, ret=1.85e-5, glen=55.7, tlen=216, kl=0.114, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:15<00:05,  1.17it/s, pg=-0.0307, ret=1.85e-5, glen=55.7, tlen=216, kl=0.114, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18/24 [00:16<00:05,  1.17it/s, pg=-0.0216, ret=1.48e-5, glen=56, tlen=216, kl=0.0909, act_lr=1e-6, ent=0.903] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:16<00:04,  1.17it/s, pg=-0.0216, ret=1.48e-5, glen=56, tlen=216, kl=0.0909, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19/24 [00:17<00:04,  1.17it/s, pg=-0.0327, ret=2.06e-5, glen=56.3, tlen=217, kl=0.104, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:17<00:03,  1.17it/s, pg=-0.0327, ret=2.06e-5, glen=56.3, tlen=217, kl=0.104, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 20/24 [00:18<00:03,  1.17it/s, pg=0.11, ret=-9.49e-5, glen=54.5, tlen=215, kl=0.123, act_lr=1e-6, ent=0.921]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:18<00:02,  1.17it/s, pg=0.11, ret=-9.49e-5, glen=54.5, tlen=215, kl=0.123, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21/24 [00:19<00:02,  1.17it/s, pg=-0.036, ret=2.4e-5, glen=55.8, tlen=216, kl=0.116, act_lr=1e-6, ent=0.875]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:19<00:01,  1.15it/s, pg=-0.036, ret=2.4e-5, glen=55.8, tlen=216, kl=0.116, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22/24 [00:19<00:01,  1.15it/s, pg=-0.0127, ret=7.47e-6, glen=56.3, tlen=217, kl=0.115, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:19<00:00,  1.16it/s, pg=-0.0127, ret=7.47e-6, glen=56.3, tlen=217, kl=0.115, act_lr=1e-6, ent=0.915]
2025-07-24 22:05:27.728 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 20.86s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.16it/s, pg=0.153, ret=-9.9e-5, glen=56.2, tlen=216, kl=0.119, act_lr=1e-6, ent=0.907]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23/24 [00:20<00:00,  1.11it/s, pg=0.153, ret=-9.9e-5, glen=56.2, tlen=216, kl=0.119, act_lr=1e-6, ent=0.907]
2025-07-24 22:05:28.401 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 22:05:30.637 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.24s
2025-07-24 22:05:30.969 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 24.22s
2025-07-24 22:05:30.972 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 2.6226043701171875e-05, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9082476447025934, 'kl': 0.11678059895833333, 'response_length': 55.938168366750084, 'total_length': 216.5890909830729, 'teacher_total_length': 228.69494438171387, 'return': 8.417101942844359e-07, 'policy_update_steps': 1.0}

Episode [12/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [24:49<00:00, 109.54s/it][A[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,981] [INFO] [utils.py:782:see_memory_usage] MA 8.79 GB         Max_MA 8.79 GB         CA 9.83 GB         Max_CA 10 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:1003:print]   activation_checkpointing_config  {

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 21:40:40,982] [INFO] [config.py:999:print] DeepSpeedEngine configuration:[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
2025-07-24 22:05:39.973 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:42:29,340] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 21:40:40,225] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:46:12,899] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:48:03,838] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:49:55,068] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:51:45,979] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:53:36,686] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:55:37,718] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 21:57:25,075] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 22:02:37,141] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 22:05:27,721] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:37,577] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:37,776] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 4407, num_elems = 23.10B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,276] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,277] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,283] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,285] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,634] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,634] [INFO] [utils.py:782:see_memory_usage] MA 9.52 GB         Max_MA 14.48 GB         CA 10.56 GB         Max_CA 55 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,635] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 111.02 GB, percent = 22.1%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [13/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [12/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [24:58<00:00, 115.31s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,967] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,967] [INFO] [utils.py:782:see_memory_usage] MA 9.52 GB         Max_MA 9.52 GB         CA 10.56 GB         Max_CA 11 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,967] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 111.01 GB, percent = 22.1%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,968] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,968] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,968] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,968] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,968] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee79cb4d7f0>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,969] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 22:05:39,970] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 22:05:40.274 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:06:12.809 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:06:12.982 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 22:06:12.983 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.71s
2025-07-24 22:06:15.024 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0010,avg_pass_at_n: 1.0000,avg_num_tokens: 55.1429,std_num_tokens: 12.2583,avg_correct_num_tokens: 55.1346,std_correct_num_tokens: 12.2502,avg_incorrect_num_tokens: 60.8333,std_incorrect_num_tokens: 15.9103
2025-07-24 22:06:15.436 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.45s
2025-07-24 22:06:17.889 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.45s
2025-07-24 22:06:41.591 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 22:06:41.592 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.70s
2025-07-24 22:06:42.838 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.84s
2025-07-24 22:06:42.838 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -1.3182284061933e-06, avg_kl: 0.0, avg_response_length: 55.158950441339044, avg_orm_score: 0.0, avg_custom_rewards: -1.3182284061933e-06
2025-07-24 22:06:42.872 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter156_replay_buffer.jsonl
2025-07-24 22:06:44.036 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=0.197, ret=-8.71e-5, glen=55, tlen=215, kl=0, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:45,  1.04s/it, pg=0.197, ret=-8.71e-5, glen=55, tlen=215, kl=0, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:45,  1.04s/it, pg=-0.0347, ret=2.75e-5, glen=54.5, tlen=215, kl=0, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:40,  1.07it/s, pg=-0.0347, ret=2.75e-5, glen=54.5, tlen=215, kl=0, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:40,  1.07it/s, pg=-0.0441, ret=3.32e-5, glen=54.6, tlen=215, kl=0, act_lr=1e-6, ent=0.88] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.08it/s, pg=-0.0441, ret=3.32e-5, glen=54.6, tlen=215, kl=0, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.08it/s, pg=0.0991, ret=-9.23e-5, glen=56.1, tlen=216, kl=0, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.11it/s, pg=0.0991, ret=-9.23e-5, glen=56.1, tlen=216, kl=0, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.11it/s, pg=-0.0454, ret=3.88e-5, glen=56, tlen=216, kl=0, act_lr=1e-6, ent=0.915]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.13it/s, pg=-0.0454, ret=3.88e-5, glen=56, tlen=216, kl=0, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.13it/s, pg=-0.057, ret=4.45e-5, glen=56.2, tlen=216, kl=0, act_lr=1e-6, ent=0.9] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:34,  1.13it/s, pg=-0.057, ret=4.45e-5, glen=56.2, tlen=216, kl=0, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:34,  1.13it/s, pg=-0.0401, ret=2.91e-5, glen=54.4, tlen=214, kl=0, act_lr=1e-6, ent=0.89]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.14it/s, pg=-0.0401, ret=2.91e-5, glen=54.4, tlen=214, kl=0, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.14it/s, pg=0.0823, ret=-7.65e-5, glen=56.7, tlen=217, kl=0, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=0.0823, ret=-7.65e-5, glen=56.7, tlen=217, kl=0, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.15it/s, pg=-0.0457, ret=3.8e-5, glen=53.3, tlen=213, kl=0, act_lr=1e-6, ent=0.887] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.16it/s, pg=-0.0457, ret=3.8e-5, glen=53.3, tlen=213, kl=0, act_lr=1e-6, ent=0.887]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.16it/s, pg=-0.0361, ret=3.1e-5, glen=54.7, tlen=215, kl=0, act_lr=1e-6, ent=0.897]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.17it/s, pg=-0.0361, ret=3.1e-5, glen=54.7, tlen=215, kl=0, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.17it/s, pg=-0.0164, ret=1.28e-5, glen=54, tlen=214, kl=0, act_lr=1e-6, ent=0.908] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.17it/s, pg=-0.0164, ret=1.28e-5, glen=54, tlen=214, kl=0, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.17it/s, pg=-0.0295, ret=2.4e-5, glen=54.8, tlen=215, kl=0, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.0295, ret=2.4e-5, glen=54.8, tlen=215, kl=0, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=-0.0268, ret=2.31e-5, glen=52.4, tlen=212, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.0268, ret=2.31e-5, glen=52.4, tlen=212, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=-0.0277, ret=2.03e-5, glen=55.6, tlen=216, kl=0, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.0277, ret=2.03e-5, glen=55.6, tlen=216, kl=0, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=0.121, ret=-9.23e-5, glen=55.9, tlen=216, kl=0, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.14it/s, pg=0.121, ret=-9.23e-5, glen=55.9, tlen=216, kl=0, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:26,  1.14it/s, pg=-0.0311, ret=2.51e-5, glen=53.9, tlen=214, kl=0, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=-0.0311, ret=2.51e-5, glen=53.9, tlen=214, kl=0, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.14it/s, pg=-0.0427, ret=3.12e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.14it/s, pg=-0.0427, ret=3.12e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.14it/s, pg=-0.0577, ret=4.3e-5, glen=53.4, tlen=214, kl=0, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.15it/s, pg=-0.0577, ret=4.3e-5, glen=53.4, tlen=214, kl=0, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.15it/s, pg=-0.0409, ret=3.28e-5, glen=55.8, tlen=216, kl=0, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.16it/s, pg=-0.0409, ret=3.28e-5, glen=55.8, tlen=216, kl=0, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.16it/s, pg=-0.0266, ret=1.81e-5, glen=55.2, tlen=215, kl=0, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=-0.0266, ret=1.81e-5, glen=55.2, tlen=215, kl=0, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=0.0656, ret=-7.32e-5, glen=54.6, tlen=215, kl=0, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:21,  1.14it/s, pg=0.0656, ret=-7.32e-5, glen=54.6, tlen=215, kl=0, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:21,  1.14it/s, pg=0.221, ret=-0.000206, glen=55.2, tlen=216, kl=0, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:20,  1.15it/s, pg=0.221, ret=-0.000206, glen=55.2, tlen=216, kl=0, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:20,  1.15it/s, pg=-0.0196, ret=1.48e-5, glen=54.1, tlen=214, kl=0, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=-0.0196, ret=1.48e-5, glen=54.1, tlen=214, kl=0, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=0.266, ret=-0.000211, glen=57, tlen=217, kl=0, act_lr=1e-6, ent=0.919]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.16it/s, pg=0.266, ret=-0.000211, glen=57, tlen=217, kl=0, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.16it/s, pg=-0.0441, ret=3.69e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0441, ret=3.69e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=0.0623, ret=-8.18e-5, glen=57.7, tlen=218, kl=0, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=0.0623, ret=-8.18e-5, glen=57.7, tlen=218, kl=0, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.138, ret=-8.35e-5, glen=54.9, tlen=215, kl=0, act_lr=1e-6, ent=0.906] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=0.138, ret=-8.35e-5, glen=54.9, tlen=215, kl=0, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0505, ret=4.98e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.18it/s, pg=-0.0505, ret=4.98e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.18it/s, pg=-0.0475, ret=3.57e-5, glen=56.8, tlen=217, kl=0, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0475, ret=3.57e-5, glen=56.8, tlen=217, kl=0, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0417, ret=3.14e-5, glen=54.8, tlen=215, kl=0, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0417, ret=3.14e-5, glen=54.8, tlen=215, kl=0, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0638, ret=4.8e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.925] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.13it/s, pg=-0.0638, ret=4.8e-5, glen=55.5, tlen=216, kl=0, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.13it/s, pg=-0.0416, ret=3.51e-5, glen=55.1, tlen=216, kl=0, act_lr=1e-6, ent=0.9] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.0416, ret=3.51e-5, glen=55.1, tlen=216, kl=0, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0273, ret=2.22e-5, glen=57.6, tlen=218, kl=0, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0273, ret=2.22e-5, glen=57.6, tlen=218, kl=0, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0278, ret=2.08e-5, glen=56, tlen=217, kl=0, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0278, ret=2.08e-5, glen=56, tlen=217, kl=0, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0377, ret=2.89e-5, glen=54, tlen=214, kl=0, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0377, ret=2.89e-5, glen=54, tlen=214, kl=0, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0416, ret=2.89e-5, glen=54.4, tlen=215, kl=0, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0416, ret=2.89e-5, glen=54.4, tlen=215, kl=0, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.17it/s, pg=0.0557, ret=-7.69e-5, glen=56.3, tlen=216, kl=0, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=0.0557, ret=-7.69e-5, glen=56.3, tlen=216, kl=0, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=0.0948, ret=-8.52e-5, glen=55.6, tlen=216, kl=0, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=0.0948, ret=-8.52e-5, glen=55.6, tlen=216, kl=0, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.045, ret=3.64e-5, glen=55.1, tlen=215, kl=0, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.045, ret=3.64e-5, glen=55.1, tlen=215, kl=0, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0382, ret=2.72e-5, glen=53.8, tlen=214, kl=0, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.18it/s, pg=-0.0382, ret=2.72e-5, glen=53.8, tlen=214, kl=0, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.18it/s, pg=-0.0553, ret=4.34e-5, glen=53.2, tlen=214, kl=0, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.18it/s, pg=-0.0553, ret=4.34e-5, glen=53.2, tlen=214, kl=0, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.18it/s, pg=-0.0266, ret=2.03e-5, glen=55.8, tlen=216, kl=0, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.0266, ret=2.03e-5, glen=55.8, tlen=216, kl=0, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.0427, ret=3.29e-5, glen=55.3, tlen=216, kl=0, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0427, ret=3.29e-5, glen=55.3, tlen=216, kl=0, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=-0.0419, ret=3.28e-5, glen=54.6, tlen=215, kl=0, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.0419, ret=3.28e-5, glen=54.6, tlen=215, kl=0, act_lr=1e-6, ent=0.894]
2025-07-24 22:07:23.361 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.15s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.18it/s, pg=-0.0569, ret=4.56e-5, glen=55.6, tlen=216, kl=0, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.0569, ret=4.56e-5, glen=55.6, tlen=216, kl=0, act_lr=1e-6, ent=0.912]
2025-07-24 22:07:24.044 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 22:07:26.374 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.33s
2025-07-24 22:07:26.768 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.68s
2025-07-24 22:07:26.774 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.0011349148220486111, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9046885795063443, 'kl': 0.0, 'response_length': 55.158846876356336, 'total_length': 215.34010416666666, 'teacher_total_length': 227.2882548014323, 'return': -2.2679306211001756e-06, 'policy_update_steps': 1.0}
Episode [13/20]:   8%|‚ñä         | 1/13 [01:46<21:21, 106.80s/it]2025-07-24 22:07:26.823 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:07:59.843 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:08:00.021 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 22:08:00.022 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.20s
2025-07-24 22:08:01.682 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0009,avg_pass_at_n: 1.0000,avg_num_tokens: 55.8749,std_num_tokens: 12.0465,avg_correct_num_tokens: 55.8835,std_correct_num_tokens: 12.0439,avg_incorrect_num_tokens: 51.2000,std_incorrect_num_tokens: 12.5762
2025-07-24 22:08:01.983 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.96s
2025-07-24 22:08:04.661 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.67s
2025-07-24 22:08:27.786 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 22:08:27.787 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.12s
2025-07-24 22:08:29.087 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.87s
2025-07-24 22:08:29.088 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 1.3489190485151536e-05, avg_kl: 0.002073021574393331, avg_response_length: 55.886559406472315, avg_orm_score: 0.0, avg_custom_rewards: 1.3489190485151536e-05
2025-07-24 22:08:29.116 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter157_replay_buffer.jsonl
2025-07-24 22:08:30.288 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=-0.0301, ret=3.21e-5, glen=57.9, tlen=219, kl=0.00384, act_lr=1e-6, ent=0.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.00it/s, pg=-0.0301, ret=3.21e-5, glen=57.9, tlen=219, kl=0.00384, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.00it/s, pg=-0.0329, ret=3.32e-5, glen=55.8, tlen=217, kl=0.00177, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=-0.0329, ret=3.32e-5, glen=55.8, tlen=217, kl=0.00177, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=0.061, ret=-7.71e-5, glen=57.1, tlen=218, kl=0.00243, act_lr=1e-6, ent=0.918] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.13it/s, pg=0.061, ret=-7.71e-5, glen=57.1, tlen=218, kl=0.00243, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.13it/s, pg=-0.0434, ret=3.88e-5, glen=55, tlen=216, kl=0.00176, act_lr=1e-6, ent=0.888] Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.0434, ret=3.88e-5, glen=55, tlen=216, kl=0.00176, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=-0.0458, ret=4.25e-5, glen=56.2, tlen=217, kl=0.00183, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.13it/s, pg=-0.0458, ret=4.25e-5, glen=56.2, tlen=217, kl=0.00183, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.13it/s, pg=-0.0395, ret=3.69e-5, glen=55.8, tlen=217, kl=0.00184, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=-0.0395, ret=3.69e-5, glen=55.8, tlen=217, kl=0.00184, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=-0.0346, ret=3.41e-5, glen=57.4, tlen=218, kl=0.00187, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:32,  1.16it/s, pg=-0.0346, ret=3.41e-5, glen=57.4, tlen=218, kl=0.00187, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:32,  1.16it/s, pg=-0.0497, ret=4.94e-5, glen=56.2, tlen=217, kl=0.00237, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0497, ret=4.94e-5, glen=56.2, tlen=217, kl=0.00237, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0535, ret=5.17e-5, glen=54.9, tlen=216, kl=0.00218, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:30,  1.17it/s, pg=-0.0535, ret=5.17e-5, glen=54.9, tlen=216, kl=0.00218, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:30,  1.17it/s, pg=0.0872, ret=-8.49e-5, glen=56, tlen=217, kl=0.00184, act_lr=1e-6, ent=0.9]    Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=0.0872, ret=-8.49e-5, glen=56, tlen=217, kl=0.00184, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=0.0674, ret=-8.25e-5, glen=57.5, tlen=218, kl=0.00204, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=0.0674, ret=-8.25e-5, glen=57.5, tlen=218, kl=0.00204, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.0321, ret=3.14e-5, glen=55.6, tlen=216, kl=0.00203, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=-0.0321, ret=3.14e-5, glen=55.6, tlen=216, kl=0.00203, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=0.241, ret=-0.000198, glen=55.1, tlen=216, kl=0.00243, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=0.241, ret=-0.000198, glen=55.1, tlen=216, kl=0.00243, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=0.0826, ret=-7.64e-5, glen=56.5, tlen=217, kl=0.00277, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=0.0826, ret=-7.64e-5, glen=56.5, tlen=217, kl=0.00277, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.17it/s, pg=0.0538, ret=-8.31e-5, glen=55, tlen=216, kl=0.00196, act_lr=1e-6, ent=0.911]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.15it/s, pg=0.0538, ret=-8.31e-5, glen=55, tlen=216, kl=0.00196, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:26,  1.15it/s, pg=0.00946, ret=-7.15e-5, glen=56.7, tlen=218, kl=0.00192, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:25,  1.15it/s, pg=0.00946, ret=-7.15e-5, glen=56.7, tlen=218, kl=0.00192, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:25,  1.15it/s, pg=-0.0655, ret=7.64e-5, glen=56, tlen=217, kl=0.00187, act_lr=1e-6, ent=0.913]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.16it/s, pg=-0.0655, ret=7.64e-5, glen=56, tlen=217, kl=0.00187, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.16it/s, pg=-0.0373, ret=3.69e-5, glen=55.8, tlen=216, kl=0.00192, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.14it/s, pg=-0.0373, ret=3.69e-5, glen=55.8, tlen=216, kl=0.00192, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.14it/s, pg=-0.05, ret=4.66e-5, glen=56.5, tlen=217, kl=0.00179, act_lr=1e-6, ent=0.895]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.15it/s, pg=-0.05, ret=4.66e-5, glen=56.5, tlen=217, kl=0.00179, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.15it/s, pg=0.0881, ret=-7.54e-5, glen=57, tlen=218, kl=0.0018, act_lr=1e-6, ent=0.93]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=0.0881, ret=-7.54e-5, glen=57, tlen=218, kl=0.0018, act_lr=1e-6, ent=0.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=-0.0427, ret=4.09e-5, glen=56.1, tlen=217, kl=0.00182, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.16it/s, pg=-0.0427, ret=4.09e-5, glen=56.1, tlen=217, kl=0.00182, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.16it/s, pg=-0.048, ret=4.47e-5, glen=55.6, tlen=216, kl=0.00198, act_lr=1e-6, ent=0.891] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:20,  1.15it/s, pg=-0.048, ret=4.47e-5, glen=55.6, tlen=216, kl=0.00198, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:20,  1.15it/s, pg=-0.0316, ret=3.18e-5, glen=56.7, tlen=217, kl=0.00185, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.16it/s, pg=-0.0316, ret=3.18e-5, glen=56.7, tlen=217, kl=0.00185, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:19,  1.16it/s, pg=-0.0479, ret=4.67e-5, glen=56.5, tlen=217, kl=0.00188, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:18,  1.16it/s, pg=-0.0479, ret=4.67e-5, glen=56.5, tlen=217, kl=0.00188, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:18,  1.16it/s, pg=-0.0359, ret=3.65e-5, glen=54.1, tlen=215, kl=0.00179, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0359, ret=3.65e-5, glen=54.1, tlen=215, kl=0.00179, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0323, ret=3.32e-5, glen=55.9, tlen=217, kl=0.00257, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0323, ret=3.32e-5, glen=55.9, tlen=217, kl=0.00257, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.0307, ret=3.14e-5, glen=56.2, tlen=217, kl=0.00217, act_lr=1e-6, ent=0.947]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0307, ret=3.14e-5, glen=56.2, tlen=217, kl=0.00217, act_lr=1e-6, ent=0.947]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0327, ret=3.36e-5, glen=55.1, tlen=216, kl=0.002, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0327, ret=3.36e-5, glen=55.1, tlen=216, kl=0.002, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0479, ret=4.43e-5, glen=55.1, tlen=216, kl=0.00193, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:15,  1.00it/s, pg=-0.0479, ret=4.43e-5, glen=55.1, tlen=216, kl=0.00193, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:15,  1.00it/s, pg=-0.0388, ret=3.78e-5, glen=56.6, tlen=218, kl=0.00172, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:14,  1.05it/s, pg=-0.0388, ret=3.78e-5, glen=56.6, tlen=218, kl=0.00172, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:14,  1.05it/s, pg=-0.0438, ret=4.2e-5, glen=54.3, tlen=215, kl=0.00256, act_lr=1e-6, ent=0.913] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.08it/s, pg=-0.0438, ret=4.2e-5, glen=54.3, tlen=215, kl=0.00256, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.08it/s, pg=0.0543, ret=-6.65e-5, glen=55.2, tlen=216, kl=0.00213, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.11it/s, pg=0.0543, ret=-6.65e-5, glen=55.2, tlen=216, kl=0.00213, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:29<00:11,  1.11it/s, pg=0.0595, ret=-6.01e-5, glen=56.7, tlen=217, kl=0.0025, act_lr=1e-6, ent=0.919] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.13it/s, pg=0.0595, ret=-6.01e-5, glen=56.7, tlen=217, kl=0.0025, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.13it/s, pg=0.112, ret=-7.45e-5, glen=57.4, tlen=219, kl=0.00205, act_lr=1e-6, ent=0.943]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.14it/s, pg=0.112, ret=-7.45e-5, glen=57.4, tlen=219, kl=0.00205, act_lr=1e-6, ent=0.943]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.14it/s, pg=-0.0526, ret=4.98e-5, glen=55.5, tlen=216, kl=0.00207, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0526, ret=4.98e-5, glen=55.5, tlen=216, kl=0.00207, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0371, ret=3.59e-5, glen=56, tlen=217, kl=0.00197, act_lr=1e-6, ent=0.889]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.0371, ret=3.59e-5, glen=56, tlen=217, kl=0.00197, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.03, ret=2.95e-5, glen=55.7, tlen=217, kl=0.0018, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.03, ret=2.95e-5, glen=55.7, tlen=217, kl=0.0018, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0296, ret=3.11e-5, glen=54, tlen=215, kl=0.00196, act_lr=1e-6, ent=0.891]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0296, ret=3.11e-5, glen=54, tlen=215, kl=0.00196, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:05,  1.17it/s, pg=-0.0291, ret=2.74e-5, glen=54.7, tlen=215, kl=0.00216, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0291, ret=2.74e-5, glen=54.7, tlen=215, kl=0.00216, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0406, ret=3.88e-5, glen=55, tlen=216, kl=0.00188, act_lr=1e-6, ent=0.888]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0406, ret=3.88e-5, glen=55, tlen=216, kl=0.00188, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.149, ret=-8.68e-5, glen=55.1, tlen=216, kl=0.00208, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.18it/s, pg=0.149, ret=-8.68e-5, glen=55.1, tlen=216, kl=0.00208, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.18it/s, pg=0.105, ret=-6.65e-5, glen=55.1, tlen=216, kl=0.00257, act_lr=1e-6, ent=0.936]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=0.105, ret=-6.65e-5, glen=55.1, tlen=216, kl=0.00257, act_lr=1e-6, ent=0.936]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.0444, ret=4.09e-5, glen=56.4, tlen=217, kl=0.00206, act_lr=1e-6, ent=0.925]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0444, ret=4.09e-5, glen=56.4, tlen=217, kl=0.00206, act_lr=1e-6, ent=0.925]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=-0.0477, ret=4.43e-5, glen=55.7, tlen=216, kl=0.00173, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0477, ret=4.43e-5, glen=55.7, tlen=216, kl=0.00173, act_lr=1e-6, ent=0.908]
2025-07-24 22:09:09.745 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.28s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=0.0607, ret=-7.9e-5, glen=55.7, tlen=217, kl=0.00177, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=0.0607, ret=-7.9e-5, glen=55.7, tlen=217, kl=0.00177, act_lr=1e-6, ent=0.917]
2025-07-24 22:09:10.579 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 22:09:12.897 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-24 22:09:13.226 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.89s
2025-07-24 22:09:13.232 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0005803002251519098, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9134248680538601, 'kl': 0.0020724402533637155, 'response_length': 55.873421817355684, 'total_length': 216.7306864420573, 'teacher_total_length': 228.78325941297743, 'return': 1.0843118313156688e-06, 'policy_update_steps': 1.0}
Episode [13/20]:  15%|‚ñà‚ñå        | 2/13 [03:33<19:32, 106.60s/it]2025-07-24 22:09:13.277 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:09:46.019 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:09:46.205 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 22:09:46.206 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 32.93s
2025-07-24 22:09:48.115 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0150,avg_reflection_pattern_score: 0.0012,avg_pass_at_n: 1.0000,avg_num_tokens: 55.7561,std_num_tokens: 12.1713,avg_correct_num_tokens: 55.7564,std_correct_num_tokens: 12.1706,avg_incorrect_num_tokens: 55.4286,std_incorrect_num_tokens: 12.9599
2025-07-24 22:09:48.526 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.32s
2025-07-24 22:09:51.000 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.47s
2025-07-24 22:10:14.111 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 178
2025-07-24 22:10:14.113 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.11s
2025-07-24 22:10:15.385 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.81s
2025-07-24 22:10:15.385 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -4.7602159693167453e-07, avg_kl: 0.01145343566208743, avg_response_length: 55.77093441566724, avg_orm_score: 0.0, avg_custom_rewards: -4.7602159693167453e-07
2025-07-24 22:10:15.414 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter158_replay_buffer.jsonl
2025-07-24 22:10:16.607 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.20s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0255, ret=1.62e-5, glen=55.4, tlen=216, kl=0.0124, act_lr=1e-6, ent=0.907]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:45,  1.04s/it, pg=-0.0255, ret=1.62e-5, glen=55.4, tlen=216, kl=0.0124, act_lr=1e-6, ent=0.907]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:45,  1.04s/it, pg=0.14, ret=-9.97e-5, glen=55.8, tlen=216, kl=0.00378, act_lr=1e-6, ent=0.875] Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:40,  1.07it/s, pg=0.14, ret=-9.97e-5, glen=55.8, tlen=216, kl=0.00378, act_lr=1e-6, ent=0.875]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:40,  1.07it/s, pg=-0.027, ret=1.64e-5, glen=55.1, tlen=215, kl=0.0038, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.11it/s, pg=-0.027, ret=1.64e-5, glen=55.1, tlen=215, kl=0.0038, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.11it/s, pg=-0.0161, ret=1.11e-5, glen=55.9, tlen=216, kl=0.00908, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.13it/s, pg=-0.0161, ret=1.11e-5, glen=55.9, tlen=216, kl=0.00908, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.13it/s, pg=0.142, ret=-0.000103, glen=56.5, tlen=217, kl=0.00384, act_lr=1e-6, ent=0.874]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:34,  1.15it/s, pg=0.142, ret=-0.000103, glen=56.5, tlen=217, kl=0.00384, act_lr=1e-6, ent=0.874]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:34,  1.15it/s, pg=-0.0223, ret=1.29e-5, glen=56.5, tlen=217, kl=0.0108, act_lr=1e-6, ent=0.889] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=-0.0223, ret=1.29e-5, glen=56.5, tlen=217, kl=0.0108, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=0.138, ret=-9.84e-5, glen=56.4, tlen=217, kl=0.00367, act_lr=1e-6, ent=0.87] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:32,  1.16it/s, pg=0.138, ret=-9.84e-5, glen=56.4, tlen=217, kl=0.00367, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:32,  1.16it/s, pg=-0.0134, ret=9.23e-6, glen=54.9, tlen=215, kl=0.0145, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0134, ret=9.23e-6, glen=54.9, tlen=215, kl=0.0145, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.06, ret=3.65e-5, glen=55.5, tlen=216, kl=0.0069, act_lr=1e-6, ent=0.893]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:30,  1.17it/s, pg=-0.06, ret=3.65e-5, glen=55.5, tlen=216, kl=0.0069, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:30,  1.17it/s, pg=-0.0185, ret=1.12e-5, glen=56.4, tlen=217, kl=0.0197, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:29,  1.17it/s, pg=-0.0185, ret=1.12e-5, glen=56.4, tlen=217, kl=0.0197, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:29,  1.17it/s, pg=-0.0352, ret=2.41e-5, glen=55.3, tlen=216, kl=0.00386, act_lr=1e-6, ent=0.892]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.17it/s, pg=-0.0352, ret=2.41e-5, glen=55.3, tlen=216, kl=0.00386, act_lr=1e-6, ent=0.892]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.17it/s, pg=-0.0597, ret=3.14e-5, glen=55.6, tlen=216, kl=0.0526, act_lr=1e-6, ent=0.905] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.17it/s, pg=-0.0597, ret=3.14e-5, glen=55.6, tlen=216, kl=0.0526, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.17it/s, pg=-0.0249, ret=1.66e-5, glen=55.3, tlen=216, kl=0.00396, act_lr=1e-6, ent=0.928]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.17it/s, pg=-0.0249, ret=1.66e-5, glen=55.3, tlen=216, kl=0.00396, act_lr=1e-6, ent=0.928]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.17it/s, pg=-0.0344, ret=2.03e-5, glen=56.3, tlen=216, kl=0.00898, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.0344, ret=2.03e-5, glen=56.3, tlen=216, kl=0.00898, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.17it/s, pg=-0.0308, ret=2.09e-5, glen=56.3, tlen=217, kl=0.00374, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:12<00:25,  1.17it/s, pg=-0.0308, ret=2.09e-5, glen=56.3, tlen=217, kl=0.00374, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0157, ret=1.08e-5, glen=54.8, tlen=215, kl=0.00408, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=-0.0157, ret=1.08e-5, glen=54.8, tlen=215, kl=0.00408, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0237, ret=1.46e-5, glen=55.2, tlen=216, kl=0.00365, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.0237, ret=1.46e-5, glen=55.2, tlen=216, kl=0.00365, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0274, ret=1.91e-5, glen=58.2, tlen=218, kl=0.012, act_lr=1e-6, ent=0.914]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:22,  1.17it/s, pg=-0.0274, ret=1.91e-5, glen=58.2, tlen=218, kl=0.012, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:22,  1.17it/s, pg=-0.0572, ret=3.32e-5, glen=55.1, tlen=215, kl=0.018, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.0572, ret=3.32e-5, glen=55.1, tlen=215, kl=0.018, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0309, ret=2.22e-5, glen=55.7, tlen=216, kl=0.00609, act_lr=1e-6, ent=0.94]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0309, ret=2.22e-5, glen=55.7, tlen=216, kl=0.00609, act_lr=1e-6, ent=0.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.0231, ret=1.66e-5, glen=55.5, tlen=216, kl=0.0476, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.18it/s, pg=-0.0231, ret=1.66e-5, glen=55.5, tlen=216, kl=0.0476, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.18it/s, pg=-0.0301, ret=2.05e-5, glen=56.3, tlen=217, kl=0.00388, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:18<00:19,  1.18it/s, pg=-0.0301, ret=2.05e-5, glen=56.3, tlen=217, kl=0.00388, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.18it/s, pg=-0.0382, ret=2.73e-5, glen=54.3, tlen=215, kl=0.00394, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.18it/s, pg=-0.0382, ret=2.73e-5, glen=54.3, tlen=215, kl=0.00394, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.18it/s, pg=-0.0352, ret=2.19e-5, glen=55.5, tlen=215, kl=0.00384, act_lr=1e-6, ent=0.881]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.18it/s, pg=-0.0352, ret=2.19e-5, glen=55.5, tlen=215, kl=0.00384, act_lr=1e-6, ent=0.881]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.18it/s, pg=0.269, ret=-0.000105, glen=56.2, tlen=216, kl=0.0036, act_lr=1e-6, ent=0.9]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=0.269, ret=-0.000105, glen=56.2, tlen=216, kl=0.0036, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0417, ret=2.18e-5, glen=54.9, tlen=215, kl=0.0215, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.18it/s, pg=-0.0417, ret=2.18e-5, glen=54.9, tlen=215, kl=0.0215, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.18it/s, pg=-0.0258, ret=1.66e-5, glen=56.5, tlen=217, kl=0.00402, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.18it/s, pg=-0.0258, ret=1.66e-5, glen=56.5, tlen=217, kl=0.00402, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.18it/s, pg=-0.0135, ret=7.86e-6, glen=55.6, tlen=216, kl=0.0572, act_lr=1e-6, ent=0.891] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0135, ret=7.86e-6, glen=55.6, tlen=216, kl=0.0572, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0223, ret=1.29e-5, glen=55.4, tlen=216, kl=0.014, act_lr=1e-6, ent=0.933] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0223, ret=1.29e-5, glen=55.4, tlen=216, kl=0.014, act_lr=1e-6, ent=0.933]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0325, ret=2e-5, glen=55, tlen=215, kl=0.00444, act_lr=1e-6, ent=0.918]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0325, ret=2e-5, glen=55, tlen=215, kl=0.00444, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0154, ret=1.11e-5, glen=56.5, tlen=217, kl=0.0174, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:26<00:12,  1.12it/s, pg=-0.0154, ret=1.11e-5, glen=56.5, tlen=217, kl=0.0174, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=0.164, ret=-0.000107, glen=56.4, tlen=217, kl=0.00424, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=0.164, ret=-0.000107, glen=56.4, tlen=217, kl=0.00424, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0327, ret=2.02e-5, glen=54.9, tlen=215, kl=0.0178, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0327, ret=2.02e-5, glen=54.9, tlen=215, kl=0.0178, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.161, ret=-9.82e-5, glen=55.6, tlen=216, kl=0.0311, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=0.161, ret=-9.82e-5, glen=55.6, tlen=216, kl=0.0311, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0231, ret=1.45e-5, glen=54.7, tlen=215, kl=0.0192, act_lr=1e-6, ent=0.924]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0231, ret=1.45e-5, glen=54.7, tlen=215, kl=0.0192, act_lr=1e-6, ent=0.924]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0208, ret=1.48e-5, glen=54.6, tlen=215, kl=0.00748, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0208, ret=1.48e-5, glen=54.6, tlen=215, kl=0.00748, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.17it/s, pg=-0.0268, ret=1.69e-5, glen=56.2, tlen=217, kl=0.00368, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:31<00:06,  1.17it/s, pg=-0.0268, ret=1.69e-5, glen=56.2, tlen=217, kl=0.00368, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0345, ret=2.22e-5, glen=56.5, tlen=217, kl=0.014, act_lr=1e-6, ent=0.9]    Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:32<00:05,  1.17it/s, pg=-0.0345, ret=2.22e-5, glen=56.5, tlen=217, kl=0.014, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.043, ret=2.4e-5, glen=56.6, tlen=217, kl=0.0121, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.043, ret=2.4e-5, glen=56.6, tlen=217, kl=0.0121, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.0975, ret=-8.68e-5, glen=56.9, tlen=217, kl=0.00953, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=0.0975, ret=-8.68e-5, glen=56.9, tlen=217, kl=0.00953, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=-0.0223, ret=1.48e-5, glen=54.4, tlen=215, kl=0.0042, act_lr=1e-6, ent=0.883] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=-0.0223, ret=1.48e-5, glen=54.4, tlen=215, kl=0.0042, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0283, ret=1.85e-5, glen=56.2, tlen=217, kl=0.00379, act_lr=1e-6, ent=0.914]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.0283, ret=1.85e-5, glen=56.2, tlen=217, kl=0.00379, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=-0.0296, ret=2e-5, glen=55.4, tlen=216, kl=0.00378, act_lr=1e-6, ent=0.902]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0296, ret=2e-5, glen=55.4, tlen=216, kl=0.00378, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=-0.0301, ret=1.87e-5, glen=57.5, tlen=218, kl=0.00364, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:37<00:00,  1.17it/s, pg=-0.0301, ret=1.87e-5, glen=57.5, tlen=218, kl=0.00364, act_lr=1e-6, ent=0.896]
2025-07-24 22:10:55.674 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 38.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0363, ret=2.4e-5, glen=55.5, tlen=216, kl=0.00399, act_lr=1e-6, ent=0.895] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.13it/s, pg=-0.0363, ret=2.4e-5, glen=55.5, tlen=216, kl=0.00399, act_lr=1e-6, ent=0.895]
2025-07-24 22:10:56.354 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 22:10:58.754 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.40s
2025-07-24 22:10:59.088 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.43s
2025-07-24 22:10:59.094 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.00038320753309461803, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9044088072246975, 'kl': 0.011677296956380208, 'response_length': 55.760918850368924, 'total_length': 216.05190734863282, 'teacher_total_length': 228.00001763237847, 'return': 3.0824002250382265e-07, 'policy_update_steps': 1.0}
Episode [13/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [05:19<17:42, 106.26s/it]2025-07-24 22:10:59.139 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:11:32.638 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:11:32.825 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 22:11:32.826 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.69s
2025-07-24 22:11:34.651 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 56.4939,std_num_tokens: 12.5836,avg_correct_num_tokens: 56.4944,std_correct_num_tokens: 12.5862,avg_incorrect_num_tokens: 55.8333,std_incorrect_num_tokens: 8.3150
2025-07-24 22:11:35.078 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.25s
2025-07-24 22:11:37.558 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.48s
2025-07-24 22:12:00.799 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 180
2025-07-24 22:12:00.799 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.24s
2025-07-24 22:12:02.259 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.78s
2025-07-24 22:12:02.259 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.105667307558987e-06, avg_kl: 0.07728017171223958, avg_response_length: 56.43055894639757, avg_orm_score: 0.0, avg_custom_rewards: 2.105667307558987e-06
2025-07-24 22:12:02.288 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter159_replay_buffer.jsonl
2025-07-24 22:12:03.444 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.16s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s, pg=-0.0191, ret=1.12e-5, glen=57, tlen=218, kl=0.00746, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:00<00:43,  1.00it/s, pg=-0.0191, ret=1.12e-5, glen=57, tlen=218, kl=0.00746, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:43,  1.00it/s, pg=-0.0164, ret=9.23e-6, glen=57.1, tlen=217, kl=0.155, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:40,  1.07it/s, pg=-0.0164, ret=9.23e-6, glen=57.1, tlen=217, kl=0.155, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:40,  1.07it/s, pg=-0.0248, ret=1.66e-5, glen=56.9, tlen=218, kl=0.0121, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.09it/s, pg=-0.0248, ret=1.66e-5, glen=56.9, tlen=218, kl=0.0121, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.09it/s, pg=-0.0345, ret=1.85e-5, glen=56.4, tlen=217, kl=0.00717, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.12it/s, pg=-0.0345, ret=1.85e-5, glen=56.4, tlen=217, kl=0.00717, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.12it/s, pg=-0.0118, ret=7.38e-6, glen=56, tlen=217, kl=0.00738, act_lr=1e-6, ent=0.942]  Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.14it/s, pg=-0.0118, ret=7.38e-6, glen=56, tlen=217, kl=0.00738, act_lr=1e-6, ent=0.942]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.14it/s, pg=-0.0377, ret=2.26e-5, glen=56, tlen=217, kl=0.107, act_lr=1e-6, ent=0.879]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.15it/s, pg=-0.0377, ret=2.26e-5, glen=56, tlen=217, kl=0.107, act_lr=1e-6, ent=0.879]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.15it/s, pg=-0.0512, ret=3.02e-5, glen=57.4, tlen=218, kl=0.00664, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:32,  1.16it/s, pg=-0.0512, ret=3.02e-5, glen=57.4, tlen=218, kl=0.00664, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:32,  1.16it/s, pg=0.209, ret=-0.000107, glen=55.7, tlen=217, kl=0.0063, act_lr=1e-6, ent=0.901] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=0.209, ret=-0.000107, glen=55.7, tlen=217, kl=0.0063, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:31,  1.16it/s, pg=-0.0263, ret=1.48e-5, glen=55.8, tlen=216, kl=0.525, act_lr=1e-6, ent=0.91]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:30,  1.16it/s, pg=-0.0263, ret=1.48e-5, glen=55.8, tlen=216, kl=0.525, act_lr=1e-6, ent=0.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:30,  1.16it/s, pg=-0.024, ret=1.51e-5, glen=56.9, tlen=218, kl=0.00781, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:31,  1.13it/s, pg=-0.024, ret=1.51e-5, glen=56.9, tlen=218, kl=0.00781, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:31,  1.13it/s, pg=0.176, ret=-0.000109, glen=57, tlen=218, kl=0.0179, act_lr=1e-6, ent=0.918]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.13it/s, pg=0.176, ret=-0.000109, glen=57, tlen=218, kl=0.0179, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.13it/s, pg=-0.0393, ret=2.27e-5, glen=56.8, tlen=218, kl=0.215, act_lr=1e-6, ent=0.912]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.15it/s, pg=-0.0393, ret=2.27e-5, glen=56.8, tlen=218, kl=0.215, act_lr=1e-6, ent=0.912]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.15it/s, pg=-0.0204, ret=1.12e-5, glen=57.7, tlen=218, kl=0.0327, act_lr=1e-6, ent=0.917]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.15it/s, pg=-0.0204, ret=1.12e-5, glen=57.7, tlen=218, kl=0.0327, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.15it/s, pg=-0.011, ret=7.47e-6, glen=56.6, tlen=217, kl=0.0606, act_lr=1e-6, ent=0.914] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=-0.011, ret=7.47e-6, glen=56.6, tlen=217, kl=0.0606, act_lr=1e-6, ent=0.914]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=-0.0333, ret=1.9e-5, glen=56.6, tlen=218, kl=0.00696, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.16it/s, pg=-0.0333, ret=1.9e-5, glen=56.6, tlen=218, kl=0.00696, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.16it/s, pg=-0.0234, ret=1.32e-5, glen=57.5, tlen=218, kl=0.00749, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=-0.0234, ret=1.32e-5, glen=57.5, tlen=218, kl=0.00749, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0195, ret=1.29e-5, glen=55.1, tlen=216, kl=0.49, act_lr=1e-6, ent=0.921]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.0195, ret=1.29e-5, glen=55.1, tlen=216, kl=0.49, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0189, ret=1.14e-5, glen=56, tlen=217, kl=0.00742, act_lr=1e-6, ent=0.923]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.17it/s, pg=-0.0189, ret=1.14e-5, glen=56, tlen=217, kl=0.00742, act_lr=1e-6, ent=0.923]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.17it/s, pg=-0.0295, ret=1.72e-5, glen=58.2, tlen=219, kl=0.00643, act_lr=1e-6, ent=0.922]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.17it/s, pg=-0.0295, ret=1.72e-5, glen=58.2, tlen=219, kl=0.00643, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.17it/s, pg=-0.0261, ret=1.48e-5, glen=55.7, tlen=216, kl=0.0852, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.17it/s, pg=-0.0261, ret=1.48e-5, glen=55.7, tlen=216, kl=0.0852, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.17it/s, pg=-0.0179, ret=9.4e-6, glen=56.4, tlen=217, kl=0.00688, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.15it/s, pg=-0.0179, ret=9.4e-6, glen=56.4, tlen=217, kl=0.00688, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.15it/s, pg=0.195, ret=-0.000105, glen=57.3, tlen=218, kl=0.0058, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.16it/s, pg=0.195, ret=-0.000105, glen=57.3, tlen=218, kl=0.0058, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.16it/s, pg=-0.0137, ret=9.23e-6, glen=54.6, tlen=216, kl=0.00638, act_lr=1e-6, ent=0.92]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.16it/s, pg=-0.0137, ret=9.23e-6, glen=54.6, tlen=216, kl=0.00638, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.16it/s, pg=-0.00846, ret=7.55e-6, glen=57.4, tlen=218, kl=0.133, act_lr=1e-6, ent=0.915]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.00846, ret=7.55e-6, glen=57.4, tlen=218, kl=0.133, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0181, ret=9.48e-6, glen=56.7, tlen=218, kl=0.0133, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0181, ret=9.48e-6, glen=56.7, tlen=218, kl=0.0133, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0488, ret=3.01e-5, glen=57.7, tlen=219, kl=0.213, act_lr=1e-6, ent=0.897] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0488, ret=3.01e-5, glen=57.7, tlen=219, kl=0.213, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.0514, ret=2.99e-5, glen=57.1, tlen=217, kl=0.102, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0514, ret=2.99e-5, glen=57.1, tlen=217, kl=0.102, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0338, ret=1.92e-5, glen=58.2, tlen=219, kl=0.00827, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0338, ret=1.92e-5, glen=58.2, tlen=219, kl=0.00827, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0256, ret=1.49e-5, glen=56.3, tlen=217, kl=0.179, act_lr=1e-6, ent=0.92]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0256, ret=1.49e-5, glen=56.3, tlen=217, kl=0.179, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0475, ret=2.44e-5, glen=56.8, tlen=218, kl=0.00654, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0475, ret=2.44e-5, glen=56.8, tlen=218, kl=0.00654, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0379, ret=2.23e-5, glen=55.8, tlen=217, kl=0.00731, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0379, ret=2.23e-5, glen=55.8, tlen=217, kl=0.00731, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0199, ret=1.3e-5, glen=55.6, tlen=216, kl=0.0306, act_lr=1e-6, ent=0.897]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:27<00:11,  1.14it/s, pg=-0.0199, ret=1.3e-5, glen=55.6, tlen=216, kl=0.0306, act_lr=1e-6, ent=0.897]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.14it/s, pg=-0.0164, ret=9.4e-6, glen=53.3, tlen=214, kl=0.00628, act_lr=1e-6, ent=0.932]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0164, ret=9.4e-6, glen=53.3, tlen=214, kl=0.00628, act_lr=1e-6, ent=0.932]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=-0.0381, ret=2.18e-5, glen=55.2, tlen=216, kl=0.00676, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.16it/s, pg=-0.0381, ret=2.18e-5, glen=55.2, tlen=216, kl=0.00676, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.16it/s, pg=-0.0049, ret=3.69e-6, glen=55.1, tlen=216, kl=0.00684, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0049, ret=3.69e-6, glen=55.1, tlen=216, kl=0.00684, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=-0.0268, ret=1.68e-5, glen=56.6, tlen=217, kl=0.0462, act_lr=1e-6, ent=0.922] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.0268, ret=1.68e-5, glen=56.6, tlen=217, kl=0.0462, act_lr=1e-6, ent=0.922]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0309, ret=1.85e-5, glen=56.5, tlen=217, kl=0.0065, act_lr=1e-6, ent=0.931]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0309, ret=1.85e-5, glen=56.5, tlen=217, kl=0.0065, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0145, ret=9.24e-6, glen=56.1, tlen=217, kl=0.0063, act_lr=1e-6, ent=0.927]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0145, ret=9.24e-6, glen=56.1, tlen=217, kl=0.0063, act_lr=1e-6, ent=0.927]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0331, ret=2.06e-5, glen=56.5, tlen=217, kl=0.209, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.0331, ret=2.06e-5, glen=56.5, tlen=217, kl=0.209, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=0.165, ret=-0.000103, glen=55.1, tlen=216, kl=0.0076, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=0.165, ret=-0.000103, glen=55.1, tlen=216, kl=0.0076, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.2, ret=-9.73e-5, glen=57.7, tlen=218, kl=0.0578, act_lr=1e-6, ent=0.917]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=0.2, ret=-9.73e-5, glen=57.7, tlen=218, kl=0.0578, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.0404, ret=2.43e-5, glen=56.9, tlen=217, kl=0.227, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.18it/s, pg=-0.0404, ret=2.43e-5, glen=56.9, tlen=217, kl=0.227, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.18it/s, pg=0.136, ret=-0.000107, glen=55.6, tlen=217, kl=0.0708, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.18it/s, pg=0.136, ret=-0.000107, glen=55.6, tlen=217, kl=0.0708, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.18it/s, pg=-0.0432, ret=2.42e-5, glen=55.6, tlen=217, kl=0.0486, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.18it/s, pg=-0.0432, ret=2.42e-5, glen=55.6, tlen=217, kl=0.0486, act_lr=1e-6, ent=0.911]
2025-07-24 22:12:42.722 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.11s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.18it/s, pg=-0.0327, ret=1.88e-5, glen=57.1, tlen=218, kl=0.292, act_lr=1e-6, ent=0.899] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.13it/s, pg=-0.0327, ret=1.88e-5, glen=57.1, tlen=218, kl=0.292, act_lr=1e-6, ent=0.899]
2025-07-24 22:12:43.383 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-24 22:12:45.488 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.10s
2025-07-24 22:12:45.823 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.33s
2025-07-24 22:12:45.842 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00021464029947916666, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9106219013532003, 'kl': 0.07728017171223958, 'response_length': 56.43055877685547, 'total_length': 217.23489583333333, 'teacher_total_length': 229.29840189615885, 'return': 9.275787912580806e-08, 'policy_update_steps': 1.0}
Episode [13/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [07:05<15:58, 106.46s/it]2025-07-24 22:12:45.854 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<01:20,  2.12it/s, est. speed input: 394.55 toks/s, output: 31.82 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/172 [00:02<00:00, 109.72it/s, est. speed input: 11528.95 toks/s, output: 3709.44 toks/s]
2025-07-24 22:12:49.789 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 294.4410,strategyqa_test/accuracy: 0.5138,eval_accuracy: 0.5138
2025-07-24 22:12:50.051 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:13:24.198 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:13:24.381 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 22:13:24.381 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 34.33s
2025-07-24 22:13:26.311 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0015,avg_pass_at_n: 1.0000,avg_num_tokens: 57.4053,std_num_tokens: 12.5646,avg_correct_num_tokens: 57.3934,std_correct_num_tokens: 12.5523,avg_incorrect_num_tokens: 71.2857,std_incorrect_num_tokens: 18.0453
2025-07-24 22:13:26.736 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.35s
2025-07-24 22:13:29.260 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.52s
2025-07-24 22:13:52.416 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 180
2025-07-24 22:13:52.417 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.15s
2025-07-24 22:13:53.733 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.85s
2025-07-24 22:13:53.734 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -6.614390359674062e-07, avg_kl: 0.893817138671875, avg_response_length: 57.43469840155707, avg_orm_score: 0.0, avg_custom_rewards: -6.614390359674062e-07
2025-07-24 22:13:53.763 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter160_replay_buffer.jsonl
2025-07-24 22:13:54.962 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.20s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:02<00:00, 82.34it/s, est. speed input: 10872.86 toks/s, output: 3638.33 toks/s][32m [repeated 56x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:03<00:00, 56.04it/s, est. speed input: 10152.51 toks/s, output: 3342.13 toks/s][32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.0374, ret=2.04e-5, glen=57.6, tlen=218, kl=0.0116, act_lr=1e-6, ent=0.886]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.0374, ret=2.04e-5, glen=57.6, tlen=218, kl=0.0116, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.0236, ret=1.32e-5, glen=57.4, tlen=217, kl=0.0116, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=-0.0236, ret=1.32e-5, glen=57.4, tlen=217, kl=0.0116, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.0247, ret=1.3e-5, glen=59.4, tlen=219, kl=0.0116, act_lr=1e-6, ent=0.917] Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:37,  1.13it/s, pg=-0.0247, ret=1.3e-5, glen=59.4, tlen=219, kl=0.0116, act_lr=1e-6, ent=0.917]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:37,  1.13it/s, pg=-0.0406, ret=2.63e-5, glen=57.9, tlen=218, kl=0.52, act_lr=1e-6, ent=0.92]  Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:35,  1.14it/s, pg=-0.0406, ret=2.63e-5, glen=57.9, tlen=218, kl=0.52, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:35,  1.14it/s, pg=-0.0326, ret=1.92e-5, glen=58.5, tlen=219, kl=0.0112, act_lr=1e-6, ent=0.919]Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:34,  1.15it/s, pg=-0.0326, ret=1.92e-5, glen=58.5, tlen=219, kl=0.0112, act_lr=1e-6, ent=0.919]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:34,  1.15it/s, pg=-0.0214, ret=1.32e-5, glen=57.6, tlen=218, kl=2.85, act_lr=1e-6, ent=0.904]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:33,  1.16it/s, pg=-0.0214, ret=1.32e-5, glen=57.6, tlen=218, kl=2.85, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:33,  1.16it/s, pg=-0.0245, ret=1.53e-5, glen=57.9, tlen=218, kl=1.34, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:34,  1.11it/s, pg=-0.0245, ret=1.53e-5, glen=57.9, tlen=218, kl=1.34, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:34,  1.11it/s, pg=-0.0305, ret=1.73e-5, glen=58.1, tlen=219, kl=0.0104, act_lr=1e-6, ent=0.905]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=-0.0305, ret=1.73e-5, glen=58.1, tlen=219, kl=0.0104, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.13it/s, pg=-0.0283, ret=1.69e-5, glen=56, tlen=217, kl=0.0129, act_lr=1e-6, ent=0.915]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:07<00:31,  1.14it/s, pg=-0.0283, ret=1.69e-5, glen=56, tlen=217, kl=0.0129, act_lr=1e-6, ent=0.915]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.14it/s, pg=-0.0309, ret=1.7e-5, glen=57.5, tlen=218, kl=0.206, act_lr=1e-6, ent=0.926]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.15it/s, pg=-0.0309, ret=1.7e-5, glen=57.5, tlen=218, kl=0.206, act_lr=1e-6, ent=0.926]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.15it/s, pg=-0.022, ret=1.33e-5, glen=58.4, tlen=219, kl=3.08, act_lr=1e-6, ent=0.895] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.16it/s, pg=-0.022, ret=1.33e-5, glen=58.4, tlen=219, kl=3.08, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.16it/s, pg=-0.0255, ret=1.49e-5, glen=56.9, tlen=217, kl=0.779, act_lr=1e-6, ent=0.911]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.16it/s, pg=-0.0255, ret=1.49e-5, glen=56.9, tlen=217, kl=0.779, act_lr=1e-6, ent=0.911]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.16it/s, pg=0.133, ret=-9.74e-5, glen=57.6, tlen=218, kl=0.0114, act_lr=1e-6, ent=0.92] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=0.133, ret=-9.74e-5, glen=57.6, tlen=218, kl=0.0114, act_lr=1e-6, ent=0.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=-0.0238, ret=1.12e-5, glen=57.7, tlen=218, kl=10.5, act_lr=1e-6, ent=0.898]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=-0.0238, ret=1.12e-5, glen=57.7, tlen=218, kl=10.5, act_lr=1e-6, ent=0.898]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=-0.0293, ret=1.85e-5, glen=56.3, tlen=217, kl=4.23, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=-0.0293, ret=1.85e-5, glen=56.3, tlen=217, kl=4.23, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.491, ret=-0.000219, glen=56.9, tlen=217, kl=1.51, act_lr=1e-6, ent=0.9]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:13<00:24,  1.17it/s, pg=0.491, ret=-0.000219, glen=56.9, tlen=217, kl=1.51, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0464, ret=2.85e-5, glen=58.9, tlen=219, kl=0.0118, act_lr=1e-6, ent=0.908]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:24,  1.14it/s, pg=-0.0464, ret=2.85e-5, glen=58.9, tlen=219, kl=0.0118, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:24,  1.14it/s, pg=-0.0325, ret=1.9e-5, glen=57.4, tlen=218, kl=3.29, act_lr=1e-6, ent=0.905]   Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.15it/s, pg=-0.0325, ret=1.9e-5, glen=57.4, tlen=218, kl=3.29, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.15it/s, pg=-0.0366, ret=2.08e-5, glen=57.7, tlen=218, kl=0.0108, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.16it/s, pg=-0.0366, ret=2.08e-5, glen=57.7, tlen=218, kl=0.0108, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.16it/s, pg=-0.0459, ret=2.66e-5, glen=57.8, tlen=218, kl=2.42, act_lr=1e-6, ent=0.904]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.16it/s, pg=-0.0459, ret=2.66e-5, glen=57.8, tlen=218, kl=2.42, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.16it/s, pg=0.154, ret=-9.52e-5, glen=58.1, tlen=219, kl=0.332, act_lr=1e-6, ent=0.893]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.17it/s, pg=0.154, ret=-9.52e-5, glen=58.1, tlen=219, kl=0.332, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.17it/s, pg=-0.0408, ret=2.42e-5, glen=56.2, tlen=217, kl=1.32, act_lr=1e-6, ent=0.913]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0408, ret=2.42e-5, glen=56.2, tlen=217, kl=1.32, act_lr=1e-6, ent=0.913]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.17it/s, pg=-0.0314, ret=1.67e-5, glen=56.2, tlen=217, kl=0.0119, act_lr=1e-6, ent=0.909]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:19<00:18,  1.17it/s, pg=-0.0314, ret=1.67e-5, glen=56.2, tlen=217, kl=0.0119, act_lr=1e-6, ent=0.909]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.028, ret=1.48e-5, glen=56.5, tlen=217, kl=1.13, act_lr=1e-6, ent=0.904]   Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=-0.028, ret=1.48e-5, glen=56.5, tlen=217, kl=1.13, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0572, ret=3.14e-5, glen=56.3, tlen=217, kl=0.164, act_lr=1e-6, ent=0.88]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0572, ret=3.14e-5, glen=56.3, tlen=217, kl=0.164, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=-0.0362, ret=2.26e-5, glen=56.8, tlen=217, kl=0.0433, act_lr=1e-6, ent=0.873]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=-0.0362, ret=2.26e-5, glen=56.8, tlen=217, kl=0.0433, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=0.174, ret=-9.38e-5, glen=57, tlen=217, kl=0.366, act_lr=1e-6, ent=0.906]    Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=0.174, ret=-9.38e-5, glen=57, tlen=217, kl=0.366, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0239, ret=1.5e-5, glen=57.7, tlen=218, kl=0.13, act_lr=1e-6, ent=0.903]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0239, ret=1.5e-5, glen=57.7, tlen=218, kl=0.13, act_lr=1e-6, ent=0.903]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=0.151, ret=-0.000101, glen=57.3, tlen=218, kl=0.24, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:16,  1.00s/it, pg=0.151, ret=-0.000101, glen=57.3, tlen=218, kl=0.24, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:16,  1.00s/it, pg=-0.0222, ret=1.12e-5, glen=57.4, tlen=218, kl=0.0124, act_lr=1e-6, ent=0.899]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:14,  1.05it/s, pg=-0.0222, ret=1.12e-5, glen=57.4, tlen=218, kl=0.0124, act_lr=1e-6, ent=0.899]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:14,  1.05it/s, pg=-0.023, ret=1.12e-5, glen=57.1, tlen=217, kl=0.0118, act_lr=1e-6, ent=0.902] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.08it/s, pg=-0.023, ret=1.12e-5, glen=57.1, tlen=217, kl=0.0118, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.08it/s, pg=-0.0261, ret=1.3e-5, glen=57.6, tlen=218, kl=0.0112, act_lr=1e-6, ent=0.918]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.11it/s, pg=-0.0261, ret=1.3e-5, glen=57.6, tlen=218, kl=0.0112, act_lr=1e-6, ent=0.918]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.11it/s, pg=-0.0386, ret=2.27e-5, glen=57.5, tlen=218, kl=0.57, act_lr=1e-6, ent=0.916] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.13it/s, pg=-0.0386, ret=2.27e-5, glen=57.5, tlen=218, kl=0.57, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.13it/s, pg=-0.0266, ret=1.68e-5, glen=56.6, tlen=217, kl=0.0131, act_lr=1e-6, ent=0.916]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.14it/s, pg=-0.0266, ret=1.68e-5, glen=56.6, tlen=217, kl=0.0131, act_lr=1e-6, ent=0.916]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.14it/s, pg=-0.0145, ret=5.54e-6, glen=57.2, tlen=217, kl=1.11, act_lr=1e-6, ent=0.9]    Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.15it/s, pg=-0.0145, ret=5.54e-6, glen=57.2, tlen=217, kl=1.11, act_lr=1e-6, ent=0.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.15it/s, pg=-0.0268, ret=1.48e-5, glen=56.3, tlen=217, kl=0.0113, act_lr=1e-6, ent=0.906]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=-0.0268, ret=1.48e-5, glen=56.3, tlen=217, kl=0.0113, act_lr=1e-6, ent=0.906]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0262, ret=1.54e-5, glen=59.6, tlen=220, kl=0.0117, act_lr=1e-6, ent=0.904]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.16it/s, pg=-0.0262, ret=1.54e-5, glen=59.6, tlen=220, kl=0.0117, act_lr=1e-6, ent=0.904]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.16it/s, pg=-0.0396, ret=2.03e-5, glen=56.5, tlen=217, kl=0.0112, act_lr=1e-6, ent=0.921]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:06,  1.17it/s, pg=-0.0396, ret=2.03e-5, glen=56.5, tlen=217, kl=0.0112, act_lr=1e-6, ent=0.921]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:34<00:06,  1.17it/s, pg=-0.0402, ret=2.45e-5, glen=58, tlen=218, kl=0.0118, act_lr=1e-6, ent=0.931]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0402, ret=2.45e-5, glen=58, tlen=218, kl=0.0118, act_lr=1e-6, ent=0.931]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0216, ret=1.15e-5, glen=56.9, tlen=217, kl=0.774, act_lr=1e-6, ent=0.929]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0216, ret=1.15e-5, glen=56.9, tlen=217, kl=0.774, act_lr=1e-6, ent=0.929]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.147, ret=-0.000101, glen=57, tlen=218, kl=0.0109, act_lr=1e-6, ent=0.886] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.17it/s, pg=0.147, ret=-0.000101, glen=57, tlen=218, kl=0.0109, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.17it/s, pg=-0.039, ret=2.31e-5, glen=59.1, tlen=220, kl=0.908, act_lr=1e-6, ent=0.944]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.17it/s, pg=-0.039, ret=2.31e-5, glen=59.1, tlen=220, kl=0.908, act_lr=1e-6, ent=0.944]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.17it/s, pg=-0.0242, ret=1.48e-5, glen=56.3, tlen=216, kl=0.105, act_lr=1e-6, ent=0.901]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.17it/s, pg=-0.0242, ret=1.48e-5, glen=56.3, tlen=216, kl=0.105, act_lr=1e-6, ent=0.901]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.17it/s, pg=-0.0491, ret=2.81e-5, glen=57.1, tlen=218, kl=2, act_lr=1e-6, ent=0.89]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.16it/s, pg=-0.0491, ret=2.81e-5, glen=57.1, tlen=218, kl=2, act_lr=1e-6, ent=0.89]
2025-07-24 22:14:34.514 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.16it/s, pg=-0.0432, ret=2.44e-5, glen=58.4, tlen=219, kl=0.0895, act_lr=1e-6, ent=0.887]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0432, ret=2.44e-5, glen=58.4, tlen=219, kl=0.0895, act_lr=1e-6, ent=0.887]
2025-07-24 22:14:35.187 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 22:14:37.417 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.23s
2025-07-24 22:14:37.752 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.72s
2025-07-24 22:14:37.758 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': 0.00032780965169270835, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.9051555871963501, 'kl': 0.8938171335392529, 'response_length': 57.434698486328124, 'total_length': 217.85454440646703, 'teacher_total_length': 229.84275885687933, 'return': -2.886127832526755e-08, 'policy_update_steps': 1.0}
Episode [13/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [08:57<14:27, 108.42s/it]2025-07-24 22:14:37.804 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 22:15:11.299 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 22:15:11.488 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 22:15:11.488 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 33.68s
2025-07-24 22:15:13.161 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0147,avg_reflection_pattern_score: 0.0005,avg_pass_at_n: 1.0000,avg_num_tokens: 56.6414,std_num_tokens: 12.7461,avg_correct_num_tokens: 56.6288,std_correct_num_tokens: 12.7300,avg_incorrect_num_tokens: 64.0000,std_incorrect_num_tokens: 18.6203
2025-07-24 22:15:13.590 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.10s
2025-07-24 22:15:16.094 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.50s
2025-07-24 22:15:39.368 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 179
2025-07-24 22:15:39.368 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 23.27s
2025-07-24 22:15:40.823 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.02s
2025-07-24 22:15:40.824 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 2.180297589762001e-05, avg_kl: 1.1130282439333101, avg_response_length: 56.66537030182737, avg_orm_score: 0.0, avg_custom_rewards: 2.180297589762001e-05
2025-07-24 22:15:40.854 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter161_replay_buffer.jsonl
2025-07-24 22:15:42.024 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.17s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/45 [00:01<?, ?it/s, pg=-0.017, ret=1.29e-5, glen=56.9, tlen=217, kl=0.0208, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.017, ret=1.29e-5, glen=56.9, tlen=217, kl=0.0208, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/45 [00:01<00:44,  1.00s/it, pg=-0.0567, ret=4.63e-5, glen=57.2, tlen=217, kl=1.05, act_lr=1e-6, ent=0.867] Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:01<00:39,  1.09it/s, pg=-0.0567, ret=4.63e-5, glen=57.2, tlen=217, kl=1.05, act_lr=1e-6, ent=0.867]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/45 [00:02<00:39,  1.09it/s, pg=-0.0209, ret=1.46e-5, glen=55.7, tlen=215, kl=0.0154, act_lr=1e-6, ent=0.865]Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:02<00:38,  1.10it/s, pg=-0.0209, ret=1.46e-5, glen=55.7, tlen=215, kl=0.0154, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 3/45 [00:03<00:38,  1.10it/s, pg=-0.0443, ret=4.25e-5, glen=57.2, tlen=217, kl=0.0165, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:03<00:36,  1.13it/s, pg=-0.0443, ret=4.25e-5, glen=57.2, tlen=217, kl=0.0165, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 4/45 [00:04<00:36,  1.13it/s, pg=-0.0572, ret=5.18e-5, glen=58, tlen=218, kl=0.429, act_lr=1e-6, ent=0.86]    Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:04<00:35,  1.12it/s, pg=-0.0572, ret=5.18e-5, glen=58, tlen=218, kl=0.429, act_lr=1e-6, ent=0.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 5/45 [00:05<00:35,  1.12it/s, pg=0.119, ret=-7.2e-5, glen=57.2, tlen=217, kl=5.08, act_lr=1e-6, ent=0.894]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:05<00:35,  1.10it/s, pg=0.119, ret=-7.2e-5, glen=57.2, tlen=217, kl=5.08, act_lr=1e-6, ent=0.894]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 6/45 [00:06<00:35,  1.10it/s, pg=0.223, ret=-0.000184, glen=58, tlen=218, kl=0.0171, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:06<00:33,  1.12it/s, pg=0.223, ret=-0.000184, glen=58, tlen=218, kl=0.0171, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 7/45 [00:07<00:33,  1.12it/s, pg=-0.0308, ret=2.4e-5, glen=56, tlen=216, kl=0.0181, act_lr=1e-6, ent=0.858] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:07<00:32,  1.14it/s, pg=-0.0308, ret=2.4e-5, glen=56, tlen=216, kl=0.0181, act_lr=1e-6, ent=0.858]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 8/45 [00:08<00:32,  1.14it/s, pg=-0.0274, ret=2.01e-5, glen=55, tlen=214, kl=0.0216, act_lr=1e-6, ent=0.869]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=-0.0274, ret=2.01e-5, glen=55, tlen=214, kl=0.0216, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 9/45 [00:08<00:31,  1.15it/s, pg=-0.0307, ret=2.77e-5, glen=56, tlen=216, kl=0.0176, act_lr=1e-6, ent=0.868]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:08<00:30,  1.16it/s, pg=-0.0307, ret=2.77e-5, glen=56, tlen=216, kl=0.0176, act_lr=1e-6, ent=0.868]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 10/45 [00:09<00:30,  1.16it/s, pg=-0.0491, ret=3.81e-5, glen=55.8, tlen=215, kl=0.635, act_lr=1e-6, ent=0.877]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:09<00:29,  1.14it/s, pg=-0.0491, ret=3.81e-5, glen=55.8, tlen=215, kl=0.635, act_lr=1e-6, ent=0.877]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 11/45 [00:10<00:29,  1.14it/s, pg=-0.0328, ret=3.12e-5, glen=56, tlen=216, kl=2.57, act_lr=1e-6, ent=0.873]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:10<00:28,  1.15it/s, pg=-0.0328, ret=3.12e-5, glen=56, tlen=216, kl=2.57, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 12/45 [00:11<00:28,  1.15it/s, pg=-0.0398, ret=3.44e-5, glen=55.7, tlen=215, kl=0.0164, act_lr=1e-6, ent=0.856]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:11<00:27,  1.16it/s, pg=-0.0398, ret=3.44e-5, glen=55.7, tlen=215, kl=0.0164, act_lr=1e-6, ent=0.856]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 13/45 [00:12<00:27,  1.16it/s, pg=0.0431, ret=-8.09e-5, glen=57.1, tlen=217, kl=0.0213, act_lr=1e-6, ent=0.878]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:12<00:26,  1.16it/s, pg=0.0431, ret=-8.09e-5, glen=57.1, tlen=217, kl=0.0213, act_lr=1e-6, ent=0.878]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 14/45 [00:13<00:26,  1.16it/s, pg=0.0796, ret=-9.05e-5, glen=57.4, tlen=217, kl=0.0247, act_lr=1e-6, ent=0.871]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:13<00:25,  1.17it/s, pg=0.0796, ret=-9.05e-5, glen=57.4, tlen=217, kl=0.0247, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [00:14<00:25,  1.17it/s, pg=0.177, ret=-5.38e-5, glen=54.3, tlen=214, kl=0.0213, act_lr=1e-6, ent=0.873] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=0.177, ret=-5.38e-5, glen=54.3, tlen=214, kl=0.0213, act_lr=1e-6, ent=0.873]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [00:14<00:24,  1.17it/s, pg=-0.0544, ret=4.47e-5, glen=57.3, tlen=217, kl=0.0212, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:14<00:23,  1.17it/s, pg=-0.0544, ret=4.47e-5, glen=57.3, tlen=217, kl=0.0212, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [00:15<00:23,  1.17it/s, pg=-0.0512, ret=4.22e-5, glen=56.3, tlen=216, kl=0.0185, act_lr=1e-6, ent=0.89] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:15<00:23,  1.14it/s, pg=-0.0512, ret=4.22e-5, glen=56.3, tlen=216, kl=0.0185, act_lr=1e-6, ent=0.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [00:16<00:23,  1.14it/s, pg=-0.0463, ret=4.09e-5, glen=56.9, tlen=216, kl=6.16, act_lr=1e-6, ent=0.886] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:16<00:22,  1.15it/s, pg=-0.0463, ret=4.09e-5, glen=56.9, tlen=216, kl=6.16, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [00:17<00:22,  1.15it/s, pg=-0.0225, ret=1.66e-5, glen=58, tlen=218, kl=0.0144, act_lr=1e-6, ent=0.888]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:17<00:21,  1.15it/s, pg=-0.0225, ret=1.66e-5, glen=58, tlen=218, kl=0.0144, act_lr=1e-6, ent=0.888]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [00:18<00:21,  1.15it/s, pg=-0.0507, ret=4.03e-5, glen=57.4, tlen=217, kl=0.0303, act_lr=1e-6, ent=0.889]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:18<00:20,  1.16it/s, pg=-0.0507, ret=4.03e-5, glen=57.4, tlen=217, kl=0.0303, act_lr=1e-6, ent=0.889]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [00:19<00:20,  1.16it/s, pg=-0.0371, ret=3e-5, glen=56.8, tlen=217, kl=9.51, act_lr=1e-6, ent=0.865]     Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:19<00:19,  1.16it/s, pg=-0.0371, ret=3e-5, glen=56.8, tlen=217, kl=9.51, act_lr=1e-6, ent=0.865]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [00:20<00:19,  1.16it/s, pg=-0.0396, ret=2.79e-5, glen=56.6, tlen=217, kl=0.0361, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=-0.0396, ret=2.79e-5, glen=56.6, tlen=217, kl=0.0361, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [00:20<00:18,  1.17it/s, pg=0.224, ret=-0.000198, glen=57.1, tlen=217, kl=0.0147, act_lr=1e-6, ent=0.884]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:20<00:17,  1.17it/s, pg=0.224, ret=-0.000198, glen=57.1, tlen=217, kl=0.0147, act_lr=1e-6, ent=0.884]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [00:21<00:17,  1.17it/s, pg=-0.0343, ret=3.09e-5, glen=56.4, tlen=216, kl=0.0149, act_lr=1e-6, ent=0.896]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:21<00:17,  1.17it/s, pg=-0.0343, ret=3.09e-5, glen=56.4, tlen=216, kl=0.0149, act_lr=1e-6, ent=0.896]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [00:22<00:17,  1.17it/s, pg=0.0735, ret=-7.59e-5, glen=57.7, tlen=218, kl=1.37, act_lr=1e-6, ent=0.869]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:22<00:16,  1.17it/s, pg=0.0735, ret=-7.59e-5, glen=57.7, tlen=218, kl=1.37, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [00:23<00:16,  1.17it/s, pg=-0.0488, ret=4.39e-5, glen=56.4, tlen=216, kl=3.62, act_lr=1e-6, ent=0.883]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:23<00:15,  1.17it/s, pg=-0.0488, ret=4.39e-5, glen=56.4, tlen=216, kl=3.62, act_lr=1e-6, ent=0.883]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [00:24<00:15,  1.17it/s, pg=-0.0278, ret=2.22e-5, glen=56.2, tlen=216, kl=0.018, act_lr=1e-6, ent=0.863]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:24<00:14,  1.17it/s, pg=-0.0278, ret=2.22e-5, glen=56.2, tlen=216, kl=0.018, act_lr=1e-6, ent=0.863]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [00:25<00:14,  1.17it/s, pg=-0.0234, ret=1.87e-5, glen=57.2, tlen=217, kl=0.0163, act_lr=1e-6, ent=0.858]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:25<00:14,  1.07it/s, pg=-0.0234, ret=1.87e-5, glen=57.2, tlen=217, kl=0.0163, act_lr=1e-6, ent=0.858]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [00:26<00:14,  1.07it/s, pg=-0.0698, ret=5.17e-5, glen=56.2, tlen=216, kl=2.12, act_lr=1e-6, ent=0.905]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:26<00:13,  1.10it/s, pg=-0.0698, ret=5.17e-5, glen=56.2, tlen=216, kl=2.12, act_lr=1e-6, ent=0.905]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [00:27<00:13,  1.10it/s, pg=-0.0726, ret=0.000108, glen=57.3, tlen=217, kl=0.444, act_lr=1e-6, ent=0.866]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:27<00:12,  1.12it/s, pg=-0.0726, ret=0.000108, glen=57.3, tlen=217, kl=0.444, act_lr=1e-6, ent=0.866]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [00:28<00:12,  1.12it/s, pg=-0.0293, ret=2.51e-5, glen=55.8, tlen=216, kl=5.26, act_lr=1e-6, ent=0.886]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.0293, ret=2.51e-5, glen=55.8, tlen=216, kl=5.26, act_lr=1e-6, ent=0.886]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [00:28<00:11,  1.13it/s, pg=-0.0454, ret=3.68e-5, glen=55.2, tlen=215, kl=0.0137, act_lr=1e-6, ent=0.876]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:28<00:10,  1.15it/s, pg=-0.0454, ret=3.68e-5, glen=55.2, tlen=215, kl=0.0137, act_lr=1e-6, ent=0.876]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [00:29<00:10,  1.15it/s, pg=0.111, ret=-0.000102, glen=55.5, tlen=215, kl=0.0178, act_lr=1e-6, ent=0.88] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:29<00:09,  1.15it/s, pg=0.111, ret=-0.000102, glen=55.5, tlen=215, kl=0.0178, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [00:30<00:09,  1.15it/s, pg=-0.0513, ret=4.14e-5, glen=57.4, tlen=217, kl=0.0257, act_lr=1e-6, ent=0.871]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:30<00:08,  1.16it/s, pg=-0.0513, ret=4.14e-5, glen=57.4, tlen=217, kl=0.0257, act_lr=1e-6, ent=0.871]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [00:31<00:08,  1.16it/s, pg=0.0752, ret=-7e-5, glen=56.9, tlen=217, kl=0.0155, act_lr=1e-6, ent=0.88]    Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:31<00:07,  1.16it/s, pg=0.0752, ret=-7e-5, glen=56.9, tlen=217, kl=0.0155, act_lr=1e-6, ent=0.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [00:32<00:07,  1.16it/s, pg=-0.0504, ret=4.74e-5, glen=57.1, tlen=217, kl=0.0184, act_lr=1e-6, ent=0.895]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:32<00:06,  1.17it/s, pg=-0.0504, ret=4.74e-5, glen=57.1, tlen=217, kl=0.0184, act_lr=1e-6, ent=0.895]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [00:33<00:06,  1.17it/s, pg=-0.0868, ret=7.38e-5, glen=56.7, tlen=217, kl=0.016, act_lr=1e-6, ent=0.891] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.0868, ret=7.38e-5, glen=56.7, tlen=217, kl=0.016, act_lr=1e-6, ent=0.891]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [00:33<00:05,  1.17it/s, pg=-0.00725, ret=3.69e-6, glen=56, tlen=216, kl=1.77, act_lr=1e-6, ent=0.893]  Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:33<00:05,  1.17it/s, pg=-0.00725, ret=3.69e-6, glen=56, tlen=216, kl=1.77, act_lr=1e-6, ent=0.893]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [00:34<00:05,  1.17it/s, pg=-0.0363, ret=2.95e-5, glen=56.9, tlen=217, kl=0.0171, act_lr=1e-6, ent=0.869]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:34<00:04,  1.17it/s, pg=-0.0363, ret=2.95e-5, glen=56.9, tlen=217, kl=0.0171, act_lr=1e-6, ent=0.869]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [00:35<00:04,  1.17it/s, pg=0.117, ret=-7.14e-5, glen=57.9, tlen=218, kl=0.279, act_lr=1e-6, ent=0.87]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:35<00:03,  1.15it/s, pg=0.117, ret=-7.14e-5, glen=57.9, tlen=218, kl=0.279, act_lr=1e-6, ent=0.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [00:36<00:03,  1.15it/s, pg=0.0962, ret=-8.25e-5, glen=56.4, tlen=216, kl=0.0162, act_lr=1e-6, ent=0.902]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:36<00:02,  1.16it/s, pg=0.0962, ret=-8.25e-5, glen=56.4, tlen=216, kl=0.0162, act_lr=1e-6, ent=0.902]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [00:37<00:02,  1.16it/s, pg=0.0535, ret=-7.2e-5, glen=57, tlen=217, kl=0.0181, act_lr=1e-6, ent=0.908]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:37<00:01,  1.16it/s, pg=0.0535, ret=-7.2e-5, glen=57, tlen=217, kl=0.0181, act_lr=1e-6, ent=0.908]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [00:38<00:01,  1.16it/s, pg=-0.0733, ret=6.73e-5, glen=57.2, tlen=217, kl=0.0199, act_lr=1e-6, ent=0.857]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:38<00:00,  1.17it/s, pg=-0.0733, ret=6.73e-5, glen=57.2, tlen=217, kl=0.0199, act_lr=1e-6, ent=0.857]
2025-07-24 22:16:21.456 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 39.26s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.17it/s, pg=-0.0768, ret=5.91e-5, glen=56.7, tlen=217, kl=8.92, act_lr=1e-6, ent=0.878]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [00:39<00:00,  1.12it/s, pg=-0.0768, ret=5.91e-5, glen=56.7, tlen=217, kl=8.92, act_lr=1e-6, ent=0.878]
2025-07-24 22:16:22.133 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 22:16:24.395 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.26s
2025-07-24 22:16:24.726 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 42.65s
2025-07-24 22:16:24.732 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0011118570963541666, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 0.8785028616587321, 'kl': 1.1070617622799344, 'response_length': 56.66517596774631, 'total_length': 216.45456882052952, 'teacher_total_length': 228.43585883246527, 'return': 2.065705474605137e-06, 'policy_update_steps': 1.0}
Episode [13/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [10:44<12:35, 107.93s/it]2025-07-24 22:16:24.778 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts

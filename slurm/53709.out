[2025-07-24 16:04:20,314] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-07-24 16:04:25.876 | INFO     | __main__:<module>:136 - --------- config key ---------        ------ value ------
seed                                  42
ref_num_nodes                         4
ref_num_gpus_per_node                 1
reward_num_nodes                      1
reward_num_gpus_per_node              2
actor_num_nodes                       4
actor_num_gpus_per_node               1
critic_num_nodes                      4
critic_num_gpus_per_node              1
colocate_critic_reward                True
colocate_actor_ref                    True
colocate_all                          True
vllm_num_engines                      4
vllm_tensor_parallel_size             1
vllm_sync_backend                     nccl
local_rank                            -1
pretrain                              /home/a/anokhin/links/scratch/Qwen2.5-1.5B
critic_pretrain
reward_pretrain                       <class 'NoneType'>
ckpt_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
save_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
tensorboard_log_dir                   /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
prompt_data                           <class 'omegaconf.listconfig.ListConfig'>
load_checkpoint                       False
zero_stage                            3
bf16                                  True
zpg                                   1
adam_offload                          False
flash_attn                            True
grad_accum_dtype                      <class 'NoneType'>
disable_trace_cache                   False
gradient_checkpointing                True
gradient_checkpointing_use_reentrant  False
disable_fast_tokenizer                False
target_modules                        all-linear
enable_prefix_caching                 True
enable_chunked_prefill                False
max_num_batched_tokens                2048
enforce_eager                         False
gpu_memory_utilization                0.25
eval_steps                            -1
save_steps                            -1
save_interval                         50
actor_learning_rate                   1e-06
critic_learning_rate                  5e-06
num_episodes                          20
max_epochs                            1
prompt_max_len                        2048
generate_max_len                      8000
train_batch_size                      256
micro_train_batch_size                1
rollout_batch_size                    128
micro_rollout_batch_size              128
micro_forward_batch_size              1
policy_update_steps                   1
critic_update_steps                   12
max_len                               8192
max_norm                              1.0
num_warmup_steps                      50
l2                                    0.0
eps_clip                              0.2
value_clip                            0.2
lambd                                 1.0
gamma                                 1.0
normalize_reward                      True
top_p                                 1.0
temperature                           1.0
freezing_actor_steps                  -1
n_samples_per_prompt                  64
kl_target                             <class 'NoneType'>
init_kl_coef                          0
use_kl_estimator_k3                   True
use_abs_kl                            False
use_kl_loss                           True
kl_loss_coef                          0.0
adam_betas                            (0.9, 0.95)
reward_clip_range                     (-10, 10)
use_compute_reward_fn                 True
advantage_normalize                   True
value_head_prefix                     value_head
ref_reward_offload                    False
enable_eval                           True
eval_interval                         10
update_ref_every_epoch                True
use_orm_score                         False
total_num_nodes                       4
exp_name                              binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
eval_prompt_data                      <class 'omegaconf.listconfig.ListConfig'>
prompt_data_probs                     <class 'omegaconf.listconfig.ListConfig'>
packing_max_len                       10048
top_k                                 -1
stop                                  <class 'omegaconf.listconfig.ListConfig'>
use_grpo                              True
wandb: Currently logged in as: avecplezir (irina-rish). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.21.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/wandb/run-20250724_160427-8405gkw1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/irina-rish/open-reasoner-zero
wandb: üöÄ View run at https://wandb.ai/irina-rish/open-reasoner-zero/runs/8405gkw1
2025-07-24 16:04:30,542	INFO worker.py:1841 -- Started a local Ray instance.
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it]
[36m(LLMActor pid=1435060)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it]
[36m(LLMActor pid=1435060)[0m 
[36m(LLMActor pid=1435061)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m 
[36m(LLMActor pid=1435062)[0m 
[36m(LLMActor pid=1435061)[0m 
2025-07-24 16:05:08.022 | INFO     | orz.ppo.utils:create_vllm_engines:452 - Offloaded all vLLM engines to CPU
2025-07-24 16:05:08.244 | INFO     | playground.orz_7b_ppo:train_dataset:580 - Start processing 1603 dialogues
2025-07-24 16:05:10.312 | INFO     | playground.orz_7b_ppo:train_dataset:589 - Finished processing 1603 dialogues
2025-07-24 16:05:10.316 | INFO     | playground.orz_7b_ppo:eval_dataset:603 - Start processing 687 dialogues
2025-07-24 16:05:10.753 | INFO     | playground.orz_7b_ppo:eval_dataset:612 - Finished processing 687 dialogues
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1
[36m(RefRayActorBase pid=1436501)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(LLMActor pid=1435061)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.91s/it][32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 4x across cluster][0m
[36m(PolicyRayActorBase pid=1435816)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...
[36m(PolicyRayActorBase pid=1435815)[0m Detected CUDA files, patching ldflags
[36m(PolicyRayActorBase pid=1435815)[0m Emitting ninja build file /home/a/anokhin/.cache/torch_extensions/py312_cu122/fused_adam/build.ninja...
[36m(PolicyRayActorBase pid=1435815)[0m /home/a/anokhin/envs/reasoner3.12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(PolicyRayActorBase pid=1435815)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(PolicyRayActorBase pid=1435815)[0m   warnings.warn(
[36m(PolicyRayActorBase pid=1435815)[0m Building extension module fused_adam...
[36m(PolicyRayActorBase pid=1435815)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(PolicyRayActorBase pid=1435815)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435815)[0m Loading extension module fused_adam...
2025-07-24 16:05:48.652 | INFO     | orz.ppo.trainer:build_models:815 - init policy/ref/critic/reward models done
2025-07-24 16:05:49.316 | INFO     | orz.ppo.trainer:train:74 - Create vllm engine gourps done.
2025-07-24 16:05:51.913 | INFO     | orz.ppo.trainer:train:76 - Sync actor weights to vllm engines, time cost: 2.60s
2025-07-24 16:05:52.208 | INFO     | orz.ppo.trainer:train:80 - Offload policy model to cpu, time cost: 0.29s
math_verify is not installed in this environment
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:38 __init__.py:207] Automatically detected platform cuda.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435061)[0m WARNING 07-24 16:04:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=1435061)[0m WARNING 07-24 16:04:54 config.py:685] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=44, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:39 __init__.py:207] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(LLMActor pid=1435060)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435062)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:54 config.py:549] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:55 cuda.py:229] Using Flash Attention backend.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:04:56 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...
[36m(LLMActor pid=1435062)[0m INFO 07-24 16:05:04 model_runner.py:1115] Loading model weights took 2.9105 GB
[36m(LLMActor pid=1435059)[0m WARNING 07-24 16:04:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m WARNING 07-24 16:04:54 config.py:685] Async output processing is not supported on the current platform type cuda.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=43, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, [32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:55 cuda.py:229] Using Flash Attention backend.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:04:56 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] Memory profiling takes 0.51 seconds
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:04 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:05 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:05 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x
[36m(LLMActor pid=1435061)[0m INFO 07-24 16:05:07 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.60 seconds
[36m(pid=1435640)[0m [2025-07-24 16:05:13,702] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 model_runner.py:1115] Loading model weights took 2.9105 GB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] Memory profiling takes 0.49 seconds[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:04 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:05 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:05 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m INFO 07-24 16:05:07 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.58 seconds[32m [repeated 3x across cluster][0m
[36m(pid=1435816)[0m [2025-07-24 16:05:21,478] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=1435818)[0m [2025-07-24 16:05:21,675] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:25,874] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:25,874] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(pid=1436500)[0m [2025-07-24 16:05:29,104] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 3x across cluster][0m
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:33,170] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:33,280] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1436500)[0m [2025-07-24 16:05:33,199] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:36,755] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(pid=1436499)[0m [2025-07-24 16:05:29,310] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 2x across cluster][0m
[36m(RefRayActorBase pid=1436501)[0m [2025-07-24 16:05:38,261] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 4x across cluster][0m
[36m(RefRayActorBase pid=1436499)[0m [2025-07-24 16:05:33,311] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,358] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,359] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,367] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,368] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,568] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,569] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.71 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,569] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.38 GB, percent = 10.4%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,709] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,710] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,710] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.38 GB, percent = 10.4%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,711] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7e4375e80>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,712] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:05:38,713] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:38,832] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:38,833] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:42,327] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(PolicyRayActorBase pid=1435815)[0m ninja: no work to do.
[36m(PolicyRayActorBase pid=1435815)[0m Time to load fused_adam op: 1.1178452968597412 seconds
[36m(PolicyRayActorBase pid=1435815)[0m [2025-07-24 16:05:44,887] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1435815)[0m [2025-07-24 16:05:38,841] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,958] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,959] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,981] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,984] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:44,984] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,008] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,242] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,243] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.6 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,243] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,244] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,244] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,417] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,418] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,418] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,567] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,567] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,568] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,697] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,697] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:45,698] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.8 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.73 GB         CA 0.72 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,528] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.72 GB         Max_CA 1 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,659] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.88 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,792] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,922] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,923] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:46,923] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,054] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 3.59 GB         CA 4.32 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.47 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,055] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,286] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,286] [INFO] [utils.py:782:see_memory_usage] MA 4.53 GB         Max_MA 5.39 GB         CA 5.76 GB         Max_CA 6 GB 
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.46 GB, percent = 11.4%
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7e72ecb7a690>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,287] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(PolicyRayActorBase pid=1435640)[0m     "partition_activations": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "contiguous_memory_optimization": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "cpu_checkpointing": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "number_checkpoints": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "synchronize_checkpoint_boundary": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "profile": false
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "start_step": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "end_step": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "metric_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "arg_mappings": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "metric": "throughput", 
[36m(PolicyRayActorBase pid=1435640)[0m     "model_info": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "results_dir": "autotuning_results", 
[36m(PolicyRayActorBase pid=1435640)[0m     "exps_dir": "autotuning_exps", 
[36m(PolicyRayActorBase pid=1435640)[0m     "overwrite": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "fast": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "start_profile_step": 3, 
[36m(PolicyRayActorBase pid=1435640)[0m     "end_profile_step": 5, 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_type": "gridsearch", 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_early_stopping": 5, 
[36m(PolicyRayActorBase pid=1435640)[0m     "tuner_num_trials": 50, 
[36m(PolicyRayActorBase pid=1435640)[0m     "model_info_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "mp_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "max_train_batch_size": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "min_train_batch_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(PolicyRayActorBase pid=1435640)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "num_tuning_micro_batch_sizes": 3
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7e72ecb79a90>
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "recompute_fwd_factor": 0.0, 
[36m(PolicyRayActorBase pid=1435640)[0m     "profile_step": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "module_depth": -1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "top_modules": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "detailed": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "output_file": null
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,288] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(PolicyRayActorBase pid=1435640)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "persistent_storage_path": null, 
[36m(PolicyRayActorBase pid=1435640)[0m     "persistent_time_interval": 100, 
[36m(PolicyRayActorBase pid=1435640)[0m     "num_of_version_in_retention": 2, 
[36m(PolicyRayActorBase pid=1435640)[0m     "enable_nebula_load": true, 
[36m(PolicyRayActorBase pid=1435640)[0m     "load_path": null
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:05:47,289] [INFO] [config.py:989:print_user_config]   json = {
[36m(PolicyRayActorBase pid=1435640)[0m     "steps_per_print": 100, 
[36m(PolicyRayActorBase pid=1435640)[0m     "zero_optimization": {
[36m(PolicyRayActorBase pid=1435640)[0m         "stage": 3, 
[36m(PolicyRayActorBase pid=1435640)[0m         "offload_param": {
[36m(PolicyRayActorBase pid=1435640)[0m             "device": "none"
[36m(PolicyRayActorBase pid=1435640)[0m         }, 
[36m(PolicyRayActorBase pid=1435640)[0m         "offload_optimizer": {
[36m(PolicyRayActorBase pid=1435640)[0m             "device": "none", 
[36m(PolicyRayActorBase pid=1435640)[0m             "pin_memory": true
[36m(PolicyRayActorBase pid=1435640)[0m         }, 
[36m(PolicyRayActorBase pid=1435640)[0m         "sub_group_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_max_live_parameters": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_max_reuse_distance": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "reduce_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_hpz_partition_size": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_quantized_weights": false, 
[36m(PolicyRayActorBase pid=1435640)[0m         "zero_quantized_gradients": false
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "bf16": {
[36m(PolicyRayActorBase pid=1435640)[0m         "enabled": true
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "gradient_clipping": 1.0, 
[36m(PolicyRayActorBase pid=1435640)[0m     "prescale_gradients": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "wall_clock_breakdown": false, 
[36m(PolicyRayActorBase pid=1435640)[0m     "data_types": {
[36m(PolicyRayActorBase pid=1435640)[0m         "grad_accum_dtype": "fp32"
[36m(PolicyRayActorBase pid=1435640)[0m     }, 
[36m(PolicyRayActorBase pid=1435640)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1435640)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=1435640)[0m }
[36m(LLMActor pid=1435061)[0m init_process_group: master_address=10.224.3.58, master_port=41739,  rank=3, world_size=5, group_name=openrlhf
[36m(PolicyRayActorBase pid=1435640)[0m WARNING:using --vllm_sync_backend=gloo for vLLM version > 0.4.2 (or export NCCL_P2P_DISABLE=1)
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435818)[0m Time to load fused_adam op: 1.216843605041504 seconds[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435818)[0m [2025-07-24 16:05:44,986] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 3x across cluster][0m
Episode [1/20]:   0%|          | 0/13 [00:00<?, ?it/s]2025-07-24 16:05:52.312 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(PolicyRayActorBase pid=1435640)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=1435818)[0m Loading extension module fused_adam...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 1/172 [00:00<00:59,  2.87it/s, est. speed input: 512.95 toks/s, output: 20.06 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:   1%|          | 2/172 [00:00<00:36,  4.64it/s, est. speed input: 767.99 toks/s, output: 40.31 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:04<00:00, 20.63it/s, est. speed input: 6303.86 toks/s, output: 2760.14 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:05<00:01, 12.39it/s, est. speed input: 5149.97 toks/s, output: 2432.96 toks/s][32m [repeated 110x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00,  1.88it/s, est. speed input: 3136.79 toks/s, output: 1923.58 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00, 17.30it/s, est. speed input: 3136.79 toks/s, output: 1923.58 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:08<00:00,  2.74it/s, est. speed input: 3468.63 toks/s, output: 1831.23 toks/s][32m [repeated 30x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:05<00:01,  8.87it/s, est. speed input: 5068.50 toks/s, output: 2273.45 toks/s]
2025-07-24 16:06:07.922 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 494.2853,strategyqa_test/accuracy: 0.3115,eval_accuracy: 0.3115
2025-07-24 16:06:08.418 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:07:55.866 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:07:56.045 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:07:56.046 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 107.63s
2025-07-24 16:08:14.553 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0094,avg_pass_at_n: 1.0000,avg_num_tokens: 114.4545,std_num_tokens: 132.9845,avg_correct_num_tokens: 105.4537,std_correct_num_tokens: 84.9028,avg_incorrect_num_tokens: 129.5815,std_incorrect_num_tokens: 186.8873
2025-07-24 16:08:14.903 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 18.86s
2025-07-24 16:08:18.212 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.31s
2025-07-24 16:08:47.873 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:08:47.874 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.66s
2025-07-24 16:08:49.264 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 16:08:49.265 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0006267745350284185, avg_kl: 0.0, avg_response_length: 116.57365384164335, avg_orm_score: 0.0, avg_custom_rewards: 0.0006267745350284185
2025-07-24 16:08:49.328 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter0_replay_buffer.jsonl
2025-07-24 16:08:51.234 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00,  1.23s/it, est. speed input: 2194.44 toks/s, output: 1255.84 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00, 12.11it/s, est. speed input: 2194.44 toks/s, output: 1255.84 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:12<00:00,  1.09it/s, est. speed input: 2445.90 toks/s, output: 1474.87 toks/s][32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.14, ret=-0.00164, glen=103, tlen=264, kl=0, act_lr=0, ent=1.73]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<01:24,  1.49s/it, pg=0.14, ret=-0.00164, glen=103, tlen=264, kl=0, act_lr=0, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:02<01:24,  1.49s/it, pg=0.057, ret=-0.000267, glen=104, tlen=264, kl=0, act_lr=0, ent=1.67]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<01:09,  1.24s/it, pg=0.057, ret=-0.000267, glen=104, tlen=264, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:04<01:09,  1.24s/it, pg=0.069, ret=-0.001, glen=139, tlen=299, kl=0, act_lr=0, ent=1.95]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:04<01:15,  1.37s/it, pg=0.069, ret=-0.001, glen=139, tlen=299, kl=0, act_lr=0, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:04<01:15,  1.37s/it, pg=0.287, ret=-0.000979, glen=126, tlen=287, kl=0, act_lr=0, ent=1.76]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<01:03,  1.17s/it, pg=0.287, ret=-0.000979, glen=126, tlen=287, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:05<01:03,  1.17s/it, pg=-0.055, ret=-0.000206, glen=105, tlen=265, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:56,  1.07s/it, pg=-0.055, ret=-0.000206, glen=105, tlen=265, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:06<00:56,  1.07s/it, pg=-0.107, ret=-0.000409, glen=115, tlen=275, kl=0, act_lr=0, ent=1.63]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:52,  1.01s/it, pg=-0.107, ret=-0.000409, glen=115, tlen=275, kl=0, act_lr=0, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:07<00:52,  1.01s/it, pg=-0.0383, ret=-0.000243, glen=112, tlen=272, kl=0, act_lr=0, ent=1.64]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:49,  1.04it/s, pg=-0.0383, ret=-0.000243, glen=112, tlen=272, kl=0, act_lr=0, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:08<00:49,  1.04it/s, pg=-0.0483, ret=0.00091, glen=104, tlen=264, kl=0, act_lr=0, ent=1.78]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:46,  1.08it/s, pg=-0.0483, ret=0.00091, glen=104, tlen=264, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:09<00:46,  1.08it/s, pg=-0.0175, ret=-0.000894, glen=106, tlen=266, kl=0, act_lr=0, ent=1.59]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:09<00:44,  1.11it/s, pg=-0.0175, ret=-0.000894, glen=106, tlen=266, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:10<00:44,  1.11it/s, pg=0.0853, ret=-0.000294, glen=117, tlen=277, kl=0, act_lr=0, ent=1.73] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:10<00:43,  1.10it/s, pg=0.0853, ret=-0.000294, glen=117, tlen=277, kl=0, act_lr=0, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:11<00:43,  1.10it/s, pg=0.118, ret=-0.000719, glen=116, tlen=276, kl=0, act_lr=0, ent=1.75] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:11<00:41,  1.12it/s, pg=0.118, ret=-0.000719, glen=116, tlen=276, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:11<00:41,  1.12it/s, pg=0.00751, ret=0.000226, glen=112, tlen=273, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.14it/s, pg=0.00751, ret=0.000226, glen=112, tlen=273, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:12<00:40,  1.14it/s, pg=0.126, ret=-0.00022, glen=108, tlen=268, kl=0, act_lr=0, ent=1.69]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:39,  1.15it/s, pg=0.126, ret=-0.00022, glen=108, tlen=268, kl=0, act_lr=0, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:13<00:39,  1.15it/s, pg=-0.179, ret=0.00049, glen=98.3, tlen=259, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.16it/s, pg=-0.179, ret=0.00049, glen=98.3, tlen=259, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:14<00:37,  1.16it/s, pg=-0.0616, ret=3.93e-5, glen=109, tlen=269, kl=0, act_lr=0, ent=1.77]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:36,  1.17it/s, pg=-0.0616, ret=3.93e-5, glen=109, tlen=269, kl=0, act_lr=0, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:15<00:36,  1.17it/s, pg=0.0941, ret=-0.000905, glen=123, tlen=284, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:15<00:35,  1.17it/s, pg=0.0941, ret=-0.000905, glen=123, tlen=284, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:16<00:35,  1.17it/s, pg=-0.103, ret=-0.000127, glen=94.4, tlen=255, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:16<00:34,  1.17it/s, pg=-0.103, ret=-0.000127, glen=94.4, tlen=255, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:16<00:34,  1.17it/s, pg=0.0759, ret=0.000215, glen=123, tlen=284, kl=0, act_lr=0, ent=1.76]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=0.0759, ret=0.000215, glen=123, tlen=284, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:17<00:34,  1.17it/s, pg=-0.154, ret=0.00225, glen=121, tlen=282, kl=0, act_lr=0, ent=1.62] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.18it/s, pg=-0.154, ret=0.00225, glen=121, tlen=282, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:18<00:33,  1.18it/s, pg=0.0566, ret=-0.000261, glen=116, tlen=276, kl=0, act_lr=0, ent=1.72]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.18it/s, pg=0.0566, ret=-0.000261, glen=116, tlen=276, kl=0, act_lr=0, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:19<00:32,  1.18it/s, pg=-0.0662, ret=0.000428, glen=101, tlen=262, kl=0, act_lr=0, ent=1.59]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.18it/s, pg=-0.0662, ret=0.000428, glen=101, tlen=262, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:20<00:31,  1.18it/s, pg=-0.102, ret=-0.000148, glen=124, tlen=284, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.18it/s, pg=-0.102, ret=-0.000148, glen=124, tlen=284, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:21<00:30,  1.18it/s, pg=0.0445, ret=-0.000576, glen=91.7, tlen=252, kl=0, act_lr=0, ent=1.65]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:21<00:29,  1.18it/s, pg=0.0445, ret=-0.000576, glen=91.7, tlen=252, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:22<00:29,  1.18it/s, pg=-0.0389, ret=-0.000265, glen=112, tlen=273, kl=0, act_lr=0, ent=1.82]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:22<00:28,  1.18it/s, pg=-0.0389, ret=-0.000265, glen=112, tlen=273, kl=0, act_lr=0, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:22<00:28,  1.18it/s, pg=-0.00873, ret=0.00307, glen=179, tlen=339, kl=0, act_lr=0, ent=2.23] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.17it/s, pg=-0.00873, ret=0.00307, glen=179, tlen=339, kl=0, act_lr=0, ent=2.23]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:23<00:28,  1.17it/s, pg=-0.203, ret=0.00135, glen=116, tlen=277, kl=0, act_lr=0, ent=1.68]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.17it/s, pg=-0.203, ret=0.00135, glen=116, tlen=277, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:24<00:27,  1.17it/s, pg=0.111, ret=-0.00221, glen=118, tlen=278, kl=0, act_lr=0, ent=1.84]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.111, ret=-0.00221, glen=118, tlen=278, kl=0, act_lr=0, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:25<00:26,  1.17it/s, pg=-0.00208, ret=-0.000165, glen=115, tlen=276, kl=0, act_lr=0, ent=1.54]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.18it/s, pg=-0.00208, ret=-0.000165, glen=115, tlen=276, kl=0, act_lr=0, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:26<00:25,  1.18it/s, pg=-0.113, ret=0.000108, glen=107, tlen=267, kl=0, act_lr=0, ent=1.67]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.113, ret=0.000108, glen=107, tlen=267, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:27<00:27,  1.07it/s, pg=-0.0209, ret=-0.000352, glen=108, tlen=269, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.10it/s, pg=-0.0209, ret=-0.000352, glen=108, tlen=269, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:28<00:25,  1.10it/s, pg=-0.00781, ret=-0.00215, glen=91.8, tlen=252, kl=0, act_lr=0, ent=1.54]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.12it/s, pg=-0.00781, ret=-0.00215, glen=91.8, tlen=252, kl=0, act_lr=0, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:29<00:24,  1.12it/s, pg=0.244, ret=-0.000828, glen=137, tlen=298, kl=0, act_lr=0, ent=2.15]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:22,  1.14it/s, pg=0.244, ret=-0.000828, glen=137, tlen=298, kl=0, act_lr=0, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:30<00:22,  1.14it/s, pg=-0.0299, ret=0.000116, glen=122, tlen=282, kl=0, act_lr=0, ent=2.02]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:30<00:21,  1.15it/s, pg=-0.0299, ret=0.000116, glen=122, tlen=282, kl=0, act_lr=0, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:30<00:21,  1.15it/s, pg=0.0894, ret=-0.000191, glen=108, tlen=268, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.16it/s, pg=0.0894, ret=-0.000191, glen=108, tlen=268, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:31<00:20,  1.16it/s, pg=0.102, ret=0.00018, glen=116, tlen=276, kl=0, act_lr=0, ent=1.83]   Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=0.102, ret=0.00018, glen=116, tlen=276, kl=0, act_lr=0, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:32<00:19,  1.16it/s, pg=-0.0702, ret=-0.000848, glen=108, tlen=269, kl=0, act_lr=0, ent=1.78]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=-0.0702, ret=-0.000848, glen=108, tlen=269, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:33<00:18,  1.17it/s, pg=-0.0553, ret=-0.000724, glen=101, tlen=261, kl=0, act_lr=0, ent=1.8] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=-0.0553, ret=-0.000724, glen=101, tlen=261, kl=0, act_lr=0, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:34<00:17,  1.17it/s, pg=-0.0718, ret=-0.000542, glen=102, tlen=263, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.17it/s, pg=-0.0718, ret=-0.000542, glen=102, tlen=263, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:35<00:17,  1.17it/s, pg=-0.136, ret=0.00152, glen=128, tlen=288, kl=0, act_lr=0, ent=1.91]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.17it/s, pg=-0.136, ret=0.00152, glen=128, tlen=288, kl=0, act_lr=0, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.17it/s, pg=0.176, ret=-0.000703, glen=119, tlen=280, kl=0, act_lr=0, ent=1.81]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.18it/s, pg=0.176, ret=-0.000703, glen=119, tlen=280, kl=0, act_lr=0, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:36<00:15,  1.18it/s, pg=0.217, ret=0.000893, glen=163, tlen=324, kl=0, act_lr=0, ent=2.15] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.217, ret=0.000893, glen=163, tlen=324, kl=0, act_lr=0, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:37<00:14,  1.16it/s, pg=-0.0223, ret=-0.00191, glen=116, tlen=277, kl=0, act_lr=0, ent=1.77]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.15it/s, pg=-0.0223, ret=-0.00191, glen=116, tlen=277, kl=0, act_lr=0, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:38<00:13,  1.15it/s, pg=-0.11, ret=0.00112, glen=116, tlen=277, kl=0, act_lr=0, ent=1.66]   Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:13,  1.15it/s, pg=-0.11, ret=0.00112, glen=116, tlen=277, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:39<00:13,  1.15it/s, pg=-0.045, ret=0.0016, glen=125, tlen=286, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.16it/s, pg=-0.045, ret=0.0016, glen=125, tlen=286, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:40<00:12,  1.16it/s, pg=-0.22, ret=0.0033, glen=127, tlen=288, kl=0, act_lr=0, ent=2.1]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=-0.22, ret=0.0033, glen=127, tlen=288, kl=0, act_lr=0, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:41<00:11,  1.17it/s, pg=-0.105, ret=0.000793, glen=140, tlen=300, kl=0, act_lr=0, ent=1.99]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.17it/s, pg=-0.105, ret=0.000793, glen=140, tlen=300, kl=0, act_lr=0, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:42<00:10,  1.17it/s, pg=-0.178, ret=0.00118, glen=128, tlen=288, kl=0, act_lr=0, ent=1.88] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:42<00:09,  1.17it/s, pg=-0.178, ret=0.00118, glen=128, tlen=288, kl=0, act_lr=0, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:42<00:09,  1.17it/s, pg=0.188, ret=-0.00214, glen=113, tlen=274, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=0.188, ret=-0.00214, glen=113, tlen=274, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:43<00:08,  1.17it/s, pg=0.219, ret=-0.0027, glen=122, tlen=283, kl=0, act_lr=0, ent=1.52] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=0.219, ret=-0.0027, glen=122, tlen=283, kl=0, act_lr=0, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:44<00:07,  1.17it/s, pg=0.0356, ret=0.00127, glen=114, tlen=274, kl=0, act_lr=0, ent=1.93]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=0.0356, ret=0.00127, glen=114, tlen=274, kl=0, act_lr=0, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:45<00:06,  1.17it/s, pg=-0.0473, ret=0.001, glen=108, tlen=268, kl=0, act_lr=0, ent=1.62] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=-0.0473, ret=0.001, glen=108, tlen=268, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:46<00:05,  1.17it/s, pg=-0.137, ret=0.00144, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.18it/s, pg=-0.137, ret=0.00144, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:47<00:05,  1.18it/s, pg=-0.071, ret=-8.45e-5, glen=137, tlen=297, kl=0, act_lr=0, ent=1.98]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.18it/s, pg=-0.071, ret=-8.45e-5, glen=137, tlen=297, kl=0, act_lr=0, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.18it/s, pg=-0.0575, ret=0.000681, glen=113, tlen=274, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.18it/s, pg=-0.0575, ret=0.000681, glen=113, tlen=274, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:48<00:03,  1.18it/s, pg=0.00867, ret=-0.000882, glen=122, tlen=282, kl=0, act_lr=0, ent=1.91]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.18it/s, pg=0.00867, ret=-0.000882, glen=122, tlen=282, kl=0, act_lr=0, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:49<00:02,  1.18it/s, pg=-0.179, ret=0.00125, glen=111, tlen=271, kl=0, act_lr=0, ent=1.59]   Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.18it/s, pg=-0.179, ret=0.00125, glen=111, tlen=271, kl=0, act_lr=0, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:50<00:01,  1.18it/s, pg=-0.203, ret=0.00141, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.18it/s, pg=-0.203, ret=0.00141, glen=108, tlen=269, kl=0, act_lr=0, ent=1.61]
2025-07-24 16:09:42.917 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 51.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:51<00:00,  1.18it/s, pg=0.0981, ret=-0.00132, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:51<00:00,  1.11it/s, pg=0.0981, ret=-0.00132, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.77]
2025-07-24 16:09:43.796 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 16:09:46.357 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-24 16:09:48.828 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 57.52s
2025-07-24 16:09:48.836 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.007191941655915359, 'actor_lr': 3.448275841112169e-10, 'clip_ratio': 0.0, 'entropy': 1.7657057540170078, 'kl': 0.0, 'response_length': 116.28467454581425, 'total_length': 276.7488821621599, 'teacher_total_length': 289.0060219600283, 'return': -1.278092644622967e-06, 'policy_update_steps': 1.0}
Episode [1/20]:   8%|‚ñä         | 1/13 [03:56<47:19, 236.63s/it]2025-07-24 16:09:48.895 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:11:20.756 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:11:20.934 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:11:20.934 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 92.04s
2025-07-24 16:11:23.095 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 109.7526,std_num_tokens: 111.6655,avg_correct_num_tokens: 102.7106,std_correct_num_tokens: 88.0398,avg_incorrect_num_tokens: 121.4100,std_incorrect_num_tokens: 141.6397
2025-07-24 16:11:23.425 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.49s
2025-07-24 16:11:26.606 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 16:11:55.557 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 225
2025-07-24 16:11:55.558 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 16:11:56.918 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.97s
2025-07-24 16:11:56.918 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0004019819732962383, avg_kl: 0.0, avg_response_length: 111.20896928575304, avg_orm_score: 0.0, avg_custom_rewards: -0.0004019819732962383
2025-07-24 16:11:56.953 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter1_replay_buffer.jsonl
2025-07-24 16:11:58.810 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.86s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.122, ret=0.00139, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.62]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.122, ret=0.00139, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.229, ret=0.00258, glen=99.7, tlen=261, kl=0, act_lr=2e-8, ent=1.64]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.08it/s, pg=-0.229, ret=0.00258, glen=99.7, tlen=261, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.08it/s, pg=-0.00248, ret=-0.000607, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.54]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=-0.00248, ret=-0.000607, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=-0.146, ret=0.00126, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.64]    Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=-0.146, ret=0.00126, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=-0.136, ret=-1.2e-5, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.136, ret=-1.2e-5, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.0627, ret=-0.00105, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0627, ret=-0.00105, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.0963, ret=0.000658, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.14it/s, pg=-0.0963, ret=0.000658, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.14it/s, pg=0.046, ret=0.00101, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.98]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=0.046, ret=0.00101, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=-0.0628, ret=-0.000412, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.0628, ret=-0.000412, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.134, ret=-0.000758, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.88]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=0.134, ret=-0.000758, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=0.0674, ret=-0.00141, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.74]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.17it/s, pg=0.0674, ret=-0.00141, glen=127, tlen=288, kl=0, act_lr=2e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.17it/s, pg=0.0914, ret=0.00054, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.62] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=0.0914, ret=0.00054, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=-0.0233, ret=-0.000625, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.75]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.0233, ret=-0.000625, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.256, ret=0.00166, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.59]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=-0.256, ret=0.00166, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0753, ret=-0.000334, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.59]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0753, ret=-0.000334, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.101, ret=0.000645, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.57] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.101, ret=0.000645, glen=108, tlen=269, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.1, ret=0.00133, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.1, ret=0.00133, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.0258, ret=-0.000727, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=0.0258, ret=-0.000727, glen=108, tlen=268, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=0.107, ret=-0.00128, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.65]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.18it/s, pg=0.107, ret=-0.00128, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.18it/s, pg=-0.0837, ret=0.000457, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.18it/s, pg=-0.0837, ret=0.000457, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.18it/s, pg=-0.184, ret=0.00276, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.69]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.18it/s, pg=-0.184, ret=0.00276, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.18it/s, pg=0.0781, ret=5.91e-5, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.82]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.18it/s, pg=0.0781, ret=5.91e-5, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.18it/s, pg=0.253, ret=-0.00223, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.81]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.18it/s, pg=0.253, ret=-0.00223, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.18it/s, pg=0.0104, ret=-0.00124, glen=110, tlen=271, kl=0, act_lr=2e-8, ent=1.8]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.18it/s, pg=0.0104, ret=-0.00124, glen=110, tlen=271, kl=0, act_lr=2e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.18it/s, pg=0.171, ret=-0.00157, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=2.06]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.18it/s, pg=0.171, ret=-0.00157, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.18it/s, pg=-0.0759, ret=0.00144, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.78]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.18it/s, pg=-0.0759, ret=0.00144, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.18it/s, pg=0.0108, ret=1.68e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.86] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=0.0108, ret=1.68e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=0.277, ret=-0.0011, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.86] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.18it/s, pg=0.277, ret=-0.0011, glen=118, tlen=279, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.18it/s, pg=0.035, ret=-0.000798, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:27,  1.02it/s, pg=0.035, ret=-0.000798, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:27,  1.02it/s, pg=-0.104, ret=0.00169, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.53] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.06it/s, pg=-0.104, ret=0.00169, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.06it/s, pg=0.151, ret=-0.00124, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=1.89]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=0.151, ret=-0.00124, glen=117, tlen=278, kl=0, act_lr=2e-8, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.0665, ret=0.000756, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.11it/s, pg=-0.0665, ret=0.000756, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.00211, ret=-0.00148, glen=125, tlen=285, kl=0, act_lr=2e-8, ent=1.78]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.00211, ret=-0.00148, glen=125, tlen=285, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=0.0208, ret=-0.000786, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.66]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.13it/s, pg=0.0208, ret=-0.000786, glen=109, tlen=269, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.13it/s, pg=-0.198, ret=0.000905, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=2.12] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=-0.198, ret=0.000905, glen=121, tlen=282, kl=0, act_lr=2e-8, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=-0.00299, ret=0.000586, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.59]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=-0.00299, ret=0.000586, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.163, ret=0.00107, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.58]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.163, ret=0.00107, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0769, ret=-2.25e-5, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0769, ret=-2.25e-5, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.194, ret=-0.000224, glen=114, tlen=275, kl=0, act_lr=2e-8, ent=1.74]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.194, ret=-0.000224, glen=114, tlen=275, kl=0, act_lr=2e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0107, ret=-0.000238, glen=96.3, tlen=257, kl=0, act_lr=2e-8, ent=1.57]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0107, ret=-0.000238, glen=96.3, tlen=257, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0047, ret=6.06e-5, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.6]     Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0047, ret=6.06e-5, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.00222, ret=0.000656, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.00222, ret=0.000656, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=0.0798, ret=-0.000233, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.58] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=0.0798, ret=-0.000233, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.0158, ret=-0.0013, glen=99.7, tlen=260, kl=0, act_lr=2e-8, ent=1.58]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=-0.0158, ret=-0.0013, glen=99.7, tlen=260, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=0.202, ret=-0.00102, glen=116, tlen=278, kl=0, act_lr=2e-8, ent=2.01]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.202, ret=-0.00102, glen=116, tlen=278, kl=0, act_lr=2e-8, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.136, ret=-0.00071, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.64]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.18it/s, pg=0.136, ret=-0.00071, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.0179, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.0179, ret=-0.00115, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=-0.0343, ret=0.000563, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.6] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=-0.0343, ret=0.000563, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=-0.125, ret=0.000842, glen=103, tlen=263, kl=0, act_lr=2e-8, ent=1.97]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.125, ret=0.000842, glen=103, tlen=263, kl=0, act_lr=2e-8, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=0.0212, ret=-0.00376, glen=120, tlen=282, kl=0, act_lr=2e-8, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.0212, ret=-0.00376, glen=120, tlen=282, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.0927, ret=0.000311, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.18it/s, pg=-0.0927, ret=0.000311, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.18it/s, pg=0.163, ret=-0.000887, glen=129, tlen=290, kl=0, act_lr=2e-8, ent=2.04]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.163, ret=-0.000887, glen=129, tlen=290, kl=0, act_lr=2e-8, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.057, ret=0.000675, glen=98.6, tlen=259, kl=0, act_lr=2e-8, ent=1.57]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.16it/s, pg=-0.057, ret=0.000675, glen=98.6, tlen=259, kl=0, act_lr=2e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.16it/s, pg=0.0166, ret=-0.00109, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.67] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.16it/s, pg=0.0166, ret=-0.00109, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=-0.0638, ret=0.00119, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0638, ret=0.00119, glen=103, tlen=264, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.16, ret=-0.00194, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.16, ret=-0.00194, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.73]
2025-07-24 16:12:48.426 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.43s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.181, ret=0.00102, glen=143, tlen=304, kl=0, act_lr=4e-8, ent=1.55]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.181, ret=0.00102, glen=143, tlen=304, kl=0, act_lr=4e-8, ent=1.55]
2025-07-24 16:12:49.303 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:12:51.663 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 16:12:52.008 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.13s
2025-07-24 16:12:52.015 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0045301872387267, 'actor_lr': 2.035087706930059e-08, 'clip_ratio': 0.0, 'entropy': 1.7246172030766804, 'kl': 0.0, 'response_length': 111.14298368755139, 'total_length': 271.7544030975877, 'teacher_total_length': 283.5741668165776, 'return': -7.169973717904405e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  15%|‚ñà‚ñå        | 2/13 [06:59<37:37, 205.19s/it]2025-07-24 16:12:52.058 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:15:32.344 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:15:32.523 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:15:32.524 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 160.47s
2025-07-24 16:15:34.999 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 112.6627,std_num_tokens: 145.2815,avg_correct_num_tokens: 103.4203,std_correct_num_tokens: 86.0646,avg_incorrect_num_tokens: 127.2372,std_incorrect_num_tokens: 205.8219
2025-07-24 16:15:35.377 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.85s
2025-07-24 16:15:38.311 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.93s
2025-07-24 16:16:07.259 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 226
2025-07-24 16:16:07.260 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.95s
2025-07-24 16:16:08.665 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 16:16:08.665 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0024170190746488, avg_kl: 0.000875793727098313, avg_response_length: 118.18322855181398, avg_orm_score: 0.0, avg_custom_rewards: -0.0024170190746488
2025-07-24 16:16:08.703 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter2_replay_buffer.jsonl
2025-07-24 16:16:10.603 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.0745, ret=-8.99e-5, glen=129, tlen=290, kl=0.000856, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0745, ret=-8.99e-5, glen=129, tlen=290, kl=0.000856, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=-0.0279, ret=1.71e-5, glen=111, tlen=271, kl=0.000875, act_lr=4e-8, ent=1.76] Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=-0.0279, ret=1.71e-5, glen=111, tlen=271, kl=0.000875, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=-0.117, ret=-0.000587, glen=116, tlen=276, kl=0.000868, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=-0.117, ret=-0.000587, glen=116, tlen=276, kl=0.000868, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=0.0988, ret=-0.0011, glen=118, tlen=278, kl=0.000878, act_lr=4e-8, ent=1.68]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=0.0988, ret=-0.0011, glen=118, tlen=278, kl=0.000878, act_lr=4e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=0.0905, ret=0.000838, glen=136, tlen=296, kl=0.000888, act_lr=4e-8, ent=1.94]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=0.0905, ret=0.000838, glen=136, tlen=296, kl=0.000888, act_lr=4e-8, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=-0.0579, ret=-0.000396, glen=104, tlen=264, kl=0.000914, act_lr=4e-8, ent=1.84]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.16it/s, pg=-0.0579, ret=-0.000396, glen=104, tlen=264, kl=0.000914, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.16it/s, pg=-0.195, ret=0.00119, glen=120, tlen=281, kl=0.000845, act_lr=4e-8, ent=1.85]   Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=-0.195, ret=0.00119, glen=120, tlen=281, kl=0.000845, act_lr=4e-8, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=-0.14, ret=0.000105, glen=103, tlen=264, kl=0.000918, act_lr=4e-8, ent=1.6] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:41,  1.17it/s, pg=-0.14, ret=0.000105, glen=103, tlen=264, kl=0.000918, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:41,  1.17it/s, pg=-0.183, ret=0.000671, glen=104, tlen=264, kl=0.00087, act_lr=4e-8, ent=1.6]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.183, ret=0.000671, glen=104, tlen=264, kl=0.00087, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.0739, ret=-0.000439, glen=112, tlen=273, kl=0.000913, act_lr=4e-8, ent=1.9]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=-0.0739, ret=-0.000439, glen=112, tlen=273, kl=0.000913, act_lr=4e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.139, ret=0.00231, glen=121, tlen=281, kl=0.000895, act_lr=4e-8, ent=1.6]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.15it/s, pg=-0.139, ret=0.00231, glen=121, tlen=281, kl=0.000895, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.15it/s, pg=-0.012, ret=0.000149, glen=110, tlen=270, kl=0.000875, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.012, ret=0.000149, glen=110, tlen=270, kl=0.000875, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.26, ret=-0.00287, glen=114, tlen=273, kl=0.000817, act_lr=4e-8, ent=1.48]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=0.26, ret=-0.00287, glen=114, tlen=273, kl=0.000817, act_lr=4e-8, ent=1.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=0.00775, ret=0.000437, glen=112, tlen=273, kl=0.000891, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.16it/s, pg=0.00775, ret=0.000437, glen=112, tlen=273, kl=0.000891, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.16it/s, pg=-0.13, ret=0.000705, glen=103, tlen=263, kl=0.000855, act_lr=4e-8, ent=1.8]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.13, ret=0.000705, glen=103, tlen=263, kl=0.000855, act_lr=4e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.0493, ret=0.00138, glen=137, tlen=297, kl=0.000841, act_lr=4e-8, ent=2.22]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0493, ret=0.00138, glen=137, tlen=297, kl=0.000841, act_lr=4e-8, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.276, ret=-0.000972, glen=120, tlen=280, kl=0.000904, act_lr=4e-8, ent=1.86]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.276, ret=-0.000972, glen=120, tlen=280, kl=0.000904, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.086, ret=0.000553, glen=103, tlen=263, kl=0.00088, act_lr=4e-8, ent=1.61] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.086, ret=0.000553, glen=103, tlen=263, kl=0.00088, act_lr=4e-8, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.195, ret=0.00143, glen=118, tlen=278, kl=0.000857, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.195, ret=0.00143, glen=118, tlen=278, kl=0.000857, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.188, ret=0.00116, glen=116, tlen=277, kl=0.000867, act_lr=4e-8, ent=1.73]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.188, ret=0.00116, glen=116, tlen=277, kl=0.000867, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.00564, ret=0.00012, glen=113, tlen=274, kl=0.000859, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.00564, ret=0.00012, glen=113, tlen=274, kl=0.000859, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.113, ret=-0.00015, glen=131, tlen=291, kl=0.000892, act_lr=4e-8, ent=1.78]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.113, ret=-0.00015, glen=131, tlen=291, kl=0.000892, act_lr=4e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0288, ret=-0.0014, glen=108, tlen=267, kl=0.000916, act_lr=4e-8, ent=1.64]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.17it/s, pg=0.0288, ret=-0.0014, glen=108, tlen=267, kl=0.000916, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0328, ret=-0.000227, glen=130, tlen=290, kl=0.000848, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0328, ret=-0.000227, glen=130, tlen=290, kl=0.000848, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0914, ret=-0.00149, glen=113, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.73] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0914, ret=-0.00149, glen=113, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0418, ret=0.000939, glen=111, tlen=271, kl=0.000896, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0418, ret=0.000939, glen=111, tlen=271, kl=0.000896, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.108, ret=-0.00106, glen=103, tlen=263, kl=0.000921, act_lr=4e-8, ent=1.7]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.108, ret=-0.00106, glen=103, tlen=263, kl=0.000921, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=-0.0739, ret=0.00138, glen=116, tlen=277, kl=0.000908, act_lr=4e-8, ent=1.85]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=-0.0739, ret=0.00138, glen=116, tlen=277, kl=0.000908, act_lr=4e-8, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0544, ret=-0.000121, glen=108, tlen=269, kl=0.00084, act_lr=4e-8, ent=1.63]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:27,  1.01it/s, pg=-0.0544, ret=-0.000121, glen=108, tlen=269, kl=0.00084, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:27,  1.01it/s, pg=-0.159, ret=0.00108, glen=114, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.65]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:25,  1.05it/s, pg=-0.159, ret=0.00108, glen=114, tlen=274, kl=0.000867, act_lr=4e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:25,  1.05it/s, pg=-0.207, ret=0.00114, glen=99, tlen=260, kl=0.00088, act_lr=4e-8, ent=1.63]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.09it/s, pg=-0.207, ret=0.00114, glen=99, tlen=260, kl=0.00088, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.09it/s, pg=-0.171, ret=0.00131, glen=108, tlen=269, kl=0.000911, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=-0.171, ret=0.00131, glen=108, tlen=269, kl=0.000911, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.11it/s, pg=0.233, ret=-0.00321, glen=108, tlen=268, kl=0.000928, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.233, ret=-0.00321, glen=108, tlen=268, kl=0.000928, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=-0.00635, ret=0.00115, glen=124, tlen=284, kl=0.000877, act_lr=4e-8, ent=1.86]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.14it/s, pg=-0.00635, ret=0.00115, glen=124, tlen=284, kl=0.000877, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.14it/s, pg=-0.00122, ret=0.000491, glen=102, tlen=262, kl=0.000856, act_lr=4e-8, ent=1.66]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.15it/s, pg=-0.00122, ret=0.000491, glen=102, tlen=262, kl=0.000856, act_lr=4e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.15it/s, pg=-0.0938, ret=0.000208, glen=112, tlen=273, kl=0.000917, act_lr=4e-8, ent=1.84] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0938, ret=0.000208, glen=112, tlen=273, kl=0.000917, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.0966, ret=-0.000281, glen=93.6, tlen=254, kl=0.000879, act_lr=4e-8, ent=1.52]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.0966, ret=-0.000281, glen=93.6, tlen=254, kl=0.000879, act_lr=4e-8, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0652, ret=-0.000276, glen=96.6, tlen=257, kl=0.000896, act_lr=4e-8, ent=1.67]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0652, ret=-0.000276, glen=96.6, tlen=257, kl=0.000896, act_lr=4e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=-0.0344, ret=0.00203, glen=109, tlen=270, kl=0.000886, act_lr=4e-8, ent=1.98]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0344, ret=0.00203, glen=109, tlen=270, kl=0.000886, act_lr=4e-8, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.11, ret=0.00118, glen=114, tlen=274, kl=0.000892, act_lr=4e-8, ent=1.7]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.11, ret=0.00118, glen=114, tlen=274, kl=0.000892, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.242, ret=-0.00316, glen=115, tlen=275, kl=0.000875, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.242, ret=-0.00316, glen=115, tlen=275, kl=0.000875, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0181, ret=3.29e-5, glen=107, tlen=266, kl=0.000891, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0181, ret=3.29e-5, glen=107, tlen=266, kl=0.000891, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.154, ret=0.000425, glen=101, tlen=261, kl=0.000907, act_lr=4e-8, ent=1.64]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.154, ret=0.000425, glen=101, tlen=261, kl=0.000907, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.00129, glen=109, tlen=269, kl=0.00086, act_lr=4e-8, ent=1.84]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.00129, glen=109, tlen=269, kl=0.00086, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.0341, ret=0.000607, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.77]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0341, ret=0.000607, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.218, ret=0.00251, glen=115, tlen=276, kl=0.000878, act_lr=4e-8, ent=1.86]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.218, ret=0.00251, glen=115, tlen=276, kl=0.000878, act_lr=4e-8, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.244, ret=-0.00315, glen=125, tlen=285, kl=0.000886, act_lr=4e-8, ent=1.7] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.244, ret=-0.00315, glen=125, tlen=285, kl=0.000886, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.149, ret=-0.00317, glen=126, tlen=286, kl=0.000907, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.149, ret=-0.00317, glen=126, tlen=286, kl=0.000907, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.431, ret=-0.0183, glen=410, tlen=570, kl=0.000707, act_lr=4e-8, ent=2.71] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:07,  1.14it/s, pg=0.431, ret=-0.0183, glen=410, tlen=570, kl=0.000707, act_lr=4e-8, ent=2.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:07,  1.14it/s, pg=0.14, ret=-0.00188, glen=118, tlen=278, kl=0.000865, act_lr=4e-8, ent=1.57]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.15it/s, pg=0.14, ret=-0.00188, glen=118, tlen=278, kl=0.000865, act_lr=4e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.15it/s, pg=-0.0471, ret=-0.000547, glen=108, tlen=269, kl=0.000831, act_lr=4e-8, ent=1.8]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.0471, ret=-0.000547, glen=108, tlen=269, kl=0.000831, act_lr=4e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=-0.0851, ret=0.00087, glen=112, tlen=272, kl=0.00089, act_lr=4e-8, ent=1.74]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=-0.0851, ret=0.00087, glen=112, tlen=272, kl=0.00089, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.151, ret=0.000737, glen=110, tlen=270, kl=0.000897, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.151, ret=0.000737, glen=110, tlen=270, kl=0.000897, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.00233, ret=0.000999, glen=112, tlen=272, kl=0.000839, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.00233, ret=0.000999, glen=112, tlen=272, kl=0.000839, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0515, ret=-0.000648, glen=108, tlen=269, kl=0.000858, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0515, ret=-0.000648, glen=108, tlen=269, kl=0.000858, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.148, ret=-0.000656, glen=128, tlen=289, kl=0.000846, act_lr=4e-8, ent=1.94]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.148, ret=-0.000656, glen=128, tlen=289, kl=0.000846, act_lr=4e-8, ent=1.94]
2025-07-24 16:17:00.357 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.57s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.00983, ret=0.000313, glen=110, tlen=271, kl=0.00084, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.00983, ret=0.000313, glen=110, tlen=271, kl=0.00084, act_lr=6e-8, ent=1.69]
2025-07-24 16:17:01.248 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:17:03.773 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 16:17:04.101 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.43s
2025-07-24 16:17:04.151 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01652477080361885, 'actor_lr': 4.0350876916587013e-08, 'clip_ratio': 0.0, 'entropy': 1.7695354474218268, 'kl': 0.0008754730224609375, 'response_length': 118.31446369907312, 'total_length': 278.56398947197096, 'teacher_total_length': 290.05270921138293, 'return': -0.0002886613505675964, 'policy_update_steps': 1.0}
Episode [1/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:11<37:46, 226.63s/it]2025-07-24 16:17:04.191 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:18:50.122 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:18:50.301 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:18:50.302 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 106.11s
2025-07-24 16:18:52.281 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 109.1659,std_num_tokens: 128.0053,avg_correct_num_tokens: 101.0823,std_correct_num_tokens: 86.4976,avg_incorrect_num_tokens: 121.6989,std_incorrect_num_tokens: 173.0066
2025-07-24 16:18:52.733 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.43s
2025-07-24 16:18:55.681 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.95s
2025-07-24 16:19:24.217 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 224
2025-07-24 16:19:24.218 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.53s
2025-07-24 16:19:25.583 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.95s
2025-07-24 16:19:25.583 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0010229884017982321, avg_kl: 0.0009026101657322475, avg_response_length: 111.71290295464652, avg_orm_score: 0.0, avg_custom_rewards: -0.0010229884017982321
2025-07-24 16:19:25.637 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter3_replay_buffer.jsonl
2025-07-24 16:19:27.483 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.85s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.169, ret=-0.00226, glen=187, tlen=347, kl=0.000835, act_lr=6e-8, ent=2.19]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:59,  1.08s/it, pg=0.169, ret=-0.00226, glen=187, tlen=347, kl=0.000835, act_lr=6e-8, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:59,  1.08s/it, pg=-0.114, ret=0.00101, glen=100, tlen=260, kl=0.000943, act_lr=6e-8, ent=1.74]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:51,  1.06it/s, pg=-0.114, ret=0.00101, glen=100, tlen=260, kl=0.000943, act_lr=6e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:51,  1.06it/s, pg=0.0869, ret=0.000845, glen=111, tlen=272, kl=0.000916, act_lr=6e-8, ent=1.79]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.10it/s, pg=0.0869, ret=0.000845, glen=111, tlen=272, kl=0.000916, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.10it/s, pg=-0.111, ret=0.0013, glen=106, tlen=267, kl=0.000919, act_lr=6e-8, ent=1.65]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.13it/s, pg=-0.111, ret=0.0013, glen=106, tlen=267, kl=0.000919, act_lr=6e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.13it/s, pg=0.12, ret=-0.00134, glen=120, tlen=280, kl=0.000899, act_lr=6e-8, ent=2.09]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.12, ret=-0.00134, glen=120, tlen=280, kl=0.000899, act_lr=6e-8, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=0.0226, ret=-0.000796, glen=97.6, tlen=258, kl=0.000943, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=0.0226, ret=-0.000796, glen=97.6, tlen=258, kl=0.000943, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.102, ret=0.0021, glen=108, tlen=269, kl=0.000917, act_lr=6e-8, ent=1.66]    Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=-0.102, ret=0.0021, glen=108, tlen=269, kl=0.000917, act_lr=6e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.0961, ret=-0.000353, glen=94.6, tlen=255, kl=0.000871, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0961, ret=-0.000353, glen=94.6, tlen=255, kl=0.000871, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.157, ret=0.00116, glen=108, tlen=269, kl=0.000896, act_lr=6e-8, ent=1.64]    Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.157, ret=0.00116, glen=108, tlen=269, kl=0.000896, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0488, ret=0.000599, glen=103, tlen=264, kl=0.000929, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0488, ret=0.000599, glen=103, tlen=264, kl=0.000929, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=0.173, ret=-0.00187, glen=118, tlen=278, kl=0.00083, act_lr=6e-8, ent=1.93]   Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=0.173, ret=-0.00187, glen=118, tlen=278, kl=0.00083, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=-0.108, ret=0.000618, glen=91.4, tlen=252, kl=0.000933, act_lr=6e-8, ent=1.7]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.16it/s, pg=-0.108, ret=0.000618, glen=91.4, tlen=252, kl=0.000933, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.16it/s, pg=0.0664, ret=-0.000441, glen=116, tlen=277, kl=0.000853, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.16it/s, pg=0.0664, ret=-0.000441, glen=116, tlen=277, kl=0.000853, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.16it/s, pg=-0.137, ret=0.000305, glen=146, tlen=306, kl=0.000909, act_lr=6e-8, ent=2.09] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.17it/s, pg=-0.137, ret=0.000305, glen=146, tlen=306, kl=0.000909, act_lr=6e-8, ent=2.09]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.17it/s, pg=-0.0267, ret=-0.00147, glen=109, tlen=269, kl=0.000905, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.0267, ret=-0.00147, glen=109, tlen=269, kl=0.000905, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=-0.00988, ret=-0.000825, glen=96.3, tlen=257, kl=0.000897, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.16it/s, pg=-0.00988, ret=-0.000825, glen=96.3, tlen=257, kl=0.000897, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.00235, ret=-0.000202, glen=91.3, tlen=252, kl=0.000913, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.00235, ret=-0.000202, glen=91.3, tlen=252, kl=0.000913, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.0131, ret=-0.000867, glen=109, tlen=269, kl=0.000899, act_lr=6e-8, ent=1.79]   Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.0131, ret=-0.000867, glen=109, tlen=269, kl=0.000899, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0363, ret=-0.000524, glen=108, tlen=269, kl=0.000918, act_lr=6e-8, ent=1.61]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0363, ret=-0.000524, glen=108, tlen=269, kl=0.000918, act_lr=6e-8, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.086, ret=-0.000445, glen=158, tlen=318, kl=0.000839, act_lr=6e-8, ent=2.21]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.086, ret=-0.000445, glen=158, tlen=318, kl=0.000839, act_lr=6e-8, ent=2.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.201, ret=0.00272, glen=115, tlen=275, kl=0.000888, act_lr=6e-8, ent=1.93] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.201, ret=0.00272, glen=115, tlen=275, kl=0.000888, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=0.148, ret=-0.0012, glen=127, tlen=287, kl=0.000909, act_lr=6e-8, ent=1.76] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.148, ret=-0.0012, glen=127, tlen=287, kl=0.000909, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.021, ret=0.00115, glen=105, tlen=266, kl=0.000918, act_lr=6e-8, ent=1.78]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=0.021, ret=0.00115, glen=105, tlen=266, kl=0.000918, act_lr=6e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.104, ret=-0.000369, glen=98.4, tlen=259, kl=0.000913, act_lr=6e-8, ent=1.53]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.104, ret=-0.000369, glen=98.4, tlen=259, kl=0.000913, act_lr=6e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.125, ret=-0.000261, glen=137, tlen=297, kl=0.000834, act_lr=6e-8, ent=2.22]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.125, ret=-0.000261, glen=137, tlen=297, kl=0.000834, act_lr=6e-8, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.0552, ret=0.00125, glen=109, tlen=268, kl=0.00092, act_lr=6e-8, ent=1.68] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.0552, ret=0.00125, glen=109, tlen=268, kl=0.00092, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.045, ret=-0.00055, glen=97.8, tlen=258, kl=0.000957, act_lr=6e-8, ent=1.59]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=-0.045, ret=-0.00055, glen=97.8, tlen=258, kl=0.000957, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.00842, ret=0.000603, glen=119, tlen=279, kl=0.00087, act_lr=6e-8, ent=1.87]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.00842, ret=0.000603, glen=119, tlen=279, kl=0.00087, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.0585, ret=0.000429, glen=98, tlen=258, kl=0.000882, act_lr=6e-8, ent=1.65] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.0585, ret=0.000429, glen=98, tlen=258, kl=0.000882, act_lr=6e-8, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=0.256, ret=-0.00329, glen=105, tlen=266, kl=0.00092, act_lr=6e-8, ent=1.7]   Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.256, ret=-0.00329, glen=105, tlen=266, kl=0.00092, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=0.00259, ret=-0.000603, glen=113, tlen=274, kl=0.000949, act_lr=6e-8, ent=1.77]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.00259, ret=-0.000603, glen=113, tlen=274, kl=0.000949, act_lr=6e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.115, ret=0.000115, glen=112, tlen=272, kl=0.000887, act_lr=6e-8, ent=1.8]   Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.13it/s, pg=-0.115, ret=0.000115, glen=112, tlen=272, kl=0.000887, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.13it/s, pg=-0.0431, ret=0.00139, glen=113, tlen=273, kl=0.000855, act_lr=6e-8, ent=1.82]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=-0.0431, ret=0.00139, glen=113, tlen=273, kl=0.000855, act_lr=6e-8, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=0.0608, ret=-0.000599, glen=102, tlen=263, kl=0.000956, act_lr=6e-8, ent=1.67]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.15it/s, pg=0.0608, ret=-0.000599, glen=102, tlen=263, kl=0.000956, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.15it/s, pg=-0.0305, ret=-0.000302, glen=98.9, tlen=259, kl=0.000947, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.0305, ret=-0.000302, glen=98.9, tlen=259, kl=0.000947, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=-0.0984, ret=0.00109, glen=118, tlen=279, kl=0.000922, act_lr=6e-8, ent=1.87]   Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=-0.0984, ret=0.00109, glen=118, tlen=279, kl=0.000922, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=0.0214, ret=0.000823, glen=123, tlen=284, kl=0.000901, act_lr=6e-8, ent=1.78]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=0.0214, ret=0.000823, glen=123, tlen=284, kl=0.000901, act_lr=6e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.234, ret=0.00201, glen=112, tlen=272, kl=0.000901, act_lr=6e-8, ent=1.79] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:32<00:15,  1.17it/s, pg=-0.234, ret=0.00201, glen=112, tlen=272, kl=0.000901, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0181, ret=-0.00049, glen=109, tlen=269, kl=0.000919, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.16it/s, pg=0.0181, ret=-0.00049, glen=109, tlen=269, kl=0.000919, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=0.115, ret=-0.000841, glen=89.2, tlen=249, kl=0.000929, act_lr=6e-8, ent=1.64]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.16it/s, pg=0.115, ret=-0.000841, glen=89.2, tlen=249, kl=0.000929, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.16it/s, pg=0.253, ret=-0.00188, glen=136, tlen=296, kl=0.000793, act_lr=6e-8, ent=1.51]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.16it/s, pg=0.253, ret=-0.00188, glen=136, tlen=296, kl=0.000793, act_lr=6e-8, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.16it/s, pg=0.0101, ret=-0.0004, glen=110, tlen=270, kl=0.000902, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.16it/s, pg=0.0101, ret=-0.0004, glen=110, tlen=270, kl=0.000902, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.16it/s, pg=-0.148, ret=3.85e-5, glen=97.2, tlen=257, kl=0.000912, act_lr=6e-8, ent=1.58]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=3.85e-5, glen=97.2, tlen=257, kl=0.000912, act_lr=6e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0665, ret=-0.00255, glen=96, tlen=256, kl=0.000932, act_lr=6e-8, ent=1.59] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0665, ret=-0.00255, glen=96, tlen=256, kl=0.000932, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.19, ret=-0.00182, glen=113, tlen=273, kl=0.000876, act_lr=6e-8, ent=1.84] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.17it/s, pg=0.19, ret=-0.00182, glen=113, tlen=273, kl=0.000876, act_lr=6e-8, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.079, ret=0.000555, glen=107, tlen=268, kl=0.000908, act_lr=6e-8, ent=1.62]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.079, ret=0.000555, glen=107, tlen=268, kl=0.000908, act_lr=6e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.113, ret=0.00129, glen=130, tlen=291, kl=0.000882, act_lr=6e-8, ent=1.73] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.18it/s, pg=-0.113, ret=0.00129, glen=130, tlen=291, kl=0.000882, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.18it/s, pg=0.0896, ret=-0.000923, glen=97.5, tlen=258, kl=0.000916, act_lr=6e-8, ent=1.54]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=0.0896, ret=-0.000923, glen=97.5, tlen=258, kl=0.000916, act_lr=6e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.0272, ret=0.00111, glen=114, tlen=274, kl=0.00092, act_lr=6e-8, ent=1.67]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.0272, ret=0.00111, glen=114, tlen=274, kl=0.00092, act_lr=6e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=-0.0159, ret=-0.000171, glen=120, tlen=280, kl=0.00091, act_lr=6e-8, ent=2.06]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.0159, ret=-0.000171, glen=120, tlen=280, kl=0.00091, act_lr=6e-8, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0125, ret=-0.000249, glen=101, tlen=261, kl=0.000925, act_lr=6e-8, ent=1.59]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0125, ret=-0.000249, glen=101, tlen=261, kl=0.000925, act_lr=6e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0162, ret=0.000158, glen=114, tlen=274, kl=0.000901, act_lr=6e-8, ent=1.88]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=-0.0162, ret=0.000158, glen=114, tlen=274, kl=0.000901, act_lr=6e-8, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.0827, ret=0.000486, glen=119, tlen=280, kl=0.000891, act_lr=6e-8, ent=1.73]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.0827, ret=0.000486, glen=119, tlen=280, kl=0.000891, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0355, ret=-0.000383, glen=96.4, tlen=257, kl=0.000915, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0355, ret=-0.000383, glen=96.4, tlen=257, kl=0.000915, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.0361, ret=-0.000192, glen=115, tlen=275, kl=0.000886, act_lr=6e-8, ent=1.7]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=-0.0361, ret=-0.000192, glen=115, tlen=275, kl=0.000886, act_lr=6e-8, ent=1.7]
2025-07-24 16:20:16.148 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=-0.194, ret=0.000859, glen=109, tlen=270, kl=0.000936, act_lr=8e-8, ent=1.97] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.194, ret=0.000859, glen=109, tlen=270, kl=0.000936, act_lr=8e-8, ent=1.97]
2025-07-24 16:20:17.016 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.82s
2025-07-24 16:20:19.425 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.41s
2025-07-24 16:20:19.750 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.20s
2025-07-24 16:20:19.758 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.010056640420641218, 'actor_lr': 6.03571407456879e-08, 'clip_ratio': 0.0, 'entropy': 1.7597153910568781, 'kl': 0.0009026101657322475, 'response_length': 111.71290343148368, 'total_length': 272.0588773999895, 'teacher_total_length': 284.70700345720564, 'return': -7.95320956967771e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:27<32:09, 214.38s/it]2025-07-24 16:20:19.806 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:23:00.801 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:23:00.983 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:23:00.984 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 161.18s
2025-07-24 16:23:03.047 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0100,avg_pass_at_n: 1.0000,avg_num_tokens: 113.3406,std_num_tokens: 152.2395,avg_correct_num_tokens: 103.6449,std_correct_num_tokens: 88.5879,avg_incorrect_num_tokens: 128.1518,std_incorrect_num_tokens: 215.0134
2025-07-24 16:23:03.500 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.52s
2025-07-24 16:23:06.581 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.08s
2025-07-24 16:23:36.209 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:23:36.209 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.63s
2025-07-24 16:23:37.920 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.24s
2025-07-24 16:23:37.920 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0005573580287067488, avg_kl: 0.0008828129830839332, avg_response_length: 118.33821242553178, avg_orm_score: 0.0, avg_custom_rewards: -0.0005573580287067488
2025-07-24 16:23:37.956 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter4_replay_buffer.jsonl
2025-07-24 16:23:39.880 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.93s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.198, ret=0.00151, glen=102, tlen=263, kl=0.000929, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.198, ret=0.00151, glen=102, tlen=263, kl=0.000929, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.00903, ret=-0.00139, glen=109, tlen=270, kl=0.000844, act_lr=8e-8, ent=1.6]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.06it/s, pg=-0.00903, ret=-0.00139, glen=109, tlen=270, kl=0.000844, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.06it/s, pg=-0.138, ret=0.000485, glen=110, tlen=270, kl=0.000888, act_lr=8e-8, ent=1.75] Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.11it/s, pg=-0.138, ret=0.000485, glen=110, tlen=270, kl=0.000888, act_lr=8e-8, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.11it/s, pg=0.0961, ret=-0.00124, glen=117, tlen=278, kl=0.000882, act_lr=8e-8, ent=1.58]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.13it/s, pg=0.0961, ret=-0.00124, glen=117, tlen=278, kl=0.000882, act_lr=8e-8, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.13it/s, pg=-0.0509, ret=-0.000247, glen=103, tlen=264, kl=0.000858, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:46,  1.15it/s, pg=-0.0509, ret=-0.000247, glen=103, tlen=264, kl=0.000858, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:46,  1.15it/s, pg=-0.0633, ret=0.00111, glen=111, tlen=272, kl=0.000883, act_lr=8e-8, ent=1.76]  Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:44,  1.16it/s, pg=-0.0633, ret=0.00111, glen=111, tlen=272, kl=0.000883, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:44,  1.16it/s, pg=0.114, ret=-0.00224, glen=356, tlen=517, kl=0.000704, act_lr=8e-8, ent=1.33] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.13it/s, pg=0.114, ret=-0.00224, glen=356, tlen=517, kl=0.000704, act_lr=8e-8, ent=1.33]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.13it/s, pg=-0.0398, ret=-0.000177, glen=102, tlen=262, kl=0.000905, act_lr=8e-8, ent=1.64]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=-0.0398, ret=-0.000177, glen=102, tlen=262, kl=0.000905, act_lr=8e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=-0.0136, ret=-0.000271, glen=111, tlen=272, kl=0.000905, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.14it/s, pg=-0.0136, ret=-0.000271, glen=111, tlen=272, kl=0.000905, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.14it/s, pg=0.171, ret=-0.00211, glen=106, tlen=266, kl=0.000896, act_lr=8e-8, ent=1.54]   Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.15it/s, pg=0.171, ret=-0.00211, glen=106, tlen=266, kl=0.000896, act_lr=8e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.15it/s, pg=0.0506, ret=-0.000404, glen=111, tlen=272, kl=0.000831, act_lr=8e-8, ent=1.95]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.16it/s, pg=0.0506, ret=-0.000404, glen=111, tlen=272, kl=0.000831, act_lr=8e-8, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.16it/s, pg=0.0435, ret=-0.00103, glen=105, tlen=265, kl=0.000893, act_lr=8e-8, ent=1.59] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.16it/s, pg=0.0435, ret=-0.00103, glen=105, tlen=265, kl=0.000893, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.16it/s, pg=0.0495, ret=-0.00215, glen=108, tlen=269, kl=0.000892, act_lr=8e-8, ent=1.71]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=0.0495, ret=-0.00215, glen=108, tlen=269, kl=0.000892, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=0.0609, ret=-0.00107, glen=106, tlen=267, kl=0.000899, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=0.0609, ret=-0.00107, glen=106, tlen=267, kl=0.000899, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=-0.142, ret=0.00198, glen=118, tlen=278, kl=0.000916, act_lr=8e-8, ent=1.74] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:37,  1.15it/s, pg=-0.142, ret=0.00198, glen=118, tlen=278, kl=0.000916, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:37,  1.15it/s, pg=-0.0829, ret=-0.000248, glen=108, tlen=269, kl=0.000873, act_lr=8e-8, ent=1.57]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.0829, ret=-0.000248, glen=108, tlen=269, kl=0.000873, act_lr=8e-8, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.051, ret=0.000987, glen=112, tlen=273, kl=0.000872, act_lr=8e-8, ent=1.72]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.051, ret=0.000987, glen=112, tlen=273, kl=0.000872, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.0227, ret=-0.00132, glen=101, tlen=262, kl=0.000894, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=0.0227, ret=-0.00132, glen=101, tlen=262, kl=0.000894, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=0.153, ret=-0.00147, glen=116, tlen=276, kl=0.000903, act_lr=8e-8, ent=1.78] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=0.153, ret=-0.00147, glen=116, tlen=276, kl=0.000903, act_lr=8e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.00244, ret=0.000159, glen=126, tlen=287, kl=0.000901, act_lr=8e-8, ent=1.92]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.00244, ret=0.000159, glen=126, tlen=287, kl=0.000901, act_lr=8e-8, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.105, ret=0.000885, glen=113, tlen=274, kl=0.000881, act_lr=8e-8, ent=1.66] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.105, ret=0.000885, glen=113, tlen=274, kl=0.000881, act_lr=8e-8, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=-0.161, ret=0.00229, glen=122, tlen=282, kl=0.000855, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.161, ret=0.00229, glen=122, tlen=282, kl=0.000855, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0652, ret=0.000451, glen=111, tlen=272, kl=0.000863, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.17it/s, pg=-0.0652, ret=0.000451, glen=111, tlen=272, kl=0.000863, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.168, ret=0.00189, glen=129, tlen=290, kl=0.000857, act_lr=8e-8, ent=2.03]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:28,  1.17it/s, pg=-0.168, ret=0.00189, glen=129, tlen=290, kl=0.000857, act_lr=8e-8, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:28,  1.17it/s, pg=-0.166, ret=0.00144, glen=132, tlen=294, kl=0.000873, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.18it/s, pg=-0.166, ret=0.00144, glen=132, tlen=294, kl=0.000873, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.18it/s, pg=0.239, ret=-0.00231, glen=134, tlen=295, kl=0.000848, act_lr=8e-8, ent=1.9] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.18it/s, pg=0.239, ret=-0.00231, glen=134, tlen=295, kl=0.000848, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.18it/s, pg=-0.0405, ret=-3.71e-5, glen=132, tlen=293, kl=0.000905, act_lr=8e-8, ent=1.81]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.18it/s, pg=-0.0405, ret=-3.71e-5, glen=132, tlen=293, kl=0.000905, act_lr=8e-8, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.18it/s, pg=-0.126, ret=-0.000624, glen=134, tlen=294, kl=0.000897, act_lr=8e-8, ent=1.87]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=-0.126, ret=-0.000624, glen=134, tlen=294, kl=0.000897, act_lr=8e-8, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.102, ret=-0.000284, glen=106, tlen=267, kl=0.000874, act_lr=8e-8, ent=1.62]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.07it/s, pg=-0.102, ret=-0.000284, glen=106, tlen=267, kl=0.000874, act_lr=8e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.00371, ret=9.82e-5, glen=126, tlen=286, kl=0.000901, act_lr=8e-8, ent=1.91]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=-0.00371, ret=9.82e-5, glen=126, tlen=286, kl=0.000901, act_lr=8e-8, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.10it/s, pg=0.149, ret=-0.00141, glen=138, tlen=298, kl=0.000907, act_lr=8e-8, ent=2]     Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.149, ret=-0.00141, glen=138, tlen=298, kl=0.000907, act_lr=8e-8, ent=2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.0511, ret=2.9e-5, glen=123, tlen=284, kl=0.000866, act_lr=8e-8, ent=1.71]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:22,  1.14it/s, pg=0.0511, ret=2.9e-5, glen=123, tlen=284, kl=0.000866, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.14it/s, pg=0.0269, ret=-0.000232, glen=118, tlen=279, kl=0.000917, act_lr=8e-8, ent=1.78]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.15it/s, pg=0.0269, ret=-0.000232, glen=118, tlen=279, kl=0.000917, act_lr=8e-8, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.15it/s, pg=0.081, ret=-0.000541, glen=105, tlen=266, kl=0.000872, act_lr=8e-8, ent=1.6]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.16it/s, pg=0.081, ret=-0.000541, glen=105, tlen=266, kl=0.000872, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.16it/s, pg=0.0626, ret=-4.4e-5, glen=96.2, tlen=257, kl=0.00088, act_lr=8e-8, ent=1.55]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=0.0626, ret=-4.4e-5, glen=96.2, tlen=257, kl=0.00088, act_lr=8e-8, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=-0.187, ret=0.000526, glen=114, tlen=275, kl=0.000891, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.17it/s, pg=-0.187, ret=0.000526, glen=114, tlen=275, kl=0.000891, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=0.107, ret=-0.000542, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:17,  1.17it/s, pg=0.107, ret=-0.000542, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=0.218, ret=-0.00381, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.218, ret=-0.00381, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=-0.301, ret=0.00221, glen=105, tlen=266, kl=0.000881, act_lr=8e-8, ent=1.59]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.17it/s, pg=-0.301, ret=0.00221, glen=105, tlen=266, kl=0.000881, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=-0.159, ret=0.00148, glen=117, tlen=278, kl=0.000934, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=-0.159, ret=0.00148, glen=117, tlen=278, kl=0.000934, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=-0.0958, ret=8.63e-5, glen=112, tlen=273, kl=0.000878, act_lr=8e-8, ent=1.49]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0958, ret=8.63e-5, glen=112, tlen=273, kl=0.000878, act_lr=8e-8, ent=1.49]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=-0.134, ret=0.00187, glen=111, tlen=273, kl=0.000899, act_lr=8e-8, ent=1.74] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.16it/s, pg=-0.134, ret=0.00187, glen=111, tlen=273, kl=0.000899, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.16it/s, pg=-0.0825, ret=-0.000351, glen=116, tlen=278, kl=0.000887, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=-0.0825, ret=-0.000351, glen=116, tlen=278, kl=0.000887, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=0.0884, ret=0.000805, glen=116, tlen=277, kl=0.000852, act_lr=8e-8, ent=1.9]   Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:12,  1.17it/s, pg=0.0884, ret=0.000805, glen=116, tlen=277, kl=0.000852, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.17it/s, pg=-0.0308, ret=6.35e-5, glen=95.1, tlen=256, kl=0.000897, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0308, ret=6.35e-5, glen=95.1, tlen=256, kl=0.000897, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.0798, ret=-0.000667, glen=120, tlen=281, kl=0.000873, act_lr=8e-8, ent=1.91]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.17it/s, pg=0.0798, ret=-0.000667, glen=120, tlen=281, kl=0.000873, act_lr=8e-8, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=-0.188, ret=0.000712, glen=97.3, tlen=258, kl=0.000892, act_lr=8e-8, ent=1.51]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.188, ret=0.000712, glen=97.3, tlen=258, kl=0.000892, act_lr=8e-8, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=-0.0296, ret=0.000783, glen=108, tlen=269, kl=0.000907, act_lr=8e-8, ent=1.62]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.17it/s, pg=-0.0296, ret=0.000783, glen=108, tlen=269, kl=0.000907, act_lr=8e-8, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=0.0599, ret=-0.000454, glen=98.9, tlen=260, kl=0.000945, act_lr=8e-8, ent=1.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.17it/s, pg=0.0599, ret=-0.000454, glen=98.9, tlen=260, kl=0.000945, act_lr=8e-8, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=-0.0551, ret=0.0086, glen=115, tlen=277, kl=0.000934, act_lr=8e-8, ent=2.12]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.18it/s, pg=-0.0551, ret=0.0086, glen=115, tlen=277, kl=0.000934, act_lr=8e-8, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.18it/s, pg=-0.0998, ret=0.000231, glen=108, tlen=268, kl=0.000911, act_lr=8e-8, ent=1.54]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.18it/s, pg=-0.0998, ret=0.000231, glen=108, tlen=268, kl=0.000911, act_lr=8e-8, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.18it/s, pg=0.0715, ret=-0.00159, glen=108, tlen=269, kl=0.000924, act_lr=8e-8, ent=1.53] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:44<00:05,  1.18it/s, pg=0.0715, ret=-0.00159, glen=108, tlen=269, kl=0.000924, act_lr=8e-8, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.18it/s, pg=-0.0443, ret=0.000486, glen=108, tlen=269, kl=0.00084, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.18it/s, pg=-0.0443, ret=0.000486, glen=108, tlen=269, kl=0.00084, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.18it/s, pg=-0.0603, ret=0.00058, glen=105, tlen=266, kl=0.000932, act_lr=8e-8, ent=1.64]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.18it/s, pg=-0.0603, ret=0.00058, glen=105, tlen=266, kl=0.000932, act_lr=8e-8, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.18it/s, pg=0.134, ret=0.000693, glen=156, tlen=317, kl=0.000748, act_lr=8e-8, ent=2.34] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.16it/s, pg=0.134, ret=0.000693, glen=156, tlen=317, kl=0.000748, act_lr=8e-8, ent=2.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.16it/s, pg=0.141, ret=-0.000664, glen=113, tlen=273, kl=0.000916, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.16it/s, pg=0.141, ret=-0.000664, glen=113, tlen=273, kl=0.000916, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.16it/s, pg=0.0629, ret=-0.000506, glen=122, tlen=283, kl=0.000863, act_lr=8e-8, ent=1.75]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=0.0629, ret=-0.000506, glen=122, tlen=283, kl=0.000863, act_lr=8e-8, ent=1.75]
2025-07-24 16:24:30.303 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.22s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.0569, ret=0.0016, glen=116, tlen=277, kl=0.000896, act_lr=1e-7, ent=1.78]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.14it/s, pg=-0.0569, ret=0.0016, glen=116, tlen=277, kl=0.000896, act_lr=1e-7, ent=1.78]
2025-07-24 16:24:31.175 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-24 16:24:33.619 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.44s
2025-07-24 16:24:33.946 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.99s
2025-07-24 16:24:33.953 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.015775351688779634, 'actor_lr': 8.034482712854038e-08, 'clip_ratio': 0.0, 'entropy': 1.7241236361963996, 'kl': 0.0008834004402160645, 'response_length': 118.16882981925175, 'total_length': 278.961176115891, 'teacher_total_length': 291.66929363382275, 'return': 7.941008265459396e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:41<30:29, 228.74s/it]2025-07-24 16:24:33.997 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:27:03.390 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:27:03.580 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 16:27:03.581 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 149.58s
2025-07-24 16:27:05.690 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0079,avg_pass_at_n: 1.0000,avg_num_tokens: 112.2317,std_num_tokens: 149.0196,avg_correct_num_tokens: 101.6415,std_correct_num_tokens: 81.0229,avg_incorrect_num_tokens: 129.6903,std_incorrect_num_tokens: 217.9545
2025-07-24 16:27:06.131 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.55s
2025-07-24 16:27:09.389 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.25s
2025-07-24 16:27:38.368 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 16:27:38.369 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.98s
2025-07-24 16:27:39.785 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.01s
2025-07-24 16:27:39.786 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0008574851802940017, avg_kl: 0.0008850013632081153, avg_response_length: 116.88514386819848, avg_orm_score: 0.0, avg_custom_rewards: 0.0008574851802940017
2025-07-24 16:27:39.818 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter5_replay_buffer.jsonl
2025-07-24 16:27:41.721 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0393, ret=-0.00192, glen=107, tlen=267, kl=0.000883, act_lr=1e-7, ent=1.81]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.0393, ret=-0.00192, glen=107, tlen=267, kl=0.000883, act_lr=1e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.00902, ret=0.000127, glen=118, tlen=278, kl=0.000901, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.00902, ret=0.000127, glen=118, tlen=278, kl=0.000901, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.0243, ret=-0.00238, glen=96.6, tlen=256, kl=0.000892, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=0.0243, ret=-0.00238, glen=96.6, tlen=256, kl=0.000892, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=0.248, ret=-0.00191, glen=118, tlen=278, kl=0.000877, act_lr=1e-7, ent=2.01]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=0.248, ret=-0.00191, glen=118, tlen=278, kl=0.000877, act_lr=1e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=0.0458, ret=-0.00112, glen=140, tlen=301, kl=0.000861, act_lr=1e-7, ent=1.6]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.13it/s, pg=0.0458, ret=-0.00112, glen=140, tlen=301, kl=0.000861, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.13it/s, pg=0.038, ret=0.00013, glen=108, tlen=269, kl=0.000864, act_lr=1e-7, ent=1.7]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.14it/s, pg=0.038, ret=0.00013, glen=108, tlen=269, kl=0.000864, act_lr=1e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.14it/s, pg=-0.133, ret=0.00173, glen=121, tlen=281, kl=0.00088, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=-0.133, ret=0.00173, glen=121, tlen=281, kl=0.00088, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=-0.173, ret=0.00139, glen=111, tlen=272, kl=0.000902, act_lr=1e-7, ent=1.72]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.173, ret=0.00139, glen=111, tlen=272, kl=0.000902, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.075, ret=-0.000734, glen=111, tlen=271, kl=0.000881, act_lr=1e-7, ent=1.8]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.16it/s, pg=0.075, ret=-0.000734, glen=111, tlen=271, kl=0.000881, act_lr=1e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.16it/s, pg=0.0364, ret=-0.0013, glen=121, tlen=281, kl=0.000901, act_lr=1e-7, ent=1.63]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=0.0364, ret=-0.0013, glen=121, tlen=281, kl=0.000901, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.0686, ret=-2.51e-6, glen=116, tlen=277, kl=0.000928, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.15it/s, pg=-0.0686, ret=-2.51e-6, glen=116, tlen=277, kl=0.000928, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.15it/s, pg=-0.0676, ret=0.00107, glen=108, tlen=269, kl=0.000891, act_lr=1e-7, ent=1.68] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=-0.0676, ret=0.00107, glen=108, tlen=269, kl=0.000891, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.082, ret=0.000271, glen=126, tlen=286, kl=0.000856, act_lr=1e-7, ent=2.07]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.082, ret=0.000271, glen=126, tlen=286, kl=0.000856, act_lr=1e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.00671, ret=-0.000421, glen=94.4, tlen=255, kl=0.000852, act_lr=1e-7, ent=1.55]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=-0.00671, ret=-0.000421, glen=94.4, tlen=255, kl=0.000852, act_lr=1e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=-0.25, ret=0.000238, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.68]     Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.25, ret=0.000238, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=0.338, ret=4.2e-5, glen=353, tlen=514, kl=0.000729, act_lr=1e-7, ent=2.82]  Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.13it/s, pg=0.338, ret=4.2e-5, glen=353, tlen=514, kl=0.000729, act_lr=1e-7, ent=2.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.13it/s, pg=0.0494, ret=0.000201, glen=119, tlen=279, kl=0.000907, act_lr=1e-7, ent=1.92]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:35,  1.14it/s, pg=0.0494, ret=0.000201, glen=119, tlen=279, kl=0.000907, act_lr=1e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.122, ret=0.000526, glen=113, tlen=273, kl=0.000918, act_lr=1e-7, ent=1.61]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.122, ret=0.000526, glen=113, tlen=273, kl=0.000918, act_lr=1e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=0.101, ret=-0.00188, glen=114, tlen=275, kl=0.000894, act_lr=1e-7, ent=1.69] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.101, ret=-0.00188, glen=114, tlen=275, kl=0.000894, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0151, ret=0.00018, glen=125, tlen=285, kl=0.000873, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0151, ret=0.00018, glen=125, tlen=285, kl=0.000873, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.234, ret=0.000414, glen=113, tlen=273, kl=0.000904, act_lr=1e-7, ent=1.72]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.234, ret=0.000414, glen=113, tlen=273, kl=0.000904, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.049, ret=-0.000156, glen=106, tlen=266, kl=0.000912, act_lr=1e-7, ent=1.77]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.049, ret=-0.000156, glen=106, tlen=266, kl=0.000912, act_lr=1e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.0792, ret=0.000538, glen=106, tlen=267, kl=0.000945, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0792, ret=0.000538, glen=106, tlen=267, kl=0.000945, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0269, ret=7.95e-5, glen=110, tlen=271, kl=0.000875, act_lr=1e-7, ent=1.59]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0269, ret=7.95e-5, glen=110, tlen=271, kl=0.000875, act_lr=1e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.135, ret=-1.89e-5, glen=123, tlen=284, kl=0.000855, act_lr=1e-7, ent=1.78]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.135, ret=-1.89e-5, glen=123, tlen=284, kl=0.000855, act_lr=1e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0774, ret=0.00101, glen=103, tlen=263, kl=0.000931, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0774, ret=0.00101, glen=103, tlen=263, kl=0.000931, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0476, ret=-0.00123, glen=121, tlen=282, kl=0.000851, act_lr=1e-7, ent=1.85]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.0476, ret=-0.00123, glen=121, tlen=282, kl=0.000851, act_lr=1e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0303, ret=0.000846, glen=127, tlen=288, kl=0.000811, act_lr=1e-7, ent=2.13]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0303, ret=0.000846, glen=127, tlen=288, kl=0.000811, act_lr=1e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0804, ret=-0.00016, glen=98.8, tlen=259, kl=0.000901, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=-0.0804, ret=-0.00016, glen=98.8, tlen=259, kl=0.000901, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.0238, ret=-0.000203, glen=98.5, tlen=259, kl=0.000941, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=-0.0238, ret=-0.000203, glen=98.5, tlen=259, kl=0.000941, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.0546, ret=0.0011, glen=115, tlen=275, kl=0.000786, act_lr=1e-7, ent=1.43]     Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0546, ret=0.0011, glen=115, tlen=275, kl=0.000786, act_lr=1e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0159, ret=0.000236, glen=102, tlen=262, kl=0.00088, act_lr=1e-7, ent=1.59]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.14it/s, pg=0.0159, ret=0.000236, glen=102, tlen=262, kl=0.00088, act_lr=1e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.14it/s, pg=0.125, ret=-6.43e-5, glen=131, tlen=291, kl=0.0009, act_lr=1e-7, ent=1.81]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.14it/s, pg=0.125, ret=-6.43e-5, glen=131, tlen=291, kl=0.0009, act_lr=1e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.23, ret=0.0289, glen=93.6, tlen=255, kl=0.000889, act_lr=1e-7, ent=1.73]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.23, ret=0.0289, glen=93.6, tlen=255, kl=0.000889, act_lr=1e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=-0.0799, ret=0.000608, glen=109, tlen=269, kl=0.000916, act_lr=1e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.0799, ret=0.000608, glen=109, tlen=269, kl=0.000916, act_lr=1e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.0713, ret=0.000816, glen=120, tlen=281, kl=0.0009, act_lr=1e-7, ent=2.03]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0713, ret=0.000816, glen=120, tlen=281, kl=0.0009, act_lr=1e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=0.0663, ret=-0.00037, glen=112, tlen=273, kl=0.000896, act_lr=1e-7, ent=1.58]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=0.0663, ret=-0.00037, glen=112, tlen=273, kl=0.000896, act_lr=1e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.211, ret=0.00184, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.63] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.211, ret=0.00184, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.0505, ret=-0.00124, glen=115, tlen=276, kl=0.000849, act_lr=1e-7, ent=1.64]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=0.0505, ret=-0.00124, glen=115, tlen=276, kl=0.000849, act_lr=1e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0694, ret=0.00014, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.6] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0694, ret=0.00014, glen=105, tlen=265, kl=0.000875, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0265, ret=-0.000634, glen=101, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.68]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0265, ret=-0.000634, glen=101, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.104, ret=0.000431, glen=117, tlen=277, kl=0.000908, act_lr=1e-7, ent=1.76] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.104, ret=0.000431, glen=117, tlen=277, kl=0.000908, act_lr=1e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=-0.0182, ret=-0.000597, glen=120, tlen=281, kl=0.000882, act_lr=1e-7, ent=1.94]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=-0.0182, ret=-0.000597, glen=120, tlen=281, kl=0.000882, act_lr=1e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.167, ret=0.000342, glen=101, tlen=261, kl=0.000909, act_lr=1e-7, ent=1.71]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=-0.167, ret=0.000342, glen=101, tlen=261, kl=0.000909, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=-0.149, ret=0.00203, glen=112, tlen=272, kl=0.000911, act_lr=1e-7, ent=1.67] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=-0.149, ret=0.00203, glen=112, tlen=272, kl=0.000911, act_lr=1e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=0.00372, ret=-0.00123, glen=104, tlen=264, kl=0.000896, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.18it/s, pg=0.00372, ret=-0.00123, glen=104, tlen=264, kl=0.000896, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.195, ret=0.0016, glen=101, tlen=261, kl=0.00093, act_lr=1e-7, ent=1.66]    Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.195, ret=0.0016, glen=101, tlen=261, kl=0.00093, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=0.214, ret=-0.00248, glen=154, tlen=314, kl=0.000893, act_lr=1e-7, ent=2.17]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.214, ret=-0.00248, glen=154, tlen=314, kl=0.000893, act_lr=1e-7, ent=2.17]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.195, ret=-0.00141, glen=123, tlen=284, kl=0.000898, act_lr=1e-7, ent=1.95]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.195, ret=-0.00141, glen=123, tlen=284, kl=0.000898, act_lr=1e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.000458, ret=-0.000291, glen=120, tlen=281, kl=0.000905, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.000458, ret=-0.000291, glen=120, tlen=281, kl=0.000905, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.224, ret=0.00203, glen=104, tlen=264, kl=0.000823, act_lr=1e-7, ent=1.89]     Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.224, ret=0.00203, glen=104, tlen=264, kl=0.000823, act_lr=1e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.218, ret=0.00191, glen=102, tlen=262, kl=0.000894, act_lr=1e-7, ent=1.57]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.218, ret=0.00191, glen=102, tlen=262, kl=0.000894, act_lr=1e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.105, ret=-0.0017, glen=103, tlen=263, kl=0.000896, act_lr=1e-7, ent=1.5]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.18it/s, pg=0.105, ret=-0.0017, glen=103, tlen=263, kl=0.000896, act_lr=1e-7, ent=1.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=-0.0441, ret=0.000576, glen=115, tlen=276, kl=0.000901, act_lr=1e-7, ent=1.71]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.18it/s, pg=-0.0441, ret=0.000576, glen=115, tlen=276, kl=0.000901, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.18it/s, pg=-0.0565, ret=-0.000726, glen=106, tlen=266, kl=0.000917, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.18it/s, pg=-0.0565, ret=-0.000726, glen=106, tlen=266, kl=0.000917, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.18it/s, pg=0.0109, ret=0.000953, glen=103, tlen=263, kl=0.000872, act_lr=1e-7, ent=1.69]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0109, ret=0.000953, glen=103, tlen=263, kl=0.000872, act_lr=1e-7, ent=1.69]
2025-07-24 16:28:31.524 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.39s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.123, ret=-0.000452, glen=125, tlen=286, kl=0.000888, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=0.123, ret=-0.000452, glen=125, tlen=286, kl=0.000888, act_lr=1.2e-7, ent=1.83]
2025-07-24 16:28:32.376 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 16:28:34.954 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-24 16:28:35.298 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.29s
2025-07-24 16:28:35.309 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01867935113739549, 'actor_lr': 1.0035087826596729e-07, 'clip_ratio': 0.0, 'entropy': 1.7527947906862225, 'kl': 0.0008854698716548451, 'response_length': 116.7514379400956, 'total_length': 277.13355670058934, 'teacher_total_length': 289.843635425233, 'return': 0.0004855551092636265, 'policy_update_steps': 1.0}
Episode [1/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:43<27:11, 233.03s/it]2025-07-24 16:28:35.353 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:29:59.237 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:29:59.416 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:29:59.417 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 84.06s
2025-07-24 16:30:01.723 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 108.2264,std_num_tokens: 106.4321,avg_correct_num_tokens: 101.3536,std_correct_num_tokens: 80.9237,avg_incorrect_num_tokens: 119.4747,std_incorrect_num_tokens: 137.6432
2025-07-24 16:30:02.195 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.78s
2025-07-24 16:30:05.160 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.96s
2025-07-24 16:30:33.561 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 16:30:33.562 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.40s
2025-07-24 16:30:35.031 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.00s
2025-07-24 16:30:35.032 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.00016510206292437665, avg_kl: 0.000943119750429162, avg_response_length: 109.35092505211253, avg_orm_score: 0.0, avg_custom_rewards: -0.00016510206292437665
2025-07-24 16:30:35.065 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter6_replay_buffer.jsonl
2025-07-24 16:30:36.896 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.83s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0025, ret=0.000914, glen=100, tlen=261, kl=0.000953, act_lr=1.2e-7, ent=1.87]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.0025, ret=0.000914, glen=100, tlen=261, kl=0.000953, act_lr=1.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.00173, ret=-0.00164, glen=118, tlen=279, kl=0.000913, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=-0.00173, ret=-0.00164, glen=118, tlen=279, kl=0.000913, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=0.0443, ret=-0.000604, glen=117, tlen=277, kl=0.000939, act_lr=1.2e-7, ent=1.75] Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=0.0443, ret=-0.000604, glen=117, tlen=277, kl=0.000939, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=0.102, ret=-0.000442, glen=107, tlen=267, kl=0.000919, act_lr=1.2e-7, ent=1.63] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.12it/s, pg=0.102, ret=-0.000442, glen=107, tlen=267, kl=0.000919, act_lr=1.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.12it/s, pg=0.00345, ret=-0.000125, glen=101, tlen=261, kl=0.000919, act_lr=1.2e-7, ent=1.92]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.14it/s, pg=0.00345, ret=-0.000125, glen=101, tlen=261, kl=0.000919, act_lr=1.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.14it/s, pg=-0.0453, ret=0.00017, glen=106, tlen=266, kl=0.000981, act_lr=1.2e-7, ent=1.55]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.15it/s, pg=-0.0453, ret=0.00017, glen=106, tlen=266, kl=0.000981, act_lr=1.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.15it/s, pg=-0.0955, ret=0.000365, glen=99.6, tlen=260, kl=0.000933, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=-0.0955, ret=0.000365, glen=99.6, tlen=260, kl=0.000933, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=-0.041, ret=0.000494, glen=132, tlen=292, kl=0.000901, act_lr=1.2e-7, ent=1.75]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.041, ret=0.000494, glen=132, tlen=292, kl=0.000901, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.0201, ret=-0.000161, glen=107, tlen=267, kl=0.000984, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.0201, ret=-0.000161, glen=107, tlen=267, kl=0.000984, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0221, ret=0.000481, glen=104, tlen=265, kl=0.000927, act_lr=1.2e-7, ent=1.68] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0221, ret=0.000481, glen=104, tlen=265, kl=0.000927, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=-0.184, ret=0.000986, glen=105, tlen=265, kl=0.000944, act_lr=1.2e-7, ent=1.77] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=-0.184, ret=0.000986, glen=105, tlen=265, kl=0.000944, act_lr=1.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0591, ret=7.01e-5, glen=109, tlen=270, kl=0.000937, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0591, ret=7.01e-5, glen=109, tlen=270, kl=0.000937, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0573, ret=0.000754, glen=124, tlen=285, kl=0.000931, act_lr=1.2e-7, ent=2.08]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0573, ret=0.000754, glen=124, tlen=285, kl=0.000931, act_lr=1.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.116, ret=0.000816, glen=99.4, tlen=260, kl=0.000991, act_lr=1.2e-7, ent=1.6]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.116, ret=0.000816, glen=99.4, tlen=260, kl=0.000991, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=0.14, ret=-0.00184, glen=110, tlen=271, kl=0.000939, act_lr=1.2e-7, ent=1.68]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=0.14, ret=-0.00184, glen=110, tlen=271, kl=0.000939, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.05, ret=0.000554, glen=99.9, tlen=260, kl=0.000954, act_lr=1.2e-7, ent=1.71]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=-0.05, ret=0.000554, glen=99.9, tlen=260, kl=0.000954, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.129, ret=0.00148, glen=129, tlen=289, kl=0.000969, act_lr=1.2e-7, ent=1.93] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.129, ret=0.00148, glen=129, tlen=289, kl=0.000969, act_lr=1.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=-0.117, ret=0.00104, glen=98.2, tlen=259, kl=0.000943, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.117, ret=0.00104, glen=98.2, tlen=259, kl=0.000943, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.0033, ret=9.83e-5, glen=108, tlen=269, kl=0.00092, act_lr=1.2e-7, ent=1.76]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0033, ret=9.83e-5, glen=108, tlen=269, kl=0.00092, act_lr=1.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.135, ret=0.000641, glen=104, tlen=265, kl=0.00101, act_lr=1.2e-7, ent=1.59]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.16it/s, pg=-0.135, ret=0.000641, glen=104, tlen=265, kl=0.00101, act_lr=1.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.16it/s, pg=0.11, ret=-0.000124, glen=110, tlen=270, kl=0.000933, act_lr=1.2e-7, ent=1.82]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=0.11, ret=-0.000124, glen=110, tlen=270, kl=0.000933, act_lr=1.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=0.0835, ret=-0.0011, glen=101, tlen=262, kl=0.00097, act_lr=1.2e-7, ent=1.6]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=0.0835, ret=-0.0011, glen=101, tlen=262, kl=0.00097, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.173, ret=0.00124, glen=101, tlen=262, kl=0.000942, act_lr=1.2e-7, ent=1.59]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.173, ret=0.00124, glen=101, tlen=262, kl=0.000942, act_lr=1.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.173, ret=0.000238, glen=101, tlen=261, kl=0.000931, act_lr=1.2e-7, ent=1.73]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.173, ret=0.000238, glen=101, tlen=261, kl=0.000931, act_lr=1.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.00574, ret=0.000516, glen=120, tlen=281, kl=0.000922, act_lr=1.2e-7, ent=1.95]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=0.00574, ret=0.000516, glen=120, tlen=281, kl=0.000922, act_lr=1.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.105, ret=6.01e-5, glen=134, tlen=295, kl=0.00092, act_lr=1.2e-7, ent=1.94]    Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.105, ret=6.01e-5, glen=134, tlen=295, kl=0.00092, act_lr=1.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.205, ret=-0.00114, glen=117, tlen=278, kl=0.000961, act_lr=1.2e-7, ent=1.79]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.205, ret=-0.00114, glen=117, tlen=278, kl=0.000961, act_lr=1.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.115, ret=0.000597, glen=95.7, tlen=256, kl=0.000937, act_lr=1.2e-7, ent=1.72]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:24,  1.15it/s, pg=-0.115, ret=0.000597, glen=95.7, tlen=256, kl=0.000937, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:24,  1.15it/s, pg=-0.0523, ret=-0.000468, glen=110, tlen=271, kl=0.000978, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.06it/s, pg=-0.0523, ret=-0.000468, glen=110, tlen=271, kl=0.000978, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.06it/s, pg=0.0282, ret=-0.000835, glen=99.3, tlen=259, kl=0.000975, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.09it/s, pg=0.0282, ret=-0.000835, glen=99.3, tlen=259, kl=0.000975, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.09it/s, pg=-0.0337, ret=0.00046, glen=108, tlen=269, kl=0.00094, act_lr=1.2e-7, ent=1.71]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.09it/s, pg=-0.0337, ret=0.00046, glen=108, tlen=269, kl=0.00094, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.09it/s, pg=0.122, ret=-0.0013, glen=128, tlen=289, kl=0.00095, act_lr=1.2e-7, ent=2.01]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.12it/s, pg=0.122, ret=-0.0013, glen=128, tlen=289, kl=0.00095, act_lr=1.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=0.0417, ret=-0.000591, glen=112, tlen=273, kl=0.000932, act_lr=1.2e-7, ent=1.78]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.13it/s, pg=0.0417, ret=-0.000591, glen=112, tlen=273, kl=0.000932, act_lr=1.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.13it/s, pg=0.0226, ret=-6.05e-5, glen=104, tlen=264, kl=0.000957, act_lr=1.2e-7, ent=1.69] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.14it/s, pg=0.0226, ret=-6.05e-5, glen=104, tlen=264, kl=0.000957, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.14it/s, pg=0.0665, ret=-0.00216, glen=110, tlen=270, kl=0.000896, act_lr=1.2e-7, ent=1.61]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.15it/s, pg=0.0665, ret=-0.00216, glen=110, tlen=270, kl=0.000896, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.15it/s, pg=0.175, ret=-0.000352, glen=123, tlen=283, kl=0.00091, act_lr=1.2e-7, ent=1.54] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.175, ret=-0.000352, glen=123, tlen=283, kl=0.00091, act_lr=1.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.103, ret=0.000979, glen=102, tlen=262, kl=0.000901, act_lr=1.2e-7, ent=1.64]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.103, ret=0.000979, glen=102, tlen=262, kl=0.000901, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.0288, ret=0.00044, glen=104, tlen=264, kl=0.000969, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.0288, ret=0.00044, glen=104, tlen=264, kl=0.000969, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.277, ret=-0.000879, glen=118, tlen=279, kl=0.000906, act_lr=1.2e-7, ent=2.15]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.277, ret=-0.000879, glen=118, tlen=279, kl=0.000906, act_lr=1.2e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0346, ret=0.000369, glen=104, tlen=265, kl=0.000946, act_lr=1.2e-7, ent=1.62]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0346, ret=0.000369, glen=104, tlen=265, kl=0.000946, act_lr=1.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0407, ret=0.00145, glen=103, tlen=263, kl=0.000937, act_lr=1.2e-7, ent=1.61] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.0407, ret=0.00145, glen=103, tlen=263, kl=0.000937, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.0691, ret=1.65e-5, glen=105, tlen=265, kl=0.000992, act_lr=1.2e-7, ent=1.66] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.0691, ret=1.65e-5, glen=105, tlen=265, kl=0.000992, act_lr=1.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.215, ret=0.00128, glen=114, tlen=274, kl=0.000937, act_lr=1.2e-7, ent=1.64]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.18it/s, pg=-0.215, ret=0.00128, glen=114, tlen=274, kl=0.000937, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.18it/s, pg=0.158, ret=-0.0014, glen=125, tlen=285, kl=0.000907, act_lr=1.2e-7, ent=1.84] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.18it/s, pg=0.158, ret=-0.0014, glen=125, tlen=285, kl=0.000907, act_lr=1.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.18it/s, pg=-0.0244, ret=-0.000285, glen=101, tlen=261, kl=0.000997, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.18it/s, pg=-0.0244, ret=-0.000285, glen=101, tlen=261, kl=0.000997, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.101, ret=-0.000606, glen=109, tlen=270, kl=0.000928, act_lr=1.2e-7, ent=1.69] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.18it/s, pg=-0.101, ret=-0.000606, glen=109, tlen=270, kl=0.000928, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.18it/s, pg=-0.025, ret=0.00108, glen=101, tlen=261, kl=0.000947, act_lr=1.2e-7, ent=1.64]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.18it/s, pg=-0.025, ret=0.00108, glen=101, tlen=261, kl=0.000947, act_lr=1.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.18it/s, pg=-0.0969, ret=0.00216, glen=98.2, tlen=259, kl=0.000931, act_lr=1.2e-7, ent=1.8]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=-0.0969, ret=0.00216, glen=98.2, tlen=259, kl=0.000931, act_lr=1.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.138, ret=0.000337, glen=114, tlen=274, kl=0.000929, act_lr=1.2e-7, ent=1.67]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.138, ret=0.000337, glen=114, tlen=274, kl=0.000929, act_lr=1.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=0.0835, ret=-0.00118, glen=117, tlen=277, kl=0.000963, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.18it/s, pg=0.0835, ret=-0.00118, glen=117, tlen=277, kl=0.000963, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.18it/s, pg=-0.0207, ret=0.000118, glen=108, tlen=268, kl=0.000925, act_lr=1.2e-7, ent=1.61]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.0207, ret=0.000118, glen=108, tlen=268, kl=0.000925, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=0.123, ret=-0.000666, glen=109, tlen=269, kl=0.000966, act_lr=1.2e-7, ent=1.71] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.18it/s, pg=0.123, ret=-0.000666, glen=109, tlen=269, kl=0.000966, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.18it/s, pg=-0.0159, ret=-0.00061, glen=115, tlen=276, kl=0.000943, act_lr=1.2e-7, ent=1.78]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.0159, ret=-0.00061, glen=115, tlen=276, kl=0.000943, act_lr=1.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0294, ret=0.000539, glen=99.7, tlen=260, kl=0.000939, act_lr=1.2e-7, ent=1.55]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0294, ret=0.000539, glen=99.7, tlen=260, kl=0.000939, act_lr=1.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0861, ret=-0.00142, glen=118, tlen=278, kl=0.000899, act_lr=1.2e-7, ent=1.7]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=0.0861, ret=-0.00142, glen=118, tlen=278, kl=0.000899, act_lr=1.2e-7, ent=1.7]
2025-07-24 16:31:25.535 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.45s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=-0.0397, ret=-0.000993, glen=106, tlen=267, kl=0.000998, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.0397, ret=-0.000993, glen=106, tlen=267, kl=0.000998, act_lr=1.4e-7, ent=1.82]
2025-07-24 16:31:26.394 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-24 16:31:28.969 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-24 16:31:29.294 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.33s
2025-07-24 16:31:29.301 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.006455455507550921, 'actor_lr': 1.203571386046828e-07, 'clip_ratio': 0.0, 'entropy': 1.7342252433300018, 'kl': 0.0009432690484183175, 'response_length': 109.31775610787528, 'total_length': 269.7792137690953, 'teacher_total_length': 281.8311091831752, 'return': -4.152664457381304e-06, 'policy_update_steps': 1.0}
Episode [1/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [25:37<21:22, 213.73s/it]2025-07-24 16:31:29.328 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:33:05.119 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:33:05.300 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:33:05.301 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 95.97s
2025-07-24 16:33:07.358 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0112,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6497,std_num_tokens: 116.1839,avg_correct_num_tokens: 105.9321,std_correct_num_tokens: 86.2368,avg_incorrect_num_tokens: 128.7628,std_incorrect_num_tokens: 151.6254
2025-07-24 16:33:07.688 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.39s
2025-07-24 16:33:10.874 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.18s
2025-07-24 16:33:39.737 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 228
2025-07-24 16:33:39.738 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.86s
2025-07-24 16:33:41.246 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.07s
2025-07-24 16:33:41.246 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008591915036065569, avg_kl: 0.0009288536874871505, avg_response_length: 116.01782658225612, avg_orm_score: 0.0, avg_custom_rewards: -0.0008591915036065569
2025-07-24 16:33:41.288 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter7_replay_buffer.jsonl
2025-07-24 16:33:43.242 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.96s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.0303, ret=-0.000445, glen=108, tlen=269, kl=0.000888, act_lr=1.4e-7, ent=1.61]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=-0.0303, ret=-0.000445, glen=108, tlen=269, kl=0.000888, act_lr=1.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.03s/it, pg=0.156, ret=-0.000947, glen=115, tlen=275, kl=0.000926, act_lr=1.4e-7, ent=1.67]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.156, ret=-0.000947, glen=115, tlen=275, kl=0.000926, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.00891, ret=-3.33e-5, glen=106, tlen=266, kl=0.000901, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=0.00891, ret=-3.33e-5, glen=106, tlen=266, kl=0.000901, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.209, ret=0.0018, glen=109, tlen=269, kl=0.000939, act_lr=1.4e-7, ent=1.79]   Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.209, ret=0.0018, glen=109, tlen=269, kl=0.000939, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=0.0459, ret=-0.00124, glen=108, tlen=269, kl=0.000956, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.12it/s, pg=0.0459, ret=-0.00124, glen=108, tlen=269, kl=0.000956, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.12it/s, pg=0.0549, ret=-0.00106, glen=119, tlen=279, kl=0.000936, act_lr=1.4e-7, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.13it/s, pg=0.0549, ret=-0.00106, glen=119, tlen=279, kl=0.000936, act_lr=1.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.13it/s, pg=0.0494, ret=-0.00189, glen=124, tlen=285, kl=0.000936, act_lr=1.4e-7, ent=1.8] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=0.0494, ret=-0.00189, glen=124, tlen=285, kl=0.000936, act_lr=1.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=0.0083, ret=-0.0017, glen=126, tlen=287, kl=0.000961, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=0.0083, ret=-0.0017, glen=126, tlen=287, kl=0.000961, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=-0.00143, ret=0.000171, glen=107, tlen=267, kl=0.000928, act_lr=1.4e-7, ent=1.58]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.16it/s, pg=-0.00143, ret=0.000171, glen=107, tlen=267, kl=0.000928, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.16it/s, pg=-0.0471, ret=0.000764, glen=114, tlen=274, kl=0.000972, act_lr=1.4e-7, ent=1.78] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.0471, ret=0.000764, glen=114, tlen=274, kl=0.000972, act_lr=1.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.0348, ret=0.000309, glen=107, tlen=268, kl=0.000876, act_lr=1.4e-7, ent=1.65] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.0348, ret=0.000309, glen=107, tlen=268, kl=0.000876, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=0.0833, ret=-0.00116, glen=119, tlen=279, kl=0.000929, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=0.0833, ret=-0.00116, glen=119, tlen=279, kl=0.000929, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.152, ret=0.00148, glen=117, tlen=277, kl=0.00094, act_lr=1.4e-7, ent=1.74]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=-0.152, ret=0.00148, glen=117, tlen=277, kl=0.00094, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=0.209, ret=-0.00144, glen=142, tlen=302, kl=0.000803, act_lr=1.4e-7, ent=1.46]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=0.209, ret=-0.00144, glen=142, tlen=302, kl=0.000803, act_lr=1.4e-7, ent=1.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0901, ret=-0.00105, glen=126, tlen=286, kl=0.00094, act_lr=1.4e-7, ent=1.84]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=0.0901, ret=-0.00105, glen=126, tlen=286, kl=0.00094, act_lr=1.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=-0.0391, ret=-0.000464, glen=104, tlen=264, kl=0.000998, act_lr=1.4e-7, ent=1.7]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0391, ret=-0.000464, glen=104, tlen=264, kl=0.000998, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.1, ret=0.000556, glen=120, tlen=280, kl=0.000887, act_lr=1.4e-7, ent=1.77]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.1, ret=0.000556, glen=120, tlen=280, kl=0.000887, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.175, ret=0.00107, glen=104, tlen=265, kl=0.000903, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.175, ret=0.00107, glen=104, tlen=265, kl=0.000903, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.156, ret=0.000737, glen=113, tlen=274, kl=0.00091, act_lr=1.4e-7, ent=1.6] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.156, ret=0.000737, glen=113, tlen=274, kl=0.00091, act_lr=1.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.22, ret=-0.00184, glen=136, tlen=296, kl=0.000958, act_lr=1.4e-7, ent=2.18]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.22, ret=-0.00184, glen=136, tlen=296, kl=0.000958, act_lr=1.4e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.124, ret=0.00102, glen=121, tlen=282, kl=0.000892, act_lr=1.4e-7, ent=1.89]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.124, ret=0.00102, glen=121, tlen=282, kl=0.000892, act_lr=1.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.00397, ret=-0.00198, glen=112, tlen=272, kl=0.000965, act_lr=1.4e-7, ent=1.64]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.00397, ret=-0.00198, glen=112, tlen=272, kl=0.000965, act_lr=1.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0125, ret=-0.000189, glen=117, tlen=278, kl=0.000898, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.17it/s, pg=0.0125, ret=-0.000189, glen=117, tlen=278, kl=0.000898, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.17it/s, pg=-0.171, ret=0.000803, glen=116, tlen=276, kl=0.000919, act_lr=1.4e-7, ent=1.77] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.171, ret=0.000803, glen=116, tlen=276, kl=0.000919, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.241, ret=-0.00267, glen=115, tlen=275, kl=0.00094, act_lr=1.4e-7, ent=1.69]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.241, ret=-0.00267, glen=115, tlen=275, kl=0.00094, act_lr=1.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.0515, ret=-5.73e-5, glen=112, tlen=272, kl=0.000903, act_lr=1.4e-7, ent=1.63]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.0515, ret=-5.73e-5, glen=112, tlen=272, kl=0.000903, act_lr=1.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.071, ret=-0.000173, glen=114, tlen=274, kl=0.000935, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=-0.071, ret=-0.000173, glen=114, tlen=274, kl=0.000935, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=0.0312, ret=-0.000752, glen=123, tlen=283, kl=0.000926, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0312, ret=-0.000752, glen=123, tlen=283, kl=0.000926, act_lr=1.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=-0.0782, ret=0.000848, glen=106, tlen=267, kl=0.000965, act_lr=1.4e-7, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=-0.0782, ret=0.000848, glen=106, tlen=267, kl=0.000965, act_lr=1.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=0.0155, ret=-0.000318, glen=110, tlen=270, kl=0.000912, act_lr=1.4e-7, ent=1.62]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=0.0155, ret=-0.000318, glen=110, tlen=270, kl=0.000912, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.0761, ret=-0.000297, glen=118, tlen=278, kl=0.000931, act_lr=1.4e-7, ent=1.64]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.0761, ret=-0.000297, glen=118, tlen=278, kl=0.000931, act_lr=1.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.162, ret=-0.00244, glen=114, tlen=274, kl=0.000969, act_lr=1.4e-7, ent=1.62]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:22,  1.14it/s, pg=0.162, ret=-0.00244, glen=114, tlen=274, kl=0.000969, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.14it/s, pg=-0.00757, ret=-0.000327, glen=125, tlen=286, kl=0.000869, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=-0.00757, ret=-0.000327, glen=125, tlen=286, kl=0.000869, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=0.212, ret=-0.00124, glen=109, tlen=270, kl=0.000921, act_lr=1.4e-7, ent=1.72]    Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=0.212, ret=-0.00124, glen=109, tlen=270, kl=0.000921, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=-0.09, ret=0.00143, glen=117, tlen=277, kl=0.000926, act_lr=1.4e-7, ent=1.74] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.09, ret=0.00143, glen=117, tlen=277, kl=0.000926, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.131, ret=5.03e-5, glen=147, tlen=308, kl=0.000883, act_lr=1.4e-7, ent=2.02]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=0.131, ret=5.03e-5, glen=147, tlen=308, kl=0.000883, act_lr=1.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.179, ret=0.00152, glen=106, tlen=266, kl=0.00092, act_lr=1.4e-7, ent=1.67]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.179, ret=0.00152, glen=106, tlen=266, kl=0.00092, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.000244, ret=8.56e-5, glen=113, tlen=274, kl=0.000953, act_lr=1.4e-7, ent=1.82]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:32<00:16,  1.17it/s, pg=-0.000244, ret=8.56e-5, glen=113, tlen=274, kl=0.000953, act_lr=1.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.149, ret=0.0015, glen=101, tlen=261, kl=0.000969, act_lr=1.4e-7, ent=1.58]    Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.149, ret=0.0015, glen=101, tlen=261, kl=0.000969, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.22, ret=0.00216, glen=124, tlen=283, kl=0.000974, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.22, ret=0.00216, glen=124, tlen=283, kl=0.000974, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0248, ret=0.000524, glen=118, tlen=279, kl=0.000932, act_lr=1.4e-7, ent=1.71]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0248, ret=0.000524, glen=118, tlen=279, kl=0.000932, act_lr=1.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0811, ret=0.000216, glen=124, tlen=285, kl=0.000938, act_lr=1.4e-7, ent=1.58]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0811, ret=0.000216, glen=124, tlen=285, kl=0.000938, act_lr=1.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.084, ret=0.000508, glen=105, tlen=266, kl=0.000934, act_lr=1.4e-7, ent=1.7] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.084, ret=0.000508, glen=105, tlen=266, kl=0.000934, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.201, ret=0.00257, glen=104, tlen=264, kl=0.000941, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.201, ret=0.00257, glen=104, tlen=264, kl=0.000941, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.161, ret=-0.00166, glen=114, tlen=275, kl=0.000942, act_lr=1.4e-7, ent=1.75]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=0.161, ret=-0.00166, glen=114, tlen=275, kl=0.000942, act_lr=1.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=0.017, ret=-0.000343, glen=118, tlen=279, kl=0.000933, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=0.017, ret=-0.000343, glen=118, tlen=279, kl=0.000933, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.105, ret=-0.0001, glen=131, tlen=292, kl=0.000915, act_lr=1.4e-7, ent=1.76]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.105, ret=-0.0001, glen=131, tlen=292, kl=0.000915, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.0308, ret=0.000512, glen=113, tlen=274, kl=0.000984, act_lr=1.4e-7, ent=1.83]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.0308, ret=0.000512, glen=113, tlen=274, kl=0.000984, act_lr=1.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.318, ret=0.00119, glen=98.4, tlen=259, kl=0.000909, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.318, ret=0.00119, glen=98.4, tlen=259, kl=0.000909, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=0.162, ret=-0.00163, glen=125, tlen=286, kl=0.000945, act_lr=1.4e-7, ent=2.02] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.18it/s, pg=0.162, ret=-0.00163, glen=125, tlen=286, kl=0.000945, act_lr=1.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.18it/s, pg=-0.0259, ret=0.00063, glen=115, tlen=275, kl=0.000906, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.0259, ret=0.00063, glen=115, tlen=275, kl=0.000906, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.106, ret=-0.000578, glen=122, tlen=282, kl=0.000878, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.17it/s, pg=0.106, ret=-0.000578, glen=122, tlen=282, kl=0.000878, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0309, ret=-0.000328, glen=104, tlen=264, kl=0.000916, act_lr=1.4e-7, ent=1.51]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0309, ret=-0.000328, glen=104, tlen=264, kl=0.000916, act_lr=1.4e-7, ent=1.51]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.046, ret=-0.000683, glen=110, tlen=271, kl=0.000943, act_lr=1.4e-7, ent=1.79]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.15it/s, pg=0.046, ret=-0.000683, glen=110, tlen=271, kl=0.000943, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.15it/s, pg=-0.169, ret=0.000447, glen=126, tlen=286, kl=0.000958, act_lr=1.4e-7, ent=1.87]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.16it/s, pg=-0.169, ret=0.000447, glen=126, tlen=286, kl=0.000958, act_lr=1.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.16it/s, pg=-0.0964, ret=0.000802, glen=107, tlen=268, kl=0.000969, act_lr=1.4e-7, ent=1.74]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.16it/s, pg=-0.0964, ret=0.000802, glen=107, tlen=268, kl=0.000969, act_lr=1.4e-7, ent=1.74]
2025-07-24 16:34:32.772 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.34s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.16it/s, pg=-0.107, ret=0.00246, glen=134, tlen=294, kl=0.000942, act_lr=1.6e-7, ent=1.87]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=-0.107, ret=0.00246, glen=134, tlen=294, kl=0.000942, act_lr=1.6e-7, ent=1.87]
2025-07-24 16:34:33.586 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.74s
2025-07-24 16:34:36.209 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.62s
2025-07-24 16:34:36.542 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.24s
2025-07-24 16:34:36.549 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.006998446949741296, 'actor_lr': 1.403508745948114e-07, 'clip_ratio': 0.0, 'entropy': 1.7287561015078896, 'kl': 0.0009288536874871505, 'response_length': 116.01782654879386, 'total_length': 276.4504688999109, 'teacher_total_length': 288.38180488452576, 'return': -5.060130525523339e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [28:44<17:06, 205.30s/it]2025-07-24 16:34:36.594 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:37:20.213 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:37:20.385 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:37:20.386 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 163.79s
2025-07-24 16:37:22.438 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 117.0878,std_num_tokens: 178.9064,avg_correct_num_tokens: 106.5291,std_correct_num_tokens: 138.5686,avg_incorrect_num_tokens: 132.9484,std_incorrect_num_tokens: 225.4444
2025-07-24 16:37:22.886 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.50s
2025-07-24 16:37:26.296 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.41s
2025-07-24 16:37:55.841 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 231
2025-07-24 16:37:55.843 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.54s
2025-07-24 16:37:57.408 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.17s
2025-07-24 16:37:57.409 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0001567989220899163, avg_kl: 0.0009046550436969444, avg_response_length: 124.27245621454148, avg_orm_score: 0.0, avg_custom_rewards: 0.0001567989220899163
2025-07-24 16:37:57.468 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter8_replay_buffer.jsonl
2025-07-24 16:37:59.484 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 2.02s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.0107, ret=0.000161, glen=104, tlen=264, kl=0.000959, act_lr=1.6e-7, ent=1.62]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.0107, ret=0.000161, glen=104, tlen=264, kl=0.000959, act_lr=1.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:58,  1.03s/it, pg=-0.22, ret=0.00211, glen=106, tlen=266, kl=0.000923, act_lr=1.6e-7, ent=1.69]   Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:51,  1.08it/s, pg=-0.22, ret=0.00211, glen=106, tlen=266, kl=0.000923, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:51,  1.08it/s, pg=-0.16, ret=0.000603, glen=115, tlen=274, kl=0.000951, act_lr=1.6e-7, ent=1.87]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.12it/s, pg=-0.16, ret=0.000603, glen=115, tlen=274, kl=0.000951, act_lr=1.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.12it/s, pg=-0.183, ret=2.14e-6, glen=108, tlen=268, kl=0.000919, act_lr=1.6e-7, ent=1.76]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:48,  1.12it/s, pg=-0.183, ret=2.14e-6, glen=108, tlen=268, kl=0.000919, act_lr=1.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:48,  1.12it/s, pg=0.0267, ret=0.00127, glen=134, tlen=295, kl=0.0009, act_lr=1.6e-7, ent=2.15]  Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:47,  1.11it/s, pg=0.0267, ret=0.00127, glen=134, tlen=295, kl=0.0009, act_lr=1.6e-7, ent=2.15]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:47,  1.11it/s, pg=0.195, ret=-0.00178, glen=119, tlen=279, kl=0.000922, act_lr=1.6e-7, ent=1.67]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:46,  1.13it/s, pg=0.195, ret=-0.00178, glen=119, tlen=279, kl=0.000922, act_lr=1.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:46,  1.13it/s, pg=0.00659, ret=-5.91e-5, glen=125, tlen=285, kl=0.000896, act_lr=1.6e-7, ent=1.76]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:44,  1.14it/s, pg=0.00659, ret=-5.91e-5, glen=125, tlen=285, kl=0.000896, act_lr=1.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:44,  1.14it/s, pg=0.0305, ret=-0.000431, glen=105, tlen=265, kl=0.00092, act_lr=1.6e-7, ent=1.81] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=0.0305, ret=-0.000431, glen=105, tlen=265, kl=0.00092, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:44,  1.13it/s, pg=0.234, ret=-0.000956, glen=175, tlen=335, kl=0.00068, act_lr=1.6e-7, ent=1.43] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.13it/s, pg=0.234, ret=-0.000956, glen=175, tlen=335, kl=0.00068, act_lr=1.6e-7, ent=1.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.13it/s, pg=-0.129, ret=0.0026, glen=262, tlen=422, kl=0.00078, act_lr=1.6e-7, ent=2.04]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:43,  1.12it/s, pg=-0.129, ret=0.0026, glen=262, tlen=422, kl=0.00078, act_lr=1.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:43,  1.12it/s, pg=-0.149, ret=0.000181, glen=114, tlen=275, kl=0.000896, act_lr=1.6e-7, ent=1.66]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:41,  1.13it/s, pg=-0.149, ret=0.000181, glen=114, tlen=275, kl=0.000896, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:41,  1.13it/s, pg=0.0181, ret=-0.000662, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.69]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:40,  1.14it/s, pg=0.0181, ret=-0.000662, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.14it/s, pg=-0.0212, ret=-0.000789, glen=131, tlen=291, kl=0.000903, act_lr=1.6e-7, ent=1.92]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:39,  1.15it/s, pg=-0.0212, ret=-0.000789, glen=131, tlen=291, kl=0.000903, act_lr=1.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:39,  1.15it/s, pg=-0.0698, ret=0.000806, glen=121, tlen=281, kl=0.000921, act_lr=1.6e-7, ent=1.83] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.16it/s, pg=-0.0698, ret=0.000806, glen=121, tlen=281, kl=0.000921, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.16it/s, pg=0.114, ret=-0.00101, glen=124, tlen=285, kl=0.000932, act_lr=1.6e-7, ent=1.77]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.16it/s, pg=0.114, ret=-0.00101, glen=124, tlen=285, kl=0.000932, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:36,  1.16it/s, pg=-0.0747, ret=0.00157, glen=139, tlen=299, kl=0.000863, act_lr=1.6e-7, ent=2.13]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.0747, ret=0.00157, glen=139, tlen=299, kl=0.000863, act_lr=1.6e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=-0.141, ret=0.00161, glen=127, tlen=287, kl=0.000916, act_lr=1.6e-7, ent=1.78] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.141, ret=0.00161, glen=127, tlen=287, kl=0.000916, act_lr=1.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=-0.164, ret=0.00207, glen=92.1, tlen=252, kl=0.000936, act_lr=1.6e-7, ent=1.63]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=-0.164, ret=0.00207, glen=92.1, tlen=252, kl=0.000936, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.143, ret=0.00126, glen=121, tlen=281, kl=0.000937, act_lr=1.6e-7, ent=1.65] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=-0.143, ret=0.00126, glen=121, tlen=281, kl=0.000937, act_lr=1.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.17, ret=-0.00114, glen=130, tlen=291, kl=0.000927, act_lr=1.6e-7, ent=1.9]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.17, ret=-0.00114, glen=130, tlen=291, kl=0.000927, act_lr=1.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=0.0278, ret=-0.00047, glen=107, tlen=267, kl=0.000916, act_lr=1.6e-7, ent=1.63]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=0.0278, ret=-0.00047, glen=107, tlen=267, kl=0.000916, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=0.0596, ret=-0.000442, glen=102, tlen=262, kl=0.000951, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=0.0596, ret=-0.000442, glen=102, tlen=262, kl=0.000951, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.17it/s, pg=-0.108, ret=-0.000339, glen=107, tlen=267, kl=0.000957, act_lr=1.6e-7, ent=1.68]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:30,  1.16it/s, pg=-0.108, ret=-0.000339, glen=107, tlen=267, kl=0.000957, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:30,  1.16it/s, pg=0.0728, ret=1.22e-5, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.79]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.16it/s, pg=0.0728, ret=1.22e-5, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.16it/s, pg=-0.234, ret=0.00178, glen=114, tlen=274, kl=0.000922, act_lr=1.6e-7, ent=1.68]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.17it/s, pg=-0.234, ret=0.00178, glen=114, tlen=274, kl=0.000922, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.17it/s, pg=0.0123, ret=-0.000921, glen=103, tlen=263, kl=0.000914, act_lr=1.6e-7, ent=1.62]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.17it/s, pg=0.0123, ret=-0.000921, glen=103, tlen=263, kl=0.000914, act_lr=1.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.17it/s, pg=0.0591, ret=0.000343, glen=128, tlen=289, kl=0.000909, act_lr=1.6e-7, ent=1.81] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.17it/s, pg=0.0591, ret=0.000343, glen=128, tlen=289, kl=0.000909, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.304, ret=-0.00378, glen=352, tlen=512, kl=0.00068, act_lr=1.6e-7, ent=1.55]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:26,  1.14it/s, pg=0.304, ret=-0.00378, glen=352, tlen=512, kl=0.00068, act_lr=1.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:26,  1.14it/s, pg=-0.114, ret=0.000583, glen=98.5, tlen=258, kl=0.00094, act_lr=1.6e-7, ent=1.75]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.05it/s, pg=-0.114, ret=0.000583, glen=98.5, tlen=258, kl=0.00094, act_lr=1.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.05it/s, pg=-0.0344, ret=0.00102, glen=126, tlen=286, kl=0.000917, act_lr=1.6e-7, ent=1.69]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:26,  1.07it/s, pg=-0.0344, ret=0.00102, glen=126, tlen=286, kl=0.000917, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:26,  1.07it/s, pg=0.0116, ret=-0.000544, glen=144, tlen=304, kl=0.000899, act_lr=1.6e-7, ent=1.64]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.10it/s, pg=0.0116, ret=-0.000544, glen=144, tlen=304, kl=0.000899, act_lr=1.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.10it/s, pg=0.128, ret=-0.000817, glen=154, tlen=314, kl=0.00088, act_lr=1.6e-7, ent=2.21]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:23,  1.11it/s, pg=0.128, ret=-0.000817, glen=154, tlen=314, kl=0.00088, act_lr=1.6e-7, ent=2.21]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:23,  1.11it/s, pg=-0.212, ret=0.00158, glen=93.9, tlen=254, kl=0.000908, act_lr=1.6e-7, ent=1.57]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:22,  1.13it/s, pg=-0.212, ret=0.00158, glen=93.9, tlen=254, kl=0.000908, act_lr=1.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:22,  1.13it/s, pg=-0.028, ret=-0.00102, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.85]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.14it/s, pg=-0.028, ret=-0.00102, glen=105, tlen=265, kl=0.000938, act_lr=1.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.14it/s, pg=0.116, ret=-0.00131, glen=109, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.68] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.15it/s, pg=0.116, ret=-0.00131, glen=109, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.15it/s, pg=0.0752, ret=-9.92e-5, glen=120, tlen=280, kl=0.000905, act_lr=1.6e-7, ent=1.71]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:19,  1.16it/s, pg=0.0752, ret=-9.92e-5, glen=120, tlen=280, kl=0.000905, act_lr=1.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:19,  1.16it/s, pg=-0.105, ret=-0.000179, glen=118, tlen=278, kl=0.000948, act_lr=1.6e-7, ent=1.72]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.14it/s, pg=-0.105, ret=-0.000179, glen=118, tlen=278, kl=0.000948, act_lr=1.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.14it/s, pg=-0.00171, ret=0.000523, glen=112, tlen=273, kl=0.00096, act_lr=1.6e-7, ent=1.91]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.15it/s, pg=-0.00171, ret=0.000523, glen=112, tlen=273, kl=0.00096, act_lr=1.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.15it/s, pg=-0.19, ret=0.000197, glen=114, tlen=274, kl=0.000903, act_lr=1.6e-7, ent=1.81]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.16it/s, pg=-0.19, ret=0.000197, glen=114, tlen=274, kl=0.000903, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:35<00:16,  1.16it/s, pg=0.0858, ret=-0.000674, glen=115, tlen=275, kl=0.000908, act_lr=1.6e-7, ent=1.88]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=0.0858, ret=-0.000674, glen=115, tlen=275, kl=0.000908, act_lr=1.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=-0.0663, ret=0.000468, glen=97.9, tlen=258, kl=0.000859, act_lr=1.6e-7, ent=1.55]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0663, ret=0.000468, glen=97.9, tlen=258, kl=0.000859, act_lr=1.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.0156, ret=4.56e-5, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.98]   Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=0.0156, ret=4.56e-5, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=-0.0187, ret=-0.00026, glen=128, tlen=289, kl=0.000949, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=-0.0187, ret=-0.00026, glen=128, tlen=289, kl=0.000949, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=-0.119, ret=0.000105, glen=115, tlen=275, kl=0.000877, act_lr=1.6e-7, ent=1.66] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=-0.119, ret=0.000105, glen=115, tlen=275, kl=0.000877, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.107, ret=0.000863, glen=148, tlen=308, kl=0.00092, act_lr=1.6e-7, ent=1.95]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.107, ret=0.000863, glen=148, tlen=308, kl=0.00092, act_lr=1.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=0.171, ret=-0.00138, glen=116, tlen=276, kl=0.000943, act_lr=1.6e-7, ent=1.96]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=0.171, ret=-0.00138, glen=116, tlen=276, kl=0.000943, act_lr=1.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.17it/s, pg=0.0599, ret=-0.000645, glen=97.8, tlen=258, kl=0.000915, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.0599, ret=-0.000645, glen=97.8, tlen=258, kl=0.000915, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.0975, ret=-0.00115, glen=116, tlen=276, kl=0.000877, act_lr=1.6e-7, ent=1.81]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.17it/s, pg=0.0975, ret=-0.00115, glen=116, tlen=276, kl=0.000877, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=-0.0564, ret=0.000432, glen=96.9, tlen=257, kl=0.000917, act_lr=1.6e-7, ent=1.66]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.17it/s, pg=-0.0564, ret=0.000432, glen=96.9, tlen=257, kl=0.000917, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=-0.114, ret=0.000657, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.96]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.17it/s, pg=-0.114, ret=0.000657, glen=120, tlen=280, kl=0.000907, act_lr=1.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=-0.154, ret=0.00122, glen=113, tlen=273, kl=0.000901, act_lr=1.6e-7, ent=1.75] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.17it/s, pg=-0.154, ret=0.00122, glen=113, tlen=273, kl=0.000901, act_lr=1.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=0.0724, ret=-0.000144, glen=118, tlen=277, kl=0.000937, act_lr=1.6e-7, ent=1.77]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.0724, ret=-0.000144, glen=118, tlen=277, kl=0.000937, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.17it/s, pg=0.0834, ret=-0.00107, glen=108, tlen=268, kl=0.000936, act_lr=1.6e-7, ent=1.54] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.17it/s, pg=0.0834, ret=-0.00107, glen=108, tlen=268, kl=0.000936, act_lr=1.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.17it/s, pg=0.0294, ret=-0.00163, glen=99.9, tlen=260, kl=0.000909, act_lr=1.6e-7, ent=1.65]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=0.0294, ret=-0.00163, glen=99.9, tlen=260, kl=0.000909, act_lr=1.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=-0.0415, ret=0.000174, glen=118, tlen=277, kl=0.000902, act_lr=1.6e-7, ent=1.82]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=-0.0415, ret=0.000174, glen=118, tlen=277, kl=0.000902, act_lr=1.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=0.162, ret=-0.00143, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.77]  Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=0.162, ret=-0.00143, glen=117, tlen=277, kl=0.000905, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=0.0326, ret=-0.000676, glen=121, tlen=281, kl=0.00092, act_lr=1.6e-7, ent=1.71]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=0.0326, ret=-0.000676, glen=121, tlen=281, kl=0.00092, act_lr=1.6e-7, ent=1.71]
2025-07-24 16:38:50.254 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.58s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.178, ret=0.00199, glen=162, tlen=322, kl=0.000824, act_lr=1.8e-7, ent=2.21] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.178, ret=0.00199, glen=162, tlen=322, kl=0.000824, act_lr=1.8e-7, ent=2.21]
2025-07-24 16:38:50.919 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 16:38:53.281 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.36s
2025-07-24 16:38:53.609 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 54.06s
2025-07-24 16:38:53.617 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011384635136045259, 'actor_lr': 1.603448266423427e-07, 'clip_ratio': 0.0, 'entropy': 1.779008370021294, 'kl': 0.0009049485469686574, 'response_length': 124.24242677359746, 'total_length': 284.38584926210603, 'teacher_total_length': 297.4944489577721, 'return': 7.362090651315219e-06, 'policy_update_steps': 1.0}
Episode [1/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [33:01<14:45, 221.48s/it]2025-07-24 16:38:53.661 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:41:35.895 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:41:36.080 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:41:36.081 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 162.42s
2025-07-24 16:41:38.141 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0114,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6687,std_num_tokens: 156.2734,avg_correct_num_tokens: 103.5393,std_correct_num_tokens: 85.3857,avg_incorrect_num_tokens: 132.2278,std_incorrect_num_tokens: 225.7101
2025-07-24 16:41:38.489 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.41s
2025-07-24 16:41:41.718 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.23s
2025-07-24 16:42:11.152 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:42:11.152 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.43s
2025-07-24 16:42:12.802 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.23s
2025-07-24 16:42:12.802 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.002387216028591667, avg_kl: 0.000913978143550423, avg_response_length: 120.19191138921346, avg_orm_score: 0.0, avg_custom_rewards: -0.002387216028591667
2025-07-24 16:42:12.881 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter9_replay_buffer.jsonl
2025-07-24 16:42:14.834 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.95s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.167, ret=0.00094, glen=112, tlen=272, kl=0.000929, act_lr=1.8e-7, ent=1.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.04s/it, pg=-0.167, ret=0.00094, glen=112, tlen=272, kl=0.000929, act_lr=1.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.04s/it, pg=-0.0608, ret=-0.0012, glen=110, tlen=270, kl=0.000912, act_lr=1.8e-7, ent=1.69]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.07it/s, pg=-0.0608, ret=-0.0012, glen=110, tlen=270, kl=0.000912, act_lr=1.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.07it/s, pg=0.0568, ret=-0.00149, glen=124, tlen=284, kl=0.000892, act_lr=1.8e-7, ent=1.75]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.11it/s, pg=0.0568, ret=-0.00149, glen=124, tlen=284, kl=0.000892, act_lr=1.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.11it/s, pg=-0.03, ret=-0.000764, glen=103, tlen=263, kl=0.000978, act_lr=1.8e-7, ent=1.78]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:48,  1.11it/s, pg=-0.03, ret=-0.000764, glen=103, tlen=263, kl=0.000978, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:48,  1.11it/s, pg=0.0298, ret=2.33e-5, glen=110, tlen=270, kl=0.000919, act_lr=1.8e-7, ent=1.68] Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:47,  1.11it/s, pg=0.0298, ret=2.33e-5, glen=110, tlen=270, kl=0.000919, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:47,  1.11it/s, pg=-0.0338, ret=0.00151, glen=121, tlen=281, kl=0.000902, act_lr=1.8e-7, ent=1.93]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:46,  1.12it/s, pg=-0.0338, ret=0.00151, glen=121, tlen=281, kl=0.000902, act_lr=1.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:46,  1.12it/s, pg=0.0105, ret=-0.00113, glen=109, tlen=269, kl=0.000882, act_lr=1.8e-7, ent=1.78]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.12it/s, pg=0.0105, ret=-0.00113, glen=109, tlen=269, kl=0.000882, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.12it/s, pg=0.0318, ret=0.000172, glen=113, tlen=274, kl=0.000943, act_lr=1.8e-7, ent=1.82]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:44,  1.13it/s, pg=0.0318, ret=0.000172, glen=113, tlen=274, kl=0.000943, act_lr=1.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:08<00:44,  1.13it/s, pg=0.043, ret=-0.00067, glen=101, tlen=261, kl=0.000973, act_lr=1.8e-7, ent=1.63] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.12it/s, pg=0.043, ret=-0.00067, glen=101, tlen=261, kl=0.000973, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:43,  1.12it/s, pg=-0.114, ret=0.000238, glen=103, tlen=264, kl=0.000927, act_lr=1.8e-7, ent=1.57]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:42,  1.13it/s, pg=-0.114, ret=0.000238, glen=103, tlen=264, kl=0.000927, act_lr=1.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:42,  1.13it/s, pg=-0.113, ret=0.000905, glen=113, tlen=273, kl=0.000938, act_lr=1.8e-7, ent=1.82]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:41,  1.14it/s, pg=-0.113, ret=0.000905, glen=113, tlen=273, kl=0.000938, act_lr=1.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:41,  1.14it/s, pg=-0.0438, ret=0.0011, glen=118, tlen=279, kl=0.000937, act_lr=1.8e-7, ent=1.74] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.15it/s, pg=-0.0438, ret=0.0011, glen=118, tlen=279, kl=0.000937, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.15it/s, pg=-0.117, ret=-0.000279, glen=125, tlen=286, kl=0.000884, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.16it/s, pg=-0.117, ret=-0.000279, glen=125, tlen=286, kl=0.000884, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.16it/s, pg=-0.0399, ret=0.00097, glen=107, tlen=268, kl=0.000889, act_lr=1.8e-7, ent=1.74] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:38,  1.14it/s, pg=-0.0399, ret=0.00097, glen=107, tlen=268, kl=0.000889, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:38,  1.14it/s, pg=-0.061, ret=0.00064, glen=112, tlen=272, kl=0.000845, act_lr=1.8e-7, ent=1.87] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:37,  1.15it/s, pg=-0.061, ret=0.00064, glen=112, tlen=272, kl=0.000845, act_lr=1.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:14<00:37,  1.15it/s, pg=-0.0618, ret=1.3e-5, glen=112, tlen=272, kl=0.000928, act_lr=1.8e-7, ent=1.81]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.15it/s, pg=-0.0618, ret=1.3e-5, glen=112, tlen=272, kl=0.000928, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:15<00:36,  1.15it/s, pg=0.0719, ret=-0.000909, glen=106, tlen=266, kl=0.000894, act_lr=1.8e-7, ent=1.63]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.0719, ret=-0.000909, glen=106, tlen=266, kl=0.000894, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.119, ret=-0.000221, glen=128, tlen=288, kl=0.000912, act_lr=1.8e-7, ent=1.81] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.16it/s, pg=0.119, ret=-0.000221, glen=128, tlen=288, kl=0.000912, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.16it/s, pg=0.0474, ret=0.00186, glen=120, tlen=279, kl=0.000905, act_lr=1.8e-7, ent=1.87] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=0.0474, ret=0.00186, glen=120, tlen=279, kl=0.000905, act_lr=1.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=-0.0794, ret=-0.0006, glen=102, tlen=263, kl=0.000916, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=-0.0794, ret=-0.0006, glen=102, tlen=263, kl=0.000916, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.182, ret=0.000858, glen=138, tlen=299, kl=0.000923, act_lr=1.8e-7, ent=1.91]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.182, ret=0.000858, glen=138, tlen=299, kl=0.000923, act_lr=1.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=0.174, ret=-0.00192, glen=104, tlen=264, kl=0.000955, act_lr=1.8e-7, ent=1.59] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=0.174, ret=-0.00192, glen=104, tlen=264, kl=0.000955, act_lr=1.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:30,  1.17it/s, pg=0.0674, ret=3e-5, glen=117, tlen=277, kl=0.000915, act_lr=1.8e-7, ent=1.85]   Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=0.0674, ret=3e-5, glen=117, tlen=277, kl=0.000915, act_lr=1.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.232, ret=0.00165, glen=107, tlen=267, kl=0.000911, act_lr=1.8e-7, ent=1.75]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.17it/s, pg=-0.232, ret=0.00165, glen=107, tlen=267, kl=0.000911, act_lr=1.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.17it/s, pg=-0.00453, ret=-0.000212, glen=98.1, tlen=258, kl=0.000957, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.16it/s, pg=-0.00453, ret=-0.000212, glen=98.1, tlen=258, kl=0.000957, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.16it/s, pg=-0.0513, ret=-0.000126, glen=100, tlen=261, kl=0.000911, act_lr=1.8e-7, ent=1.64]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.16it/s, pg=-0.0513, ret=-0.000126, glen=100, tlen=261, kl=0.000911, act_lr=1.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.16it/s, pg=-0.0331, ret=-0.000246, glen=123, tlen=283, kl=0.000939, act_lr=1.8e-7, ent=1.72]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.16it/s, pg=-0.0331, ret=-0.000246, glen=123, tlen=283, kl=0.000939, act_lr=1.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.16it/s, pg=0.0574, ret=0.000944, glen=173, tlen=334, kl=0.000851, act_lr=1.8e-7, ent=2.42]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:26,  1.15it/s, pg=0.0574, ret=0.000944, glen=173, tlen=334, kl=0.000851, act_lr=1.8e-7, ent=2.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:26,  1.15it/s, pg=-0.1, ret=0.000148, glen=97.4, tlen=257, kl=0.000938, act_lr=1.8e-7, ent=1.71] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.06it/s, pg=-0.1, ret=0.000148, glen=97.4, tlen=257, kl=0.000938, act_lr=1.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.06it/s, pg=0.298, ret=-0.00334, glen=142, tlen=302, kl=0.000829, act_lr=1.8e-7, ent=1.68]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.09it/s, pg=0.298, ret=-0.00334, glen=142, tlen=302, kl=0.000829, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.09it/s, pg=-0.0555, ret=0.00156, glen=118, tlen=278, kl=0.00091, act_lr=1.8e-7, ent=1.81]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.11it/s, pg=-0.0555, ret=0.00156, glen=118, tlen=278, kl=0.00091, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.11it/s, pg=0.0816, ret=-0.000133, glen=112, tlen=272, kl=0.000849, act_lr=1.8e-7, ent=2.1]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:23,  1.13it/s, pg=0.0816, ret=-0.000133, glen=112, tlen=272, kl=0.000849, act_lr=1.8e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:29<00:23,  1.13it/s, pg=0.108, ret=-0.00123, glen=108, tlen=268, kl=0.000927, act_lr=1.8e-7, ent=1.57] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.14it/s, pg=0.108, ret=-0.00123, glen=108, tlen=268, kl=0.000927, act_lr=1.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.14it/s, pg=-0.104, ret=0.00119, glen=122, tlen=282, kl=0.000923, act_lr=1.8e-7, ent=1.76]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.15it/s, pg=-0.104, ret=0.00119, glen=122, tlen=282, kl=0.000923, act_lr=1.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.15it/s, pg=-0.355, ret=0.00279, glen=114, tlen=274, kl=0.000935, act_lr=1.8e-7, ent=1.73]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=-0.355, ret=0.00279, glen=114, tlen=274, kl=0.000935, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=0.105, ret=-0.00129, glen=104, tlen=264, kl=0.000938, act_lr=1.8e-7, ent=1.59]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.16it/s, pg=0.105, ret=-0.00129, glen=104, tlen=264, kl=0.000938, act_lr=1.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.16it/s, pg=-0.0505, ret=-0.000461, glen=111, tlen=271, kl=0.000882, act_lr=1.8e-7, ent=1.73]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.17it/s, pg=-0.0505, ret=-0.000461, glen=111, tlen=271, kl=0.000882, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.17it/s, pg=0.0753, ret=-0.000717, glen=117, tlen=277, kl=0.000923, act_lr=1.8e-7, ent=1.73] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.0753, ret=-0.000717, glen=117, tlen=277, kl=0.000923, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.17it/s, pg=0.0708, ret=-0.00128, glen=113, tlen=273, kl=0.000932, act_lr=1.8e-7, ent=1.81] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=0.0708, ret=-0.00128, glen=113, tlen=273, kl=0.000932, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=0.0461, ret=-5.98e-5, glen=128, tlen=288, kl=0.000878, act_lr=1.8e-7, ent=1.92]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=0.0461, ret=-5.98e-5, glen=128, tlen=288, kl=0.000878, act_lr=1.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=0.203, ret=-0.000602, glen=172, tlen=332, kl=0.000859, act_lr=1.8e-7, ent=2.6] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=0.203, ret=-0.000602, glen=172, tlen=332, kl=0.000859, act_lr=1.8e-7, ent=2.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=0.0457, ret=-0.00189, glen=108, tlen=268, kl=0.000929, act_lr=1.8e-7, ent=1.64]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.16it/s, pg=0.0457, ret=-0.00189, glen=108, tlen=268, kl=0.000929, act_lr=1.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.16it/s, pg=-0.143, ret=-0.000868, glen=120, tlen=280, kl=0.000915, act_lr=1.8e-7, ent=1.75]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.16it/s, pg=-0.143, ret=-0.000868, glen=120, tlen=280, kl=0.000915, act_lr=1.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.16it/s, pg=-0.0614, ret=0.000739, glen=108, tlen=268, kl=0.000954, act_lr=1.8e-7, ent=1.72]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:12,  1.17it/s, pg=-0.0614, ret=0.000739, glen=108, tlen=268, kl=0.000954, act_lr=1.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.17it/s, pg=-0.0588, ret=0.000803, glen=115, tlen=275, kl=0.00093, act_lr=1.8e-7, ent=1.88] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0588, ret=0.000803, glen=115, tlen=275, kl=0.00093, act_lr=1.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.17it/s, pg=0.169, ret=-0.0013, glen=170, tlen=331, kl=0.000835, act_lr=1.8e-7, ent=2.43]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.16it/s, pg=0.169, ret=-0.0013, glen=170, tlen=331, kl=0.000835, act_lr=1.8e-7, ent=2.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:41<00:10,  1.16it/s, pg=0.0458, ret=-0.000339, glen=117, tlen=278, kl=0.000936, act_lr=1.8e-7, ent=1.74]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.16it/s, pg=0.0458, ret=-0.000339, glen=117, tlen=278, kl=0.000936, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.16it/s, pg=0.176, ret=-0.00183, glen=120, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.94]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.16it/s, pg=0.176, ret=-0.00183, glen=120, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.16it/s, pg=-0.0997, ret=0.00109, glen=115, tlen=276, kl=0.000915, act_lr=1.8e-7, ent=1.66]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.16it/s, pg=-0.0997, ret=0.00109, glen=115, tlen=276, kl=0.000915, act_lr=1.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.16it/s, pg=-0.058, ret=0.00078, glen=119, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.71] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.17it/s, pg=-0.058, ret=0.00078, glen=119, tlen=280, kl=0.000897, act_lr=1.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=0.112, ret=-0.0111, glen=354, tlen=515, kl=0.000912, act_lr=1.8e-7, ent=2.26] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:06,  1.14it/s, pg=0.112, ret=-0.0111, glen=354, tlen=515, kl=0.000912, act_lr=1.8e-7, ent=2.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:06,  1.14it/s, pg=-0.0525, ret=0.000687, glen=115, tlen=275, kl=0.000942, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.15it/s, pg=-0.0525, ret=0.000687, glen=115, tlen=275, kl=0.000942, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.15it/s, pg=0.0139, ret=0.000187, glen=114, tlen=274, kl=0.000918, act_lr=1.8e-7, ent=1.78] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.15it/s, pg=0.0139, ret=0.000187, glen=114, tlen=274, kl=0.000918, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:47<00:04,  1.15it/s, pg=-0.0474, ret=-0.000178, glen=114, tlen=274, kl=0.000901, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.16it/s, pg=-0.0474, ret=-0.000178, glen=114, tlen=274, kl=0.000901, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.16it/s, pg=-0.161, ret=0.00186, glen=115, tlen=275, kl=0.000966, act_lr=1.8e-7, ent=1.79]   Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.16it/s, pg=-0.161, ret=0.00186, glen=115, tlen=275, kl=0.000966, act_lr=1.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.16it/s, pg=-0.0821, ret=-0.000283, glen=118, tlen=278, kl=0.000918, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=-0.0821, ret=-0.000283, glen=118, tlen=278, kl=0.000918, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=-0.0858, ret=0.000249, glen=115, tlen=275, kl=0.00095, act_lr=1.8e-7, ent=1.68]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=-0.0858, ret=0.000249, glen=115, tlen=275, kl=0.00095, act_lr=1.8e-7, ent=1.68]
2025-07-24 16:43:05.612 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.60s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=0.167, ret=-0.0059, glen=122, tlen=282, kl=0.000868, act_lr=2e-7, ent=1.83]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=0.167, ret=-0.0059, glen=122, tlen=282, kl=0.000868, act_lr=2e-7, ent=1.83]
2025-07-24 16:43:06.421 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-24 16:43:08.950 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-24 16:43:09.295 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 54.40s
2025-07-24 16:43:09.306 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.008862774947593952, 'actor_lr': 1.8034482829715012e-07, 'clip_ratio': 0.0, 'entropy': 1.796474444455114, 'kl': 0.000913324027225889, 'response_length': 121.13656826676994, 'total_length': 281.3766242717874, 'teacher_total_length': 292.80887893150594, 'return': -0.00032191864930014005, 'policy_update_steps': 1.0}
Episode [1/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [37:17<11:36, 232.04s/it]2025-07-24 16:43:09.311 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   1%|          | 1/172 [00:00<01:06,  2.57it/s, est. speed input: 456.75 toks/s, output: 28.22 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:05<00:00, 14.31it/s, est. speed input: 5341.39 toks/s, output: 2771.51 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:00, 19.94it/s, est. speed input: 5616.74 toks/s, output: 2821.74 toks/s][32m [repeated 119x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:10<00:00,  2.38it/s, est. speed input: 2889.48 toks/s, output: 1905.21 toks/s][32m [repeated 27x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:01,  8.16it/s, est. speed input: 4712.16 toks/s, output: 2455.32 toks/s][32m [repeated 2x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00,  1.86it/s, est. speed input: 2619.69 toks/s, output: 1697.83 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00, 14.45it/s, est. speed input: 2619.69 toks/s, output: 1697.83 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:12<00:00,  1.71it/s, est. speed input: 2551.21 toks/s, output: 1730.78 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:23<00:00,  2.48s/it, est. speed input: 1348.37 toks/s, output: 975.15 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:23<00:00,  7.44it/s, est. speed input: 1348.37 toks/s, output: 975.15 toks/s][32m [repeated 3x across cluster][0m
2025-07-24 16:43:34.003 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 553.0451,strategyqa_test/accuracy: 0.3100,eval_accuracy: 0.3100
2025-07-24 16:43:34.277 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:45:19.372 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:45:19.541 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:45:19.541 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 105.26s
2025-07-24 16:45:21.827 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0083,avg_pass_at_n: 1.0000,avg_num_tokens: 110.7008,std_num_tokens: 112.9670,avg_correct_num_tokens: 103.3926,std_correct_num_tokens: 85.9129,avg_incorrect_num_tokens: 122.1896,std_incorrect_num_tokens: 144.9279
2025-07-24 16:45:22.278 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.74s
2025-07-24 16:45:25.395 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.11s
2025-07-24 16:45:54.301 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 225
2025-07-24 16:45:54.301 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.90s
2025-07-24 16:45:55.984 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.20s
2025-07-24 16:45:55.985 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008869843830406252, avg_kl: 0.0009025743272569445, avg_response_length: 112.13507792154948, avg_orm_score: 0.0, avg_custom_rewards: -0.0008869843830406252
2025-07-24 16:45:56.042 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter10_replay_buffer.jsonl
2025-07-24 16:45:57.965 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.92s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0414, ret=0.00175, glen=122, tlen=282, kl=0.000906, act_lr=2e-7, ent=1.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.0414, ret=0.00175, glen=122, tlen=282, kl=0.000906, act_lr=2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.107, ret=-0.00114, glen=108, tlen=268, kl=0.000941, act_lr=2e-7, ent=1.81]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.107, ret=-0.00114, glen=108, tlen=268, kl=0.000941, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.0516, ret=-0.000559, glen=123, tlen=284, kl=0.000924, act_lr=2e-7, ent=1.88]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=0.0516, ret=-0.000559, glen=123, tlen=284, kl=0.000924, act_lr=2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.0151, ret=-0.000345, glen=113, tlen=273, kl=0.000877, act_lr=2e-7, ent=1.63]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.0151, ret=-0.000345, glen=113, tlen=273, kl=0.000877, act_lr=2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=-0.0871, ret=0.000792, glen=111, tlen=271, kl=0.000837, act_lr=2e-7, ent=1.59] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.0871, ret=0.000792, glen=111, tlen=271, kl=0.000837, act_lr=2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.0622, ret=-0.00134, glen=107, tlen=267, kl=0.000955, act_lr=2e-7, ent=1.68] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0622, ret=-0.00134, glen=107, tlen=267, kl=0.000955, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=0.095, ret=-0.000418, glen=129, tlen=289, kl=0.000929, act_lr=2e-7, ent=1.81]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=0.095, ret=-0.000418, glen=129, tlen=289, kl=0.000929, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=0.133, ret=-0.00107, glen=108, tlen=268, kl=0.000939, act_lr=2e-7, ent=1.7]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.133, ret=-0.00107, glen=108, tlen=268, kl=0.000939, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.218, ret=0.0014, glen=94.6, tlen=256, kl=0.000877, act_lr=2e-7, ent=1.65]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:42,  1.14it/s, pg=-0.218, ret=0.0014, glen=94.6, tlen=256, kl=0.000877, act_lr=2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=-0.0128, ret=-0.000764, glen=109, tlen=269, kl=0.000888, act_lr=2e-7, ent=1.78]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.0128, ret=-0.000764, glen=109, tlen=269, kl=0.000888, act_lr=2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.132, ret=-0.00123, glen=134, tlen=294, kl=0.000905, act_lr=2e-7, ent=2.43]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.132, ret=-0.00123, glen=134, tlen=294, kl=0.000905, act_lr=2e-7, ent=2.43]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0379, ret=-0.000525, glen=98.7, tlen=259, kl=0.000902, act_lr=2e-7, ent=1.8]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.0379, ret=-0.000525, glen=98.7, tlen=259, kl=0.000902, act_lr=2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.0292, ret=0.000107, glen=118, tlen=279, kl=0.000882, act_lr=2e-7, ent=1.82]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=0.0292, ret=0.000107, glen=118, tlen=279, kl=0.000882, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=0.106, ret=-0.000375, glen=109, tlen=269, kl=0.000896, act_lr=2e-7, ent=1.75]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=0.106, ret=-0.000375, glen=109, tlen=269, kl=0.000896, act_lr=2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=0.168, ret=-0.000278, glen=101, tlen=261, kl=0.0009, act_lr=2e-7, ent=1.71]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.168, ret=-0.000278, glen=101, tlen=261, kl=0.0009, act_lr=2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0755, ret=-0.00168, glen=115, tlen=276, kl=0.000895, act_lr=2e-7, ent=1.82]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=0.0755, ret=-0.00168, glen=115, tlen=276, kl=0.000895, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.14, ret=-0.000119, glen=110, tlen=271, kl=0.000889, act_lr=2e-7, ent=1.71]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.14, ret=-0.000119, glen=110, tlen=271, kl=0.000889, act_lr=2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.17, ret=-0.000759, glen=128, tlen=288, kl=0.000891, act_lr=2e-7, ent=1.84] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=0.17, ret=-0.000759, glen=128, tlen=288, kl=0.000891, act_lr=2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.168, ret=0.000796, glen=102, tlen=262, kl=0.000935, act_lr=2e-7, ent=1.67]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=-0.168, ret=0.000796, glen=102, tlen=262, kl=0.000935, act_lr=2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0351, ret=0.000656, glen=108, tlen=268, kl=0.000875, act_lr=2e-7, ent=1.54]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0351, ret=0.000656, glen=108, tlen=268, kl=0.000875, act_lr=2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.0476, ret=0.000906, glen=110, tlen=270, kl=0.00092, act_lr=2e-7, ent=1.69] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.0476, ret=0.000906, glen=110, tlen=270, kl=0.00092, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.217, ret=0.00145, glen=105, tlen=266, kl=0.000929, act_lr=2e-7, ent=1.82] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.217, ret=0.00145, glen=105, tlen=266, kl=0.000929, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.162, ret=0.000478, glen=97, tlen=258, kl=0.000879, act_lr=2e-7, ent=1.63]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.17it/s, pg=-0.162, ret=0.000478, glen=97, tlen=258, kl=0.000879, act_lr=2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.193, ret=0.00145, glen=97.3, tlen=258, kl=0.000923, act_lr=2e-7, ent=1.7]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.193, ret=0.00145, glen=97.3, tlen=258, kl=0.000923, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0632, ret=-0.000921, glen=110, tlen=270, kl=0.000922, act_lr=2e-7, ent=1.93]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0632, ret=-0.000921, glen=110, tlen=270, kl=0.000922, act_lr=2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.189, ret=0.000405, glen=113, tlen=273, kl=0.000966, act_lr=2e-7, ent=1.82] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.189, ret=0.000405, glen=113, tlen=273, kl=0.000966, act_lr=2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.118, ret=-0.00109, glen=118, tlen=279, kl=0.000927, act_lr=2e-7, ent=1.89] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.118, ret=-0.00109, glen=118, tlen=279, kl=0.000927, act_lr=2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0112, ret=-0.000626, glen=117, tlen=277, kl=0.000939, act_lr=2e-7, ent=1.73]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=0.0112, ret=-0.000626, glen=117, tlen=277, kl=0.000939, act_lr=2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=0.0182, ret=0.000292, glen=103, tlen=263, kl=0.000883, act_lr=2e-7, ent=1.69] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=0.0182, ret=0.000292, glen=103, tlen=263, kl=0.000883, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=0.0637, ret=0.000526, glen=121, tlen=281, kl=0.0009, act_lr=2e-7, ent=1.85]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=0.0637, ret=0.000526, glen=121, tlen=281, kl=0.0009, act_lr=2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=-0.0696, ret=0.00077, glen=115, tlen=276, kl=0.000937, act_lr=2e-7, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.0696, ret=0.00077, glen=115, tlen=276, kl=0.000937, act_lr=2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.0938, ret=0.0018, glen=105, tlen=265, kl=0.000905, act_lr=2e-7, ent=1.58] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.14it/s, pg=-0.0938, ret=0.0018, glen=105, tlen=265, kl=0.000905, act_lr=2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=0.232, ret=-0.00113, glen=127, tlen=287, kl=0.000861, act_lr=2e-7, ent=1.72]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.14it/s, pg=0.232, ret=-0.00113, glen=127, tlen=287, kl=0.000861, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.0125, ret=-0.000442, glen=98.9, tlen=259, kl=0.000895, act_lr=2e-7, ent=1.57]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.0125, ret=-0.000442, glen=98.9, tlen=259, kl=0.000895, act_lr=2e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.069, ret=-0.00163, glen=118, tlen=278, kl=0.000866, act_lr=2e-7, ent=1.71]    Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.069, ret=-0.00163, glen=118, tlen=278, kl=0.000866, act_lr=2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.15, ret=0.00151, glen=103, tlen=264, kl=0.000925, act_lr=2e-7, ent=1.7]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.15, ret=0.00151, glen=103, tlen=264, kl=0.000925, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=0.0533, ret=-0.000197, glen=107, tlen=268, kl=0.000916, act_lr=2e-7, ent=1.73]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=0.0533, ret=-0.000197, glen=107, tlen=268, kl=0.000916, act_lr=2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.239, ret=0.00188, glen=107, tlen=268, kl=0.000933, act_lr=2e-7, ent=1.84]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:32<00:16,  1.17it/s, pg=-0.239, ret=0.00188, glen=107, tlen=268, kl=0.000933, act_lr=2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0502, ret=-0.000372, glen=108, tlen=268, kl=0.000896, act_lr=2e-7, ent=1.68]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.0502, ret=-0.000372, glen=108, tlen=268, kl=0.000896, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0953, ret=-0.000232, glen=108, tlen=269, kl=0.000927, act_lr=2e-7, ent=1.76]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.0953, ret=-0.000232, glen=108, tlen=269, kl=0.000927, act_lr=2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.135, ret=-0.000272, glen=99.9, tlen=260, kl=0.000921, act_lr=2e-7, ent=1.73] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.135, ret=-0.000272, glen=99.9, tlen=260, kl=0.000921, act_lr=2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.145, ret=-0.000124, glen=100, tlen=260, kl=0.000901, act_lr=2e-7, ent=1.69]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.145, ret=-0.000124, glen=100, tlen=260, kl=0.000901, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.0598, ret=3.61e-5, glen=110, tlen=271, kl=0.000921, act_lr=2e-7, ent=1.59] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.0598, ret=3.61e-5, glen=110, tlen=271, kl=0.000921, act_lr=2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=0.139, ret=-0.00268, glen=150, tlen=310, kl=0.000731, act_lr=2e-7, ent=1.56] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.139, ret=-0.00268, glen=150, tlen=310, kl=0.000731, act_lr=2e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.171, ret=-0.00168, glen=113, tlen=273, kl=0.0009, act_lr=2e-7, ent=1.75]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=0.171, ret=-0.00168, glen=113, tlen=273, kl=0.0009, act_lr=2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=6.1e-5, ret=-0.000907, glen=117, tlen=277, kl=0.000917, act_lr=2e-7, ent=1.72]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=6.1e-5, ret=-0.000907, glen=117, tlen=277, kl=0.000917, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.0767, ret=0.000404, glen=110, tlen=270, kl=0.000897, act_lr=2e-7, ent=1.74]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.0767, ret=0.000404, glen=110, tlen=270, kl=0.000897, act_lr=2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.0562, ret=4.53e-5, glen=127, tlen=287, kl=0.000876, act_lr=2e-7, ent=2.16] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=-0.0562, ret=4.53e-5, glen=127, tlen=287, kl=0.000876, act_lr=2e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.0868, ret=0.00218, glen=115, tlen=275, kl=0.000899, act_lr=2e-7, ent=1.68]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=-0.0868, ret=0.00218, glen=115, tlen=275, kl=0.000899, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=0.132, ret=-0.00134, glen=118, tlen=279, kl=0.000875, act_lr=2e-7, ent=1.89] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.132, ret=-0.00134, glen=118, tlen=279, kl=0.000875, act_lr=2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=0.0215, ret=-0.00107, glen=119, tlen=280, kl=0.000937, act_lr=2e-7, ent=1.77]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.0215, ret=-0.00107, glen=119, tlen=280, kl=0.000937, act_lr=2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.00391, ret=-0.000157, glen=105, tlen=265, kl=0.000881, act_lr=2e-7, ent=1.76]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.17it/s, pg=-0.00391, ret=-0.000157, glen=105, tlen=265, kl=0.000881, act_lr=2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.151, ret=0.000789, glen=105, tlen=265, kl=0.000913, act_lr=2e-7, ent=1.68]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.151, ret=0.000789, glen=105, tlen=265, kl=0.000913, act_lr=2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.101, ret=-0.00112, glen=114, tlen=275, kl=0.000936, act_lr=2e-7, ent=1.81] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.101, ret=-0.00112, glen=114, tlen=275, kl=0.000936, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0388, ret=0.000642, glen=125, tlen=286, kl=0.000809, act_lr=2e-7, ent=1.62]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0388, ret=0.000642, glen=125, tlen=286, kl=0.000809, act_lr=2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=-0.115, ret=0.000798, glen=111, tlen=271, kl=0.000903, act_lr=2e-7, ent=1.65] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=-0.115, ret=0.000798, glen=111, tlen=271, kl=0.000903, act_lr=2e-7, ent=1.65]
2025-07-24 16:46:47.480 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.31s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.1, ret=0.00206, glen=110, tlen=271, kl=0.000926, act_lr=2.2e-7, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.14it/s, pg=-0.1, ret=0.00206, glen=110, tlen=271, kl=0.000926, act_lr=2.2e-7, ent=1.73]
2025-07-24 16:46:48.308 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 16:46:50.839 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-24 16:46:51.163 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.12s
2025-07-24 16:46:51.170 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.009948998166803728, 'actor_lr': 2.0035087955923365e-07, 'clip_ratio': 0.0, 'entropy': 1.7507029165301407, 'kl': 0.0009024268702456826, 'response_length': 111.99827147366707, 'total_length': 272.4189592327988, 'teacher_total_length': 284.16753775613347, 'return': -4.6558342434939715e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [40:58<07:37, 228.93s/it]2025-07-24 16:46:51.217 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:48:47.349 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:48:47.532 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 16:48:47.532 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 116.32s
2025-07-24 16:48:49.726 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 114.9939,std_num_tokens: 149.9138,avg_correct_num_tokens: 102.9947,std_correct_num_tokens: 81.3509,avg_incorrect_num_tokens: 134.7240,std_incorrect_num_tokens: 218.9069
2025-07-24 16:48:50.164 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.63s
2025-07-24 16:48:53.390 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.22s
2025-07-24 16:49:22.835 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:49:22.836 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.44s
2025-07-24 16:49:24.415 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.17s
2025-07-24 16:49:24.416 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.0012488026775858939, avg_kl: 0.0009306695263458652, avg_response_length: 117.93109827166562, avg_orm_score: 0.0, avg_custom_rewards: 0.0012488026775858939
2025-07-24 16:49:24.482 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter11_replay_buffer.jsonl
2025-07-24 16:49:26.461 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.98s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.0697, ret=-0.000943, glen=101, tlen=262, kl=0.00094, act_lr=2.2e-7, ent=1.68]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.05s/it, pg=0.0697, ret=-0.000943, glen=101, tlen=262, kl=0.00094, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:59,  1.05s/it, pg=-0.27, ret=0.012, glen=122, tlen=282, kl=0.000855, act_lr=2.2e-7, ent=1.76]    Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:52,  1.06it/s, pg=-0.27, ret=0.012, glen=122, tlen=282, kl=0.000855, act_lr=2.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:52,  1.06it/s, pg=-0.146, ret=0.000722, glen=114, tlen=275, kl=0.000962, act_lr=2.2e-7, ent=1.82]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.10it/s, pg=-0.146, ret=0.000722, glen=114, tlen=275, kl=0.000962, act_lr=2.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.10it/s, pg=-0.0287, ret=-0.000316, glen=126, tlen=287, kl=0.000924, act_lr=2.2e-7, ent=1.9]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.13it/s, pg=-0.0287, ret=-0.000316, glen=126, tlen=287, kl=0.000924, act_lr=2.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.13it/s, pg=0.13, ret=-0.00114, glen=117, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.72]   Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:46,  1.14it/s, pg=0.13, ret=-0.00114, glen=117, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:46,  1.14it/s, pg=-0.139, ret=0.00188, glen=112, tlen=272, kl=0.000925, act_lr=2.2e-7, ent=1.66]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:45,  1.13it/s, pg=-0.139, ret=0.00188, glen=112, tlen=272, kl=0.000925, act_lr=2.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:45,  1.13it/s, pg=0.2, ret=-0.000557, glen=211, tlen=372, kl=0.00091, act_lr=2.2e-7, ent=2.19]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:45,  1.13it/s, pg=0.2, ret=-0.000557, glen=211, tlen=372, kl=0.00091, act_lr=2.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:45,  1.13it/s, pg=-0.0331, ret=5.92e-5, glen=116, tlen=277, kl=0.000958, act_lr=2.2e-7, ent=1.78]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.14it/s, pg=-0.0331, ret=5.92e-5, glen=116, tlen=277, kl=0.000958, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.14it/s, pg=0.0508, ret=-0.00177, glen=100, tlen=260, kl=0.000978, act_lr=2.2e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.15it/s, pg=0.0508, ret=-0.00177, glen=100, tlen=260, kl=0.000978, act_lr=2.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.15it/s, pg=-0.127, ret=0.00028, glen=93.1, tlen=253, kl=0.000921, act_lr=2.2e-7, ent=1.59]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.16it/s, pg=-0.127, ret=0.00028, glen=93.1, tlen=253, kl=0.000921, act_lr=2.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.16it/s, pg=-0.0802, ret=-0.000248, glen=114, tlen=274, kl=0.000965, act_lr=2.2e-7, ent=1.87]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.16it/s, pg=-0.0802, ret=-0.000248, glen=114, tlen=274, kl=0.000965, act_lr=2.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.16it/s, pg=-0.012, ret=0.000218, glen=117, tlen=277, kl=0.000941, act_lr=2.2e-7, ent=1.72]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.16it/s, pg=-0.012, ret=0.000218, glen=117, tlen=277, kl=0.000941, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.16it/s, pg=-0.29, ret=0.000731, glen=98.3, tlen=259, kl=0.000941, act_lr=2.2e-7, ent=1.64]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=-0.29, ret=0.000731, glen=98.3, tlen=259, kl=0.000941, act_lr=2.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=0.0443, ret=0.000299, glen=127, tlen=288, kl=0.000971, act_lr=2.2e-7, ent=1.65]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=0.0443, ret=0.000299, glen=127, tlen=288, kl=0.000971, act_lr=2.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=0.111, ret=-0.00198, glen=114, tlen=275, kl=0.00098, act_lr=2.2e-7, ent=1.7]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=0.111, ret=-0.00198, glen=114, tlen=275, kl=0.00098, act_lr=2.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=-0.336, ret=0.00294, glen=130, tlen=290, kl=0.000918, act_lr=2.2e-7, ent=1.76]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:13<00:35,  1.17it/s, pg=-0.336, ret=0.00294, glen=130, tlen=290, kl=0.000918, act_lr=2.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:35,  1.17it/s, pg=-0.125, ret=0.00119, glen=112, tlen=273, kl=0.000945, act_lr=2.2e-7, ent=1.6] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.17it/s, pg=-0.125, ret=0.00119, glen=112, tlen=273, kl=0.000945, act_lr=2.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.17it/s, pg=-0.0329, ret=0.000899, glen=125, tlen=285, kl=0.000925, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=-0.0329, ret=0.000899, glen=125, tlen=285, kl=0.000925, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.0841, ret=0.000657, glen=95.8, tlen=256, kl=0.000859, act_lr=2.2e-7, ent=1.55]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.18it/s, pg=-0.0841, ret=0.000657, glen=95.8, tlen=256, kl=0.000859, act_lr=2.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.18it/s, pg=-0.042, ret=-7.61e-5, glen=101, tlen=262, kl=0.000952, act_lr=2.2e-7, ent=1.63]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.16it/s, pg=-0.042, ret=-7.61e-5, glen=101, tlen=262, kl=0.000952, act_lr=2.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.16it/s, pg=-0.0566, ret=-0.00022, glen=93.8, tlen=254, kl=0.000958, act_lr=2.2e-7, ent=1.64]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.16it/s, pg=-0.0566, ret=-0.00022, glen=93.8, tlen=254, kl=0.000958, act_lr=2.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.16it/s, pg=-0.152, ret=0.000598, glen=102, tlen=262, kl=0.00098, act_lr=2.2e-7, ent=1.68]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.152, ret=0.000598, glen=102, tlen=262, kl=0.00098, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=0.193, ret=-0.00152, glen=105, tlen=266, kl=0.000911, act_lr=2.2e-7, ent=1.71]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.17it/s, pg=0.193, ret=-0.00152, glen=105, tlen=266, kl=0.000911, act_lr=2.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.0391, ret=0.00202, glen=135, tlen=295, kl=0.000947, act_lr=2.2e-7, ent=2.08]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.17it/s, pg=-0.0391, ret=0.00202, glen=135, tlen=295, kl=0.000947, act_lr=2.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.17it/s, pg=0.0673, ret=-0.00154, glen=112, tlen=273, kl=0.000904, act_lr=2.2e-7, ent=1.9] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.15it/s, pg=0.0673, ret=-0.00154, glen=112, tlen=273, kl=0.000904, act_lr=2.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.15it/s, pg=0.113, ret=-0.00118, glen=126, tlen=286, kl=0.000923, act_lr=2.2e-7, ent=1.86]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.16it/s, pg=0.113, ret=-0.00118, glen=126, tlen=286, kl=0.000923, act_lr=2.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.16it/s, pg=-0.189, ret=0.000218, glen=112, tlen=272, kl=0.000908, act_lr=2.2e-7, ent=1.66]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.17it/s, pg=-0.189, ret=0.000218, glen=112, tlen=272, kl=0.000908, act_lr=2.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=0.00732, ret=-0.000992, glen=114, tlen=275, kl=0.000945, act_lr=2.2e-7, ent=1.82]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=0.00732, ret=-0.000992, glen=114, tlen=275, kl=0.000945, act_lr=2.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.0691, ret=0.00184, glen=137, tlen=299, kl=0.000942, act_lr=2.2e-7, ent=1.91]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.06it/s, pg=-0.0691, ret=0.00184, glen=137, tlen=299, kl=0.000942, act_lr=2.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.06it/s, pg=0.0711, ret=0.00011, glen=125, tlen=286, kl=0.000912, act_lr=2.2e-7, ent=1.79] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.09it/s, pg=0.0711, ret=0.00011, glen=125, tlen=286, kl=0.000912, act_lr=2.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:25,  1.09it/s, pg=0.0966, ret=0.000325, glen=125, tlen=285, kl=0.000935, act_lr=2.2e-7, ent=1.99]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.0966, ret=0.000325, glen=125, tlen=285, kl=0.000935, act_lr=2.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.245, ret=-0.00271, glen=124, tlen=284, kl=0.000927, act_lr=2.2e-7, ent=1.91] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:22,  1.13it/s, pg=0.245, ret=-0.00271, glen=124, tlen=284, kl=0.000927, act_lr=2.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.13it/s, pg=0.123, ret=-0.00078, glen=126, tlen=286, kl=0.000937, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.14it/s, pg=0.123, ret=-0.00078, glen=126, tlen=286, kl=0.000937, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.14it/s, pg=0.103, ret=-0.000709, glen=125, tlen=286, kl=0.000891, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.15it/s, pg=0.103, ret=-0.000709, glen=125, tlen=286, kl=0.000891, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.15it/s, pg=0.0511, ret=0.00163, glen=159, tlen=319, kl=0.000774, act_lr=2.2e-7, ent=1.45] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:20,  1.15it/s, pg=0.0511, ret=0.00163, glen=159, tlen=319, kl=0.000774, act_lr=2.2e-7, ent=1.45]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:20,  1.15it/s, pg=-0.108, ret=0.000659, glen=113, tlen=275, kl=0.000937, act_lr=2.2e-7, ent=1.76]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:19,  1.16it/s, pg=-0.108, ret=0.000659, glen=113, tlen=275, kl=0.000937, act_lr=2.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:19,  1.16it/s, pg=0.136, ret=-0.00108, glen=107, tlen=267, kl=0.000952, act_lr=2.2e-7, ent=1.58] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.16it/s, pg=0.136, ret=-0.00108, glen=107, tlen=267, kl=0.000952, act_lr=2.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.16it/s, pg=0.098, ret=-0.00133, glen=166, tlen=326, kl=0.000774, act_lr=2.2e-7, ent=2.52]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.15it/s, pg=0.098, ret=-0.00133, glen=166, tlen=326, kl=0.000774, act_lr=2.2e-7, ent=2.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:34<00:17,  1.15it/s, pg=-0.00522, ret=-8.73e-5, glen=116, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.73]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.15it/s, pg=-0.00522, ret=-8.73e-5, glen=116, tlen=277, kl=0.000925, act_lr=2.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.15it/s, pg=0.0577, ret=-0.00109, glen=107, tlen=267, kl=0.000923, act_lr=2.2e-7, ent=1.75]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.16it/s, pg=0.0577, ret=-0.00109, glen=107, tlen=267, kl=0.000923, act_lr=2.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=-0.0135, ret=-0.000361, glen=117, tlen=277, kl=0.000985, act_lr=2.2e-7, ent=1.84]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.16it/s, pg=-0.0135, ret=-0.000361, glen=117, tlen=277, kl=0.000985, act_lr=2.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.16it/s, pg=-0.104, ret=0.000264, glen=107, tlen=267, kl=0.000985, act_lr=2.2e-7, ent=1.81]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=-0.104, ret=0.000264, glen=107, tlen=267, kl=0.000985, act_lr=2.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=0.0083, ret=-0.000608, glen=113, tlen=273, kl=0.000937, act_lr=2.2e-7, ent=1.72]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=0.0083, ret=-0.000608, glen=113, tlen=273, kl=0.000937, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=-0.0266, ret=0.000425, glen=102, tlen=263, kl=0.000929, act_lr=2.2e-7, ent=1.54]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=-0.0266, ret=0.000425, glen=102, tlen=263, kl=0.000929, act_lr=2.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.204, ret=-0.0019, glen=129, tlen=289, kl=0.000926, act_lr=2.2e-7, ent=2.19]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.204, ret=-0.0019, glen=129, tlen=289, kl=0.000926, act_lr=2.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.019, ret=0.000675, glen=109, tlen=270, kl=0.000972, act_lr=2.2e-7, ent=1.81]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.17it/s, pg=-0.019, ret=0.000675, glen=109, tlen=270, kl=0.000972, act_lr=2.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=-0.0184, ret=-9.74e-5, glen=111, tlen=272, kl=0.000907, act_lr=2.2e-7, ent=1.47]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.0184, ret=-9.74e-5, glen=111, tlen=272, kl=0.000907, act_lr=2.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.116, ret=-0.000519, glen=138, tlen=299, kl=0.000902, act_lr=2.2e-7, ent=2.31] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.15it/s, pg=0.116, ret=-0.000519, glen=138, tlen=299, kl=0.000902, act_lr=2.2e-7, ent=2.31]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.15it/s, pg=-0.0499, ret=0.000151, glen=110, tlen=270, kl=0.000923, act_lr=2.2e-7, ent=1.62]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.16it/s, pg=-0.0499, ret=0.000151, glen=110, tlen=270, kl=0.000923, act_lr=2.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.16it/s, pg=-0.0669, ret=-0.000424, glen=102, tlen=262, kl=0.000954, act_lr=2.2e-7, ent=1.58]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.16it/s, pg=-0.0669, ret=-0.000424, glen=102, tlen=262, kl=0.000954, act_lr=2.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.16it/s, pg=0.0181, ret=0.000197, glen=120, tlen=281, kl=0.000949, act_lr=2.2e-7, ent=1.78]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.17it/s, pg=0.0181, ret=0.000197, glen=120, tlen=281, kl=0.000949, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=-0.0667, ret=-0.00023, glen=113, tlen=273, kl=0.000981, act_lr=2.2e-7, ent=1.68]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=-0.0667, ret=-0.00023, glen=113, tlen=273, kl=0.000981, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=-0.132, ret=0.000294, glen=97.7, tlen=258, kl=0.000955, act_lr=2.2e-7, ent=1.59]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.17it/s, pg=-0.132, ret=0.000294, glen=97.7, tlen=258, kl=0.000955, act_lr=2.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.17it/s, pg=-0.128, ret=-9.17e-5, glen=104, tlen=265, kl=0.00097, act_lr=2.2e-7, ent=1.83]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.17it/s, pg=-0.128, ret=-9.17e-5, glen=104, tlen=265, kl=0.00097, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=0.0692, ret=-0.000887, glen=117, tlen=278, kl=0.000918, act_lr=2.2e-7, ent=1.6]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=0.0692, ret=-0.000887, glen=117, tlen=278, kl=0.000918, act_lr=2.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=-0.0249, ret=-3.51e-5, glen=112, tlen=273, kl=0.000934, act_lr=2.2e-7, ent=1.72]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=-0.0249, ret=-3.51e-5, glen=112, tlen=273, kl=0.000934, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=-0.237, ret=0.00239, glen=139, tlen=300, kl=0.000922, act_lr=2.2e-7, ent=1.98]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=-0.237, ret=0.00239, glen=139, tlen=300, kl=0.000922, act_lr=2.2e-7, ent=1.98]
2025-07-24 16:50:17.071 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.39s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.13, ret=0.00169, glen=115, tlen=275, kl=0.000914, act_lr=2.4e-7, ent=1.73] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.13, ret=0.00169, glen=115, tlen=275, kl=0.000914, act_lr=2.4e-7, ent=1.73]
2025-07-24 16:50:17.735 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 16:50:20.111 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.38s
2025-07-24 16:50:20.442 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.89s
2025-07-24 16:50:20.452 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.017218655553357356, 'actor_lr': 2.2034483136175022e-07, 'clip_ratio': 0.0, 'entropy': 1.7747623344947552, 'kl': 0.0009304408379428988, 'response_length': 117.81621248968716, 'total_length': 278.36411364325164, 'teacher_total_length': 290.11740059688174, 'return': 0.00017178226247277302, 'policy_update_steps': 1.0}
Episode [1/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [44:28<03:42, 222.95s/it]2025-07-24 16:50:20.458 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:06,  2.55it/s, est. speed input: 459.36 toks/s, output: 28.07 toks/s]
[36m(LLMActor pid=1435061)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/172 [00:05<00:00, 16.83it/s, est. speed input: 5649.25 toks/s, output: 2822.09 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:01, 14.44it/s, est. speed input: 5320.78 toks/s, output: 2624.76 toks/s][32m [repeated 116x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:10<00:03,  1.79it/s, est. speed input: 2919.33 toks/s, output: 1755.87 toks/s][32m [repeated 27x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:05<00:01, 13.02it/s, est. speed input: 4765.75 toks/s, output: 2392.88 toks/s][32m [repeated 4x across cluster][0m
[36m(LLMActor pid=1435059)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:10<00:00,  1.69it/s, est. speed input: 2883.94 toks/s, output: 1736.57 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:10<00:00, 15.89it/s, est. speed input: 2883.94 toks/s, output: 1736.57 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:15<00:00,  1.03it/s, est. speed input: 1964.50 toks/s, output: 1389.90 toks/s][32m [repeated 10x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:16<00:00,  1.16it/s, est. speed input: 1908.70 toks/s, output: 1402.62 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:16<00:00, 10.53it/s, est. speed input: 1908.70 toks/s, output: 1402.62 toks/s][32m [repeated 3x across cluster][0m
2025-07-24 16:50:38.129 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 563.0422,strategyqa_test/accuracy: 0.3217,eval_accuracy: 0.3217
2025-07-24 16:50:38.383 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:51:31.029 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:51:31.203 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:51:31.204 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 52.82s
2025-07-24 16:51:32.319 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0091,avg_pass_at_n: 1.0000,avg_num_tokens: 104.6768,std_num_tokens: 104.4267,avg_correct_num_tokens: 99.6748,std_correct_num_tokens: 83.4959,avg_incorrect_num_tokens: 112.9474,std_incorrect_num_tokens: 131.5251
2025-07-24 16:51:32.649 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 1.44s
2025-07-24 16:51:34.214 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 1.56s
2025-07-24 16:51:49.298 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 115
2025-07-24 16:51:49.299 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 15.08s
2025-07-24 16:51:50.428 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.71s
2025-07-24 16:51:50.429 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: 0.00014084085477920978, avg_kl: 0.0009495610776154891, avg_response_length: 105.95155016028363, avg_orm_score: 0.0, avg_custom_rewards: 0.00014084085477920978
2025-07-24 16:51:50.461 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter12_replay_buffer.jsonl
2025-07-24 16:51:51.402 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 0.94s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s, pg=0.105, ret=-0.000926, glen=115, tlen=275, kl=0.000928, act_lr=2.4e-7, ent=1.84]Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:00<00:27,  1.00it/s, pg=0.105, ret=-0.000926, glen=115, tlen=275, kl=0.000928, act_lr=2.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:01<00:27,  1.00it/s, pg=-0.172, ret=0.000433, glen=90.8, tlen=250, kl=0.00099, act_lr=2.4e-7, ent=1.65]Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:01<00:24,  1.09it/s, pg=-0.172, ret=0.000433, glen=90.8, tlen=250, kl=0.00099, act_lr=2.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:02<00:24,  1.09it/s, pg=0.125, ret=0.000343, glen=124, tlen=283, kl=0.000853, act_lr=2.4e-7, ent=1.91] Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:02<00:23,  1.12it/s, pg=0.125, ret=0.000343, glen=124, tlen=283, kl=0.000853, act_lr=2.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:03<00:23,  1.12it/s, pg=0.0442, ret=-0.00187, glen=101, tlen=260, kl=0.000957, act_lr=2.4e-7, ent=1.8]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:03<00:22,  1.12it/s, pg=0.0442, ret=-0.00187, glen=101, tlen=260, kl=0.000957, act_lr=2.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:04<00:22,  1.12it/s, pg=0.108, ret=-0.000293, glen=98.1, tlen=258, kl=0.000941, act_lr=2.4e-7, ent=1.82]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:04<00:21,  1.14it/s, pg=0.108, ret=-0.000293, glen=98.1, tlen=258, kl=0.000941, act_lr=2.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:05<00:21,  1.14it/s, pg=0.0654, ret=0.000206, glen=122, tlen=281, kl=0.000998, act_lr=2.4e-7, ent=2.08] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:05<00:20,  1.15it/s, pg=0.0654, ret=0.000206, glen=122, tlen=281, kl=0.000998, act_lr=2.4e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:06<00:20,  1.15it/s, pg=-0.167, ret=0.0018, glen=106, tlen=266, kl=0.000976, act_lr=2.4e-7, ent=1.7]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:06<00:19,  1.15it/s, pg=-0.167, ret=0.0018, glen=106, tlen=266, kl=0.000976, act_lr=2.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:07<00:19,  1.15it/s, pg=-0.181, ret=0.00129, glen=108, tlen=267, kl=0.000966, act_lr=2.4e-7, ent=1.84]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.16it/s, pg=-0.181, ret=0.00129, glen=108, tlen=267, kl=0.000966, act_lr=2.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.16it/s, pg=0.033, ret=0.000164, glen=96.6, tlen=257, kl=0.000953, act_lr=2.4e-7, ent=1.74]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:07<00:17,  1.16it/s, pg=0.033, ret=0.000164, glen=96.6, tlen=257, kl=0.000953, act_lr=2.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:08<00:17,  1.16it/s, pg=-0.0825, ret=-7.81e-5, glen=103, tlen=263, kl=0.000933, act_lr=2.4e-7, ent=1.63]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:08<00:16,  1.17it/s, pg=-0.0825, ret=-7.81e-5, glen=103, tlen=263, kl=0.000933, act_lr=2.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:09<00:16,  1.17it/s, pg=0.0226, ret=-0.00105, glen=97.7, tlen=257, kl=0.000984, act_lr=2.4e-7, ent=1.71]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:09<00:15,  1.17it/s, pg=0.0226, ret=-0.00105, glen=97.7, tlen=257, kl=0.000984, act_lr=2.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:10<00:15,  1.17it/s, pg=0.0446, ret=8.37e-5, glen=112, tlen=272, kl=0.00094, act_lr=2.4e-7, ent=1.66]   Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:10<00:14,  1.17it/s, pg=0.0446, ret=8.37e-5, glen=112, tlen=272, kl=0.00094, act_lr=2.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:11<00:14,  1.17it/s, pg=-0.185, ret=0.00143, glen=113, tlen=272, kl=0.000949, act_lr=2.4e-7, ent=1.59]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:11<00:13,  1.17it/s, pg=-0.185, ret=0.00143, glen=113, tlen=272, kl=0.000949, act_lr=2.4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:12<00:13,  1.17it/s, pg=0.168, ret=-0.00153, glen=98.4, tlen=258, kl=0.00095, act_lr=2.4e-7, ent=1.52]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:12<00:12,  1.17it/s, pg=0.168, ret=-0.00153, glen=98.4, tlen=258, kl=0.00095, act_lr=2.4e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:13<00:12,  1.17it/s, pg=0.0481, ret=-0.00205, glen=98.3, tlen=258, kl=0.000963, act_lr=2.4e-7, ent=1.61]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.14it/s, pg=0.0481, ret=-0.00205, glen=98.3, tlen=258, kl=0.000963, act_lr=2.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.14it/s, pg=-0.0548, ret=-0.000386, glen=97.3, tlen=257, kl=0.000949, act_lr=2.4e-7, ent=1.54]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:13<00:11,  1.15it/s, pg=-0.0548, ret=-0.000386, glen=97.3, tlen=257, kl=0.000949, act_lr=2.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:14<00:11,  1.15it/s, pg=0.132, ret=-0.000671, glen=116, tlen=276, kl=0.000998, act_lr=2.4e-7, ent=1.84]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:14<00:10,  1.16it/s, pg=0.132, ret=-0.000671, glen=116, tlen=276, kl=0.000998, act_lr=2.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:15<00:10,  1.16it/s, pg=0.0545, ret=-0.000663, glen=96.6, tlen=256, kl=0.000903, act_lr=2.4e-7, ent=1.62]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:15<00:09,  1.17it/s, pg=0.0545, ret=-0.000663, glen=96.6, tlen=256, kl=0.000903, act_lr=2.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:16<00:09,  1.17it/s, pg=0.0808, ret=-0.00105, glen=107, tlen=266, kl=0.000937, act_lr=2.4e-7, ent=1.6]   Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:16<00:08,  1.17it/s, pg=0.0808, ret=-0.00105, glen=107, tlen=266, kl=0.000937, act_lr=2.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:17<00:08,  1.17it/s, pg=-0.141, ret=0.00102, glen=93, tlen=253, kl=0.000953, act_lr=2.4e-7, ent=1.66] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:17<00:07,  1.17it/s, pg=-0.141, ret=0.00102, glen=93, tlen=253, kl=0.000953, act_lr=2.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:18<00:07,  1.17it/s, pg=0.00894, ret=-2e-5, glen=122, tlen=282, kl=0.000944, act_lr=2.4e-7, ent=1.67]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:18<00:06,  1.17it/s, pg=0.00894, ret=-2e-5, glen=122, tlen=282, kl=0.000944, act_lr=2.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:19<00:06,  1.17it/s, pg=0.107, ret=0.000842, glen=113, tlen=273, kl=0.000957, act_lr=2.4e-7, ent=1.87]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=0.107, ret=0.000842, glen=113, tlen=273, kl=0.000957, act_lr=2.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=0.0717, ret=-0.000238, glen=105, tlen=265, kl=0.000992, act_lr=2.4e-7, ent=1.79]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:19<00:05,  1.17it/s, pg=0.0717, ret=-0.000238, glen=105, tlen=265, kl=0.000992, act_lr=2.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:20<00:05,  1.17it/s, pg=0.0761, ret=-0.000323, glen=108, tlen=268, kl=0.000914, act_lr=2.4e-7, ent=1.73]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:20<00:04,  1.17it/s, pg=0.0761, ret=-0.000323, glen=108, tlen=268, kl=0.000914, act_lr=2.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:21<00:04,  1.17it/s, pg=0.191, ret=-0.00147, glen=122, tlen=281, kl=0.000879, act_lr=2.4e-7, ent=1.75]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:21<00:03,  1.17it/s, pg=0.191, ret=-0.00147, glen=122, tlen=281, kl=0.000879, act_lr=2.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:22<00:03,  1.17it/s, pg=-0.161, ret=0.000976, glen=101, tlen=261, kl=0.000937, act_lr=2.4e-7, ent=1.71]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:22<00:02,  1.17it/s, pg=-0.161, ret=0.000976, glen=101, tlen=261, kl=0.000937, act_lr=2.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:23<00:02,  1.17it/s, pg=-0.223, ret=0.00107, glen=99, tlen=259, kl=0.000966, act_lr=2.4e-7, ent=1.64]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:23<00:01,  1.17it/s, pg=-0.223, ret=0.00107, glen=99, tlen=259, kl=0.000966, act_lr=2.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:24<00:01,  1.17it/s, pg=-0.134, ret=0.00128, glen=103, tlen=263, kl=0.000971, act_lr=2.4e-7, ent=1.69]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:24<00:00,  1.17it/s, pg=-0.134, ret=0.00128, glen=103, tlen=263, kl=0.000971, act_lr=2.4e-7, ent=1.69]
2025-07-24 16:52:16.921 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 25.36s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.17it/s, pg=-0.168, ret=0.00159, glen=109, tlen=269, kl=0.000997, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.11it/s, pg=-0.168, ret=0.00159, glen=109, tlen=269, kl=0.000997, act_lr=2.6e-7, ent=1.67]
2025-07-24 16:52:17.603 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-24 16:52:19.717 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.11s
2025-07-24 16:52:20.042 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 28.60s
2025-07-24 16:52:20.046 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.0063823173786031785, 'actor_lr': 2.406896471554852e-07, 'clip_ratio': 0.0, 'entropy': 1.720232079769003, 'kl': 0.0009509119494207974, 'response_length': 106.02787728145205, 'total_length': 265.74897818729795, 'teacher_total_length': 278.236981622104, 'return': -2.9182686783566044e-06, 'policy_update_steps': 1.0}
Episode [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [46:27<00:00, 191.64s/it]2025-07-24 16:52:28.712 | INFO     | orz.ppo.trainer:train:194 - Successfully update ref model with policy model, training continue.
[36m(extract_final_answers_batch pid=1439602)[0m [2025-07-24 16:07:59,012] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[36m(LLMActor pid=1435059)[0m init_process_group: master_address=10.224.3.58, master_port=41739,  rank=2, world_size=5, group_name=openrlhf[32m [repeated 3x across cluster][0m
[36m(extract_final_answers_batch pid=1439602)[0m [2025-07-24 16:07:59,017] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[36m(pid=1440883)[0m [2025-07-24 16:08:05,597] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 15x across cluster][0m
[36m(pid=1440883)[0m [2025-07-24 16:08:05,602] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 15x across cluster][0m
[36m(get_reflection_pattern_score pid=1437884)[0m math_verify is not installed in this environment
[36m(pid=1440893)[0m [2025-07-24 16:08:09,954] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 28x across cluster][0m
[36m(pid=1440893)[0m [2025-07-24 16:08:09,957] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 28x across cluster][0m
[36m(pid=1440889)[0m math_verify is not installed in this environment[32m [repeated 15x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Invalidate trace cache @ step 370 and module 0: cache has only 370 modules
[36m(pid=1440894)[0m math_verify is not installed in this environment[32m [repeated 21x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:09:42,907] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(extract_final_answers_batch pid=1440886)[0m math_verify is not installed in this environment
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:12:48,419] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(get_reflection_pattern_score pid=1440891)[0m math_verify is not installed in this environment[32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:17:00,346] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:20:16,141] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:24:30,296] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:28:31,517] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:31:25,527] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:34:32,765] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:38:50,247] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:43:05,605] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:46:47,473] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m [2025-07-24 16:50:17,064] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=1435640)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:26,602] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:26,801] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 678, num_elems = 3.55B
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,297] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,297] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,304] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,306] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,548] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,549] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 6.45 GB         CA 2.51 GB         Max_CA 38 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,549] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.88 GB, percent = 21.6%
[36m(RefRayActorBase pid=1435817)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [2/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [46:36<00:00, 215.12s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,706] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,707] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 2.51 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,707] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.88 GB, percent = 21.6%
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1435817)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1435817)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1435817)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1435817)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1435817)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1435817)[0m     "profile": false
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1435817)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1435817)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1435817)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1435817)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1435817)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1435817)[0m     "fast": true, 
[36m(RefRayActorBase pid=1435817)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1435817)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1435817)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1435817)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1435817)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "num_tuning_micro_batch_sizes": 3
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  Falsehuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee7d9736b10>
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1435817)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1435817)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1435817)[0m     "output_file": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,708] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   nebula_config ................ {huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=1435817)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1435817)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1435817)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1435817)[0m     "load_path": null
[36m(RefRayActorBase pid=1435817)[0m }
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1435817)[0m [2025-07-24 16:52:28,709] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1435817)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1435817)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1435817)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1435817)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1435817)[0m         "offload_param": {
[36m(RefRayActorBase pid=1435817)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1435817)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1435817)[0m         }
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "bf16": {
[36m(RefRayActorBase pid=1435817)[0m         "enabled": true
[36m(RefRayActorBase pid=1435817)[0m     }, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1435817)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1435817)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1435817)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1435817)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1435817)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-24 16:52:28.966 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:55:06.949 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:55:07.121 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:55:07.121 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 158.16s
2025-07-24 16:55:09.423 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0088,avg_pass_at_n: 1.0000,avg_num_tokens: 114.6980,std_num_tokens: 158.8815,avg_correct_num_tokens: 104.4542,std_correct_num_tokens: 82.5497,avg_incorrect_num_tokens: 132.0131,std_incorrect_num_tokens: 236.4671
2025-07-24 16:55:09.866 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.74s
2025-07-24 16:55:12.897 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.03s
2025-07-24 16:55:42.940 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 229
2025-07-24 16:55:42.940 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 30.04s
2025-07-24 16:55:44.550 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.20s
2025-07-24 16:55:44.551 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0010314403766741788, avg_kl: 0.0, avg_response_length: 120.66883367013723, avg_orm_score: 0.0, avg_custom_rewards: -0.0010314403766741788
2025-07-24 16:55:44.631 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter13_replay_buffer.jsonl
2025-07-24 16:55:46.571 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.94s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=0.0809, ret=-0.00196, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.59]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:57,  1.01s/it, pg=0.0809, ret=-0.00196, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:57,  1.01s/it, pg=0.161, ret=0.000202, glen=120, tlen=280, kl=0, act_lr=2.6e-7, ent=2.14] Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:51,  1.08it/s, pg=0.161, ret=0.000202, glen=120, tlen=280, kl=0, act_lr=2.6e-7, ent=2.14]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:51,  1.08it/s, pg=-0.141, ret=0.000303, glen=110, tlen=270, kl=0, act_lr=2.6e-7, ent=1.63]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.12it/s, pg=-0.141, ret=0.000303, glen=110, tlen=270, kl=0, act_lr=2.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.12it/s, pg=0.00488, ret=-1.7e-5, glen=100, tlen=261, kl=0, act_lr=2.6e-7, ent=1.77]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:48,  1.12it/s, pg=0.00488, ret=-1.7e-5, glen=100, tlen=261, kl=0, act_lr=2.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:48,  1.12it/s, pg=0.00537, ret=-0.00154, glen=99.8, tlen=260, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:47,  1.12it/s, pg=0.00537, ret=-0.00154, glen=99.8, tlen=260, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:47,  1.12it/s, pg=-0.0101, ret=-0.000664, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:45,  1.14it/s, pg=-0.0101, ret=-0.000664, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:45,  1.14it/s, pg=-0.255, ret=0.00309, glen=117, tlen=278, kl=0, act_lr=2.6e-7, ent=1.62]   Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:44,  1.15it/s, pg=-0.255, ret=0.00309, glen=117, tlen=278, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:44,  1.15it/s, pg=0.00128, ret=-0.000769, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.56]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.16it/s, pg=0.00128, ret=-0.000769, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.16it/s, pg=-0.241, ret=0.00179, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.72]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.16it/s, pg=-0.241, ret=0.00179, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.16it/s, pg=-0.129, ret=-2.23e-5, glen=114, tlen=274, kl=0, act_lr=2.6e-7, ent=1.54]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.17it/s, pg=-0.129, ret=-2.23e-5, glen=114, tlen=274, kl=0, act_lr=2.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.17it/s, pg=0.177, ret=-0.00184, glen=122, tlen=282, kl=0, act_lr=2.6e-7, ent=2.06] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:40,  1.17it/s, pg=0.177, ret=-0.00184, glen=122, tlen=282, kl=0, act_lr=2.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:40,  1.17it/s, pg=0.231, ret=-0.00109, glen=138, tlen=299, kl=0, act_lr=2.6e-7, ent=1.6] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:39,  1.17it/s, pg=0.231, ret=-0.00109, glen=138, tlen=299, kl=0, act_lr=2.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:39,  1.17it/s, pg=0.0111, ret=-0.000123, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.99]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.17it/s, pg=0.0111, ret=-0.000123, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.17it/s, pg=-0.26, ret=0.00117, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.7]    Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.17it/s, pg=-0.26, ret=0.00117, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.17it/s, pg=-0.138, ret=0.000469, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.55]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=-0.138, ret=0.000469, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.17it/s, pg=0.0195, ret=-0.000199, glen=107, tlen=268, kl=0, act_lr=2.6e-7, ent=1.58]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:13<00:36,  1.15it/s, pg=0.0195, ret=-0.000199, glen=107, tlen=268, kl=0, act_lr=2.6e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.15it/s, pg=-0.0808, ret=-0.000727, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.79]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=-0.0808, ret=-0.000727, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=-0.0922, ret=-0.000559, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.64]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=-0.0922, ret=-0.000559, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.143, ret=0.00049, glen=97.7, tlen=258, kl=0, act_lr=2.6e-7, ent=1.66]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=-0.143, ret=0.00049, glen=97.7, tlen=258, kl=0, act_lr=2.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=0.227, ret=-0.00104, glen=126, tlen=286, kl=0, act_lr=2.6e-7, ent=1.64] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=0.227, ret=-0.00104, glen=126, tlen=286, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.104, ret=0.00078, glen=125, tlen=285, kl=0, act_lr=2.6e-7, ent=1.85]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.104, ret=0.00078, glen=125, tlen=285, kl=0, act_lr=2.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=-0.0652, ret=0.000275, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.57]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0652, ret=0.000275, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.0134, ret=0.000357, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.62]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.18it/s, pg=-0.0134, ret=0.000357, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.18it/s, pg=0.133, ret=-0.000775, glen=112, tlen=272, kl=0, act_lr=2.6e-7, ent=1.84] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:28,  1.18it/s, pg=0.133, ret=-0.000775, glen=112, tlen=272, kl=0, act_lr=2.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:28,  1.18it/s, pg=-0.0751, ret=9.99e-5, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.64]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.16it/s, pg=-0.0751, ret=9.99e-5, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.16it/s, pg=0.0591, ret=-0.000209, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.87]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.16it/s, pg=0.0591, ret=-0.000209, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.16it/s, pg=-0.0345, ret=2.93e-5, glen=96.8, tlen=257, kl=0, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:26,  1.17it/s, pg=-0.0345, ret=2.93e-5, glen=96.8, tlen=257, kl=0, act_lr=2.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:26,  1.17it/s, pg=-0.0227, ret=-0.000372, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.71]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:25,  1.17it/s, pg=-0.0227, ret=-0.000372, glen=110, tlen=271, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:25,  1.17it/s, pg=-0.0903, ret=0.00146, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.69]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:27,  1.07it/s, pg=-0.0903, ret=0.00146, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:27,  1.07it/s, pg=-0.0934, ret=-0.000907, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=-0.0934, ret=-0.000907, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:25,  1.10it/s, pg=0.0637, ret=-0.00047, glen=118, tlen=279, kl=0, act_lr=2.6e-7, ent=1.76]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:26<00:24,  1.12it/s, pg=0.0637, ret=-0.00047, glen=118, tlen=279, kl=0, act_lr=2.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.182, ret=-0.00114, glen=391, tlen=553, kl=0, act_lr=2.6e-7, ent=2.26] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:27<00:23,  1.11it/s, pg=0.182, ret=-0.00114, glen=391, tlen=553, kl=0, act_lr=2.6e-7, ent=2.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:23,  1.11it/s, pg=-0.00452, ret=0.000591, glen=130, tlen=291, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:22,  1.12it/s, pg=-0.00452, ret=0.000591, glen=130, tlen=291, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:22,  1.12it/s, pg=-0.36, ret=0.00104, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.69]    Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:21,  1.14it/s, pg=-0.36, ret=0.00104, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:21,  1.14it/s, pg=-0.0426, ret=-0.000918, glen=114, tlen=275, kl=0, act_lr=2.6e-7, ent=1.72]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.15it/s, pg=-0.0426, ret=-0.000918, glen=114, tlen=275, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.15it/s, pg=-0.122, ret=0.00126, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.64]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:19,  1.16it/s, pg=-0.122, ret=0.00126, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:19,  1.16it/s, pg=0.0905, ret=-0.000472, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.92]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:18,  1.16it/s, pg=0.0905, ret=-0.000472, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:18,  1.16it/s, pg=-0.225, ret=0.000221, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.63] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.16it/s, pg=-0.225, ret=0.000221, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.16it/s, pg=-0.0738, ret=0.000694, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.69]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.16it/s, pg=-0.0738, ret=0.000694, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.16it/s, pg=0.154, ret=0.000223, glen=124, tlen=284, kl=0, act_lr=2.6e-7, ent=1.97]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.16it/s, pg=0.154, ret=0.000223, glen=124, tlen=284, kl=0, act_lr=2.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.16it/s, pg=-0.122, ret=0.000834, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.72]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.17it/s, pg=-0.122, ret=0.000834, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.17it/s, pg=-0.164, ret=0.000858, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.54]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=-0.164, ret=0.000858, glen=118, tlen=278, kl=0, act_lr=2.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=0.316, ret=0.000223, glen=151, tlen=312, kl=0, act_lr=2.6e-7, ent=2.16] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=0.316, ret=0.000223, glen=151, tlen=312, kl=0, act_lr=2.6e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=0.0946, ret=-0.00134, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.66]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=0.0946, ret=-0.00134, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.0197, ret=-0.000257, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.88]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.0197, ret=-0.000257, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=-0.0845, ret=0.000793, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.71]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.14it/s, pg=-0.0845, ret=0.000793, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.14it/s, pg=0.139, ret=-0.00178, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.87]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.15it/s, pg=0.139, ret=-0.00178, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.15it/s, pg=0.125, ret=-0.00188, glen=186, tlen=346, kl=0, act_lr=2.6e-7, ent=1.42]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.14it/s, pg=0.125, ret=-0.00188, glen=186, tlen=346, kl=0, act_lr=2.6e-7, ent=1.42]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.14it/s, pg=0.028, ret=6.72e-5, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.83] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.15it/s, pg=0.028, ret=6.72e-5, glen=132, tlen=293, kl=0, act_lr=2.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.15it/s, pg=0.022, ret=-0.000282, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=2.34]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.16it/s, pg=0.022, ret=-0.000282, glen=122, tlen=283, kl=0, act_lr=2.6e-7, ent=2.34]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.16it/s, pg=0.0121, ret=0.000783, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.87]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:06,  1.16it/s, pg=0.0121, ret=0.000783, glen=112, tlen=273, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:06,  1.16it/s, pg=0.101, ret=-0.000986, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.101, ret=-0.000986, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.0901, ret=-0.00111, glen=123, tlen=284, kl=0, act_lr=2.6e-7, ent=1.75]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.17it/s, pg=0.0901, ret=-0.00111, glen=123, tlen=284, kl=0, act_lr=2.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.17it/s, pg=-0.032, ret=-0.000552, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.79]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.17it/s, pg=-0.032, ret=-0.000552, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=-0.117, ret=0.00115, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.63]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=-0.117, ret=0.00115, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=-0.0551, ret=-0.000221, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.76]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.17it/s, pg=-0.0551, ret=-0.000221, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.17it/s, pg=-0.133, ret=0.00125, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.56]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.17it/s, pg=-0.133, ret=0.00125, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.56]
2025-07-24 16:56:37.139 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 50.35s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.17it/s, pg=-0.0843, ret=0.000149, glen=108, tlen=269, kl=0, act_lr=2.8e-7, ent=1.7]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.0843, ret=0.000149, glen=108, tlen=269, kl=0, act_lr=2.8e-7, ent=1.7]
2025-07-24 16:56:37.974 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-24 16:56:40.579 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 16:56:40.908 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 54.24s
2025-07-24 16:56:40.915 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.018242244062752558, 'actor_lr': 2.6034483467136506e-07, 'clip_ratio': 0.0, 'entropy': 1.752143958519245, 'kl': 0.0, 'response_length': 120.72242802587049, 'total_length': 281.317540662042, 'teacher_total_length': 292.39703947922277, 'return': -6.165954067094797e-05, 'policy_update_steps': 1.0}

Episode [2/20]:   8%|‚ñä         | 1/13 [04:12<50:26, 252.20s/it][A2025-07-24 16:56:40.963 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 16:58:48.154 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 16:58:48.328 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-24 16:58:48.328 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 127.37s
2025-07-24 16:58:50.256 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0093,avg_pass_at_n: 1.0000,avg_num_tokens: 110.9402,std_num_tokens: 136.6484,avg_correct_num_tokens: 102.8409,std_correct_num_tokens: 80.3895,avg_incorrect_num_tokens: 123.4079,std_incorrect_num_tokens: 192.9054
2025-07-24 16:58:50.708 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 16:58:53.627 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.92s
2025-07-24 16:59:22.448 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 16:59:22.448 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.82s
2025-07-24 16:59:24.284 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.34s
2025-07-24 16:59:24.284 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0006905885425853719, avg_kl: 0.0009041336664544329, avg_response_length: 113.81047716854953, avg_orm_score: 0.0, avg_custom_rewards: -0.0006905885425853719
2025-07-24 16:59:24.324 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter14_replay_buffer.jsonl
2025-07-24 16:59:26.237 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.92s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0193, ret=-0.00029, glen=130, tlen=291, kl=0.000899, act_lr=2.8e-7, ent=1.82]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.02s/it, pg=0.0193, ret=-0.00029, glen=130, tlen=291, kl=0.000899, act_lr=2.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.02s/it, pg=0.156, ret=-0.00196, glen=93.7, tlen=254, kl=0.000931, act_lr=2.8e-7, ent=1.62]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.08it/s, pg=0.156, ret=-0.00196, glen=93.7, tlen=254, kl=0.000931, act_lr=2.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.08it/s, pg=-0.124, ret=9.82e-6, glen=120, tlen=280, kl=0.000926, act_lr=2.8e-7, ent=1.93] Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.10it/s, pg=-0.124, ret=9.82e-6, glen=120, tlen=280, kl=0.000926, act_lr=2.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.10it/s, pg=-0.147, ret=0.000743, glen=104, tlen=265, kl=0.000936, act_lr=2.8e-7, ent=1.71]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:48,  1.10it/s, pg=-0.147, ret=0.000743, glen=104, tlen=265, kl=0.000936, act_lr=2.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:48,  1.10it/s, pg=-0.114, ret=0.000342, glen=95.3, tlen=256, kl=0.000895, act_lr=2.8e-7, ent=1.59]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.12it/s, pg=-0.114, ret=0.000342, glen=95.3, tlen=256, kl=0.000895, act_lr=2.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.12it/s, pg=-0.121, ret=-0.00448, glen=98.3, tlen=260, kl=0.000918, act_lr=2.8e-7, ent=1.7] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:46,  1.10it/s, pg=-0.121, ret=-0.00448, glen=98.3, tlen=260, kl=0.000918, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:46,  1.10it/s, pg=-0.188, ret=0.00118, glen=103, tlen=264, kl=0.000904, act_lr=2.8e-7, ent=1.69] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.12it/s, pg=-0.188, ret=0.00118, glen=103, tlen=264, kl=0.000904, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.12it/s, pg=-0.0248, ret=0.000862, glen=121, tlen=282, kl=0.000915, act_lr=2.8e-7, ent=2.06]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.14it/s, pg=-0.0248, ret=0.000862, glen=121, tlen=282, kl=0.000915, act_lr=2.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.14it/s, pg=0.0918, ret=-0.000369, glen=102, tlen=262, kl=0.000957, act_lr=2.8e-7, ent=1.7] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.0918, ret=-0.000369, glen=102, tlen=262, kl=0.000957, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.142, ret=0.000965, glen=99.6, tlen=260, kl=0.000934, act_lr=2.8e-7, ent=1.59]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=-0.142, ret=0.000965, glen=99.6, tlen=260, kl=0.000934, act_lr=2.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=0.12, ret=-0.00182, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.61]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.12, ret=-0.00182, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0225, ret=-0.00157, glen=100, tlen=261, kl=0.000946, act_lr=2.8e-7, ent=1.69]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=-0.0225, ret=-0.00157, glen=100, tlen=261, kl=0.000946, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.092, ret=-0.000109, glen=94.5, tlen=255, kl=0.000902, act_lr=2.8e-7, ent=1.64]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.092, ret=-0.000109, glen=94.5, tlen=255, kl=0.000902, act_lr=2.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.0952, ret=0.000228, glen=104, tlen=264, kl=0.000929, act_lr=2.8e-7, ent=1.67] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.15it/s, pg=-0.0952, ret=0.000228, glen=104, tlen=264, kl=0.000929, act_lr=2.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.15it/s, pg=-0.159, ret=0.00125, glen=116, tlen=277, kl=0.00096, act_lr=2.8e-7, ent=1.88]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.159, ret=0.00125, glen=116, tlen=277, kl=0.00096, act_lr=2.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=-0.222, ret=0.000263, glen=115, tlen=276, kl=0.000889, act_lr=2.8e-7, ent=1.75]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=-0.222, ret=0.000263, glen=115, tlen=276, kl=0.000889, act_lr=2.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=0.308, ret=-0.00168, glen=140, tlen=301, kl=0.000789, act_lr=2.8e-7, ent=2.5]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.16it/s, pg=0.308, ret=-0.00168, glen=140, tlen=301, kl=0.000789, act_lr=2.8e-7, ent=2.5]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.16it/s, pg=0.152, ret=-0.00189, glen=116, tlen=276, kl=0.000853, act_lr=2.8e-7, ent=1.88]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=0.152, ret=-0.00189, glen=116, tlen=276, kl=0.000853, act_lr=2.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=0.198, ret=-0.00172, glen=102, tlen=262, kl=0.000895, act_lr=2.8e-7, ent=1.72]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=0.198, ret=-0.00172, glen=102, tlen=262, kl=0.000895, act_lr=2.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.117, ret=0.000665, glen=125, tlen=285, kl=0.000928, act_lr=2.8e-7, ent=1.9]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.117, ret=0.000665, glen=125, tlen=285, kl=0.000928, act_lr=2.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=0.132, ret=-0.000176, glen=113, tlen=274, kl=0.000902, act_lr=2.8e-7, ent=1.94]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.132, ret=-0.000176, glen=113, tlen=274, kl=0.000902, act_lr=2.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.269, ret=0.00174, glen=110, tlen=270, kl=0.000903, act_lr=2.8e-7, ent=1.58] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.269, ret=0.00174, glen=110, tlen=270, kl=0.000903, act_lr=2.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=0.000916, ret=0.000205, glen=119, tlen=280, kl=0.000903, act_lr=2.8e-7, ent=1.61]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.000916, ret=0.000205, glen=119, tlen=280, kl=0.000903, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.117, ret=-0.0026, glen=115, tlen=277, kl=0.00093, act_lr=2.8e-7, ent=1.74]     Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.117, ret=-0.0026, glen=115, tlen=277, kl=0.00093, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.025, ret=0.00111, glen=110, tlen=270, kl=0.000908, act_lr=2.8e-7, ent=1.65]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.025, ret=0.00111, glen=110, tlen=270, kl=0.000908, act_lr=2.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0581, ret=0.000316, glen=120, tlen=281, kl=0.000909, act_lr=2.8e-7, ent=1.89]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0581, ret=0.000316, glen=120, tlen=281, kl=0.000909, act_lr=2.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.179, ret=0.00196, glen=105, tlen=266, kl=0.000912, act_lr=2.8e-7, ent=1.79]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.18it/s, pg=-0.179, ret=0.00196, glen=105, tlen=266, kl=0.000912, act_lr=2.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.18it/s, pg=-0.0406, ret=0.000538, glen=103, tlen=263, kl=0.000919, act_lr=2.8e-7, ent=1.57]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.18it/s, pg=-0.0406, ret=0.000538, glen=103, tlen=263, kl=0.000919, act_lr=2.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.18it/s, pg=-0.0606, ret=0.000437, glen=98.1, tlen=259, kl=0.00091, act_lr=2.8e-7, ent=1.63]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.07it/s, pg=-0.0606, ret=0.000437, glen=98.1, tlen=259, kl=0.00091, act_lr=2.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.07it/s, pg=-0.087, ret=9.55e-5, glen=102, tlen=263, kl=0.000868, act_lr=2.8e-7, ent=1.58]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.10it/s, pg=-0.087, ret=9.55e-5, glen=102, tlen=263, kl=0.000868, act_lr=2.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.10it/s, pg=0.2, ret=-0.00238, glen=113, tlen=274, kl=0.000948, act_lr=2.8e-7, ent=1.78]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=0.2, ret=-0.00238, glen=113, tlen=274, kl=0.000948, act_lr=2.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.00183, ret=-0.000425, glen=109, tlen=269, kl=0.000917, act_lr=2.8e-7, ent=1.6]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.14it/s, pg=-0.00183, ret=-0.000425, glen=109, tlen=269, kl=0.000917, act_lr=2.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=0.0249, ret=0.000216, glen=132, tlen=293, kl=0.000913, act_lr=2.8e-7, ent=1.97]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=0.0249, ret=0.000216, glen=132, tlen=293, kl=0.000913, act_lr=2.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=-0.0125, ret=0.000189, glen=101, tlen=262, kl=0.000877, act_lr=2.8e-7, ent=1.59]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.13it/s, pg=-0.0125, ret=0.000189, glen=101, tlen=262, kl=0.000877, act_lr=2.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.13it/s, pg=-0.0887, ret=-0.000223, glen=104, tlen=265, kl=0.000878, act_lr=2.8e-7, ent=1.76]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=-0.0887, ret=-0.000223, glen=104, tlen=265, kl=0.000878, act_lr=2.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=0.0115, ret=-0.00103, glen=104, tlen=265, kl=0.000889, act_lr=2.8e-7, ent=1.67]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=0.0115, ret=-0.00103, glen=104, tlen=265, kl=0.000889, act_lr=2.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.0695, ret=0.00129, glen=121, tlen=282, kl=0.000887, act_lr=2.8e-7, ent=1.74]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.0695, ret=0.00129, glen=121, tlen=282, kl=0.000887, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.152, ret=0.0011, glen=112, tlen=273, kl=0.000863, act_lr=2.8e-7, ent=1.94]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=-0.152, ret=0.0011, glen=112, tlen=273, kl=0.000863, act_lr=2.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.16it/s, pg=0.239, ret=-0.00133, glen=110, tlen=271, kl=0.000948, act_lr=2.8e-7, ent=1.7]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.239, ret=-0.00133, glen=110, tlen=271, kl=0.000948, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.00172, ret=4.96e-5, glen=104, tlen=264, kl=0.000927, act_lr=2.8e-7, ent=1.71]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.00172, ret=4.96e-5, glen=104, tlen=264, kl=0.000927, act_lr=2.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.092, ret=0.00148, glen=110, tlen=271, kl=0.000925, act_lr=2.8e-7, ent=1.76]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.092, ret=0.00148, glen=110, tlen=271, kl=0.000925, act_lr=2.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.21, ret=-0.00164, glen=120, tlen=281, kl=0.000881, act_lr=2.8e-7, ent=1.63] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.21, ret=-0.00164, glen=120, tlen=281, kl=0.000881, act_lr=2.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.061, ret=-0.000327, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.82]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.061, ret=-0.000327, glen=108, tlen=269, kl=0.000944, act_lr=2.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.104, ret=0.000491, glen=113, tlen=274, kl=0.000908, act_lr=2.8e-7, ent=1.61] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.104, ret=0.000491, glen=113, tlen=274, kl=0.000908, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.111, ret=0.00087, glen=115, tlen=276, kl=0.000896, act_lr=2.8e-7, ent=1.94] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.111, ret=0.00087, glen=115, tlen=276, kl=0.000896, act_lr=2.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=6.1e-5, ret=-0.00167, glen=114, tlen=275, kl=0.000909, act_lr=2.8e-7, ent=1.57]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=6.1e-5, ret=-0.00167, glen=114, tlen=275, kl=0.000909, act_lr=2.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.352, ret=0.00033, glen=169, tlen=330, kl=0.000752, act_lr=2.8e-7, ent=2.41]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.352, ret=0.00033, glen=169, tlen=330, kl=0.000752, act_lr=2.8e-7, ent=2.41]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.229, ret=0.000519, glen=227, tlen=387, kl=0.00079, act_lr=2.8e-7, ent=2.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.15it/s, pg=0.229, ret=0.000519, glen=227, tlen=387, kl=0.00079, act_lr=2.8e-7, ent=2.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.15it/s, pg=0.077, ret=0.00179, glen=138, tlen=299, kl=0.000861, act_lr=2.8e-7, ent=2.07]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.16it/s, pg=0.077, ret=0.00179, glen=138, tlen=299, kl=0.000861, act_lr=2.8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.16it/s, pg=0.129, ret=-0.00103, glen=125, tlen=286, kl=0.000898, act_lr=2.8e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.16it/s, pg=0.129, ret=-0.00103, glen=125, tlen=286, kl=0.000898, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.16it/s, pg=-0.144, ret=0.00121, glen=120, tlen=281, kl=0.000937, act_lr=2.8e-7, ent=1.7] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.144, ret=0.00121, glen=120, tlen=281, kl=0.000937, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=-0.0637, ret=0.000135, glen=114, tlen=275, kl=0.000937, act_lr=2.8e-7, ent=1.78]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0637, ret=0.000135, glen=114, tlen=275, kl=0.000937, act_lr=2.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=-0.205, ret=0.00113, glen=105, tlen=266, kl=0.000887, act_lr=2.8e-7, ent=1.69]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.205, ret=0.00113, glen=105, tlen=266, kl=0.000887, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.0141, ret=-0.000547, glen=108, tlen=269, kl=0.000937, act_lr=2.8e-7, ent=1.74]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=-0.0141, ret=-0.000547, glen=108, tlen=269, kl=0.000937, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.102, ret=0.000407, glen=101, tlen=262, kl=0.000902, act_lr=2.8e-7, ent=1.6]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.102, ret=0.000407, glen=101, tlen=262, kl=0.000902, act_lr=2.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0684, ret=-0.000756, glen=109, tlen=269, kl=0.000924, act_lr=2.8e-7, ent=1.8]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0684, ret=-0.000756, glen=109, tlen=269, kl=0.000924, act_lr=2.8e-7, ent=1.8]
2025-07-24 17:00:16.021 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.0094, ret=-0.000802, glen=93.7, tlen=255, kl=0.000897, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.0094, ret=-0.000802, glen=93.7, tlen=255, kl=0.000897, act_lr=3e-7, ent=1.54]
2025-07-24 17:00:16.879 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:00:19.398 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-24 17:00:19.724 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.38s
2025-07-24 17:00:19.732 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.012011912831088952, 'actor_lr': 2.8035087221692183e-07, 'clip_ratio': 0.0, 'entropy': 1.7733097954800254, 'kl': 0.000904317487750137, 'response_length': 113.70133142304002, 'total_length': 274.47614649722453, 'teacher_total_length': 287.0512256287692, 'return': -0.00011782957128097973, 'policy_update_steps': 1.0}

Episode [2/20]:  15%|‚ñà‚ñå        | 2/13 [07:51<42:38, 232.57s/it][A2025-07-24 17:00:19.776 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:01:53.455 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:01:53.639 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:01:53.639 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 93.86s
2025-07-24 17:01:55.582 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0083,avg_pass_at_n: 1.0000,avg_num_tokens: 108.7333,std_num_tokens: 114.1218,avg_correct_num_tokens: 100.7200,std_correct_num_tokens: 84.5683,avg_incorrect_num_tokens: 123.2784,std_incorrect_num_tokens: 152.8202
2025-07-24 17:01:55.880 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.24s
2025-07-24 17:01:58.820 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 2.94s
2025-07-24 17:02:27.188 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:02:27.189 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.37s
2025-07-24 17:02:28.798 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.14s
2025-07-24 17:02:28.799 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0009074466998089572, avg_kl: 0.0010004257407423627, avg_response_length: 110.29346431851921, avg_orm_score: 0.0, avg_custom_rewards: -0.0009074466998089572
2025-07-24 17:02:28.857 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter15_replay_buffer.jsonl
2025-07-24 17:02:30.761 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0857, ret=-8.2e-5, glen=121, tlen=281, kl=0.000998, act_lr=3e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:57,  1.04s/it, pg=0.0857, ret=-8.2e-5, glen=121, tlen=281, kl=0.000998, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:57,  1.04s/it, pg=0.00806, ret=-0.00193, glen=107, tlen=268, kl=0.001, act_lr=3e-7, ent=1.74] Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.07it/s, pg=0.00806, ret=-0.00193, glen=107, tlen=268, kl=0.001, act_lr=3e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.07it/s, pg=0.153, ret=-0.000369, glen=118, tlen=278, kl=0.000999, act_lr=3e-7, ent=1.99]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.11it/s, pg=0.153, ret=-0.000369, glen=118, tlen=278, kl=0.000999, act_lr=3e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.11it/s, pg=0.145, ret=0.000795, glen=124, tlen=285, kl=0.000955, act_lr=3e-7, ent=1.66] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:45,  1.13it/s, pg=0.145, ret=0.000795, glen=124, tlen=285, kl=0.000955, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:45,  1.13it/s, pg=0.108, ret=-0.00145, glen=94.3, tlen=254, kl=0.000965, act_lr=3e-7, ent=1.63]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=0.108, ret=-0.00145, glen=94.3, tlen=254, kl=0.000965, act_lr=3e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=0.13, ret=-0.00181, glen=126, tlen=287, kl=0.000984, act_lr=3e-7, ent=1.89]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.14it/s, pg=0.13, ret=-0.00181, glen=126, tlen=287, kl=0.000984, act_lr=3e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.14it/s, pg=-0.0962, ret=-0.00144, glen=112, tlen=272, kl=0.00101, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=-0.0962, ret=-0.00144, glen=112, tlen=272, kl=0.00101, act_lr=3e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=0.0673, ret=-0.00165, glen=98.8, tlen=259, kl=0.00108, act_lr=3e-7, ent=1.66]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.0673, ret=-0.00165, glen=98.8, tlen=259, kl=0.00108, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.167, ret=-0.00193, glen=113, tlen=273, kl=0.000993, act_lr=3e-7, ent=1.7]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.16it/s, pg=0.167, ret=-0.00193, glen=113, tlen=273, kl=0.000993, act_lr=3e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=0.152, ret=-0.000931, glen=125, tlen=285, kl=0.00093, act_lr=3e-7, ent=1.94]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=0.152, ret=-0.000931, glen=125, tlen=285, kl=0.00093, act_lr=3e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=0.0105, ret=-0.000132, glen=104, tlen=264, kl=0.00108, act_lr=3e-7, ent=1.74]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=0.0105, ret=-0.000132, glen=104, tlen=264, kl=0.00108, act_lr=3e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.0103, ret=0.00046, glen=108, tlen=269, kl=0.000982, act_lr=3e-7, ent=1.69]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.0103, ret=0.00046, glen=108, tlen=269, kl=0.000982, act_lr=3e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0554, ret=0.00057, glen=117, tlen=278, kl=0.000978, act_lr=3e-7, ent=1.71] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0554, ret=0.00057, glen=117, tlen=278, kl=0.000978, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=-0.0176, ret=-0.00144, glen=105, tlen=265, kl=0.00102, act_lr=3e-7, ent=1.71]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=-0.0176, ret=-0.00144, glen=105, tlen=265, kl=0.00102, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.199, ret=-1.44e-5, glen=123, tlen=284, kl=0.000886, act_lr=3e-7, ent=1.98]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.199, ret=-1.44e-5, glen=123, tlen=284, kl=0.000886, act_lr=3e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.0967, ret=0.0017, glen=138, tlen=298, kl=0.000977, act_lr=3e-7, ent=2.16] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.16it/s, pg=-0.0967, ret=0.0017, glen=138, tlen=298, kl=0.000977, act_lr=3e-7, ent=2.16]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.063, ret=0.000135, glen=120, tlen=280, kl=0.00105, act_lr=3e-7, ent=1.82]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.16it/s, pg=-0.063, ret=0.000135, glen=120, tlen=280, kl=0.00105, act_lr=3e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.156, ret=0.000398, glen=110, tlen=270, kl=0.000978, act_lr=3e-7, ent=1.69]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.156, ret=0.000398, glen=110, tlen=270, kl=0.000978, act_lr=3e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.201, ret=0.00138, glen=109, tlen=270, kl=0.00101, act_lr=3e-7, ent=1.76]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.201, ret=0.00138, glen=109, tlen=270, kl=0.00101, act_lr=3e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.0329, ret=0.00167, glen=125, tlen=286, kl=0.001, act_lr=3e-7, ent=1.99] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=-0.0329, ret=0.00167, glen=125, tlen=286, kl=0.001, act_lr=3e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.223, ret=0.000759, glen=104, tlen=264, kl=0.00103, act_lr=3e-7, ent=1.86]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.223, ret=0.000759, glen=104, tlen=264, kl=0.00103, act_lr=3e-7, ent=1.86]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.0745, ret=0.00113, glen=106, tlen=266, kl=0.00102, act_lr=3e-7, ent=1.66]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=-0.0745, ret=0.00113, glen=106, tlen=266, kl=0.00102, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=-0.068, ret=0.000485, glen=103, tlen=263, kl=0.00102, act_lr=3e-7, ent=1.6] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.18it/s, pg=-0.068, ret=0.000485, glen=103, tlen=263, kl=0.00102, act_lr=3e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.18it/s, pg=-0.333, ret=0.00254, glen=97.8, tlen=258, kl=0.00106, act_lr=3e-7, ent=1.75]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.333, ret=0.00254, glen=97.8, tlen=258, kl=0.00106, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.0176, ret=0.0004, glen=110, tlen=270, kl=0.00103, act_lr=3e-7, ent=1.7]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0176, ret=0.0004, glen=110, tlen=270, kl=0.00103, act_lr=3e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.082, ret=-0.00287, glen=109, tlen=269, kl=0.001, act_lr=3e-7, ent=1.71] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.082, ret=-0.00287, glen=109, tlen=269, kl=0.001, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.105, ret=-0.000728, glen=94.4, tlen=255, kl=0.001, act_lr=3e-7, ent=1.58]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.105, ret=-0.000728, glen=94.4, tlen=255, kl=0.001, act_lr=3e-7, ent=1.58]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0679, ret=0.000288, glen=106, tlen=266, kl=0.001, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.18it/s, pg=-0.0679, ret=0.000288, glen=106, tlen=266, kl=0.001, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.18it/s, pg=0.0962, ret=0.000213, glen=114, tlen=274, kl=0.000996, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=0.0962, ret=0.000213, glen=114, tlen=274, kl=0.000996, act_lr=3e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.0581, ret=0.000558, glen=97.7, tlen=258, kl=0.000989, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.0581, ret=0.000558, glen=97.7, tlen=258, kl=0.000989, act_lr=3e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.0338, ret=-0.000136, glen=108, tlen=268, kl=0.000981, act_lr=3e-7, ent=1.67] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:26<00:22,  1.12it/s, pg=0.0338, ret=-0.000136, glen=108, tlen=268, kl=0.000981, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.0797, ret=-0.000401, glen=103, tlen=263, kl=0.000977, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.12it/s, pg=0.0797, ret=-0.000401, glen=103, tlen=263, kl=0.000977, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.12it/s, pg=-0.00641, ret=0.00123, glen=110, tlen=270, kl=0.001, act_lr=3e-7, ent=1.67]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.12it/s, pg=-0.00641, ret=0.00123, glen=110, tlen=270, kl=0.001, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.12it/s, pg=0.00464, ret=-0.000319, glen=118, tlen=278, kl=0.001, act_lr=3e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.12it/s, pg=0.00464, ret=-0.000319, glen=118, tlen=278, kl=0.001, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.12it/s, pg=0.0905, ret=-0.000405, glen=119, tlen=279, kl=0.00101, act_lr=3e-7, ent=1.78]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.14it/s, pg=0.0905, ret=-0.000405, glen=119, tlen=279, kl=0.00101, act_lr=3e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.14it/s, pg=-0.136, ret=0.000559, glen=97.3, tlen=258, kl=0.00101, act_lr=3e-7, ent=1.53]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=-0.136, ret=0.000559, glen=97.3, tlen=258, kl=0.00101, act_lr=3e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=0.049, ret=0.000669, glen=136, tlen=296, kl=0.000963, act_lr=3e-7, ent=1.87] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.15it/s, pg=0.049, ret=0.000669, glen=136, tlen=296, kl=0.000963, act_lr=3e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.15it/s, pg=0.112, ret=-0.000923, glen=96.2, tlen=256, kl=0.00101, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.112, ret=-0.000923, glen=96.2, tlen=256, kl=0.00101, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0203, ret=-0.00122, glen=108, tlen=268, kl=0.000957, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0203, ret=-0.00122, glen=108, tlen=268, kl=0.000957, act_lr=3e-7, ent=1.54]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=0.0566, ret=-0.00011, glen=111, tlen=270, kl=0.001, act_lr=3e-7, ent=1.62]   Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=0.0566, ret=-0.00011, glen=111, tlen=270, kl=0.001, act_lr=3e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.103, ret=-0.000102, glen=114, tlen=275, kl=0.00105, act_lr=3e-7, ent=1.79]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.103, ret=-0.000102, glen=114, tlen=275, kl=0.00105, act_lr=3e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.103, ret=0.00104, glen=97.4, tlen=258, kl=0.000996, act_lr=3e-7, ent=1.65]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.103, ret=0.00104, glen=97.4, tlen=258, kl=0.000996, act_lr=3e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.111, ret=-4.74e-5, glen=98.2, tlen=258, kl=0.00102, act_lr=3e-7, ent=1.72]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.111, ret=-4.74e-5, glen=98.2, tlen=258, kl=0.00102, act_lr=3e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.0845, ret=0.000954, glen=121, tlen=281, kl=0.00103, act_lr=3e-7, ent=1.94]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0845, ret=0.000954, glen=121, tlen=281, kl=0.00103, act_lr=3e-7, ent=1.94]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0912, ret=-7.66e-6, glen=100, tlen=261, kl=0.00105, act_lr=3e-7, ent=1.63]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.0912, ret=-7.66e-6, glen=100, tlen=261, kl=0.00105, act_lr=3e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.18it/s, pg=-0.369, ret=0.00192, glen=94.5, tlen=255, kl=0.00104, act_lr=3e-7, ent=1.55] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.18it/s, pg=-0.369, ret=0.00192, glen=94.5, tlen=255, kl=0.00104, act_lr=3e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.18it/s, pg=-0.102, ret=-0.00056, glen=93.6, tlen=253, kl=0.00102, act_lr=3e-7, ent=1.6]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.102, ret=-0.00056, glen=93.6, tlen=253, kl=0.00102, act_lr=3e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.109, ret=-0.00215, glen=112, tlen=273, kl=0.00096, act_lr=3e-7, ent=1.55] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.109, ret=-0.00215, glen=112, tlen=273, kl=0.00096, act_lr=3e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.00894, ret=0.00136, glen=108, tlen=268, kl=0.00101, act_lr=3e-7, ent=1.82]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.00894, ret=0.00136, glen=108, tlen=268, kl=0.00101, act_lr=3e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.132, ret=0.000867, glen=120, tlen=280, kl=0.000981, act_lr=3e-7, ent=1.6] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.132, ret=0.000867, glen=120, tlen=280, kl=0.000981, act_lr=3e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0342, ret=-0.000824, glen=109, tlen=269, kl=0.00101, act_lr=3e-7, ent=1.67]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0342, ret=-0.000824, glen=109, tlen=269, kl=0.00101, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0789, ret=-0.000533, glen=120, tlen=281, kl=0.000966, act_lr=3e-7, ent=1.69]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=-0.0789, ret=-0.000533, glen=120, tlen=281, kl=0.000966, act_lr=3e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.157, ret=2.5e-5, glen=103, tlen=263, kl=0.00104, act_lr=3e-7, ent=1.76]     Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.157, ret=2.5e-5, glen=103, tlen=263, kl=0.00104, act_lr=3e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.11, ret=-0.000137, glen=109, tlen=269, kl=0.000966, act_lr=3e-7, ent=1.64]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=0.11, ret=-0.000137, glen=109, tlen=269, kl=0.000966, act_lr=3e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.000244, ret=0.00159, glen=119, tlen=280, kl=0.00101, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=-0.000244, ret=0.00159, glen=119, tlen=280, kl=0.00101, act_lr=3e-7, ent=1.81]
2025-07-24 17:03:19.615 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.0649, ret=-0.00145, glen=106, tlen=266, kl=0.000973, act_lr=3.2e-7, ent=1.76]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=0.0649, ret=-0.00145, glen=106, tlen=266, kl=0.000973, act_lr=3.2e-7, ent=1.76]
2025-07-24 17:03:20.652 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.97s
2025-07-24 17:03:23.253 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 17:03:23.580 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.56s
2025-07-24 17:03:23.587 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011712959834507533, 'actor_lr': 3.003571532441843e-07, 'clip_ratio': 0.0, 'entropy': 1.7289215901068278, 'kl': 0.001000523567199707, 'response_length': 110.19897638048444, 'total_length': 270.40987750462125, 'teacher_total_length': 282.2843802315848, 'return': -4.291131816509213e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [10:54<35:03, 210.32s/it][A2025-07-24 17:03:23.630 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:05:53.580 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:05:53.764 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:05:53.765 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 150.14s
2025-07-24 17:05:55.755 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0074,avg_pass_at_n: 1.0000,avg_num_tokens: 111.1066,std_num_tokens: 174.6282,avg_correct_num_tokens: 102.2503,std_correct_num_tokens: 93.7130,avg_incorrect_num_tokens: 127.2418,std_incorrect_num_tokens: 263.9140
2025-07-24 17:05:56.201 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.44s
2025-07-24 17:05:59.618 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.42s
2025-07-24 17:06:28.701 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 226
2025-07-24 17:06:28.706 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.08s
2025-07-24 17:06:30.151 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.02s
2025-07-24 17:06:30.152 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0028858509378825044, avg_kl: 0.001027183195131015, avg_response_length: 119.2440990009139, avg_orm_score: 0.0, avg_custom_rewards: -0.0028858509378825044
2025-07-24 17:06:30.193 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter16_replay_buffer.jsonl
2025-07-24 17:06:32.094 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.90s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.059, ret=-0.00195, glen=110, tlen=270, kl=0.00101, act_lr=3.2e-7, ent=1.63]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.059, ret=-0.00195, glen=110, tlen=270, kl=0.00101, act_lr=3.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.159, ret=-0.00141, glen=119, tlen=280, kl=0.00102, act_lr=3.2e-7, ent=2.12]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.159, ret=-0.00141, glen=119, tlen=280, kl=0.00102, act_lr=3.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=-0.0616, ret=-0.000482, glen=103, tlen=264, kl=0.00101, act_lr=3.2e-7, ent=1.8]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=-0.0616, ret=-0.000482, glen=103, tlen=264, kl=0.00101, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=0.137, ret=-0.00139, glen=107, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.71]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=0.137, ret=-0.00139, glen=107, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=-0.052, ret=-0.00256, glen=106, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.68]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.052, ret=-0.00256, glen=106, tlen=267, kl=0.00104, act_lr=3.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=-0.0311, ret=-0.0021, glen=109, tlen=269, kl=0.00104, act_lr=3.2e-7, ent=1.66]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=-0.0311, ret=-0.0021, glen=109, tlen=269, kl=0.00104, act_lr=3.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.168, ret=0.00126, glen=113, tlen=273, kl=0.00106, act_lr=3.2e-7, ent=1.7]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.14it/s, pg=-0.168, ret=0.00126, glen=113, tlen=273, kl=0.00106, act_lr=3.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.14it/s, pg=-0.267, ret=0.00125, glen=119, tlen=280, kl=0.00104, act_lr=3.2e-7, ent=2.01]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=-0.267, ret=0.00125, glen=119, tlen=280, kl=0.00104, act_lr=3.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:42,  1.15it/s, pg=-0.123, ret=0.000276, glen=96.3, tlen=257, kl=0.00105, act_lr=3.2e-7, ent=1.64]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.12it/s, pg=-0.123, ret=0.000276, glen=96.3, tlen=257, kl=0.00105, act_lr=3.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.12it/s, pg=-0.0805, ret=0.000641, glen=110, tlen=271, kl=0.00106, act_lr=3.2e-7, ent=1.78]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.13it/s, pg=-0.0805, ret=0.000641, glen=110, tlen=271, kl=0.00106, act_lr=3.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.13it/s, pg=-0.0139, ret=0.000428, glen=118, tlen=279, kl=0.00102, act_lr=3.2e-7, ent=1.69]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.14it/s, pg=-0.0139, ret=0.000428, glen=118, tlen=279, kl=0.00102, act_lr=3.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.14it/s, pg=0.011, ret=-6.8e-5, glen=111, tlen=272, kl=0.00102, act_lr=3.2e-7, ent=1.75]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=0.011, ret=-6.8e-5, glen=111, tlen=272, kl=0.00102, act_lr=3.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=0.148, ret=-0.000256, glen=201, tlen=360, kl=0.00102, act_lr=3.2e-7, ent=2.25]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.14it/s, pg=0.148, ret=-0.000256, glen=201, tlen=360, kl=0.00102, act_lr=3.2e-7, ent=2.25]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.14it/s, pg=-0.133, ret=0.000837, glen=106, tlen=266, kl=0.00108, act_lr=3.2e-7, ent=1.73]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=-0.133, ret=0.000837, glen=106, tlen=266, kl=0.00108, act_lr=3.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=-0.196, ret=0.000923, glen=90.1, tlen=250, kl=0.00106, act_lr=3.2e-7, ent=1.57]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.196, ret=0.000923, glen=90.1, tlen=250, kl=0.00106, act_lr=3.2e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=-0.173, ret=0.0012, glen=118, tlen=278, kl=0.00103, act_lr=3.2e-7, ent=1.84]   Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:36,  1.12it/s, pg=-0.173, ret=0.0012, glen=118, tlen=278, kl=0.00103, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:15<00:36,  1.12it/s, pg=-0.0416, ret=-0.000252, glen=103, tlen=263, kl=0.00104, act_lr=3.2e-7, ent=1.61]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.0416, ret=-0.000252, glen=103, tlen=263, kl=0.00104, act_lr=3.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.14it/s, pg=-0.083, ret=-0.000116, glen=110, tlen=270, kl=0.00102, act_lr=3.2e-7, ent=1.87] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.083, ret=-0.000116, glen=110, tlen=270, kl=0.00102, act_lr=3.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=0.121, ret=-0.000823, glen=124, tlen=285, kl=0.00106, act_lr=3.2e-7, ent=2.04] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.121, ret=-0.000823, glen=124, tlen=285, kl=0.00106, act_lr=3.2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.072, ret=0.000149, glen=104, tlen=265, kl=0.00102, act_lr=3.2e-7, ent=1.83]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.072, ret=0.000149, glen=104, tlen=265, kl=0.00102, act_lr=3.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.168, ret=0.00113, glen=104, tlen=265, kl=0.00107, act_lr=3.2e-7, ent=1.74] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.16it/s, pg=-0.168, ret=0.00113, glen=104, tlen=265, kl=0.00107, act_lr=3.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.16it/s, pg=-0.114, ret=0.000862, glen=103, tlen=263, kl=0.00101, act_lr=3.2e-7, ent=1.65]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.114, ret=0.000862, glen=103, tlen=263, kl=0.00101, act_lr=3.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.147, ret=0.00235, glen=116, tlen=276, kl=0.00103, act_lr=3.2e-7, ent=2.07] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.147, ret=0.00235, glen=116, tlen=276, kl=0.00103, act_lr=3.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:21<00:29,  1.17it/s, pg=0.12, ret=-0.00126, glen=101, tlen=261, kl=0.00108, act_lr=3.2e-7, ent=1.78] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.12, ret=-0.00126, glen=101, tlen=261, kl=0.00108, act_lr=3.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0222, ret=5.42e-5, glen=112, tlen=272, kl=0.00106, act_lr=3.2e-7, ent=1.81]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0222, ret=5.42e-5, glen=112, tlen=272, kl=0.00106, act_lr=3.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.192, ret=-0.00131, glen=115, tlen=275, kl=0.000993, act_lr=3.2e-7, ent=1.9] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.192, ret=-0.00131, glen=115, tlen=275, kl=0.000993, act_lr=3.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.0276, ret=0.00153, glen=111, tlen=271, kl=0.00101, act_lr=3.2e-7, ent=2.1]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=-0.0276, ret=0.00153, glen=111, tlen=271, kl=0.00101, act_lr=3.2e-7, ent=2.1]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=0.0979, ret=0.000566, glen=145, tlen=305, kl=0.000959, act_lr=3.2e-7, ent=2.67]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.16it/s, pg=0.0979, ret=0.000566, glen=145, tlen=305, kl=0.000959, act_lr=3.2e-7, ent=2.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.16it/s, pg=0.00751, ret=-0.00134, glen=115, tlen=275, kl=0.00103, act_lr=3.2e-7, ent=1.82]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=0.00751, ret=-0.00134, glen=115, tlen=275, kl=0.00103, act_lr=3.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.105, ret=0.000243, glen=109, tlen=269, kl=0.00109, act_lr=3.2e-7, ent=1.84] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.09it/s, pg=-0.105, ret=0.000243, glen=109, tlen=269, kl=0.00109, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.09it/s, pg=-0.251, ret=0.00212, glen=104, tlen=264, kl=0.00108, act_lr=3.2e-7, ent=1.62] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.11it/s, pg=-0.251, ret=0.00212, glen=104, tlen=264, kl=0.00108, act_lr=3.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.11it/s, pg=-0.0129, ret=-0.000268, glen=111, tlen=271, kl=0.00102, act_lr=3.2e-7, ent=1.98]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=-0.0129, ret=-0.000268, glen=111, tlen=271, kl=0.00102, act_lr=3.2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:29<00:22,  1.13it/s, pg=0.00995, ret=-0.00117, glen=108, tlen=268, kl=0.00101, act_lr=3.2e-7, ent=1.84] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=0.00995, ret=-0.00117, glen=108, tlen=268, kl=0.00101, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.14it/s, pg=-0.14, ret=-0.000362, glen=102, tlen=262, kl=0.00111, act_lr=3.2e-7, ent=1.8]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=-0.14, ret=-0.000362, glen=102, tlen=262, kl=0.00111, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.0603, ret=-8.67e-5, glen=97, tlen=257, kl=0.00108, act_lr=3.2e-7, ent=1.64]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.0603, ret=-8.67e-5, glen=97, tlen=257, kl=0.00108, act_lr=3.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.0744, ret=-0.000316, glen=121, tlen=282, kl=0.00103, act_lr=3.2e-7, ent=1.85]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0744, ret=-0.000316, glen=121, tlen=282, kl=0.00103, act_lr=3.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.113, ret=-0.000538, glen=100, tlen=260, kl=0.00106, act_lr=3.2e-7, ent=1.64] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.113, ret=-0.000538, glen=100, tlen=260, kl=0.00106, act_lr=3.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.0381, ret=-9.95e-5, glen=110, tlen=270, kl=0.001, act_lr=3.2e-7, ent=1.84]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0381, ret=-9.95e-5, glen=110, tlen=270, kl=0.001, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=0.287, ret=-0.0167, glen=388, tlen=549, kl=0.000831, act_lr=3.2e-7, ent=1.3] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.14it/s, pg=0.287, ret=-0.0167, glen=388, tlen=549, kl=0.000831, act_lr=3.2e-7, ent=1.3]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:35<00:15,  1.14it/s, pg=0.308, ret=-0.00248, glen=282, tlen=443, kl=0.000843, act_lr=3.2e-7, ent=2.77]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:15,  1.12it/s, pg=0.308, ret=-0.00248, glen=282, tlen=443, kl=0.000843, act_lr=3.2e-7, ent=2.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:15,  1.12it/s, pg=-0.101, ret=0.000376, glen=107, tlen=268, kl=0.00103, act_lr=3.2e-7, ent=1.63]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:14,  1.14it/s, pg=-0.101, ret=0.000376, glen=107, tlen=268, kl=0.00103, act_lr=3.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:14,  1.14it/s, pg=0.0177, ret=-0.000163, glen=126, tlen=286, kl=0.00101, act_lr=3.2e-7, ent=1.91]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:13,  1.15it/s, pg=0.0177, ret=-0.000163, glen=126, tlen=286, kl=0.00101, act_lr=3.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:13,  1.15it/s, pg=0.00317, ret=-0.000533, glen=120, tlen=281, kl=0.00101, act_lr=3.2e-7, ent=1.92]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.16it/s, pg=0.00317, ret=-0.000533, glen=120, tlen=281, kl=0.00101, act_lr=3.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=0.0264, ret=0.000651, glen=111, tlen=272, kl=0.00105, act_lr=3.2e-7, ent=1.81]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.0264, ret=0.000651, glen=111, tlen=272, kl=0.00105, act_lr=3.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.16it/s, pg=-0.0828, ret=-0.000698, glen=117, tlen=278, kl=0.00101, act_lr=3.2e-7, ent=1.8]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0828, ret=-0.000698, glen=117, tlen=278, kl=0.00101, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.17it/s, pg=0.0955, ret=0.00085, glen=113, tlen=274, kl=0.00106, act_lr=3.2e-7, ent=1.89]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0955, ret=0.00085, glen=113, tlen=274, kl=0.00106, act_lr=3.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:41<00:09,  1.17it/s, pg=0.324, ret=-0.00223, glen=117, tlen=277, kl=0.00102, act_lr=3.2e-7, ent=1.92]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.324, ret=-0.00223, glen=117, tlen=277, kl=0.00102, act_lr=3.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.234, ret=0.000626, glen=128, tlen=288, kl=0.000995, act_lr=3.2e-7, ent=2.13]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.15it/s, pg=-0.234, ret=0.000626, glen=128, tlen=288, kl=0.000995, act_lr=3.2e-7, ent=2.13]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.15it/s, pg=0.00909, ret=0.000214, glen=110, tlen=270, kl=0.00104, act_lr=3.2e-7, ent=1.79]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.15it/s, pg=0.00909, ret=0.000214, glen=110, tlen=270, kl=0.00104, act_lr=3.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.15it/s, pg=-0.137, ret=0.000394, glen=93.6, tlen=254, kl=0.00102, act_lr=3.2e-7, ent=1.8] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.16it/s, pg=-0.137, ret=0.000394, glen=93.6, tlen=254, kl=0.00102, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.16it/s, pg=-0.131, ret=0.000537, glen=103, tlen=263, kl=0.00102, act_lr=3.2e-7, ent=1.66]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.131, ret=0.000537, glen=103, tlen=263, kl=0.00102, act_lr=3.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=0.00946, ret=-0.000251, glen=108, tlen=269, kl=0.000979, act_lr=3.2e-7, ent=1.8]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.00946, ret=-0.000251, glen=108, tlen=269, kl=0.000979, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=-0.0494, ret=5.67e-6, glen=104, tlen=264, kl=0.00105, act_lr=3.2e-7, ent=1.61]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.0494, ret=5.67e-6, glen=104, tlen=264, kl=0.00105, act_lr=3.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.17it/s, pg=0.0171, ret=-0.000318, glen=93.6, tlen=254, kl=0.00105, act_lr=3.2e-7, ent=1.52]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=0.0171, ret=-0.000318, glen=93.6, tlen=254, kl=0.00105, act_lr=3.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0541, ret=0.000874, glen=105, tlen=266, kl=0.00103, act_lr=3.2e-7, ent=1.67] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0541, ret=0.000874, glen=105, tlen=266, kl=0.00103, act_lr=3.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0355, ret=-0.000357, glen=101, tlen=262, kl=0.00101, act_lr=3.2e-7, ent=1.8] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.0355, ret=-0.000357, glen=101, tlen=262, kl=0.00101, act_lr=3.2e-7, ent=1.8]
2025-07-24 17:07:22.068 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.80s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=0.113, ret=-0.0006, glen=197, tlen=356, kl=0.00099, act_lr=3.4e-7, ent=2.42]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.113, ret=-0.0006, glen=197, tlen=356, kl=0.00099, act_lr=3.4e-7, ent=2.42]
2025-07-24 17:07:22.862 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.74s
2025-07-24 17:07:25.404 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 17:07:25.733 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.58s
2025-07-24 17:07:25.740 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.019771274767423932, 'actor_lr': 3.203508755265367e-07, 'clip_ratio': 0.0, 'entropy': 1.8358874362811708, 'kl': 0.001025835673014323, 'response_length': 120.7800596806041, 'total_length': 281.1828126070792, 'teacher_total_length': 292.6850355717174, 'return': -0.0003888742726861924, 'policy_update_steps': 1.0}

Episode [2/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:57<33:26, 222.89s/it][A2025-07-24 17:07:25.783 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:09:27.937 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:09:28.119 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:09:28.120 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 122.34s
2025-07-24 17:09:30.128 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0110,avg_pass_at_n: 1.0000,avg_num_tokens: 109.3993,std_num_tokens: 118.2373,avg_correct_num_tokens: 104.0115,std_correct_num_tokens: 85.2126,avg_incorrect_num_tokens: 119.8881,std_incorrect_num_tokens: 163.9925
2025-07-24 17:09:30.504 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 17:09:33.880 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.37s
2025-07-24 17:10:02.385 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:10:02.386 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.50s
2025-07-24 17:10:04.101 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 1.28s
2025-07-24 17:10:04.102 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008023398442323332, avg_kl: 0.0010367166835630955, avg_response_length: 111.21581179357965, avg_orm_score: 0.0, avg_custom_rewards: -0.0008023398442323332
2025-07-24 17:10:04.157 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter17_replay_buffer.jsonl
2025-07-24 17:10:06.061 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.123, ret=-0.00186, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.03s/it, pg=0.123, ret=-0.00186, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.03s/it, pg=-0.0354, ret=-0.000521, glen=108, tlen=268, kl=0.00107, act_lr=3.4e-7, ent=1.66]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.08it/s, pg=-0.0354, ret=-0.000521, glen=108, tlen=268, kl=0.00107, act_lr=3.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.08it/s, pg=-0.232, ret=0.00168, glen=109, tlen=269, kl=0.00106, act_lr=3.4e-7, ent=1.74]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.08it/s, pg=-0.232, ret=0.00168, glen=109, tlen=269, kl=0.00106, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.08it/s, pg=0.079, ret=0.000748, glen=104, tlen=264, kl=0.00104, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:47,  1.10it/s, pg=0.079, ret=0.000748, glen=104, tlen=264, kl=0.00104, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:47,  1.10it/s, pg=-0.179, ret=0.0017, glen=106, tlen=267, kl=0.001, act_lr=3.4e-7, ent=1.63]   Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.12it/s, pg=-0.179, ret=0.0017, glen=106, tlen=267, kl=0.001, act_lr=3.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.12it/s, pg=-0.019, ret=0.000826, glen=107, tlen=268, kl=0.00105, act_lr=3.4e-7, ent=1.72]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.11it/s, pg=-0.019, ret=0.000826, glen=107, tlen=268, kl=0.00105, act_lr=3.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.11it/s, pg=0.0769, ret=-0.00104, glen=98.4, tlen=258, kl=0.0011, act_lr=3.4e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.12it/s, pg=0.0769, ret=-0.00104, glen=98.4, tlen=258, kl=0.0011, act_lr=3.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.12it/s, pg=0.0399, ret=-1.22e-5, glen=115, tlen=275, kl=0.00102, act_lr=3.4e-7, ent=1.81]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.14it/s, pg=0.0399, ret=-1.22e-5, glen=115, tlen=275, kl=0.00102, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:42,  1.14it/s, pg=0.0515, ret=-0.000699, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.76]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=0.0515, ret=-0.000699, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=0.375, ret=-0.00287, glen=193, tlen=353, kl=0.000826, act_lr=3.4e-7, ent=1.44] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:40,  1.14it/s, pg=0.375, ret=-0.00287, glen=193, tlen=353, kl=0.000826, act_lr=3.4e-7, ent=1.44]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.14it/s, pg=0.0195, ret=-0.000691, glen=109, tlen=269, kl=0.00101, act_lr=3.4e-7, ent=1.96]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.15it/s, pg=0.0195, ret=-0.000691, glen=109, tlen=269, kl=0.00101, act_lr=3.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.15it/s, pg=0.0538, ret=-0.0011, glen=102, tlen=262, kl=0.00107, act_lr=3.4e-7, ent=1.68]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.16it/s, pg=0.0538, ret=-0.0011, glen=102, tlen=262, kl=0.00107, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.16it/s, pg=-0.0036, ret=-0.000255, glen=112, tlen=272, kl=0.00102, act_lr=3.4e-7, ent=1.67]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.16it/s, pg=-0.0036, ret=-0.000255, glen=112, tlen=272, kl=0.00102, act_lr=3.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.16it/s, pg=-0.031, ret=0.000163, glen=97.5, tlen=257, kl=0.00109, act_lr=3.4e-7, ent=1.75] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.17it/s, pg=-0.031, ret=0.000163, glen=97.5, tlen=257, kl=0.00109, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.17it/s, pg=-0.12, ret=0.000565, glen=100, tlen=260, kl=0.00106, act_lr=3.4e-7, ent=1.67]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.12, ret=0.000565, glen=100, tlen=260, kl=0.00106, act_lr=3.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.17it/s, pg=-0.0933, ret=0.000558, glen=125, tlen=286, kl=0.00101, act_lr=3.4e-7, ent=1.97]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.0933, ret=0.000558, glen=125, tlen=286, kl=0.00101, act_lr=3.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=0.0725, ret=0.000399, glen=113, tlen=273, kl=0.00101, act_lr=3.4e-7, ent=2.01] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=0.0725, ret=0.000399, glen=113, tlen=273, kl=0.00101, act_lr=3.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.19, ret=-0.000996, glen=112, tlen=272, kl=0.00111, act_lr=3.4e-7, ent=1.74] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.19, ret=-0.000996, glen=112, tlen=272, kl=0.00111, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=0.0269, ret=-0.00028, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.74]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0269, ret=-0.00028, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.197, ret=-0.00121, glen=125, tlen=285, kl=0.000987, act_lr=3.4e-7, ent=2.05]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.197, ret=-0.00121, glen=125, tlen=285, kl=0.000987, act_lr=3.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.0905, ret=-6.21e-5, glen=103, tlen=263, kl=0.00104, act_lr=3.4e-7, ent=1.7]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.0905, ret=-6.21e-5, glen=103, tlen=263, kl=0.00104, act_lr=3.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.046, ret=0.00048, glen=107, tlen=267, kl=0.00114, act_lr=3.4e-7, ent=1.99] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.046, ret=0.00048, glen=107, tlen=267, kl=0.00114, act_lr=3.4e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.17it/s, pg=-0.0386, ret=-0.000153, glen=125, tlen=285, kl=0.00107, act_lr=3.4e-7, ent=2.03]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.0386, ret=-0.000153, glen=125, tlen=285, kl=0.00107, act_lr=3.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.112, ret=0.00268, glen=124, tlen=284, kl=0.000963, act_lr=3.4e-7, ent=1.79]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.112, ret=0.00268, glen=124, tlen=284, kl=0.000963, act_lr=3.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=-0.0514, ret=0.00116, glen=103, tlen=263, kl=0.00107, act_lr=3.4e-7, ent=1.74]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0514, ret=0.00116, glen=103, tlen=263, kl=0.00107, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.128, ret=0.000102, glen=114, tlen=274, kl=0.000995, act_lr=3.4e-7, ent=1.87]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.128, ret=0.000102, glen=114, tlen=274, kl=0.000995, act_lr=3.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.00299, ret=-0.000642, glen=107, tlen=267, kl=0.00101, act_lr=3.4e-7, ent=1.66]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.00299, ret=-0.000642, glen=107, tlen=267, kl=0.00101, act_lr=3.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.134, ret=0.000584, glen=113, tlen=273, kl=0.00102, act_lr=3.4e-7, ent=2.03]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.134, ret=0.000584, glen=113, tlen=273, kl=0.00102, act_lr=3.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=-0.129, ret=0.000983, glen=96.6, tlen=256, kl=0.001, act_lr=3.4e-7, ent=1.63] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=-0.129, ret=0.000983, glen=96.6, tlen=256, kl=0.001, act_lr=3.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.14, ret=0.000262, glen=109, tlen=269, kl=0.00104, act_lr=3.4e-7, ent=1.65]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.14, ret=0.000262, glen=109, tlen=269, kl=0.00104, act_lr=3.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.10it/s, pg=-0.108, ret=0.000256, glen=108, tlen=268, kl=0.00102, act_lr=3.4e-7, ent=1.76]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=-0.108, ret=0.000256, glen=108, tlen=268, kl=0.00102, act_lr=3.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.0425, ret=-0.0015, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.64] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.14it/s, pg=0.0425, ret=-0.0015, glen=106, tlen=266, kl=0.00104, act_lr=3.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=0.111, ret=-0.00172, glen=102, tlen=262, kl=0.00109, act_lr=3.4e-7, ent=1.68]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.15it/s, pg=0.111, ret=-0.00172, glen=102, tlen=262, kl=0.00109, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.15it/s, pg=0.253, ret=-0.00118, glen=125, tlen=285, kl=0.000995, act_lr=3.4e-7, ent=1.83]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.16it/s, pg=0.253, ret=-0.00118, glen=125, tlen=285, kl=0.000995, act_lr=3.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.16it/s, pg=-0.0298, ret=-6.09e-5, glen=97.5, tlen=257, kl=0.00107, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=-0.0298, ret=-6.09e-5, glen=97.5, tlen=257, kl=0.00107, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=-0.0617, ret=-0.000393, glen=95.3, tlen=255, kl=0.00105, act_lr=3.4e-7, ent=1.68]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.17it/s, pg=-0.0617, ret=-0.000393, glen=95.3, tlen=255, kl=0.00105, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.17it/s, pg=-0.222, ret=0.00161, glen=104, tlen=264, kl=0.00107, act_lr=3.4e-7, ent=1.75]    Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.222, ret=0.00161, glen=104, tlen=264, kl=0.00107, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.17it/s, pg=0.0331, ret=-6.7e-5, glen=119, tlen=278, kl=0.00101, act_lr=3.4e-7, ent=1.99]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0331, ret=-6.7e-5, glen=119, tlen=278, kl=0.00101, act_lr=3.4e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0375, ret=-0.0007, glen=117, tlen=277, kl=0.00108, act_lr=3.4e-7, ent=2.2] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0375, ret=-0.0007, glen=117, tlen=277, kl=0.00108, act_lr=3.4e-7, ent=2.2]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0994, ret=0.000623, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.72]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0994, ret=0.000623, glen=108, tlen=268, kl=0.00106, act_lr=3.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0241, ret=-0.000573, glen=102, tlen=262, kl=0.00106, act_lr=3.4e-7, ent=1.77]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.0241, ret=-0.000573, glen=102, tlen=262, kl=0.00106, act_lr=3.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.0948, ret=-0.000905, glen=101, tlen=261, kl=0.00104, act_lr=3.4e-7, ent=1.68] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.0948, ret=-0.000905, glen=101, tlen=261, kl=0.00104, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=0.000911, glen=94.2, tlen=254, kl=0.00104, act_lr=3.4e-7, ent=1.61]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.148, ret=0.000911, glen=94.2, tlen=254, kl=0.00104, act_lr=3.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.138, ret=-0.00176, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.81]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.138, ret=-0.00176, glen=121, tlen=281, kl=0.00106, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0524, ret=0.000207, glen=116, tlen=276, kl=0.00101, act_lr=3.4e-7, ent=1.79]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0524, ret=0.000207, glen=116, tlen=276, kl=0.00101, act_lr=3.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.0659, ret=0.000175, glen=119, tlen=279, kl=0.00105, act_lr=3.4e-7, ent=1.85] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.16it/s, pg=0.0659, ret=0.000175, glen=119, tlen=279, kl=0.00105, act_lr=3.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.16it/s, pg=-0.188, ret=0.00197, glen=115, tlen=275, kl=0.00107, act_lr=3.4e-7, ent=1.81] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.16it/s, pg=-0.188, ret=0.00197, glen=115, tlen=275, kl=0.00107, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.16it/s, pg=-0.104, ret=0.00198, glen=110, tlen=270, kl=0.00102, act_lr=3.4e-7, ent=1.71]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=-0.104, ret=0.00198, glen=110, tlen=270, kl=0.00102, act_lr=3.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=0.0485, ret=-0.000417, glen=114, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=0.0485, ret=-0.000417, glen=114, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=0.106, ret=-0.000378, glen=115, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.82] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=0.106, ret=-0.000378, glen=115, tlen=275, kl=0.00104, act_lr=3.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.00607, ret=-0.000812, glen=124, tlen=285, kl=0.00101, act_lr=3.4e-7, ent=1.91]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.00607, ret=-0.000812, glen=124, tlen=285, kl=0.00101, act_lr=3.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.18it/s, pg=0.217, ret=-0.00246, glen=108, tlen=268, kl=0.000991, act_lr=3.4e-7, ent=2.08]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.217, ret=-0.00246, glen=108, tlen=268, kl=0.000991, act_lr=3.4e-7, ent=2.08]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.152, ret=0.00134, glen=116, tlen=276, kl=0.00104, act_lr=3.4e-7, ent=1.79] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.152, ret=0.00134, glen=116, tlen=276, kl=0.00104, act_lr=3.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0822, ret=0.000248, glen=110, tlen=270, kl=0.000991, act_lr=3.4e-7, ent=1.57]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.18it/s, pg=-0.0822, ret=0.000248, glen=110, tlen=270, kl=0.000991, act_lr=3.4e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.18it/s, pg=-0.233, ret=0.000735, glen=100, tlen=260, kl=0.00107, act_lr=3.4e-7, ent=1.64]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=-0.233, ret=0.000735, glen=100, tlen=260, kl=0.00107, act_lr=3.4e-7, ent=1.64]
2025-07-24 17:10:54.807 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.56s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=0.0127, ret=-0.000222, glen=102, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.76]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=0.0127, ret=-0.000222, glen=102, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.76]
2025-07-24 17:10:55.625 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-24 17:10:58.218 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-24 17:10:58.562 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.44s
2025-07-24 17:10:58.570 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011185143675122942, 'actor_lr': 3.4035715655379916e-07, 'clip_ratio': 0.0, 'entropy': 1.7742285962615694, 'kl': 0.0010373677526201522, 'response_length': 111.14685412815639, 'total_length': 271.05371638706754, 'teacher_total_length': 283.3528366088867, 'return': -4.607349507880697e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:29<29:14, 219.26s/it][A2025-07-24 17:10:58.612 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:13:25.363 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:13:25.550 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-24 17:13:25.550 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 146.94s
2025-07-24 17:13:27.545 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0122,avg_pass_at_n: 1.0000,avg_num_tokens: 112.0604,std_num_tokens: 152.0236,avg_correct_num_tokens: 100.6975,std_correct_num_tokens: 74.0874,avg_incorrect_num_tokens: 133.8238,std_incorrect_num_tokens: 236.9434
2025-07-24 17:13:27.997 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.45s
2025-07-24 17:13:31.208 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.21s
2025-07-24 17:14:00.264 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 227
2025-07-24 17:14:00.264 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.05s
2025-07-24 17:14:01.742 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.97s
2025-07-24 17:14:01.743 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008560248386752812, avg_kl: 0.0010772150518610613, avg_response_length: 116.00188835917065, avg_orm_score: 0.0, avg_custom_rewards: -0.0008560248386752812
2025-07-24 17:14:01.775 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter18_replay_buffer.jsonl
2025-07-24 17:14:03.682 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.91s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.133, ret=0.00104, glen=129, tlen=290, kl=0.00101, act_lr=3.6e-7, ent=2.26]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<01:00,  1.08s/it, pg=0.133, ret=0.00104, glen=129, tlen=290, kl=0.00101, act_lr=3.6e-7, ent=2.26]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<01:00,  1.08s/it, pg=-0.0935, ret=-0.000242, glen=101, tlen=262, kl=0.00115, act_lr=3.6e-7, ent=1.63]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:53,  1.04it/s, pg=-0.0935, ret=-0.000242, glen=101, tlen=262, kl=0.00115, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:53,  1.04it/s, pg=0.0418, ret=0.00252, glen=130, tlen=291, kl=0.00106, act_lr=3.6e-7, ent=2.18]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:51,  1.05it/s, pg=0.0418, ret=0.00252, glen=130, tlen=291, kl=0.00106, act_lr=3.6e-7, ent=2.18]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:51,  1.05it/s, pg=0.0914, ret=0.000476, glen=133, tlen=293, kl=0.000974, act_lr=3.6e-7, ent=2.28]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:49,  1.06it/s, pg=0.0914, ret=0.000476, glen=133, tlen=293, kl=0.000974, act_lr=3.6e-7, ent=2.28]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:49,  1.06it/s, pg=0.00702, ret=-0.00127, glen=119, tlen=280, kl=0.00112, act_lr=3.6e-7, ent=1.79]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:47,  1.10it/s, pg=0.00702, ret=-0.00127, glen=119, tlen=280, kl=0.00112, act_lr=3.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:47,  1.10it/s, pg=-0.124, ret=0.000265, glen=122, tlen=283, kl=0.00112, act_lr=3.6e-7, ent=1.82] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.12it/s, pg=-0.124, ret=0.000265, glen=122, tlen=283, kl=0.00112, act_lr=3.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.12it/s, pg=0.174, ret=-0.000891, glen=118, tlen=279, kl=0.00101, act_lr=3.6e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.14it/s, pg=0.174, ret=-0.000891, glen=118, tlen=279, kl=0.00101, act_lr=3.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.14it/s, pg=-0.197, ret=0.000773, glen=99, tlen=259, kl=0.00113, act_lr=3.6e-7, ent=1.66] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.13it/s, pg=-0.197, ret=0.000773, glen=99, tlen=259, kl=0.00113, act_lr=3.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.13it/s, pg=0.0272, ret=-0.000583, glen=96.6, tlen=258, kl=0.00107, act_lr=3.6e-7, ent=1.53]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.14it/s, pg=0.0272, ret=-0.000583, glen=96.6, tlen=258, kl=0.00107, act_lr=3.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.14it/s, pg=-0.212, ret=0.000716, glen=110, tlen=271, kl=0.00114, act_lr=3.6e-7, ent=1.7]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.212, ret=0.000716, glen=110, tlen=271, kl=0.00114, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=-0.135, ret=0.00133, glen=107, tlen=267, kl=0.00107, act_lr=3.6e-7, ent=1.65]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=-0.135, ret=0.00133, glen=107, tlen=267, kl=0.00107, act_lr=3.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0614, ret=-0.000299, glen=122, tlen=282, kl=0.00103, act_lr=3.6e-7, ent=2.22]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.0614, ret=-0.000299, glen=122, tlen=282, kl=0.00103, act_lr=3.6e-7, ent=2.22]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.0624, ret=0.000282, glen=137, tlen=298, kl=0.00102, act_lr=3.6e-7, ent=2.06]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=0.0624, ret=0.000282, glen=137, tlen=298, kl=0.00102, act_lr=3.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=-0.224, ret=0.000853, glen=103, tlen=264, kl=0.00109, act_lr=3.6e-7, ent=1.73]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=-0.224, ret=0.000853, glen=103, tlen=264, kl=0.00109, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=-0.0554, ret=-0.001, glen=104, tlen=265, kl=0.00112, act_lr=3.6e-7, ent=1.63] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.0554, ret=-0.001, glen=104, tlen=265, kl=0.00112, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:35,  1.17it/s, pg=-0.107, ret=5.2e-5, glen=113, tlen=274, kl=0.00109, act_lr=3.6e-7, ent=1.77] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.107, ret=5.2e-5, glen=113, tlen=274, kl=0.00109, act_lr=3.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.00143, ret=-0.00101, glen=107, tlen=268, kl=0.00114, act_lr=3.6e-7, ent=1.76]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.00143, ret=-0.00101, glen=107, tlen=268, kl=0.00114, act_lr=3.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.00244, ret=0.000598, glen=134, tlen=294, kl=0.00107, act_lr=3.6e-7, ent=2.24]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=-0.00244, ret=0.000598, glen=134, tlen=294, kl=0.00107, act_lr=3.6e-7, ent=2.24]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=0.121, ret=-0.000594, glen=133, tlen=294, kl=0.00108, act_lr=3.6e-7, ent=1.95]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=0.121, ret=-0.000594, glen=133, tlen=294, kl=0.00108, act_lr=3.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0337, ret=-0.00053, glen=104, tlen=264, kl=0.00112, act_lr=3.6e-7, ent=1.82]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0337, ret=-0.00053, glen=104, tlen=264, kl=0.00112, act_lr=3.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.127, ret=0.00152, glen=115, tlen=276, kl=0.0011, act_lr=3.6e-7, ent=1.74]   Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.16it/s, pg=-0.127, ret=0.00152, glen=115, tlen=276, kl=0.0011, act_lr=3.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.16it/s, pg=-0.0803, ret=0.00213, glen=130, tlen=291, kl=0.00103, act_lr=3.6e-7, ent=2.07]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.17it/s, pg=-0.0803, ret=0.00213, glen=130, tlen=291, kl=0.00103, act_lr=3.6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.17it/s, pg=-0.0977, ret=0.000646, glen=102, tlen=264, kl=0.0011, act_lr=3.6e-7, ent=1.75]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0977, ret=0.000646, glen=102, tlen=264, kl=0.0011, act_lr=3.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.19, ret=-0.00245, glen=142, tlen=303, kl=0.00102, act_lr=3.6e-7, ent=1.59]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.19, ret=-0.00245, glen=142, tlen=303, kl=0.00102, act_lr=3.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0645, ret=-0.000141, glen=125, tlen=286, kl=0.00101, act_lr=3.6e-7, ent=1.88]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0645, ret=-0.000141, glen=125, tlen=286, kl=0.00101, act_lr=3.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.0251, ret=0.000188, glen=115, tlen=276, kl=0.00108, act_lr=3.6e-7, ent=1.71] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.0251, ret=0.000188, glen=115, tlen=276, kl=0.00108, act_lr=3.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0215, ret=-0.000996, glen=122, tlen=283, kl=0.00104, act_lr=3.6e-7, ent=1.6] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:25,  1.17it/s, pg=0.0215, ret=-0.000996, glen=122, tlen=283, kl=0.00104, act_lr=3.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:25,  1.17it/s, pg=-0.079, ret=5e-5, glen=100, tlen=262, kl=0.0011, act_lr=3.6e-7, ent=1.69]     Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:24,  1.17it/s, pg=-0.079, ret=5e-5, glen=100, tlen=262, kl=0.0011, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:24,  1.17it/s, pg=0.145, ret=-0.00141, glen=116, tlen=277, kl=0.00105, act_lr=3.6e-7, ent=1.73]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:26,  1.06it/s, pg=0.145, ret=-0.00141, glen=116, tlen=277, kl=0.00105, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:26,  1.06it/s, pg=-0.107, ret=-0.000153, glen=116, tlen=277, kl=0.00113, act_lr=3.6e-7, ent=1.65]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.09it/s, pg=-0.107, ret=-0.000153, glen=116, tlen=277, kl=0.00113, act_lr=3.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.09it/s, pg=-0.0725, ret=-0.000543, glen=111, tlen=272, kl=0.00111, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.11it/s, pg=-0.0725, ret=-0.000543, glen=111, tlen=272, kl=0.00111, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.11it/s, pg=0.199, ret=-0.00168, glen=103, tlen=263, kl=0.0011, act_lr=3.6e-7, ent=1.73]    Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=0.199, ret=-0.00168, glen=103, tlen=263, kl=0.0011, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=0.168, ret=6.54e-5, glen=124, tlen=285, kl=0.00103, act_lr=3.6e-7, ent=1.9] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.14it/s, pg=0.168, ret=6.54e-5, glen=124, tlen=285, kl=0.00103, act_lr=3.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.14it/s, pg=0.0212, ret=-0.00105, glen=102, tlen=263, kl=0.00107, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=0.0212, ret=-0.00105, glen=102, tlen=263, kl=0.00107, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.0116, ret=0.000198, glen=123, tlen=284, kl=0.00109, act_lr=3.6e-7, ent=1.75]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.0116, ret=0.000198, glen=123, tlen=284, kl=0.00109, act_lr=3.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.179, ret=0.00332, glen=125, tlen=286, kl=0.00107, act_lr=3.6e-7, ent=1.81] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.179, ret=0.00332, glen=125, tlen=286, kl=0.00107, act_lr=3.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.188, ret=0.000264, glen=106, tlen=266, kl=0.00113, act_lr=3.6e-7, ent=1.64]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.188, ret=0.000264, glen=106, tlen=266, kl=0.00113, act_lr=3.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.0384, ret=0.00195, glen=116, tlen=277, kl=0.00103, act_lr=3.6e-7, ent=2.01]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0384, ret=0.00195, glen=116, tlen=277, kl=0.00103, act_lr=3.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=-0.0289, ret=-3.4e-5, glen=115, tlen=276, kl=0.00106, act_lr=3.6e-7, ent=1.68]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.0289, ret=-3.4e-5, glen=115, tlen=276, kl=0.00106, act_lr=3.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0486, ret=-0.0019, glen=111, tlen=272, kl=0.00109, act_lr=3.6e-7, ent=1.72] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.0486, ret=-0.0019, glen=111, tlen=272, kl=0.00109, act_lr=3.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.0193, ret=0.00135, glen=119, tlen=279, kl=0.0011, act_lr=3.6e-7, ent=1.92]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.0193, ret=0.00135, glen=119, tlen=279, kl=0.0011, act_lr=3.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.259, ret=-0.0027, glen=241, tlen=402, kl=0.000964, act_lr=3.6e-7, ent=1.62]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:13,  1.14it/s, pg=0.259, ret=-0.0027, glen=241, tlen=402, kl=0.000964, act_lr=3.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:13,  1.14it/s, pg=-0.119, ret=0.000473, glen=92.3, tlen=253, kl=0.00111, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.15it/s, pg=-0.119, ret=0.000473, glen=92.3, tlen=253, kl=0.00111, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.15it/s, pg=0.0985, ret=-0.00133, glen=93.3, tlen=254, kl=0.00114, act_lr=3.6e-7, ent=1.67]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.0985, ret=-0.00133, glen=93.3, tlen=254, kl=0.00114, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.16it/s, pg=-0.132, ret=0.0013, glen=113, tlen=274, kl=0.00107, act_lr=3.6e-7, ent=1.69]   Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.16it/s, pg=-0.132, ret=0.0013, glen=113, tlen=274, kl=0.00107, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.16it/s, pg=-0.0377, ret=-2.19e-5, glen=109, tlen=269, kl=0.00113, act_lr=3.6e-7, ent=1.78]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.12it/s, pg=-0.0377, ret=-2.19e-5, glen=109, tlen=269, kl=0.00113, act_lr=3.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:41<00:09,  1.12it/s, pg=0.0544, ret=-0.00123, glen=104, tlen=264, kl=0.00114, act_lr=3.6e-7, ent=1.63] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:09,  1.11it/s, pg=0.0544, ret=-0.00123, glen=104, tlen=264, kl=0.00114, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:42<00:09,  1.11it/s, pg=-0.104, ret=0.000615, glen=112, tlen=272, kl=0.00108, act_lr=3.6e-7, ent=1.7] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.13it/s, pg=-0.104, ret=0.000615, glen=112, tlen=272, kl=0.00108, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.13it/s, pg=0.165, ret=-0.00163, glen=128, tlen=289, kl=0.00103, act_lr=3.6e-7, ent=2.48]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:07,  1.14it/s, pg=0.165, ret=-0.00163, glen=128, tlen=289, kl=0.00103, act_lr=3.6e-7, ent=2.48]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:07,  1.14it/s, pg=-0.162, ret=0.000229, glen=98.4, tlen=259, kl=0.0011, act_lr=3.6e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.15it/s, pg=-0.162, ret=0.000229, glen=98.4, tlen=259, kl=0.0011, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.15it/s, pg=0.0644, ret=-0.000945, glen=102, tlen=263, kl=0.00108, act_lr=3.6e-7, ent=1.57]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=0.0644, ret=-0.000945, glen=102, tlen=263, kl=0.00108, act_lr=3.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=0.0893, ret=-0.00162, glen=115, tlen=276, kl=0.00107, act_lr=3.6e-7, ent=1.67] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=0.0893, ret=-0.00162, glen=115, tlen=276, kl=0.00107, act_lr=3.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.146, ret=-0.000471, glen=105, tlen=265, kl=0.00108, act_lr=3.6e-7, ent=1.77]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.16it/s, pg=-0.146, ret=-0.000471, glen=105, tlen=265, kl=0.00108, act_lr=3.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.16it/s, pg=0.0989, ret=-0.00112, glen=117, tlen=278, kl=0.00107, act_lr=3.6e-7, ent=1.72] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=0.0989, ret=-0.00112, glen=117, tlen=278, kl=0.00107, act_lr=3.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=-0.0839, ret=0.000813, glen=104, tlen=265, kl=0.00113, act_lr=3.6e-7, ent=1.65]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0839, ret=0.000813, glen=104, tlen=265, kl=0.00113, act_lr=3.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.00679, ret=-0.00102, glen=100, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.57]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.17it/s, pg=0.00679, ret=-0.00102, glen=100, tlen=261, kl=0.00104, act_lr=3.6e-7, ent=1.57]
2025-07-24 17:14:53.708 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 49.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.17it/s, pg=-0.168, ret=0.00116, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=2]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.168, ret=0.00116, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=2]
2025-07-24 17:14:54.592 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:14:57.133 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-24 17:14:57.460 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 53.72s
2025-07-24 17:14:57.467 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.01760175771880568, 'actor_lr': 3.6035087833752503e-07, 'clip_ratio': 0.0, 'entropy': 1.790599132839002, 'kl': 0.0010777439987450315, 'response_length': 115.91371034321033, 'total_length': 276.66269938151044, 'teacher_total_length': 288.8366179884526, 'return': -6.468255652866342e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:28<26:21, 225.94s/it][A2025-07-24 17:14:57.510 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:17:24.899 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:17:25.075 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:17:25.076 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 147.57s
2025-07-24 17:17:27.039 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 107.7162,std_num_tokens: 138.4706,avg_correct_num_tokens: 100.7582,std_correct_num_tokens: 80.4996,avg_incorrect_num_tokens: 121.6068,std_incorrect_num_tokens: 210.2995
2025-07-24 17:17:27.458 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.38s
2025-07-24 17:17:30.606 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.15s
2025-07-24 17:17:58.866 | INFO     | orz.ppo.trainer:make_experience:303 - experiences size: 223
2025-07-24 17:17:58.867 | INFO     | orz.ppo.trainer:make_experience:290 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.26s
2025-07-24 17:18:00.292 | INFO     | orz.ppo.trainer:make_experience:377 - Calculate advantages and returns, time cost: 0.99s
2025-07-24 17:18:00.293 | INFO     | orz.ppo.trainer:make_experience:395 - avg_raw_rewards: -0.0008467278438578506, avg_kl: 0.0011273884452511912, avg_response_length: 112.22215236783562, avg_orm_score: 0.0, avg_custom_rewards: -0.0008467278438578506
2025-07-24 17:18:00.324 | INFO     | orz.ppo.trainer:train:130 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_teacher-grpo-4gpu-v1/dumped_replay_buffer/iter19_replay_buffer.jsonl
2025-07-24 17:18:02.165 | INFO     | orz.ppo.trainer:train:125 - Dumping replay buffer, time cost: 1.84s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.139, ret=0.000396, glen=117, tlen=278, kl=0.00107, act_lr=3.8e-7, ent=1.9]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.139, ret=0.000396, glen=117, tlen=278, kl=0.00107, act_lr=3.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.00574, ret=0.000231, glen=109, tlen=269, kl=0.0011, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=0.00574, ret=0.000231, glen=109, tlen=269, kl=0.0011, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=-0.224, ret=0.00127, glen=99.7, tlen=260, kl=0.00113, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.224, ret=0.00127, glen=99.7, tlen=260, kl=0.00113, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=-0.0479, ret=0.00084, glen=125, tlen=285, kl=0.0011, act_lr=3.8e-7, ent=1.92] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.13it/s, pg=-0.0479, ret=0.00084, glen=125, tlen=285, kl=0.0011, act_lr=3.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.13it/s, pg=-0.0911, ret=0.000751, glen=108, tlen=268, kl=0.00114, act_lr=3.8e-7, ent=1.7]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=-0.0911, ret=0.000751, glen=108, tlen=268, kl=0.00114, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=-0.208, ret=0.00292, glen=109, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.73] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=-0.208, ret=0.00292, glen=109, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=0.0279, ret=-0.000519, glen=109, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=0.0279, ret=-0.000519, glen=109, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=0.0178, ret=0.000544, glen=102, tlen=262, kl=0.00116, act_lr=3.8e-7, ent=1.65] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0178, ret=0.000544, glen=102, tlen=262, kl=0.00116, act_lr=3.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0375, ret=-0.000419, glen=99.8, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.59]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.16it/s, pg=0.0375, ret=-0.000419, glen=99.8, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=0.0486, ret=0.000319, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.72]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.16it/s, pg=0.0486, ret=0.000319, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.16it/s, pg=0.116, ret=-0.000694, glen=118, tlen=278, kl=0.00108, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=0.116, ret=-0.000694, glen=118, tlen=278, kl=0.00108, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=0.0176, ret=-0.00114, glen=98.7, tlen=259, kl=0.00113, act_lr=3.8e-7, ent=1.6]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=0.0176, ret=-0.00114, glen=98.7, tlen=259, kl=0.00113, act_lr=3.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.046, ret=-0.000587, glen=113, tlen=273, kl=0.00114, act_lr=3.8e-7, ent=1.77]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.046, ret=-0.000587, glen=113, tlen=273, kl=0.00114, act_lr=3.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=0.0683, ret=0.000594, glen=116, tlen=276, kl=0.0011, act_lr=3.8e-7, ent=1.64] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=0.0683, ret=0.000594, glen=116, tlen=276, kl=0.0011, act_lr=3.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.0271, ret=-0.00186, glen=107, tlen=267, kl=0.0011, act_lr=3.8e-7, ent=1.85]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.0271, ret=-0.00186, glen=107, tlen=267, kl=0.0011, act_lr=3.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:34,  1.17it/s, pg=-0.0385, ret=0.000567, glen=100, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=-0.0385, ret=0.000567, glen=100, tlen=260, kl=0.00111, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.205, ret=0.00199, glen=111, tlen=272, kl=0.0011, act_lr=3.8e-7, ent=1.71]   Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=-0.205, ret=0.00199, glen=111, tlen=272, kl=0.0011, act_lr=3.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=-0.0138, ret=0.000123, glen=112, tlen=273, kl=0.00115, act_lr=3.8e-7, ent=1.84]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.0138, ret=0.000123, glen=112, tlen=273, kl=0.00115, act_lr=3.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0432, ret=0.000324, glen=101, tlen=261, kl=0.00118, act_lr=3.8e-7, ent=1.8] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0432, ret=0.000324, glen=101, tlen=261, kl=0.00118, act_lr=3.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.0526, ret=-0.00154, glen=113, tlen=273, kl=0.00109, act_lr=3.8e-7, ent=1.79]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.0526, ret=-0.00154, glen=113, tlen=273, kl=0.00109, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.14, ret=0.000411, glen=102, tlen=262, kl=0.00119, act_lr=3.8e-7, ent=1.79] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.18it/s, pg=-0.14, ret=0.000411, glen=102, tlen=262, kl=0.00119, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.18it/s, pg=0.0447, ret=0.000533, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=0.0447, ret=0.000533, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:28,  1.17it/s, pg=-0.179, ret=0.000435, glen=113, tlen=272, kl=0.00112, act_lr=3.8e-7, ent=1.75]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.179, ret=0.000435, glen=113, tlen=272, kl=0.00112, act_lr=3.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=0.0532, ret=-0.000346, glen=96.3, tlen=257, kl=0.00114, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.18it/s, pg=0.0532, ret=-0.000346, glen=96.3, tlen=257, kl=0.00114, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.18it/s, pg=-0.158, ret=0.00104, glen=105, tlen=265, kl=0.00114, act_lr=3.8e-7, ent=1.74]   Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.18it/s, pg=-0.158, ret=0.00104, glen=105, tlen=265, kl=0.00114, act_lr=3.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.18it/s, pg=0.13, ret=-0.000351, glen=108, tlen=269, kl=0.00111, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.13, ret=-0.000351, glen=108, tlen=269, kl=0.00111, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.168, ret=-0.000795, glen=110, tlen=270, kl=0.00117, act_lr=3.8e-7, ent=1.85]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:24,  1.17it/s, pg=0.168, ret=-0.000795, glen=110, tlen=270, kl=0.00117, act_lr=3.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:24,  1.17it/s, pg=-0.0799, ret=0.00053, glen=96.1, tlen=257, kl=0.00111, act_lr=3.8e-7, ent=1.7]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:23,  1.17it/s, pg=-0.0799, ret=0.00053, glen=96.1, tlen=257, kl=0.00111, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:23,  1.17it/s, pg=0.105, ret=-0.00204, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=1.68] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:25,  1.07it/s, pg=0.105, ret=-0.00204, glen=112, tlen=273, kl=0.00103, act_lr=3.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:25,  1.07it/s, pg=-0.0291, ret=-0.000474, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.59]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=-0.0291, ret=-0.000474, glen=107, tlen=267, kl=0.00113, act_lr=3.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.10it/s, pg=0.213, ret=-0.00221, glen=95.8, tlen=256, kl=0.0012, act_lr=3.8e-7, ent=1.64]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:26<00:22,  1.12it/s, pg=0.213, ret=-0.00221, glen=95.8, tlen=256, kl=0.0012, act_lr=3.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:22,  1.12it/s, pg=0.0385, ret=-0.00169, glen=111, tlen=271, kl=0.00114, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:21,  1.14it/s, pg=0.0385, ret=-0.00169, glen=111, tlen=271, kl=0.00114, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=0.13, ret=0.00039, glen=344, tlen=504, kl=0.00105, act_lr=3.8e-7, ent=2.46]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:20,  1.12it/s, pg=0.13, ret=0.00039, glen=344, tlen=504, kl=0.00105, act_lr=3.8e-7, ent=2.46]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.12it/s, pg=0.0659, ret=-0.00042, glen=98.6, tlen=259, kl=0.00115, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.13it/s, pg=0.0659, ret=-0.00042, glen=98.6, tlen=259, kl=0.00115, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.13it/s, pg=-0.104, ret=0.000536, glen=120, tlen=281, kl=0.00109, act_lr=3.8e-7, ent=1.61] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.14it/s, pg=-0.104, ret=0.000536, glen=120, tlen=281, kl=0.00109, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.14it/s, pg=-0.0788, ret=-0.000133, glen=103, tlen=263, kl=0.00114, act_lr=3.8e-7, ent=1.65]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=-0.0788, ret=-0.000133, glen=103, tlen=263, kl=0.00114, act_lr=3.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=-0.13, ret=0.00154, glen=96.9, tlen=257, kl=0.00117, act_lr=3.8e-7, ent=1.82]   Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.13, ret=0.00154, glen=96.9, tlen=257, kl=0.00117, act_lr=3.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=0.0087, ret=-0.000426, glen=127, tlen=288, kl=0.00113, act_lr=3.8e-7, ent=1.99]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:32<00:15,  1.16it/s, pg=0.0087, ret=-0.000426, glen=127, tlen=288, kl=0.00113, act_lr=3.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0921, ret=-0.000552, glen=117, tlen=277, kl=0.00115, act_lr=3.8e-7, ent=1.55]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=0.0921, ret=-0.000552, glen=117, tlen=277, kl=0.00115, act_lr=3.8e-7, ent=1.55]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.0944, ret=0.00084, glen=97.7, tlen=258, kl=0.00107, act_lr=3.8e-7, ent=1.92]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0944, ret=0.00084, glen=97.7, tlen=258, kl=0.00107, act_lr=3.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.132, ret=-0.00116, glen=127, tlen=288, kl=0.00112, act_lr=3.8e-7, ent=1.75]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.132, ret=-0.00116, glen=127, tlen=288, kl=0.00112, act_lr=3.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.154, ret=-0.00195, glen=123, tlen=284, kl=0.00114, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:12,  1.17it/s, pg=0.154, ret=-0.00195, glen=123, tlen=284, kl=0.00114, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:12,  1.17it/s, pg=-0.0782, ret=-0.000408, glen=94.9, tlen=255, kl=0.00114, act_lr=3.8e-7, ent=1.61]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.0782, ret=-0.000408, glen=94.9, tlen=255, kl=0.00114, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.0734, ret=0.000154, glen=99.1, tlen=259, kl=0.00119, act_lr=3.8e-7, ent=1.7]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0734, ret=0.000154, glen=99.1, tlen=259, kl=0.00119, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0541, ret=-0.000776, glen=97.2, tlen=258, kl=0.00117, act_lr=3.8e-7, ent=1.61]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.17it/s, pg=-0.0541, ret=-0.000776, glen=97.2, tlen=258, kl=0.00117, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=0.00745, ret=-0.000992, glen=108, tlen=269, kl=0.00113, act_lr=3.8e-7, ent=1.7]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=0.00745, ret=-0.000992, glen=108, tlen=269, kl=0.00113, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.169, ret=0.000857, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.62] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.169, ret=0.000857, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.0269, ret=0.00072, glen=99.8, tlen=260, kl=0.00117, act_lr=3.8e-7, ent=1.57]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=0.0269, ret=0.00072, glen=99.8, tlen=260, kl=0.00117, act_lr=3.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.034, ret=-0.000115, glen=110, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.034, ret=-0.000115, glen=110, tlen=270, kl=0.00112, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.231, ret=0.00129, glen=114, tlen=275, kl=0.00115, act_lr=3.8e-7, ent=1.87]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.231, ret=0.00129, glen=114, tlen=275, kl=0.00115, act_lr=3.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0255, ret=-0.000571, glen=108, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0255, ret=-0.000571, glen=108, tlen=269, kl=0.00114, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.211, ret=-0.00136, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.81]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=0.211, ret=-0.00136, glen=106, tlen=266, kl=0.00113, act_lr=3.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.136, ret=-0.000944, glen=94.8, tlen=255, kl=0.0012, act_lr=3.8e-7, ent=1.57]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=0.136, ret=-0.000944, glen=94.8, tlen=255, kl=0.0012, act_lr=3.8e-7, ent=1.57]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.198, ret=5.72e-6, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.7]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.198, ret=5.72e-6, glen=104, tlen=264, kl=0.00112, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.259, ret=-0.00145, glen=148, tlen=309, kl=0.000987, act_lr=3.8e-7, ent=2.46]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.16it/s, pg=0.259, ret=-0.00145, glen=148, tlen=309, kl=0.000987, act_lr=3.8e-7, ent=2.46]
2025-07-24 17:18:50.823 | INFO     | orz.ppo.trainer:ppo_local_train_policy:819 - Policy model training, time cost: 48.48s
[36m(PolicyRayActorBase pid=1435640)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.16it/s, pg=-0.218, ret=0.00211, glen=105, tlen=265, kl=0.00117, act_lr=4e-7, ent=1.66]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=-0.218, ret=0.00211, glen=105, tlen=265, kl=0.00117, act_lr=4e-7, ent=1.66]
2025-07-24 17:18:51.682 | INFO     | orz.ppo.trainer:ppo_local_train_policy:826 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-24 17:18:54.284 | INFO     | orz.ppo.trainer:ppo_local_train_policy:828 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-24 17:18:54.615 | INFO     | orz.ppo.trainer:train:149 - Actor model training, time cost: 52.39s
2025-07-24 17:18:54.622 | INFO     | orz.ppo.trainer:train:174 - {'policy_loss': -0.011518278292247228, 'actor_lr': 3.8035713194923506e-07, 'clip_ratio': 0.0, 'entropy': 1.755924410053662, 'kl': 0.0011276347296578543, 'response_length': 112.190030506679, 'total_length': 272.52758407592773, 'teacher_total_length': 284.82583400181363, 'return': -6.525290284896203e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [26:25<22:57, 229.61s/it][A2025-07-24 17:18:54.629 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=1435060)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=1435062)[0m Processed prompts:   1%|          | 1/171 [00:00<01:09,  2.44it/s, est. speed input: 441.15 toks/s, output: 29.25 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 66/172 [00:02<00:02, 46.97it/s, est. speed input: 5620.92 toks/s, output: 1382.77 toks/s]Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 75/172 [00:02<00:01, 56.40it/s, est. speed input: 6089.27 toks/s, output: 1585.80 toks/s]
[36m(LLMActor pid=1435060)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:05<00:00, 14.19it/s, est. speed input: 5633.58 toks/s, output: 2582.87 toks/s]
[36m(LLMActor pid=1435059)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=1435061)[0m Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/172 [00:04<00:00, 20.03it/s, est. speed input: 5655.93 toks/s, output: 2647.15 toks/s][32m [repeated 111x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:10<00:01,  1.72it/s, est. speed input: 2943.60 toks/s, output: 1760.61 toks/s][32m [repeated 26x across cluster][0m
[36m(LLMActor pid=1435062)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00,  2.21it/s, est. speed input: 2723.51 toks/s, output: 1776.63 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:11<00:00, 15.02it/s, est. speed input: 2723.51 toks/s, output: 1776.63 toks/s]
2025-07-24 17:19:09.888 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 505.7686,strategyqa_test/accuracy: 0.3493,eval_accuracy: 0.3493
2025-07-24 17:19:10.142 | INFO     | orz.ppo.trainer:make_experience:231 - start generation from teacher prompts
2025-07-24 17:20:39.162 | INFO     | orz.ppo.trainer:make_experience:234 - generate local rollout batch done
2025-07-24 17:20:39.345 | INFO     | orz.ppo.trainer:make_experience:238 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-24 17:20:39.345 | INFO     | orz.ppo.trainer:make_experience:218 - Generate sequences via vllm engines, time cost: 89.20s
2025-07-24 17:20:41.356 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0082,avg_pass_at_n: 1.0000,avg_num_tokens: 108.6934,std_num_tokens: 116.0161,avg_correct_num_tokens: 102.0378,std_correct_num_tokens: 81.0916,avg_incorrect_num_tokens: 122.3441,std_incorrect_num_tokens: 165.2319
2025-07-24 17:20:41.801 | INFO     | orz.ppo.trainer:make_experience:249 - Calculate custom rewards, time cost: 2.46s
2025-07-24 17:20:44.919 | INFO     | orz.ppo.trainer:make_experience:265 - Packing samples, time cost: 3.12s

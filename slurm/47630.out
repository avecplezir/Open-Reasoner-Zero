[2025-07-23 11:46:25,499] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-07-23 11:46:30.960 | INFO     | __main__:<module>:136 - --------- config key ---------        ------ value ------
seed                                  42
ref_num_nodes                         4
ref_num_gpus_per_node                 1
reward_num_nodes                      1
reward_num_gpus_per_node              2
actor_num_nodes                       4
actor_num_gpus_per_node               1
critic_num_nodes                      4
critic_num_gpus_per_node              1
colocate_critic_reward                True
colocate_actor_ref                    True
colocate_all                          True
vllm_num_engines                      4
vllm_tensor_parallel_size             1
vllm_sync_backend                     nccl
local_rank                            -1
pretrain                              /home/a/anokhin/links/scratch/Qwen2.5-1.5B
critic_pretrain
reward_pretrain                       <class 'NoneType'>
ckpt_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0
save_path                             /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0
tensorboard_log_dir                   /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_grpo-4gpu-v0
prompt_data                           <class 'omegaconf.listconfig.ListConfig'>
load_checkpoint                       False
zero_stage                            3
bf16                                  True
zpg                                   1
adam_offload                          False
flash_attn                            True
grad_accum_dtype                      <class 'NoneType'>
disable_trace_cache                   False
gradient_checkpointing                True
gradient_checkpointing_use_reentrant  False
disable_fast_tokenizer                False
target_modules                        all-linear
enable_prefix_caching                 True
enable_chunked_prefill                False
max_num_batched_tokens                2048
enforce_eager                         False
gpu_memory_utilization                0.25
eval_steps                            -1
save_steps                            -1
save_interval                         50
actor_learning_rate                   1e-06
critic_learning_rate                  5e-06
num_episodes                          20
max_epochs                            1
prompt_max_len                        2048
generate_max_len                      8000
train_batch_size                      256
micro_train_batch_size                1
rollout_batch_size                    128
micro_rollout_batch_size              128
micro_forward_batch_size              1
policy_update_steps                   1
critic_update_steps                   12
max_len                               8192
max_norm                              1.0
num_warmup_steps                      50
l2                                    0.0
eps_clip                              0.2
value_clip                            0.2
lambd                                 1.0
gamma                                 1.0
normalize_reward                      True
top_p                                 1.0
temperature                           1.0
freezing_actor_steps                  -1
n_samples_per_prompt                  64
kl_target                             <class 'NoneType'>
init_kl_coef                          0
use_kl_estimator_k3                   True
use_abs_kl                            False
use_kl_loss                           True
kl_loss_coef                          0.0
adam_betas                            (0.9, 0.95)
reward_clip_range                     (-10, 10)
use_compute_reward_fn                 True
advantage_normalize                   True
value_head_prefix                     value_head
ref_reward_offload                    False
enable_eval                           True
eval_interval                         10
update_ref_every_epoch                True
use_orm_score                         False
total_num_nodes                       4
exp_name                              binary_orz_1p5b_ppo_grpo-4gpu-v0
eval_prompt_data                      <class 'omegaconf.listconfig.ListConfig'>
prompt_data_probs                     <class 'omegaconf.listconfig.ListConfig'>
packing_max_len                       10048
top_k                                 -1
stop                                  <class 'omegaconf.listconfig.ListConfig'>
use_grpo                              True
wandb: Currently logged in as: avecplezir (irina-rish). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.21.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_grpo-4gpu-v0/wandb/run-20250723_114632-xj9cg7ms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run binary_orz_1p5b_ppo_grpo-4gpu-v0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/irina-rish/open-reasoner-zero
wandb: üöÄ View run at https://wandb.ai/irina-rish/open-reasoner-zero/runs/xj9cg7ms
2025-07-23 11:46:36,956	INFO worker.py:1841 -- Started a local Ray instance.
[36m(LLMActor pid=884796)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(LLMActor pid=884796)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.41s/it]
[36m(LLMActor pid=884796)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.41s/it]
[36m(LLMActor pid=884796)[0m 
[36m(LLMActor pid=884797)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m 
[36m(LLMActor pid=884799)[0m 
[36m(LLMActor pid=884797)[0m 
2025-07-23 11:47:15.034 | INFO     | orz.ppo.utils:create_vllm_engines:452 - Offloaded all vLLM engines to CPU
2025-07-23 11:47:15.267 | INFO     | playground.orz_7b_ppo:train_dataset:580 - Start processing 1603 dialogues
2025-07-23 11:47:16.245 | INFO     | playground.orz_7b_ppo:train_dataset:589 - Finished processing 1603 dialogues
2025-07-23 11:47:16.248 | INFO     | playground.orz_7b_ppo:eval_dataset:603 - Start processing 687 dialogues
2025-07-23 11:47:16.688 | INFO     | playground.orz_7b_ppo:eval_dataset:612 - Finished processing 687 dialogues
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/a/anokhin/links/scratch/orz_logs/binary_orz_1p5b_ppo_grpo-4gpu-v0
[36m(RefRayActorBase pid=886179)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(LLMActor pid=884797)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.41s/it][32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 4x across cluster][0m
[36m(PolicyRayActorBase pid=885530)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...
[36m(PolicyRayActorBase pid=885530)[0m Detected CUDA files, patching ldflags
[36m(PolicyRayActorBase pid=885530)[0m Emitting ninja build file /home/a/anokhin/.cache/torch_extensions/py312_cu122/fused_adam/build.ninja...
[36m(PolicyRayActorBase pid=885530)[0m /home/a/anokhin/envs/reasoner3.12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(PolicyRayActorBase pid=885530)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(PolicyRayActorBase pid=885530)[0m   warnings.warn(
[36m(PolicyRayActorBase pid=885530)[0m Building extension module fused_adam...
[36m(PolicyRayActorBase pid=885530)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(PolicyRayActorBase pid=885530)[0m Loading extension module fused_adam...
[36m(PolicyRayActorBase pid=885531)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
2025-07-23 11:47:52.486 | INFO     | orz.ppo.trainer:build_models:728 - init policy/ref/critic/reward models done
2025-07-23 11:47:53.153 | INFO     | orz.ppo.trainer:train:73 - Create vllm engine gourps done.
2025-07-23 11:47:55.732 | INFO     | orz.ppo.trainer:train:75 - Sync actor weights to vllm engines, time cost: 2.58s
2025-07-23 11:47:56.028 | INFO     | orz.ppo.trainer:train:79 - Offload policy model to cpu, time cost: 0.30s
math_verify is not installed in this environment
[36m(LLMActor pid=884796)[0m INFO 07-23 11:46:45 __init__.py:207] Automatically detected platform cuda.
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:00 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.
[36m(LLMActor pid=884799)[0m WARNING 07-23 11:47:00 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=884799)[0m WARNING 07-23 11:47:00 config.py:685] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=45, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=884797)[0m INFO 07-23 11:46:45 __init__.py:207] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(LLMActor pid=884796)[0m INFO 07-23 11:47:00 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'score', 'generate', 'classify'}. Defaulting to 'generate'.
[36m(LLMActor pid=884798)[0m INFO 07-23 11:47:00 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:00 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:01 cuda.py:229] Using Flash Attention backend.
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:03 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:11 model_runner.py:1115] Loading model weights took 2.9105 GB
[36m(LLMActor pid=884797)[0m WARNING 07-23 11:47:00 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884797)[0m WARNING 07-23 11:47:00 config.py:685] Async output processing is not supported on the current platform type cuda.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', speculative_config=None, tokenizer='/home/a/anokhin/links/scratch/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=44, served_model_name=/home/a/anokhin/links/scratch/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, [32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m INFO 07-23 11:47:01 cuda.py:229] Using Flash Attention backend.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:03 model_runner.py:1110] Starting to load model /home/a/anokhin/links/scratch/Qwen2.5-1.5B...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:12 worker.py:267] Memory profiling takes 0.52 seconds
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:12 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:12 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:12 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x
[36m(LLMActor pid=884797)[0m INFO 07-23 11:47:14 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.59 seconds
[36m(pid=885360)[0m [2025-07-23 11:47:19,158] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:11 model_runner.py:1115] Loading model weights took 2.9105 GB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m INFO 07-23 11:47:12 worker.py:267] Memory profiling takes 0.53 seconds[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m INFO 07-23 11:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.25) = 19.80GiB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m INFO 07-23 11:47:12 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 15.99GiB.[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:12 executor_base.py:111] # cuda blocks: 2338, # CPU blocks: 585[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:12 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 73.06x[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m INFO 07-23 11:47:14 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.60 seconds[32m [repeated 3x across cluster][0m
[36m(pid=885530)[0m [2025-07-23 11:47:25,648] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=885532)[0m [2025-07-23 11:47:26,509] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:30,612] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:30,612] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(pid=886179)[0m [2025-07-23 11:47:33,227] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 3x across cluster][0m
[36m(RefRayActorBase pid=886179)[0m [2025-07-23 11:47:37,209] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=886179)[0m [2025-07-23 11:47:37,319] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=886178)[0m [2025-07-23 11:47:37,644] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:41,025] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(pid=886178)[0m [2025-07-23 11:47:33,665] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 2x across cluster][0m
[36m(RefRayActorBase pid=886177)[0m [2025-07-23 11:47:42,442] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 4x across cluster][0m
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,511] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,511] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,519] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,521] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,691] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,692] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.71 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,692] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.65 GB, percent = 9.9%
[36m(RefRayActorBase pid=886177)[0m [2025-07-23 11:47:37,675] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=885532)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,818] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,819] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,819] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.65 GB, percent = 9.9%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=885532)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=885532)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=885532)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=885532)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=885532)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=885532)[0m     "profile": false
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "start_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "end_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=885532)[0m     "model_info": null, 
[36m(RefRayActorBase pid=885532)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=885532)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=885532)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=885532)[0m     "fast": true, 
[36m(RefRayActorBase pid=885532)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=885532)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=885532)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=885532)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,820] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x764c4810eb40>
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=885532)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=885532)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=885532)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=885532)[0m     "detailed": true, 
[36m(RefRayActorBase pid=885532)[0m     "output_file": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=885532)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=885532)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=885532)[0m     "load_path": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,821] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 11:47:42,822] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=885532)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=885532)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=885532)[0m         "stage": 3, 
[36m(RefRayActorBase pid=885532)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=885532)[0m         "offload_param": {
[36m(RefRayActorBase pid=885532)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=885532)[0m             "pin_memory": true
[36m(RefRayActorBase pid=885532)[0m         }
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "bf16": {
[36m(RefRayActorBase pid=885532)[0m         "enabled": true
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=885532)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=885532)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=885532)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=885532)[0m }
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:42,940] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:42,940] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:46,330] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[36m(PolicyRayActorBase pid=885530)[0m ninja: no work to do.
[36m(PolicyRayActorBase pid=885530)[0m Time to load fused_adam op: 1.1279067993164062 seconds
[36m(PolicyRayActorBase pid=885530)[0m [2025-07-23 11:47:48,873] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=885531)[0m [2025-07-23 11:47:42,946] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,925] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,925] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,950] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,952] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,953] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,976] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,977] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,977] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:48,977] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,177] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,178] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 1.71 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,178] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.05 GB, percent = 10.3%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,179] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,179] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,293] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,293] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,293] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.05 GB, percent = 10.3%
[36m(PolicyRayActorBase pid=885360)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,420] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,420] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,420] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.05 GB, percent = 10.3%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,538] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,539] [INFO] [utils.py:782:see_memory_usage] MA 0.73 GB         Max_MA 0.73 GB         CA 1.75 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:49,539] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.05 GB, percent = 10.3%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,262] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,263] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.73 GB         CA 0.72 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,263] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.94 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,378] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,379] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.72 GB         Max_CA 1 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,379] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.9 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,498] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,499] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.88 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,499] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.9 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,617] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,618] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 2.88 GB         Max_CA 3 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,618] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.9 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,734] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,735] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 3.59 GB         CA 4.32 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,735] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.9 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,735] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,947] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,948] [INFO] [utils.py:782:see_memory_usage] MA 4.53 GB         Max_MA 5.39 GB         CA 5.76 GB         Max_CA 6 GB 
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,948] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.9 GB, percent = 10.9%
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,948] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,948] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,948] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7c3dc8ff3f20>
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,948] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(PolicyRayActorBase pid=885360)[0m     "partition_activations": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "contiguous_memory_optimization": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "cpu_checkpointing": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "number_checkpoints": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "synchronize_checkpoint_boundary": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "profile": false
[36m(PolicyRayActorBase pid=885360)[0m }
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(PolicyRayActorBase pid=885360)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "start_step": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "end_step": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "metric_path": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "arg_mappings": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "metric": "throughput", 
[36m(PolicyRayActorBase pid=885360)[0m     "model_info": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "results_dir": "autotuning_results", 
[36m(PolicyRayActorBase pid=885360)[0m     "exps_dir": "autotuning_exps", 
[36m(PolicyRayActorBase pid=885360)[0m     "overwrite": true, 
[36m(PolicyRayActorBase pid=885360)[0m     "fast": true, 
[36m(PolicyRayActorBase pid=885360)[0m     "start_profile_step": 3, 
[36m(PolicyRayActorBase pid=885360)[0m     "end_profile_step": 5, 
[36m(PolicyRayActorBase pid=885360)[0m     "tuner_type": "gridsearch", 
[36m(PolicyRayActorBase pid=885360)[0m     "tuner_early_stopping": 5, 
[36m(PolicyRayActorBase pid=885360)[0m     "tuner_num_trials": 50, 
[36m(PolicyRayActorBase pid=885360)[0m     "model_info_path": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "mp_size": 1, 
[36m(PolicyRayActorBase pid=885360)[0m     "max_train_batch_size": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "min_train_batch_size": 1, 
[36m(PolicyRayActorBase pid=885360)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(PolicyRayActorBase pid=885360)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=885360)[0m     "num_tuning_micro_batch_sizes": 3
[36m(PolicyRayActorBase pid=885360)[0m }
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7c3dc8ff2780>
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,949] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(PolicyRayActorBase pid=885360)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "recompute_fwd_factor": 0.0, 
[36m(PolicyRayActorBase pid=885360)[0m     "profile_step": 1, 
[36m(PolicyRayActorBase pid=885360)[0m     "module_depth": -1, 
[36m(PolicyRayActorBase pid=885360)[0m     "top_modules": 1, 
[36m(PolicyRayActorBase pid=885360)[0m     "detailed": true, 
[36m(PolicyRayActorBase pid=885360)[0m     "output_file": null
[36m(PolicyRayActorBase pid=885360)[0m }
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(PolicyRayActorBase pid=885360)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "persistent_storage_path": null, 
[36m(PolicyRayActorBase pid=885360)[0m     "persistent_time_interval": 100, 
[36m(PolicyRayActorBase pid=885360)[0m     "num_of_version_in_retention": 2, 
[36m(PolicyRayActorBase pid=885360)[0m     "enable_nebula_load": true, 
[36m(PolicyRayActorBase pid=885360)[0m     "load_path": null
[36m(PolicyRayActorBase pid=885360)[0m }
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,950] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:47:50,951] [INFO] [config.py:989:print_user_config]   json = {
[36m(PolicyRayActorBase pid=885360)[0m     "steps_per_print": 100, 
[36m(PolicyRayActorBase pid=885360)[0m     "zero_optimization": {
[36m(PolicyRayActorBase pid=885360)[0m         "stage": 3, 
[36m(PolicyRayActorBase pid=885360)[0m         "offload_param": {
[36m(PolicyRayActorBase pid=885360)[0m             "device": "none"
[36m(PolicyRayActorBase pid=885360)[0m         }, 
[36m(PolicyRayActorBase pid=885360)[0m         "offload_optimizer": {
[36m(PolicyRayActorBase pid=885360)[0m             "device": "none", 
[36m(PolicyRayActorBase pid=885360)[0m             "pin_memory": true
[36m(PolicyRayActorBase pid=885360)[0m         }, 
[36m(PolicyRayActorBase pid=885360)[0m         "sub_group_size": "auto", 
[36m(PolicyRayActorBase pid=885360)[0m         "stage3_max_live_parameters": "auto", 
[36m(PolicyRayActorBase pid=885360)[0m         "stage3_max_reuse_distance": "auto", 
[36m(PolicyRayActorBase pid=885360)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(PolicyRayActorBase pid=885360)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=885360)[0m         "reduce_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=885360)[0m         "zero_hpz_partition_size": 1, 
[36m(PolicyRayActorBase pid=885360)[0m         "zero_quantized_weights": false, 
[36m(PolicyRayActorBase pid=885360)[0m         "zero_quantized_gradients": false
[36m(PolicyRayActorBase pid=885360)[0m     }, 
[36m(PolicyRayActorBase pid=885360)[0m     "bf16": {
[36m(PolicyRayActorBase pid=885360)[0m         "enabled": true
[36m(PolicyRayActorBase pid=885360)[0m     }, 
[36m(PolicyRayActorBase pid=885360)[0m     "gradient_clipping": 1.0, 
[36m(PolicyRayActorBase pid=885360)[0m     "prescale_gradients": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "wall_clock_breakdown": false, 
[36m(PolicyRayActorBase pid=885360)[0m     "data_types": {
[36m(PolicyRayActorBase pid=885360)[0m         "grad_accum_dtype": "fp32"
[36m(PolicyRayActorBase pid=885360)[0m     }, 
[36m(PolicyRayActorBase pid=885360)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=885360)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=885360)[0m }
[36m(LLMActor pid=884797)[0m init_process_group: master_address=10.224.3.59, master_port=59053,  rank=3, world_size=5, group_name=openrlhf
[36m(PolicyRayActorBase pid=885360)[0m WARNING:using --vllm_sync_backend=gloo for vLLM version > 0.4.2 (or export NCCL_P2P_DISABLE=1)
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885531)[0m Time to load fused_adam op: 1.2158455848693848 seconds[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=885531)[0m [2025-07-23 11:47:48,963] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 3x across cluster][0m
Episode [1/20]:   0%|          | 0/13 [00:00<?, ?it/s]2025-07-23 11:47:56.159 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(PolicyRayActorBase pid=885360)[0m Using /home/a/anokhin/.cache/torch_extensions/py312_cu122 as PyTorch extensions root...[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=885531)[0m Loading extension module fused_adam...[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 1/172 [00:00<00:58,  2.92it/s, est. speed input: 523.12 toks/s, output: 20.46 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 2/172 [00:00<00:36,  4.70it/s, est. speed input: 779.62 toks/s, output: 40.92 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 159/172 [00:04<00:00, 20.54it/s, est. speed input: 6289.75 toks/s, output: 2753.96 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:05<00:01, 12.84it/s, est. speed input: 5250.87 toks/s, output: 2410.86 toks/s][32m [repeated 109x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00,  1.88it/s, est. speed input: 3128.94 toks/s, output: 1918.77 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00, 17.26it/s, est. speed input: 3128.94 toks/s, output: 1918.77 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:08<00:00,  2.73it/s, est. speed input: 3468.77 toks/s, output: 1831.30 toks/s][32m [repeated 30x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:05<00:01, 12.36it/s, est. speed input: 5137.71 toks/s, output: 2427.17 toks/s][32m [repeated 2x across cluster][0m
2025-07-23 11:48:11.758 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 494.2853,strategyqa_test/accuracy: 0.3115,eval_accuracy: 0.3115
2025-07-23 11:48:12.385 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 11:50:52.678 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 11:50:52.868 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 11:50:52.868 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 160.48s
[36m(get_reflection_pattern_score pid=889771)[0m /home/a/anokhin/Open-Reasoner-Zero/orz/ppo/tools/math_utils.py:314: SyntaxWarning: invalid escape sequence '\%'
[36m(get_reflection_pattern_score pid=889771)[0m   string = string.replace("\%", "")
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00,  1.23s/it, est. speed input: 2191.73 toks/s, output: 1254.29 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00, 12.10it/s, est. speed input: 2191.73 toks/s, output: 1254.29 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:12<00:00,  1.11it/s, est. speed input: 2484.68 toks/s, output: 1498.25 toks/s][32m [repeated 2x across cluster][0m
2025-07-23 11:51:11.224 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0090,avg_pass_at_n: 1.0000,avg_num_tokens: 111.2201,std_num_tokens: 170.2698,avg_correct_num_tokens: 92.9702,std_correct_num_tokens: 82.8752,avg_incorrect_num_tokens: 120.4574,std_incorrect_num_tokens: 199.8397
2025-07-23 11:51:11.618 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 18.75s
2025-07-23 11:51:12.958 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.34s
2025-07-23 11:51:42.238 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 227
2025-07-23 11:51:42.239 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.28s
2025-07-23 11:51:43.079 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.84s
2025-07-23 11:51:43.079 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0013085801649524826, avg_kl: 0.0, avg_response_length: 120.11578345613857, avg_orm_score: 0.0, avg_custom_rewards: -0.0013085801649524826
2025-07-23 11:51:43.145 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter0_replay_buffer.jsonl
2025-07-23 11:51:44.807 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.66s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0516, ret=-0.00028, glen=105, tlen=265, kl=0, act_lr=0, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<01:17,  1.39s/it, pg=0.0516, ret=-0.00028, glen=105, tlen=265, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:02<01:17,  1.39s/it, pg=0.13, ret=-0.000291, glen=103, tlen=262, kl=0, act_lr=0, ent=1.6]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<01:05,  1.19s/it, pg=0.13, ret=-0.000291, glen=103, tlen=262, kl=0, act_lr=0, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:04<01:05,  1.19s/it, pg=-0.0997, ret=-2.03e-5, glen=99.3, tlen=260, kl=0, act_lr=0, ent=1.64]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:04<01:14,  1.37s/it, pg=-0.0997, ret=-2.03e-5, glen=99.3, tlen=260, kl=0, act_lr=0, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:04<01:14,  1.37s/it, pg=0.0302, ret=0.000233, glen=105, tlen=265, kl=0, act_lr=0, ent=1.71]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<01:03,  1.20s/it, pg=0.0302, ret=0.000233, glen=105, tlen=265, kl=0, act_lr=0, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:05<01:03,  1.20s/it, pg=-0.0234, ret=-0.000794, glen=115, tlen=275, kl=0, act_lr=0, ent=1.6]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:55,  1.07s/it, pg=-0.0234, ret=-0.000794, glen=115, tlen=275, kl=0, act_lr=0, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:06<00:55,  1.07s/it, pg=0.0525, ret=-0.00115, glen=105, tlen=264, kl=0, act_lr=0, ent=1.74] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:50,  1.00it/s, pg=0.0525, ret=-0.00115, glen=105, tlen=264, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:07<00:50,  1.00it/s, pg=0.168, ret=-0.00176, glen=120, tlen=279, kl=0, act_lr=0, ent=2.02] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:47,  1.05it/s, pg=0.168, ret=-0.00176, glen=120, tlen=279, kl=0, act_lr=0, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:08<00:47,  1.05it/s, pg=0.00208, ret=3.67e-5, glen=115, tlen=275, kl=0, act_lr=0, ent=1.84]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:45,  1.09it/s, pg=0.00208, ret=3.67e-5, glen=115, tlen=275, kl=0, act_lr=0, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:09<00:45,  1.09it/s, pg=-0.0483, ret=0.000585, glen=113, tlen=273, kl=0, act_lr=0, ent=1.79]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:09<00:43,  1.11it/s, pg=-0.0483, ret=0.000585, glen=113, tlen=273, kl=0, act_lr=0, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:10<00:43,  1.11it/s, pg=0.0363, ret=-0.000855, glen=112, tlen=271, kl=0, act_lr=0, ent=2.07]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:10<00:41,  1.13it/s, pg=0.0363, ret=-0.000855, glen=112, tlen=271, kl=0, act_lr=0, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:10<00:41,  1.13it/s, pg=-0.216, ret=0.00135, glen=118, tlen=278, kl=0, act_lr=0, ent=1.72]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:41,  1.12it/s, pg=-0.216, ret=0.00135, glen=118, tlen=278, kl=0, act_lr=0, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:11<00:41,  1.12it/s, pg=0.168, ret=-0.00156, glen=113, tlen=273, kl=0, act_lr=0, ent=1.88]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:40,  1.11it/s, pg=0.168, ret=-0.00156, glen=113, tlen=273, kl=0, act_lr=0, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:12<00:40,  1.11it/s, pg=0.109, ret=-0.00115, glen=110, tlen=270, kl=0, act_lr=0, ent=1.62]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:39,  1.13it/s, pg=0.109, ret=-0.00115, glen=110, tlen=270, kl=0, act_lr=0, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:13<00:39,  1.13it/s, pg=0.182, ret=-0.000611, glen=110, tlen=270, kl=0, act_lr=0, ent=1.78]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=0.182, ret=-0.000611, glen=110, tlen=270, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:14<00:37,  1.14it/s, pg=-0.103, ret=0.000497, glen=117, tlen=277, kl=0, act_lr=0, ent=1.85]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=-0.103, ret=0.000497, glen=117, tlen=277, kl=0, act_lr=0, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:15<00:36,  1.15it/s, pg=-0.0389, ret=0.000317, glen=117, tlen=276, kl=0, act_lr=0, ent=1.8]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:15<00:35,  1.16it/s, pg=-0.0389, ret=0.000317, glen=117, tlen=276, kl=0, act_lr=0, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:16<00:35,  1.16it/s, pg=0.0399, ret=-0.00142, glen=111, tlen=270, kl=0, act_lr=0, ent=1.7] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:16<00:34,  1.16it/s, pg=0.0399, ret=-0.00142, glen=111, tlen=270, kl=0, act_lr=0, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:17<00:34,  1.16it/s, pg=-0.167, ret=0.00179, glen=107, tlen=267, kl=0, act_lr=0, ent=1.72]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:17<00:33,  1.17it/s, pg=-0.167, ret=0.00179, glen=107, tlen=267, kl=0, act_lr=0, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:17<00:33,  1.17it/s, pg=0.0384, ret=-3.09e-5, glen=104, tlen=263, kl=0, act_lr=0, ent=1.69]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.0384, ret=-3.09e-5, glen=104, tlen=263, kl=0, act_lr=0, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:18<00:32,  1.17it/s, pg=-0.164, ret=0.00199, glen=108, tlen=268, kl=0, act_lr=0, ent=1.75] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.164, ret=0.00199, glen=108, tlen=268, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:19<00:31,  1.17it/s, pg=-0.176, ret=0.000925, glen=105, tlen=265, kl=0, act_lr=0, ent=1.72]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.18it/s, pg=-0.176, ret=0.000925, glen=105, tlen=265, kl=0, act_lr=0, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:20<00:30,  1.18it/s, pg=-0.0114, ret=0.000232, glen=119, tlen=279, kl=0, act_lr=0, ent=1.76]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.18it/s, pg=-0.0114, ret=0.000232, glen=119, tlen=279, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:21<00:29,  1.18it/s, pg=-0.147, ret=0.00192, glen=140, tlen=300, kl=0, act_lr=0, ent=1.75]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:21<00:28,  1.17it/s, pg=-0.147, ret=0.00192, glen=140, tlen=300, kl=0, act_lr=0, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:22<00:28,  1.17it/s, pg=0.127, ret=-0.000252, glen=111, tlen=271, kl=0, act_lr=0, ent=1.8]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:22<00:28,  1.17it/s, pg=0.127, ret=-0.000252, glen=111, tlen=271, kl=0, act_lr=0, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:23<00:28,  1.17it/s, pg=0.0728, ret=0.000675, glen=111, tlen=271, kl=0, act_lr=0, ent=1.74]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:23<00:27,  1.15it/s, pg=0.0728, ret=0.000675, glen=111, tlen=271, kl=0, act_lr=0, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:23<00:27,  1.15it/s, pg=0.109, ret=-0.000539, glen=116, tlen=276, kl=0, act_lr=0, ent=1.71]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.16it/s, pg=0.109, ret=-0.000539, glen=116, tlen=276, kl=0, act_lr=0, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:25<00:26,  1.16it/s, pg=0.0278, ret=-0.0178, glen=107, tlen=267, kl=0, act_lr=0, ent=1.89] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:25<00:28,  1.06it/s, pg=0.0278, ret=-0.0178, glen=107, tlen=267, kl=0, act_lr=0, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:25<00:28,  1.06it/s, pg=-0.0861, ret=0.00104, glen=104, tlen=263, kl=0, act_lr=0, ent=1.61]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.09it/s, pg=-0.0861, ret=0.00104, glen=104, tlen=263, kl=0, act_lr=0, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:26<00:26,  1.09it/s, pg=0.0637, ret=0.000465, glen=118, tlen=278, kl=0, act_lr=0, ent=1.81]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.12it/s, pg=0.0637, ret=0.000465, glen=118, tlen=278, kl=0, act_lr=0, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:27<00:25,  1.12it/s, pg=-0.0267, ret=0.000204, glen=114, tlen=273, kl=0, act_lr=0, ent=1.8]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.14it/s, pg=-0.0267, ret=0.000204, glen=114, tlen=273, kl=0, act_lr=0, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:28<00:23,  1.14it/s, pg=-0.0216, ret=0.00107, glen=118, tlen=278, kl=0, act_lr=0, ent=1.78]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:22,  1.15it/s, pg=-0.0216, ret=0.00107, glen=118, tlen=278, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:29<00:22,  1.15it/s, pg=-0.101, ret=0.00105, glen=107, tlen=267, kl=0, act_lr=0, ent=1.66] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:29<00:21,  1.16it/s, pg=-0.101, ret=0.00105, glen=107, tlen=267, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:30<00:21,  1.16it/s, pg=0.0845, ret=-0.00202, glen=102, tlen=261, kl=0, act_lr=0, ent=1.68]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:30<00:20,  1.16it/s, pg=0.0845, ret=-0.00202, glen=102, tlen=261, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:30<00:20,  1.16it/s, pg=-0.057, ret=-0.000461, glen=102, tlen=261, kl=0, act_lr=0, ent=1.66]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.17it/s, pg=-0.057, ret=-0.000461, glen=102, tlen=261, kl=0, act_lr=0, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:31<00:19,  1.17it/s, pg=-0.146, ret=0.000841, glen=96.6, tlen=256, kl=0, act_lr=0, ent=1.68]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.17it/s, pg=-0.146, ret=0.000841, glen=96.6, tlen=256, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:32<00:18,  1.17it/s, pg=0.0243, ret=0.000758, glen=143, tlen=303, kl=0, act_lr=0, ent=2.02] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:17,  1.17it/s, pg=0.0243, ret=0.000758, glen=143, tlen=303, kl=0, act_lr=0, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:33<00:17,  1.17it/s, pg=0.0825, ret=-0.00111, glen=128, tlen=287, kl=0, act_lr=0, ent=1.69]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=0.0825, ret=-0.00111, glen=128, tlen=287, kl=0, act_lr=0, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:34<00:17,  1.17it/s, pg=0.0373, ret=-0.000599, glen=98.8, tlen=259, kl=0, act_lr=0, ent=1.63]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=0.0373, ret=-0.000599, glen=98.8, tlen=259, kl=0, act_lr=0, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:35<00:16,  1.17it/s, pg=-0.073, ret=0.000739, glen=118, tlen=277, kl=0, act_lr=0, ent=1.65]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:35<00:15,  1.17it/s, pg=-0.073, ret=0.000739, glen=118, tlen=277, kl=0, act_lr=0, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:36<00:15,  1.17it/s, pg=0.057, ret=0.00935, glen=342, tlen=501, kl=0, act_lr=0, ent=1.46]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:36<00:14,  1.14it/s, pg=0.057, ret=0.00935, glen=342, tlen=501, kl=0, act_lr=0, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:36<00:14,  1.14it/s, pg=-0.0506, ret=-0.00152, glen=105, tlen=264, kl=0, act_lr=0, ent=1.76]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.15it/s, pg=-0.0506, ret=-0.00152, glen=105, tlen=264, kl=0, act_lr=0, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:37<00:13,  1.15it/s, pg=0.0709, ret=-0.000669, glen=103, tlen=263, kl=0, act_lr=0, ent=1.71]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.16it/s, pg=0.0709, ret=-0.000669, glen=103, tlen=263, kl=0, act_lr=0, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:38<00:12,  1.16it/s, pg=-0.00623, ret=-0.00245, glen=103, tlen=263, kl=0, act_lr=0, ent=1.68]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=-0.00623, ret=-0.00245, glen=103, tlen=263, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:39<00:12,  1.16it/s, pg=-0.022, ret=5.43e-5, glen=115, tlen=274, kl=0, act_lr=0, ent=1.95]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.022, ret=5.43e-5, glen=115, tlen=274, kl=0, act_lr=0, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:40<00:11,  1.17it/s, pg=-0.0992, ret=-0.00967, glen=395, tlen=555, kl=0, act_lr=0, ent=1.37]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.13it/s, pg=-0.0992, ret=-0.00967, glen=395, tlen=555, kl=0, act_lr=0, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:41<00:10,  1.13it/s, pg=-0.107, ret=-4.92e-5, glen=116, tlen=275, kl=0, act_lr=0, ent=1.79] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:41<00:09,  1.14it/s, pg=-0.107, ret=-4.92e-5, glen=116, tlen=275, kl=0, act_lr=0, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:42<00:09,  1.14it/s, pg=0.0699, ret=-0.000447, glen=114, tlen=273, kl=0, act_lr=0, ent=2.03]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:42<00:08,  1.15it/s, pg=0.0699, ret=-0.000447, glen=114, tlen=273, kl=0, act_lr=0, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:43<00:08,  1.15it/s, pg=0.0399, ret=-0.000726, glen=102, tlen=262, kl=0, act_lr=0, ent=1.67]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:43<00:07,  1.16it/s, pg=0.0399, ret=-0.000726, glen=102, tlen=262, kl=0, act_lr=0, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:43<00:07,  1.16it/s, pg=-0.0966, ret=-0.000205, glen=101, tlen=260, kl=0, act_lr=0, ent=1.81]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.0966, ret=-0.000205, glen=101, tlen=260, kl=0, act_lr=0, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:44<00:06,  1.17it/s, pg=-0.0347, ret=0.00101, glen=113, tlen=273, kl=0, act_lr=0, ent=1.78]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.17it/s, pg=-0.0347, ret=0.00101, glen=113, tlen=273, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:45<00:06,  1.17it/s, pg=-0.0975, ret=-0.000225, glen=114, tlen=274, kl=0, act_lr=0, ent=1.88]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.0975, ret=-0.000225, glen=114, tlen=274, kl=0, act_lr=0, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:46<00:05,  1.17it/s, pg=0.00956, ret=0.00036, glen=110, tlen=269, kl=0, act_lr=0, ent=1.68]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=0.00956, ret=0.00036, glen=110, tlen=269, kl=0, act_lr=0, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:47<00:04,  1.17it/s, pg=-0.0554, ret=0.000285, glen=116, tlen=276, kl=0, act_lr=0, ent=1.78]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.17it/s, pg=-0.0554, ret=0.000285, glen=116, tlen=276, kl=0, act_lr=0, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:48<00:03,  1.17it/s, pg=-0.0669, ret=0.000129, glen=115, tlen=275, kl=0, act_lr=0, ent=1.71]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:48<00:02,  1.18it/s, pg=-0.0669, ret=0.000129, glen=115, tlen=275, kl=0, act_lr=0, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:48<00:02,  1.18it/s, pg=-0.0446, ret=0.000445, glen=113, tlen=273, kl=0, act_lr=0, ent=1.87]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.18it/s, pg=-0.0446, ret=0.000445, glen=113, tlen=273, kl=0, act_lr=0, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:50<00:01,  1.18it/s, pg=0.109, ret=-0.0014, glen=103, tlen=263, kl=0, act_lr=0, ent=1.59]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:50<00:00,  1.08it/s, pg=0.109, ret=-0.0014, glen=103, tlen=263, kl=0, act_lr=0, ent=1.59]
2025-07-23 11:52:36.255 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 51.15s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:51<00:00,  1.08it/s, pg=-0.254, ret=-7.2e-6, glen=109, tlen=268, kl=0, act_lr=2e-8, ent=2.08]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:51<00:00,  1.10it/s, pg=-0.254, ret=-7.2e-6, glen=109, tlen=268, kl=0, act_lr=2e-8, ent=2.08]
2025-07-23 11:52:37.110 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 11:52:39.680 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-23 11:52:42.128 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 57.09s
2025-07-23 11:52:42.133 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.011347151639168723, 'actor_lr': 3.508771908500102e-10, 'clip_ratio': 0.0, 'entropy': 1.7521670994005705, 'kl': 0.0, 'response_length': 120.19222848457203, 'total_length': 279.9460706208882, 'return': -0.0003808953691061521, 'policy_update_steps': 1.0}
Episode [1/20]:   8%|‚ñä         | 1/13 [04:46<57:13, 286.10s/it]2025-07-23 11:52:42.164 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 11:55:07.791 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 11:55:07.977 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 11:55:07.977 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 145.81s
2025-07-23 11:55:10.306 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0111,avg_pass_at_n: 1.0000,avg_num_tokens: 114.8232,std_num_tokens: 169.9987,avg_correct_num_tokens: 91.6019,std_correct_num_tokens: 79.7716,avg_incorrect_num_tokens: 126.2583,std_incorrect_num_tokens: 198.9997
2025-07-23 11:55:10.782 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.80s
2025-07-23 11:55:12.137 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.35s
2025-07-23 11:55:41.883 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 229
2025-07-23 11:55:41.883 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.74s
2025-07-23 11:55:42.789 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.90s
2025-07-23 11:55:42.790 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0017023362414037005, avg_kl: 0.0, avg_response_length: 119.37104674093588, avg_orm_score: 0.0, avg_custom_rewards: -0.0017023362414037005
2025-07-23 11:55:42.829 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter1_replay_buffer.jsonl
2025-07-23 11:55:44.517 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.69s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s, pg=-0.0566, ret=0.000244, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.77]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:00<00:56,  1.01it/s, pg=-0.0566, ret=0.000244, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:56,  1.01it/s, pg=-0.156, ret=0.000259, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.91] Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:51,  1.09it/s, pg=-0.156, ret=0.000259, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:51,  1.09it/s, pg=-0.121, ret=0.000289, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.91]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:48,  1.13it/s, pg=-0.121, ret=0.000289, glen=102, tlen=262, kl=0, act_lr=2e-8, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:48,  1.13it/s, pg=-0.0524, ret=0.00071, glen=122, tlen=283, kl=0, act_lr=2e-8, ent=1.83]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.14it/s, pg=-0.0524, ret=0.00071, glen=122, tlen=283, kl=0, act_lr=2e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.14it/s, pg=-0.17, ret=0.000572, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.91] Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:45,  1.16it/s, pg=-0.17, ret=0.000572, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:45,  1.16it/s, pg=0.162, ret=-0.000299, glen=164, tlen=324, kl=0, act_lr=2e-8, ent=2.08]Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:45,  1.15it/s, pg=0.162, ret=-0.000299, glen=164, tlen=324, kl=0, act_lr=2e-8, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:45,  1.15it/s, pg=-0.249, ret=0.00107, glen=115, tlen=275, kl=0, act_lr=2e-8, ent=1.97] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:44,  1.14it/s, pg=-0.249, ret=0.00107, glen=115, tlen=275, kl=0, act_lr=2e-8, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:44,  1.14it/s, pg=0.00555, ret=0.000578, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.79]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.15it/s, pg=0.00555, ret=0.000578, glen=104, tlen=265, kl=0, act_lr=2e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.15it/s, pg=0.0402, ret=-0.0017, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.7]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.14it/s, pg=0.0402, ret=-0.0017, glen=109, tlen=270, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.14it/s, pg=-0.0956, ret=0.000232, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.15it/s, pg=-0.0956, ret=0.000232, glen=113, tlen=274, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.15it/s, pg=0.0428, ret=-0.000321, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:41,  1.14it/s, pg=0.0428, ret=-0.000321, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:41,  1.14it/s, pg=-0.0397, ret=0.000132, glen=128, tlen=289, kl=0, act_lr=2e-8, ent=1.81]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:40,  1.14it/s, pg=-0.0397, ret=0.000132, glen=128, tlen=289, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.14it/s, pg=-0.0139, ret=0.000345, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=2]   Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:39,  1.15it/s, pg=-0.0139, ret=0.000345, glen=117, tlen=277, kl=0, act_lr=2e-8, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:39,  1.15it/s, pg=0.0868, ret=-0.000958, glen=99.6, tlen=260, kl=0, act_lr=2e-8, ent=1.75]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:38,  1.16it/s, pg=0.0868, ret=-0.000958, glen=99.6, tlen=260, kl=0, act_lr=2e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:38,  1.16it/s, pg=0.02, ret=8.22e-5, glen=93, tlen=253, kl=0, act_lr=2e-8, ent=1.66]      Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.16it/s, pg=0.02, ret=8.22e-5, glen=93, tlen=253, kl=0, act_lr=2e-8, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.16it/s, pg=0.085, ret=-0.000602, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.79]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:13<00:35,  1.17it/s, pg=0.085, ret=-0.000602, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:35,  1.17it/s, pg=0.0728, ret=-0.000267, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.84]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.17it/s, pg=0.0728, ret=-0.000267, glen=105, tlen=266, kl=0, act_lr=2e-8, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.17it/s, pg=0.0131, ret=-0.000367, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.85]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.17it/s, pg=0.0131, ret=-0.000367, glen=106, tlen=266, kl=0, act_lr=2e-8, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.17it/s, pg=-0.0855, ret=0.000672, glen=124, tlen=284, kl=0, act_lr=2e-8, ent=1.54]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:33,  1.17it/s, pg=-0.0855, ret=0.000672, glen=124, tlen=284, kl=0, act_lr=2e-8, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:33,  1.17it/s, pg=-0.161, ret=0.00173, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]  Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:32,  1.17it/s, pg=-0.161, ret=0.00173, glen=116, tlen=276, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:32,  1.17it/s, pg=-0.0363, ret=-0.000811, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.75]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:31,  1.17it/s, pg=-0.0363, ret=-0.000811, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:31,  1.17it/s, pg=-0.234, ret=0.00288, glen=133, tlen=293, kl=0, act_lr=2e-8, ent=2.17]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.234, ret=0.00288, glen=133, tlen=293, kl=0, act_lr=2e-8, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:30,  1.17it/s, pg=-0.195, ret=9.07e-6, glen=101, tlen=261, kl=0, act_lr=2e-8, ent=1.73]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:19<00:29,  1.17it/s, pg=-0.195, ret=9.07e-6, glen=101, tlen=261, kl=0, act_lr=2e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=0.0658, ret=1.52e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.63]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.16it/s, pg=0.0658, ret=1.52e-5, glen=114, tlen=274, kl=0, act_lr=2e-8, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.16it/s, pg=0.104, ret=-0.00597, glen=224, tlen=384, kl=0, act_lr=2e-8, ent=2.03]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:29,  1.14it/s, pg=0.104, ret=-0.00597, glen=224, tlen=384, kl=0, act_lr=2e-8, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:29,  1.14it/s, pg=-0.154, ret=0.00035, glen=122, tlen=282, kl=0, act_lr=2e-8, ent=1.84]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.15it/s, pg=-0.154, ret=0.00035, glen=122, tlen=282, kl=0, act_lr=2e-8, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.15it/s, pg=0.229, ret=-0.00112, glen=150, tlen=310, kl=0, act_lr=2e-8, ent=2.04]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:29,  1.05it/s, pg=0.229, ret=-0.00112, glen=150, tlen=310, kl=0, act_lr=2e-8, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:29,  1.05it/s, pg=0.13, ret=-0.00101, glen=115, tlen=275, kl=0, act_lr=2e-8, ent=1.88] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:27,  1.09it/s, pg=0.13, ret=-0.00101, glen=115, tlen=275, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:27,  1.09it/s, pg=-0.0508, ret=-0.00134, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.63]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:26,  1.11it/s, pg=-0.0508, ret=-0.00134, glen=106, tlen=267, kl=0, act_lr=2e-8, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:26,  1.11it/s, pg=0.0327, ret=-0.000899, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.88]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:24,  1.13it/s, pg=0.0327, ret=-0.000899, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:24,  1.13it/s, pg=0.151, ret=7.21e-5, glen=218, tlen=378, kl=0, act_lr=2e-8, ent=2.43]   Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:24,  1.12it/s, pg=0.151, ret=7.21e-5, glen=218, tlen=378, kl=0, act_lr=2e-8, ent=2.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:24,  1.12it/s, pg=-0.0318, ret=0.00104, glen=119, tlen=279, kl=0, act_lr=2e-8, ent=1.96]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.14it/s, pg=-0.0318, ret=0.00104, glen=119, tlen=279, kl=0, act_lr=2e-8, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.14it/s, pg=0.026, ret=-0.00171, glen=131, tlen=291, kl=0, act_lr=2e-8, ent=2.02] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.15it/s, pg=0.026, ret=-0.00171, glen=131, tlen=291, kl=0, act_lr=2e-8, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.15it/s, pg=-0.0143, ret=0.000424, glen=123, tlen=284, kl=0, act_lr=2e-8, ent=2.03]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.15it/s, pg=-0.0143, ret=0.000424, glen=123, tlen=284, kl=0, act_lr=2e-8, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.15it/s, pg=-0.00974, ret=0.000414, glen=134, tlen=294, kl=0, act_lr=2e-8, ent=2.3]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=-0.00974, ret=0.000414, glen=134, tlen=294, kl=0, act_lr=2e-8, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=-0.229, ret=0.00067, glen=115, tlen=275, kl=0, act_lr=2e-8, ent=1.96]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.16it/s, pg=-0.229, ret=0.00067, glen=115, tlen=275, kl=0, act_lr=2e-8, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.16it/s, pg=0.0493, ret=-0.00036, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.86]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:17,  1.17it/s, pg=0.0493, ret=-0.00036, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=-0.0353, ret=0.000763, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.79]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=-0.0353, ret=0.000763, glen=112, tlen=272, kl=0, act_lr=2e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.122, ret=-0.00105, glen=116, tlen=277, kl=0, act_lr=2e-8, ent=1.78]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.17it/s, pg=0.122, ret=-0.00105, glen=116, tlen=277, kl=0, act_lr=2e-8, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=0.22, ret=-0.00217, glen=140, tlen=300, kl=0, act_lr=2e-8, ent=2.01] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=0.22, ret=-0.00217, glen=140, tlen=300, kl=0, act_lr=2e-8, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=0.0515, ret=-0.000581, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.84]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.17it/s, pg=0.0515, ret=-0.000581, glen=111, tlen=271, kl=0, act_lr=2e-8, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.17it/s, pg=0.137, ret=-0.00127, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.58]  Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=0.137, ret=-0.00127, glen=101, tlen=262, kl=0, act_lr=2e-8, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=-0.0181, ret=0.0017, glen=131, tlen=292, kl=0, act_lr=2e-8, ent=2.13]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:12,  1.17it/s, pg=-0.0181, ret=0.0017, glen=131, tlen=292, kl=0, act_lr=2e-8, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:12,  1.17it/s, pg=0.0478, ret=-0.00011, glen=118, tlen=278, kl=0, act_lr=2e-8, ent=1.88]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:11,  1.17it/s, pg=0.0478, ret=-0.00011, glen=118, tlen=278, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:11,  1.17it/s, pg=0.0427, ret=-0.000325, glen=124, tlen=284, kl=0, act_lr=2e-8, ent=2.08]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.0427, ret=-0.000325, glen=124, tlen=284, kl=0, act_lr=2e-8, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.17it/s, pg=0.00537, ret=-0.000418, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.71]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:39<00:10,  1.17it/s, pg=0.00537, ret=-0.000418, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.17it/s, pg=-0.138, ret=0.00017, glen=99.9, tlen=260, kl=0, act_lr=2e-8, ent=1.72]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.138, ret=0.00017, glen=99.9, tlen=260, kl=0, act_lr=2e-8, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=-0.00201, ret=0.000155, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.92]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.17it/s, pg=-0.00201, ret=0.000155, glen=115, tlen=276, kl=0, act_lr=2e-8, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.17it/s, pg=-0.136, ret=0.00163, glen=121, tlen=281, kl=0, act_lr=2e-8, ent=1.92]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.17it/s, pg=-0.136, ret=0.00163, glen=121, tlen=281, kl=0, act_lr=2e-8, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.17it/s, pg=0.0196, ret=-0.00167, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.81]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.17it/s, pg=0.0196, ret=-0.00167, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.17it/s, pg=-0.0354, ret=-0.000208, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.88]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:05,  1.17it/s, pg=-0.0354, ret=-0.000208, glen=111, tlen=272, kl=0, act_lr=2e-8, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:05,  1.17it/s, pg=0.157, ret=-0.00128, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.63]   Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.157, ret=-0.00128, glen=110, tlen=270, kl=0, act_lr=2e-8, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.17it/s, pg=0.082, ret=0.000392, glen=146, tlen=306, kl=0, act_lr=2e-8, ent=1.6] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:45<00:04,  1.16it/s, pg=0.082, ret=0.000392, glen=146, tlen=306, kl=0, act_lr=2e-8, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.16it/s, pg=-0.156, ret=0.00114, glen=99.6, tlen=260, kl=0, act_lr=2e-8, ent=1.68]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.17it/s, pg=-0.156, ret=0.00114, glen=99.6, tlen=260, kl=0, act_lr=2e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.17it/s, pg=-0.0532, ret=0.000254, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.7]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.17it/s, pg=-0.0532, ret=0.000254, glen=107, tlen=267, kl=0, act_lr=2e-8, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.17it/s, pg=-0.032, ret=5.09e-5, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.75] Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.07it/s, pg=-0.032, ret=5.09e-5, glen=113, tlen=273, kl=0, act_lr=2e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.07it/s, pg=-0.07, ret=-0.00148, glen=129, tlen=289, kl=0, act_lr=2e-8, ent=2.06]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.10it/s, pg=-0.07, ret=-0.00148, glen=129, tlen=289, kl=0, act_lr=2e-8, ent=2.06]
2025-07-23 11:56:35.257 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 50.57s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.10it/s, pg=-0.206, ret=0.00197, glen=111, tlen=271, kl=0, act_lr=4e-8, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.13it/s, pg=-0.206, ret=0.00197, glen=111, tlen=271, kl=0, act_lr=4e-8, ent=1.69]
2025-07-23 11:56:36.145 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.83s
2025-07-23 11:56:38.679 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-23 11:56:39.001 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 54.44s
2025-07-23 11:56:39.007 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.014427152173272494, 'actor_lr': 2.0344827462561796e-08, 'clip_ratio': 0.0, 'entropy': 1.8547721628485054, 'kl': 0.0, 'response_length': 119.75949122987944, 'total_length': 280.1101005817282, 'return': -0.0001258967225520133, 'policy_update_steps': 1.0}
Episode [1/20]:  15%|‚ñà‚ñå        | 2/13 [08:42<47:08, 257.15s/it]2025-07-23 11:56:39.039 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 11:59:00.862 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 11:59:01.044 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 11:59:01.045 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 142.01s
2025-07-23 11:59:03.100 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0161,avg_reflection_pattern_score: 0.0085,avg_pass_at_n: 1.0000,avg_num_tokens: 111.3912,std_num_tokens: 173.8911,avg_correct_num_tokens: 90.9494,std_correct_num_tokens: 76.4351,avg_incorrect_num_tokens: 121.9317,std_incorrect_num_tokens: 206.1330
2025-07-23 11:59:03.485 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.44s
2025-07-23 11:59:05.063 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.58s
2025-07-23 11:59:34.051 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 227
2025-07-23 11:59:34.051 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.99s
2025-07-23 11:59:34.986 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.93s
2025-07-23 11:59:34.987 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 7.432177616693572e-06, avg_kl: 0.0008685998454493048, avg_response_length: 117.6861011992467, avg_orm_score: 0.0, avg_custom_rewards: 7.432177616693572e-06
2025-07-23 11:59:35.023 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter2_replay_buffer.jsonl
2025-07-23 11:59:36.673 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.65s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0694, ret=-0.00249, glen=110, tlen=271, kl=0.000895, act_lr=4e-8, ent=1.73]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:56,  1.01s/it, pg=0.0694, ret=-0.00249, glen=110, tlen=271, kl=0.000895, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:56,  1.01s/it, pg=0.00403, ret=-0.000132, glen=103, tlen=263, kl=0.000863, act_lr=4e-8, ent=1.73]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.09it/s, pg=0.00403, ret=-0.000132, glen=103, tlen=263, kl=0.000863, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.09it/s, pg=-0.0531, ret=0.00141, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.69]  Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=-0.0531, ret=0.00141, glen=111, tlen=271, kl=0.000879, act_lr=4e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=-0.247, ret=0.00294, glen=115, tlen=275, kl=0.000881, act_lr=4e-8, ent=2.02] Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=-0.247, ret=0.00294, glen=115, tlen=275, kl=0.000881, act_lr=4e-8, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=-0.0882, ret=0.000143, glen=108, tlen=269, kl=0.000889, act_lr=4e-8, ent=1.87]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.0882, ret=0.000143, glen=108, tlen=269, kl=0.000889, act_lr=4e-8, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.142, ret=-0.000455, glen=118, tlen=279, kl=0.000734, act_lr=4e-8, ent=1.49] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.142, ret=-0.000455, glen=118, tlen=279, kl=0.000734, act_lr=4e-8, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=0.0689, ret=-0.0011, glen=112, tlen=273, kl=0.000879, act_lr=4e-8, ent=1.65] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=0.0689, ret=-0.0011, glen=112, tlen=273, kl=0.000879, act_lr=4e-8, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=-0.0189, ret=-0.000428, glen=110, tlen=270, kl=0.000892, act_lr=4e-8, ent=1.73]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.13it/s, pg=-0.0189, ret=-0.000428, glen=110, tlen=270, kl=0.000892, act_lr=4e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.13it/s, pg=0.148, ret=-0.000966, glen=111, tlen=271, kl=0.000863, act_lr=4e-8, ent=1.84]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:42,  1.14it/s, pg=0.148, ret=-0.000966, glen=111, tlen=271, kl=0.000863, act_lr=4e-8, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=0.306, ret=0.00107, glen=422, tlen=582, kl=0.000561, act_lr=4e-8, ent=2.45]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:42,  1.12it/s, pg=0.306, ret=0.00107, glen=422, tlen=582, kl=0.000561, act_lr=4e-8, ent=2.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:42,  1.12it/s, pg=-0.166, ret=0.00102, glen=94.4, tlen=255, kl=0.000892, act_lr=4e-8, ent=1.71]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.13it/s, pg=-0.166, ret=0.00102, glen=94.4, tlen=255, kl=0.000892, act_lr=4e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.13it/s, pg=0.0959, ret=-0.00107, glen=107, tlen=267, kl=0.0009, act_lr=4e-8, ent=1.79]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=0.0959, ret=-0.00107, glen=107, tlen=267, kl=0.0009, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.143, ret=-0.000179, glen=96.7, tlen=256, kl=0.000896, act_lr=4e-8, ent=1.67]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.143, ret=-0.000179, glen=96.7, tlen=256, kl=0.000896, act_lr=4e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=0.237, ret=-0.0012, glen=125, tlen=286, kl=0.000848, act_lr=4e-8, ent=2.11]    Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=0.237, ret=-0.0012, glen=125, tlen=286, kl=0.000848, act_lr=4e-8, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0886, ret=-0.00089, glen=119, tlen=279, kl=0.000803, act_lr=4e-8, ent=1.41]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.14it/s, pg=0.0886, ret=-0.00089, glen=119, tlen=279, kl=0.000803, act_lr=4e-8, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.14it/s, pg=-0.041, ret=-0.000945, glen=101, tlen=261, kl=0.000896, act_lr=4e-8, ent=1.65]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.15it/s, pg=-0.041, ret=-0.000945, glen=101, tlen=261, kl=0.000896, act_lr=4e-8, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.15it/s, pg=0.0232, ret=-0.000381, glen=109, tlen=269, kl=0.000855, act_lr=4e-8, ent=1.71]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.16it/s, pg=0.0232, ret=-0.000381, glen=109, tlen=269, kl=0.000855, act_lr=4e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.16it/s, pg=-0.105, ret=-0.00077, glen=117, tlen=278, kl=0.000889, act_lr=4e-8, ent=1.71] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=-0.105, ret=-0.00077, glen=117, tlen=278, kl=0.000889, act_lr=4e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=-0.0426, ret=0.00182, glen=124, tlen=285, kl=0.000868, act_lr=4e-8, ent=1.82]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=-0.0426, ret=0.00182, glen=124, tlen=285, kl=0.000868, act_lr=4e-8, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=0.0134, ret=-0.000583, glen=106, tlen=267, kl=0.000872, act_lr=4e-8, ent=1.67]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=0.0134, ret=-0.000583, glen=106, tlen=267, kl=0.000872, act_lr=4e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.127, ret=0.000894, glen=106, tlen=267, kl=0.000912, act_lr=4e-8, ent=1.66] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.127, ret=0.000894, glen=106, tlen=267, kl=0.000912, act_lr=4e-8, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.0452, ret=0.000439, glen=111, tlen=271, kl=0.000851, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0452, ret=0.000439, glen=111, tlen=271, kl=0.000851, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.236, ret=0.00156, glen=105, tlen=266, kl=0.000881, act_lr=4e-8, ent=1.6]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.236, ret=0.00156, glen=105, tlen=266, kl=0.000881, act_lr=4e-8, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.155, ret=0.000943, glen=116, tlen=277, kl=0.000875, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.155, ret=0.000943, glen=116, tlen=277, kl=0.000875, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0518, ret=0.00049, glen=99.8, tlen=260, kl=0.00091, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0518, ret=0.00049, glen=99.8, tlen=260, kl=0.00091, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.0659, ret=-6.09e-5, glen=133, tlen=294, kl=0.000832, act_lr=4e-8, ent=2.17]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.0659, ret=-6.09e-5, glen=133, tlen=294, kl=0.000832, act_lr=4e-8, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.161, ret=-0.00213, glen=104, tlen=264, kl=0.000928, act_lr=4e-8, ent=1.81] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:27,  1.07it/s, pg=0.161, ret=-0.00213, glen=104, tlen=264, kl=0.000928, act_lr=4e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:27,  1.07it/s, pg=-0.0608, ret=-3.44e-5, glen=101, tlen=261, kl=0.000882, act_lr=4e-8, ent=1.66]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.10it/s, pg=-0.0608, ret=-3.44e-5, glen=101, tlen=261, kl=0.000882, act_lr=4e-8, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.10it/s, pg=-0.0768, ret=-9.47e-5, glen=104, tlen=265, kl=0.000858, act_lr=4e-8, ent=1.7] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:24,  1.12it/s, pg=-0.0768, ret=-9.47e-5, glen=104, tlen=265, kl=0.000858, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:24,  1.12it/s, pg=-0.0198, ret=-0.000142, glen=115, tlen=275, kl=0.000911, act_lr=4e-8, ent=1.83]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.14it/s, pg=-0.0198, ret=-0.000142, glen=115, tlen=275, kl=0.000911, act_lr=4e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.14it/s, pg=-0.285, ret=0.00196, glen=123, tlen=284, kl=0.000854, act_lr=4e-8, ent=1.74]   Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.285, ret=0.00196, glen=123, tlen=284, kl=0.000854, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.12it/s, pg=-0.208, ret=0.00121, glen=108, tlen=269, kl=0.000868, act_lr=4e-8, ent=1.68]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=-0.208, ret=0.00121, glen=108, tlen=269, kl=0.000868, act_lr=4e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=0.149, ret=-0.00121, glen=111, tlen=272, kl=0.000903, act_lr=4e-8, ent=1.75]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=0.149, ret=-0.00121, glen=111, tlen=272, kl=0.000903, act_lr=4e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=0.158, ret=0.000119, glen=110, tlen=271, kl=0.000925, act_lr=4e-8, ent=2.03]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.15it/s, pg=0.158, ret=0.000119, glen=110, tlen=271, kl=0.000925, act_lr=4e-8, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.15it/s, pg=0.12, ret=0.00111, glen=178, tlen=338, kl=0.000722, act_lr=4e-8, ent=2.03]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.15it/s, pg=0.12, ret=0.00111, glen=178, tlen=338, kl=0.000722, act_lr=4e-8, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.15it/s, pg=-0.035, ret=-0.000189, glen=99.2, tlen=260, kl=0.000863, act_lr=4e-8, ent=1.7]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=-0.035, ret=-0.000189, glen=99.2, tlen=260, kl=0.000863, act_lr=4e-8, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=0.0106, ret=5.88e-5, glen=122, tlen=283, kl=0.000897, act_lr=4e-8, ent=1.82]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=0.0106, ret=5.88e-5, glen=122, tlen=283, kl=0.000897, act_lr=4e-8, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=0.00195, ret=1.98e-5, glen=100, tlen=261, kl=0.00087, act_lr=4e-8, ent=1.74]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=0.00195, ret=1.98e-5, glen=100, tlen=261, kl=0.00087, act_lr=4e-8, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.16it/s, pg=0.0342, ret=-0.000679, glen=113, tlen=273, kl=0.000872, act_lr=4e-8, ent=1.67]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0342, ret=-0.000679, glen=113, tlen=273, kl=0.000872, act_lr=4e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.261, ret=0.00145, glen=92.3, tlen=253, kl=0.000871, act_lr=4e-8, ent=1.64] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.261, ret=0.00145, glen=92.3, tlen=253, kl=0.000871, act_lr=4e-8, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.067, ret=0.000531, glen=102, tlen=262, kl=0.000906, act_lr=4e-8, ent=1.71]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.067, ret=0.000531, glen=102, tlen=262, kl=0.000906, act_lr=4e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0402, ret=0.000517, glen=99.8, tlen=260, kl=0.000906, act_lr=4e-8, ent=1.71]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.0402, ret=0.000517, glen=99.8, tlen=260, kl=0.000906, act_lr=4e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=0.17, ret=-0.00198, glen=119, tlen=279, kl=0.000908, act_lr=4e-8, ent=1.82]    Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=0.17, ret=-0.00198, glen=119, tlen=279, kl=0.000908, act_lr=4e-8, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=0.168, ret=-0.00203, glen=116, tlen=276, kl=0.000936, act_lr=4e-8, ent=1.96]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.168, ret=-0.00203, glen=116, tlen=276, kl=0.000936, act_lr=4e-8, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.213, ret=0.00106, glen=112, tlen=273, kl=0.000874, act_lr=4e-8, ent=1.76]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.213, ret=0.00106, glen=112, tlen=273, kl=0.000874, act_lr=4e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.17it/s, pg=0.0144, ret=0.000104, glen=112, tlen=272, kl=0.000851, act_lr=4e-8, ent=1.79]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0144, ret=0.000104, glen=112, tlen=272, kl=0.000851, act_lr=4e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.0183, ret=-0.000506, glen=111, tlen=271, kl=0.000884, act_lr=4e-8, ent=1.63]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.0183, ret=-0.000506, glen=111, tlen=271, kl=0.000884, act_lr=4e-8, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.138, ret=0.00132, glen=99.9, tlen=260, kl=0.000854, act_lr=4e-8, ent=1.59]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=-0.138, ret=0.00132, glen=99.9, tlen=260, kl=0.000854, act_lr=4e-8, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.181, ret=0.00148, glen=106, tlen=267, kl=0.000913, act_lr=4e-8, ent=1.78] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=-0.181, ret=0.00148, glen=106, tlen=267, kl=0.000913, act_lr=4e-8, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.0797, ret=0.000589, glen=107, tlen=268, kl=0.000889, act_lr=4e-8, ent=1.72]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.0797, ret=0.000589, glen=107, tlen=268, kl=0.000889, act_lr=4e-8, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.00607, ret=-0.000401, glen=110, tlen=271, kl=0.000865, act_lr=4e-8, ent=1.82]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.00607, ret=-0.000401, glen=110, tlen=271, kl=0.000865, act_lr=4e-8, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=0.0608, ret=-0.00122, glen=115, tlen=276, kl=0.000851, act_lr=4e-8, ent=1.81]   Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.0608, ret=-0.00122, glen=115, tlen=276, kl=0.000851, act_lr=4e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=-0.141, ret=0.000409, glen=103, tlen=263, kl=0.000959, act_lr=4e-8, ent=1.69]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.141, ret=0.000409, glen=103, tlen=263, kl=0.000959, act_lr=4e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.0378, ret=0.000664, glen=154, tlen=315, kl=0.000762, act_lr=4e-8, ent=1.58]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.16it/s, pg=-0.0378, ret=0.000664, glen=154, tlen=315, kl=0.000762, act_lr=4e-8, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=0.105, ret=-0.00117, glen=141, tlen=301, kl=0.000875, act_lr=4e-8, ent=2.18]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.16it/s, pg=0.105, ret=-0.00117, glen=141, tlen=301, kl=0.000875, act_lr=4e-8, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.16it/s, pg=-0.159, ret=0.00088, glen=126, tlen=287, kl=0.000861, act_lr=4e-8, ent=1.8] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.06it/s, pg=-0.159, ret=0.00088, glen=126, tlen=287, kl=0.000861, act_lr=4e-8, ent=1.8]
2025-07-23 12:00:26.653 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.82s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.06it/s, pg=0.0422, ret=0.000261, glen=108, tlen=268, kl=0.00089, act_lr=6e-8, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.0422, ret=0.000261, glen=108, tlen=268, kl=0.00089, act_lr=6e-8, ent=1.82]
2025-07-23 12:00:27.555 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.83s
2025-07-23 12:00:30.140 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-23 12:00:30.445 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.73s
2025-07-23 12:00:30.450 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.017549146685683935, 'actor_lr': 4.0350876916587013e-08, 'clip_ratio': 0.0, 'entropy': 1.7758771202020478, 'kl': 0.0008687889366819148, 'response_length': 117.76307116056743, 'total_length': 278.2671192905359, 'return': 5.3252642509617367e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [12:34<40:54, 245.41s/it]2025-07-23 12:00:30.482 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:02:04.692 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:02:04.875 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:02:04.876 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 94.39s
2025-07-23 12:02:06.903 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0099,avg_pass_at_n: 1.0000,avg_num_tokens: 110.8070,std_num_tokens: 115.8095,avg_correct_num_tokens: 96.0442,std_correct_num_tokens: 83.4463,avg_incorrect_num_tokens: 118.2141,std_incorrect_num_tokens: 128.3867
2025-07-23 12:02:07.266 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.39s
2025-07-23 12:02:08.838 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.57s
2025-07-23 12:02:37.671 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 225
2025-07-23 12:02:37.671 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.83s
2025-07-23 12:02:38.534 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.86s
2025-07-23 12:02:38.534 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00041578011703677473, avg_kl: 0.000902252197265625, avg_response_length: 112.11681576199001, avg_orm_score: 0.0, avg_custom_rewards: -0.00041578011703677473
2025-07-23 12:02:38.561 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter3_replay_buffer.jsonl
2025-07-23 12:02:40.190 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.63s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s, pg=-0.222, ret=0.00225, glen=107, tlen=268, kl=0.000852, act_lr=6e-8, ent=1.77]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:00<00:54,  1.02it/s, pg=-0.222, ret=0.00225, glen=107, tlen=268, kl=0.000852, act_lr=6e-8, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:54,  1.02it/s, pg=0.0817, ret=-0.000101, glen=110, tlen=271, kl=0.000911, act_lr=6e-8, ent=1.79]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.10it/s, pg=0.0817, ret=-0.000101, glen=110, tlen=271, kl=0.000911, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.10it/s, pg=-0.119, ret=0.000347, glen=106, tlen=267, kl=0.000925, act_lr=6e-8, ent=1.79] Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.10it/s, pg=-0.119, ret=0.000347, glen=106, tlen=267, kl=0.000925, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.10it/s, pg=-0.0694, ret=0.00025, glen=101, tlen=261, kl=0.000922, act_lr=6e-8, ent=1.79]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.0694, ret=0.00025, glen=101, tlen=261, kl=0.000922, act_lr=6e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=-0.11, ret=-6.37e-5, glen=103, tlen=263, kl=0.000939, act_lr=6e-8, ent=1.76] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.11, ret=-6.37e-5, glen=103, tlen=263, kl=0.000939, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.0898, ret=-0.00191, glen=119, tlen=279, kl=0.000919, act_lr=6e-8, ent=1.68]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0898, ret=-0.00191, glen=119, tlen=279, kl=0.000919, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=0.0242, ret=0.000868, glen=100, tlen=260, kl=0.000955, act_lr=6e-8, ent=1.66]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=0.0242, ret=0.000868, glen=100, tlen=260, kl=0.000955, act_lr=6e-8, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=-0.164, ret=0.000802, glen=114, tlen=274, kl=0.000907, act_lr=6e-8, ent=1.87]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.164, ret=0.000802, glen=114, tlen=274, kl=0.000907, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.117, ret=0.000519, glen=113, tlen=273, kl=0.000912, act_lr=6e-8, ent=1.94] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.17it/s, pg=0.117, ret=0.000519, glen=113, tlen=273, kl=0.000912, act_lr=6e-8, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.17it/s, pg=-0.0527, ret=0.000778, glen=115, tlen=276, kl=0.000896, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.17it/s, pg=-0.0527, ret=0.000778, glen=115, tlen=276, kl=0.000896, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.17it/s, pg=-0.0731, ret=0.000219, glen=110, tlen=270, kl=0.000912, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.17it/s, pg=-0.0731, ret=0.000219, glen=110, tlen=270, kl=0.000912, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.17it/s, pg=0.175, ret=-0.0017, glen=106, tlen=266, kl=0.000897, act_lr=6e-8, ent=1.86]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.17it/s, pg=0.175, ret=-0.0017, glen=106, tlen=266, kl=0.000897, act_lr=6e-8, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.17it/s, pg=0.148, ret=-0.00131, glen=104, tlen=265, kl=0.000922, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=0.148, ret=-0.00131, glen=104, tlen=265, kl=0.000922, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=-0.0219, ret=0.000142, glen=99.1, tlen=259, kl=0.000915, act_lr=6e-8, ent=1.74]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=-0.0219, ret=0.000142, glen=99.1, tlen=259, kl=0.000915, act_lr=6e-8, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=0.058, ret=-0.00121, glen=122, tlen=282, kl=0.000899, act_lr=6e-8, ent=1.76]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:12<00:35,  1.17it/s, pg=0.058, ret=-0.00121, glen=122, tlen=282, kl=0.000899, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.103, ret=0.00132, glen=108, tlen=268, kl=0.000888, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:34,  1.17it/s, pg=-0.103, ret=0.00132, glen=108, tlen=268, kl=0.000888, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:34,  1.17it/s, pg=0.124, ret=-0.000238, glen=108, tlen=269, kl=0.000929, act_lr=6e-8, ent=1.7]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.124, ret=-0.000238, glen=108, tlen=269, kl=0.000929, act_lr=6e-8, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.472, ret=0.00365, glen=109, tlen=270, kl=0.000908, act_lr=6e-8, ent=1.93]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.472, ret=0.00365, glen=109, tlen=270, kl=0.000908, act_lr=6e-8, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.0156, ret=-0.00231, glen=120, tlen=280, kl=0.000881, act_lr=6e-8, ent=1.92]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.0156, ret=-0.00231, glen=120, tlen=280, kl=0.000881, act_lr=6e-8, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.166, ret=-0.000607, glen=102, tlen=262, kl=0.000855, act_lr=6e-8, ent=2.03] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.166, ret=-0.000607, glen=102, tlen=262, kl=0.000855, act_lr=6e-8, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.00671, ret=-0.000113, glen=108, tlen=269, kl=0.000893, act_lr=6e-8, ent=1.75]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=-0.00671, ret=-0.000113, glen=108, tlen=269, kl=0.000893, act_lr=6e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.135, ret=-0.0021, glen=107, tlen=267, kl=0.000914, act_lr=6e-8, ent=1.69]     Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:18<00:30,  1.16it/s, pg=0.135, ret=-0.0021, glen=107, tlen=267, kl=0.000914, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=-0.0524, ret=0.000436, glen=112, tlen=272, kl=0.0009, act_lr=6e-8, ent=1.73]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.16it/s, pg=-0.0524, ret=0.000436, glen=112, tlen=272, kl=0.0009, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.16it/s, pg=0.0586, ret=-0.000985, glen=103, tlen=264, kl=0.000916, act_lr=6e-8, ent=1.63]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0586, ret=-0.000985, glen=103, tlen=264, kl=0.000916, act_lr=6e-8, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.092, ret=-0.00022, glen=115, tlen=275, kl=0.000902, act_lr=6e-8, ent=1.8]   Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.15it/s, pg=0.092, ret=-0.00022, glen=115, tlen=275, kl=0.000902, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.15it/s, pg=0.0486, ret=-0.000565, glen=114, tlen=275, kl=0.000883, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.16it/s, pg=0.0486, ret=-0.000565, glen=114, tlen=275, kl=0.000883, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.16it/s, pg=-0.0626, ret=0.000827, glen=106, tlen=266, kl=0.000896, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:28,  1.06it/s, pg=-0.0626, ret=0.000827, glen=106, tlen=266, kl=0.000896, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.06it/s, pg=0.143, ret=0.00113, glen=117, tlen=277, kl=0.000821, act_lr=6e-8, ent=1.57]   Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.09it/s, pg=0.143, ret=0.00113, glen=117, tlen=277, kl=0.000821, act_lr=6e-8, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.09it/s, pg=-0.107, ret=0.000122, glen=121, tlen=281, kl=0.000909, act_lr=6e-8, ent=1.84]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:25,  1.12it/s, pg=-0.107, ret=0.000122, glen=121, tlen=281, kl=0.000909, act_lr=6e-8, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.12it/s, pg=0.0716, ret=-0.00158, glen=116, tlen=276, kl=0.000931, act_lr=6e-8, ent=1.73]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=0.0716, ret=-0.00158, glen=116, tlen=276, kl=0.000931, act_lr=6e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=0.0245, ret=-0.000212, glen=108, tlen=268, kl=0.000893, act_lr=6e-8, ent=1.85]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:26<00:22,  1.15it/s, pg=0.0245, ret=-0.000212, glen=108, tlen=268, kl=0.000893, act_lr=6e-8, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.182, ret=0.000625, glen=102, tlen=262, kl=0.000937, act_lr=6e-8, ent=1.82] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.15it/s, pg=-0.182, ret=0.000625, glen=102, tlen=262, kl=0.000937, act_lr=6e-8, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.15it/s, pg=-0.204, ret=0.00104, glen=138, tlen=299, kl=0.00087, act_lr=6e-8, ent=1.89]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.16it/s, pg=-0.204, ret=0.00104, glen=138, tlen=299, kl=0.00087, act_lr=6e-8, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.16it/s, pg=-0.0376, ret=-0.000118, glen=116, tlen=277, kl=0.000841, act_lr=6e-8, ent=1.71]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.17it/s, pg=-0.0376, ret=-0.000118, glen=116, tlen=277, kl=0.000841, act_lr=6e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.17it/s, pg=0.115, ret=4.78e-5, glen=112, tlen=272, kl=0.000917, act_lr=6e-8, ent=1.64]    Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.17it/s, pg=0.115, ret=4.78e-5, glen=112, tlen=272, kl=0.000917, act_lr=6e-8, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.17it/s, pg=0.152, ret=-0.000924, glen=117, tlen=277, kl=0.00087, act_lr=6e-8, ent=2]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:17,  1.17it/s, pg=0.152, ret=-0.000924, glen=117, tlen=277, kl=0.00087, act_lr=6e-8, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:17,  1.17it/s, pg=0.0386, ret=-0.00121, glen=122, tlen=282, kl=0.000884, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=0.0386, ret=-0.00121, glen=122, tlen=282, kl=0.000884, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0247, ret=0.000636, glen=117, tlen=277, kl=0.000912, act_lr=6e-8, ent=1.86]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:32<00:16,  1.18it/s, pg=-0.0247, ret=0.000636, glen=117, tlen=277, kl=0.000912, act_lr=6e-8, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.18it/s, pg=0.136, ret=-6.35e-5, glen=128, tlen=289, kl=0.000881, act_lr=6e-8, ent=1.8]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.18it/s, pg=0.136, ret=-6.35e-5, glen=128, tlen=289, kl=0.000881, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.18it/s, pg=-0.0897, ret=0.000766, glen=113, tlen=273, kl=0.000956, act_lr=6e-8, ent=1.76]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.18it/s, pg=-0.0897, ret=0.000766, glen=113, tlen=273, kl=0.000956, act_lr=6e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.18it/s, pg=-0.000397, ret=0.000639, glen=109, tlen=269, kl=0.000894, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.18it/s, pg=-0.000397, ret=0.000639, glen=109, tlen=269, kl=0.000894, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.18it/s, pg=0.174, ret=-0.00189, glen=150, tlen=311, kl=0.000818, act_lr=6e-8, ent=1.62]    Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.174, ret=-0.00189, glen=150, tlen=311, kl=0.000818, act_lr=6e-8, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.16, ret=0.000965, glen=115, tlen=275, kl=0.000902, act_lr=6e-8, ent=1.98]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.16, ret=0.000965, glen=115, tlen=275, kl=0.000902, act_lr=6e-8, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=0.0298, ret=-0.000575, glen=137, tlen=297, kl=0.000859, act_lr=6e-8, ent=2.51]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.0298, ret=-0.000575, glen=137, tlen=297, kl=0.000859, act_lr=6e-8, ent=2.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.125, ret=0.000344, glen=113, tlen=273, kl=0.000948, act_lr=6e-8, ent=1.86] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=-0.125, ret=0.000344, glen=113, tlen=273, kl=0.000948, act_lr=6e-8, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0914, ret=0.0014, glen=109, tlen=269, kl=0.000912, act_lr=6e-8, ent=1.8]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.0914, ret=0.0014, glen=109, tlen=269, kl=0.000912, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.0149, ret=-0.000265, glen=118, tlen=278, kl=0.000909, act_lr=6e-8, ent=1.81]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.0149, ret=-0.000265, glen=118, tlen=278, kl=0.000909, act_lr=6e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.0573, ret=-0.0014, glen=128, tlen=287, kl=0.000919, act_lr=6e-8, ent=1.87]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.0573, ret=-0.0014, glen=128, tlen=287, kl=0.000919, act_lr=6e-8, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.0807, ret=-0.000397, glen=103, tlen=264, kl=0.000905, act_lr=6e-8, ent=1.77]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.0807, ret=-0.000397, glen=103, tlen=264, kl=0.000905, act_lr=6e-8, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.045, ret=0.00021, glen=99.1, tlen=260, kl=0.00091, act_lr=6e-8, ent=1.68]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.18it/s, pg=-0.045, ret=0.00021, glen=99.1, tlen=260, kl=0.00091, act_lr=6e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.18it/s, pg=0.0812, ret=0.000181, glen=122, tlen=282, kl=0.000903, act_lr=6e-8, ent=1.98]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:43<00:05,  1.18it/s, pg=0.0812, ret=0.000181, glen=122, tlen=282, kl=0.000903, act_lr=6e-8, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.18it/s, pg=0.0427, ret=-1.43e-6, glen=102, tlen=263, kl=0.000866, act_lr=6e-8, ent=1.69]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.18it/s, pg=0.0427, ret=-1.43e-6, glen=102, tlen=263, kl=0.000866, act_lr=6e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.18it/s, pg=0.223, ret=-0.00126, glen=112, tlen=272, kl=0.000914, act_lr=6e-8, ent=1.8]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.18it/s, pg=0.223, ret=-0.00126, glen=112, tlen=272, kl=0.000914, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=-0.0707, ret=0.00119, glen=109, tlen=268, kl=0.000941, act_lr=6e-8, ent=1.8]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.18it/s, pg=-0.0707, ret=0.00119, glen=109, tlen=268, kl=0.000941, act_lr=6e-8, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.18it/s, pg=-0.0821, ret=0.000351, glen=104, tlen=264, kl=0.000877, act_lr=6e-8, ent=1.6]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.18it/s, pg=-0.0821, ret=0.000351, glen=104, tlen=264, kl=0.000877, act_lr=6e-8, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.18it/s, pg=-0.0605, ret=-0.00114, glen=104, tlen=264, kl=0.000957, act_lr=6e-8, ent=1.94]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=-0.0605, ret=-0.00114, glen=104, tlen=264, kl=0.000957, act_lr=6e-8, ent=1.94]
2025-07-23 12:03:29.788 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.43s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=0.000637, ret=0.000195, glen=99.1, tlen=259, kl=0.000925, act_lr=8e-8, ent=1.79]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.000637, ret=0.000195, glen=99.1, tlen=259, kl=0.000925, act_lr=8e-8, ent=1.79]
2025-07-23 12:03:30.649 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 12:03:33.216 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-23 12:03:33.542 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.31s
2025-07-23 12:03:33.548 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0026686818976151315, 'actor_lr': 6.035087508100907e-08, 'clip_ratio': 0.0, 'entropy': 1.7948315039015652, 'kl': 0.0009022428278337445, 'response_length': 112.28867795174583, 'total_length': 272.56057899876646, 'return': -3.8748148781770286e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [15:37<33:07, 220.81s/it]2025-07-23 12:03:33.582 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:04:57.857 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:04:58.041 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:04:58.042 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 84.46s
2025-07-23 12:05:00.220 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0078,avg_pass_at_n: 1.0000,avg_num_tokens: 114.1774,std_num_tokens: 121.3945,avg_correct_num_tokens: 94.8093,std_correct_num_tokens: 84.7939,avg_incorrect_num_tokens: 123.5370,std_incorrect_num_tokens: 134.5907
2025-07-23 12:05:00.660 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.62s
2025-07-23 12:05:02.365 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.70s
2025-07-23 12:05:31.683 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 228
2025-07-23 12:05:31.684 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.31s
2025-07-23 12:05:32.692 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 1.00s
2025-07-23 12:05:32.692 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0008876868711300439, avg_kl: 0.0009047190348307291, avg_response_length: 115.75800775226794, avg_orm_score: 0.0, avg_custom_rewards: -0.0008876868711300439
2025-07-23 12:05:32.743 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter4_replay_buffer.jsonl
2025-07-23 12:05:34.490 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.75s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.213, ret=0.000348, glen=95.1, tlen=256, kl=0.00094, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:56,  1.01s/it, pg=-0.213, ret=0.000348, glen=95.1, tlen=256, kl=0.00094, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:56,  1.01s/it, pg=-0.164, ret=0.000891, glen=113, tlen=274, kl=0.000927, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.09it/s, pg=-0.164, ret=0.000891, glen=113, tlen=274, kl=0.000927, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.09it/s, pg=-0.0452, ret=-5.02e-5, glen=96.5, tlen=257, kl=0.000898, act_lr=8e-8, ent=1.58]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=-0.0452, ret=-5.02e-5, glen=96.5, tlen=257, kl=0.000898, act_lr=8e-8, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.145, ret=0.00111, glen=114, tlen=275, kl=0.00086, act_lr=8e-8, ent=1.67]    Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.12it/s, pg=-0.145, ret=0.00111, glen=114, tlen=275, kl=0.00086, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.12it/s, pg=0.0462, ret=-0.00188, glen=130, tlen=291, kl=0.000886, act_lr=8e-8, ent=1.74]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.11it/s, pg=0.0462, ret=-0.00188, glen=130, tlen=291, kl=0.000886, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.11it/s, pg=-0.0461, ret=0.000969, glen=112, tlen=272, kl=0.000921, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.13it/s, pg=-0.0461, ret=0.000969, glen=112, tlen=272, kl=0.000921, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.13it/s, pg=-0.015, ret=-0.000593, glen=101, tlen=262, kl=0.000888, act_lr=8e-8, ent=1.65]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.11it/s, pg=-0.015, ret=-0.000593, glen=101, tlen=262, kl=0.000888, act_lr=8e-8, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.11it/s, pg=0.00291, ret=0.000186, glen=106, tlen=266, kl=0.000933, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.13it/s, pg=0.00291, ret=0.000186, glen=106, tlen=266, kl=0.000933, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.13it/s, pg=0.0298, ret=0.000158, glen=119, tlen=279, kl=0.000911, act_lr=8e-8, ent=1.69] Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.0298, ret=0.000158, glen=119, tlen=279, kl=0.000911, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.0154, ret=-0.000358, glen=100, tlen=261, kl=0.000916, act_lr=8e-8, ent=1.73]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=0.0154, ret=-0.000358, glen=100, tlen=261, kl=0.000916, act_lr=8e-8, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.00421, ret=0.000596, glen=112, tlen=272, kl=0.000903, act_lr=8e-8, ent=1.67]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.00421, ret=0.000596, glen=112, tlen=272, kl=0.000903, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.178, ret=0.00191, glen=115, tlen=276, kl=0.000884, act_lr=8e-8, ent=1.65]  Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.178, ret=0.00191, glen=115, tlen=276, kl=0.000884, act_lr=8e-8, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.0435, ret=-0.000716, glen=121, tlen=282, kl=0.000918, act_lr=8e-8, ent=1.77]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=-0.0435, ret=-0.000716, glen=121, tlen=282, kl=0.000918, act_lr=8e-8, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=0.11, ret=-0.00113, glen=126, tlen=286, kl=0.000899, act_lr=8e-8, ent=1.81]    Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=0.11, ret=-0.00113, glen=126, tlen=286, kl=0.000899, act_lr=8e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=-0.0381, ret=0.00121, glen=128, tlen=288, kl=0.000902, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.17it/s, pg=-0.0381, ret=0.00121, glen=128, tlen=288, kl=0.000902, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.17it/s, pg=-0.164, ret=0.00079, glen=115, tlen=276, kl=0.000866, act_lr=8e-8, ent=1.68] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.14it/s, pg=-0.164, ret=0.00079, glen=115, tlen=276, kl=0.000866, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.14it/s, pg=0.079, ret=-0.000586, glen=139, tlen=301, kl=0.000874, act_lr=8e-8, ent=1.85]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.15it/s, pg=0.079, ret=-0.000586, glen=139, tlen=301, kl=0.000874, act_lr=8e-8, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.15it/s, pg=-0.0217, ret=0.000813, glen=106, tlen=267, kl=0.000873, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.0217, ret=0.000813, glen=106, tlen=267, kl=0.000873, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=-0.123, ret=0.000815, glen=94.4, tlen=255, kl=0.000939, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=-0.123, ret=0.000815, glen=94.4, tlen=255, kl=0.000939, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=0.00433, ret=0.000997, glen=132, tlen=292, kl=0.000906, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=0.00433, ret=0.000997, glen=132, tlen=292, kl=0.000906, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=0.0309, ret=-0.000225, glen=117, tlen=278, kl=0.000902, act_lr=8e-8, ent=1.74]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.0309, ret=-0.000225, glen=117, tlen=278, kl=0.000902, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.17, ret=-0.00122, glen=123, tlen=283, kl=0.000921, act_lr=8e-8, ent=1.71]   Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.17, ret=-0.00122, glen=123, tlen=283, kl=0.000921, act_lr=8e-8, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.0406, ret=-0.000291, glen=107, tlen=268, kl=0.000891, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0406, ret=-0.000291, glen=107, tlen=268, kl=0.000891, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.185, ret=0.000889, glen=101, tlen=261, kl=0.000892, act_lr=8e-8, ent=1.6]   Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.185, ret=0.000889, glen=101, tlen=261, kl=0.000892, act_lr=8e-8, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.145, ret=0.000904, glen=99.6, tlen=260, kl=0.00091, act_lr=8e-8, ent=1.66]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.145, ret=0.000904, glen=99.6, tlen=260, kl=0.00091, act_lr=8e-8, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.226, ret=0.000828, glen=96.5, tlen=257, kl=0.000901, act_lr=8e-8, ent=1.55]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.18it/s, pg=-0.226, ret=0.000828, glen=96.5, tlen=257, kl=0.000901, act_lr=8e-8, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.18it/s, pg=-0.0297, ret=0.000433, glen=120, tlen=280, kl=0.000937, act_lr=8e-8, ent=1.84]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:27,  1.08it/s, pg=-0.0297, ret=0.000433, glen=120, tlen=280, kl=0.000937, act_lr=8e-8, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:27,  1.08it/s, pg=-0.0536, ret=0.000151, glen=116, tlen=277, kl=0.00089, act_lr=8e-8, ent=1.75] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.10it/s, pg=-0.0536, ret=0.000151, glen=116, tlen=277, kl=0.00089, act_lr=8e-8, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.10it/s, pg=0.142, ret=-0.00143, glen=112, tlen=272, kl=0.00088, act_lr=8e-8, ent=1.79]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:24,  1.12it/s, pg=0.142, ret=-0.00143, glen=112, tlen=272, kl=0.00088, act_lr=8e-8, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:24,  1.12it/s, pg=-0.0112, ret=-8.86e-5, glen=122, tlen=282, kl=0.000908, act_lr=8e-8, ent=1.76]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.14it/s, pg=-0.0112, ret=-8.86e-5, glen=122, tlen=282, kl=0.000908, act_lr=8e-8, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.14it/s, pg=-0.0211, ret=-0.00045, glen=113, tlen=274, kl=0.000902, act_lr=8e-8, ent=1.67]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.0211, ret=-0.00045, glen=113, tlen=274, kl=0.000902, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.0709, ret=0.00012, glen=123, tlen=284, kl=0.00092, act_lr=8e-8, ent=1.83]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.16it/s, pg=-0.0709, ret=0.00012, glen=123, tlen=284, kl=0.00092, act_lr=8e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.16it/s, pg=0.161, ret=-0.000349, glen=127, tlen=288, kl=0.000926, act_lr=8e-8, ent=1.86]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.16it/s, pg=0.161, ret=-0.000349, glen=127, tlen=288, kl=0.000926, act_lr=8e-8, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.16it/s, pg=0.0371, ret=0.000624, glen=112, tlen=272, kl=0.000912, act_lr=8e-8, ent=1.87]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=0.0371, ret=0.000624, glen=112, tlen=272, kl=0.000912, act_lr=8e-8, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=0.139, ret=-0.00249, glen=138, tlen=299, kl=0.000912, act_lr=8e-8, ent=2.07] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.139, ret=-0.00249, glen=138, tlen=299, kl=0.000912, act_lr=8e-8, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.0422, ret=0.000448, glen=107, tlen=268, kl=0.000899, act_lr=8e-8, ent=1.57]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:17,  1.17it/s, pg=0.0422, ret=0.000448, glen=107, tlen=268, kl=0.000899, act_lr=8e-8, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:17,  1.17it/s, pg=-0.0566, ret=-0.000409, glen=111, tlen=272, kl=0.000873, act_lr=8e-8, ent=1.86]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0566, ret=-0.000409, glen=111, tlen=272, kl=0.000873, act_lr=8e-8, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.00623, ret=0.0015, glen=125, tlen=286, kl=0.000947, act_lr=8e-8, ent=1.93]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.00623, ret=0.0015, glen=125, tlen=286, kl=0.000947, act_lr=8e-8, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.108, ret=-0.00111, glen=119, tlen=279, kl=0.000925, act_lr=8e-8, ent=1.67] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=0.108, ret=-0.00111, glen=119, tlen=279, kl=0.000925, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.053, ret=-0.000342, glen=127, tlen=288, kl=0.000888, act_lr=8e-8, ent=1.68]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.053, ret=-0.000342, glen=127, tlen=288, kl=0.000888, act_lr=8e-8, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0167, ret=-0.000984, glen=115, tlen=275, kl=0.000937, act_lr=8e-8, ent=1.83]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0167, ret=-0.000984, glen=115, tlen=275, kl=0.000937, act_lr=8e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0631, ret=0.000486, glen=106, tlen=266, kl=0.000947, act_lr=8e-8, ent=1.72]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.0631, ret=0.000486, glen=106, tlen=266, kl=0.000947, act_lr=8e-8, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=0.0844, ret=0.00023, glen=110, tlen=270, kl=0.000887, act_lr=8e-8, ent=1.63]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=0.0844, ret=0.00023, glen=110, tlen=270, kl=0.000887, act_lr=8e-8, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=0.028, ret=-0.00122, glen=112, tlen=273, kl=0.000882, act_lr=8e-8, ent=1.67]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.18it/s, pg=0.028, ret=-0.00122, glen=112, tlen=273, kl=0.000882, act_lr=8e-8, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.18it/s, pg=0.108, ret=-0.000642, glen=137, tlen=297, kl=0.000896, act_lr=8e-8, ent=2.08]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=0.108, ret=-0.000642, glen=137, tlen=297, kl=0.000896, act_lr=8e-8, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.0406, ret=-3.2e-5, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.83]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.0406, ret=-3.2e-5, glen=123, tlen=283, kl=0.000899, act_lr=8e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.236, ret=-0.000897, glen=146, tlen=307, kl=0.000882, act_lr=8e-8, ent=2.14]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.236, ret=-0.000897, glen=146, tlen=307, kl=0.000882, act_lr=8e-8, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.162, ret=0.0012, glen=107, tlen=268, kl=0.000937, act_lr=8e-8, ent=1.59]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=-0.162, ret=0.0012, glen=107, tlen=268, kl=0.000937, act_lr=8e-8, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.0801, ret=0.000369, glen=128, tlen=288, kl=0.000881, act_lr=8e-8, ent=1.9]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.0801, ret=0.000369, glen=128, tlen=288, kl=0.000881, act_lr=8e-8, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.0603, ret=-0.000383, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.74]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.0603, ret=-0.000383, glen=113, tlen=274, kl=0.000875, act_lr=8e-8, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=0.166, ret=-0.0017, glen=105, tlen=265, kl=0.000917, act_lr=8e-8, ent=1.83]    Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.166, ret=-0.0017, glen=105, tlen=265, kl=0.000917, act_lr=8e-8, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=0.0697, ret=0.000468, glen=117, tlen=278, kl=0.000921, act_lr=8e-8, ent=1.81]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.0697, ret=0.000468, glen=117, tlen=278, kl=0.000921, act_lr=8e-8, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0517, ret=0.000172, glen=119, tlen=279, kl=0.000878, act_lr=8e-8, ent=1.88]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0517, ret=0.000172, glen=119, tlen=279, kl=0.000878, act_lr=8e-8, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.0114, ret=-0.000261, glen=114, tlen=275, kl=0.000923, act_lr=8e-8, ent=1.69]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.0114, ret=-0.000261, glen=114, tlen=275, kl=0.000923, act_lr=8e-8, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0766, ret=0.000991, glen=111, tlen=271, kl=0.000871, act_lr=8e-8, ent=1.57]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0766, ret=0.000991, glen=111, tlen=271, kl=0.000871, act_lr=8e-8, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=-0.0869, ret=-0.00224, glen=131, tlen=292, kl=0.000904, act_lr=8e-8, ent=1.87]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=-0.0869, ret=-0.00224, glen=131, tlen=292, kl=0.000904, act_lr=8e-8, ent=1.87]
2025-07-23 12:06:24.302 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.65s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=0.148, ret=-0.00137, glen=114, tlen=274, kl=0.000949, act_lr=1e-7, ent=1.78]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.148, ret=-0.00137, glen=114, tlen=274, kl=0.000949, act_lr=1e-7, ent=1.78]
2025-07-23 12:06:25.157 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 12:06:27.667 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-23 12:06:27.972 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.43s
2025-07-23 12:06:27.978 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.008066679302014802, 'actor_lr': 8.035087673581649e-08, 'clip_ratio': 0.0, 'entropy': 1.7553850893388714, 'kl': 0.0009047190348307291, 'response_length': 115.75800751803214, 'total_length': 276.41286856668034, 'return': -4.948380203067995e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:31<27:12, 204.08s/it]2025-07-23 12:06:28.010 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:09:10.365 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:09:10.535 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-23 12:09:10.535 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 162.53s
2025-07-23 12:09:12.583 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0101,avg_pass_at_n: 1.0000,avg_num_tokens: 112.6650,std_num_tokens: 147.3066,avg_correct_num_tokens: 93.9842,std_correct_num_tokens: 93.7457,avg_incorrect_num_tokens: 121.6026,std_incorrect_num_tokens: 166.2210
2025-07-23 12:09:12.931 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.40s
2025-07-23 12:09:14.625 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.69s
2025-07-23 12:09:43.746 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 227
2025-07-23 12:09:43.746 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.12s
2025-07-23 12:09:44.866 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 1.12s
2025-07-23 12:09:44.867 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.002288872872003342, avg_kl: 0.0008939079251058301, avg_response_length: 119.37300671144729, avg_orm_score: 0.0, avg_custom_rewards: -0.002288872872003342
2025-07-23 12:09:44.948 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter5_replay_buffer.jsonl
2025-07-23 12:09:46.630 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.68s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.184, ret=0.00127, glen=108, tlen=269, kl=0.000913, act_lr=1e-7, ent=1.76]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:56,  1.00s/it, pg=-0.184, ret=0.00127, glen=108, tlen=269, kl=0.000913, act_lr=1e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:56,  1.00s/it, pg=0.0807, ret=-0.000123, glen=121, tlen=281, kl=0.000885, act_lr=1e-7, ent=1.67]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.09it/s, pg=0.0807, ret=-0.000123, glen=121, tlen=281, kl=0.000885, act_lr=1e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.09it/s, pg=-0.0142, ret=-0.000108, glen=108, tlen=269, kl=0.000886, act_lr=1e-7, ent=1.75]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=-0.0142, ret=-0.000108, glen=108, tlen=269, kl=0.000886, act_lr=1e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=-0.0815, ret=0.000467, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.62] Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.14it/s, pg=-0.0815, ret=0.000467, glen=108, tlen=268, kl=0.000857, act_lr=1e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.14it/s, pg=-0.182, ret=0.000782, glen=111, tlen=271, kl=0.00094, act_lr=1e-7, ent=1.77]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.15it/s, pg=-0.182, ret=0.000782, glen=111, tlen=271, kl=0.00094, act_lr=1e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.15it/s, pg=0.0167, ret=-0.00033, glen=101, tlen=262, kl=0.000881, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.16it/s, pg=0.0167, ret=-0.00033, glen=101, tlen=262, kl=0.000881, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.16it/s, pg=-0.112, ret=0.000409, glen=107, tlen=267, kl=0.00096, act_lr=1e-7, ent=1.79] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.13it/s, pg=-0.112, ret=0.000409, glen=107, tlen=267, kl=0.00096, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.13it/s, pg=-0.251, ret=0.00236, glen=105, tlen=266, kl=0.000941, act_lr=1e-7, ent=1.82]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=-0.251, ret=0.00236, glen=105, tlen=266, kl=0.000941, act_lr=1e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.14it/s, pg=0.0978, ret=-0.000343, glen=119, tlen=280, kl=0.000905, act_lr=1e-7, ent=1.74]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=0.0978, ret=-0.000343, glen=119, tlen=280, kl=0.000905, act_lr=1e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.0669, ret=0.00143, glen=115, tlen=275, kl=0.000925, act_lr=1e-7, ent=1.69] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=-0.0669, ret=0.00143, glen=115, tlen=275, kl=0.000925, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.076, ret=-0.000109, glen=139, tlen=299, kl=0.000904, act_lr=1e-7, ent=2.16]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.14it/s, pg=0.076, ret=-0.000109, glen=139, tlen=299, kl=0.000904, act_lr=1e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.14it/s, pg=0.0758, ret=-0.00125, glen=104, tlen=264, kl=0.000903, act_lr=1e-7, ent=1.73]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=0.0758, ret=-0.00125, glen=104, tlen=264, kl=0.000903, act_lr=1e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=0.187, ret=-0.00152, glen=123, tlen=284, kl=0.000873, act_lr=1e-7, ent=1.6]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=0.187, ret=-0.00152, glen=123, tlen=284, kl=0.000873, act_lr=1e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=0.072, ret=0.000227, glen=128, tlen=289, kl=0.000901, act_lr=1e-7, ent=1.96]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.16it/s, pg=0.072, ret=0.000227, glen=128, tlen=289, kl=0.000901, act_lr=1e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.16it/s, pg=-0.148, ret=0.000453, glen=105, tlen=266, kl=0.000897, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.148, ret=0.000453, glen=105, tlen=266, kl=0.000897, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=-0.0991, ret=0.000564, glen=102, tlen=262, kl=0.000919, act_lr=1e-7, ent=1.63]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.17it/s, pg=-0.0991, ret=0.000564, glen=102, tlen=262, kl=0.000919, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.0375, ret=0.000894, glen=121, tlen=282, kl=0.000866, act_lr=1e-7, ent=1.79]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.0375, ret=0.000894, glen=121, tlen=282, kl=0.000866, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.0156, ret=-0.000407, glen=95.3, tlen=256, kl=0.000897, act_lr=1e-7, ent=1.66]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.0156, ret=-0.000407, glen=95.3, tlen=256, kl=0.000897, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=0.124, ret=-0.00174, glen=114, tlen=275, kl=0.000908, act_lr=1e-7, ent=1.64]    Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=0.124, ret=-0.00174, glen=114, tlen=275, kl=0.000908, act_lr=1e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.183, ret=-0.001, glen=137, tlen=297, kl=0.00092, act_lr=1e-7, ent=2.27]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.183, ret=-0.001, glen=137, tlen=297, kl=0.00092, act_lr=1e-7, ent=2.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=0.139, ret=-0.00025, glen=101, tlen=261, kl=0.000928, act_lr=1e-7, ent=1.71]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.139, ret=-0.00025, glen=101, tlen=261, kl=0.000928, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.0242, ret=2.98e-5, glen=118, tlen=279, kl=0.000911, act_lr=1e-7, ent=1.7] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0242, ret=2.98e-5, glen=118, tlen=279, kl=0.000911, act_lr=1e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.417, ret=0.000739, glen=126, tlen=286, kl=0.000828, act_lr=1e-7, ent=1.69]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:29,  1.17it/s, pg=-0.417, ret=0.000739, glen=126, tlen=286, kl=0.000828, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.00529, ret=-0.00115, glen=111, tlen=272, kl=0.000896, act_lr=1e-7, ent=1.75]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.00529, ret=-0.00115, glen=111, tlen=272, kl=0.000896, act_lr=1e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0175, ret=-0.000255, glen=134, tlen=294, kl=0.000802, act_lr=1e-7, ent=1.65]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0175, ret=-0.000255, glen=134, tlen=294, kl=0.000802, act_lr=1e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.172, ret=0.000266, glen=94.5, tlen=255, kl=0.000935, act_lr=1e-7, ent=1.68] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.172, ret=0.000266, glen=94.5, tlen=255, kl=0.000935, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0482, ret=-0.000565, glen=105, tlen=265, kl=0.00093, act_lr=1e-7, ent=1.69] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:27,  1.07it/s, pg=0.0482, ret=-0.000565, glen=105, tlen=265, kl=0.00093, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:27,  1.07it/s, pg=0.167, ret=-0.0112, glen=457, tlen=617, kl=0.000711, act_lr=1e-7, ent=3.02]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:27,  1.07it/s, pg=0.167, ret=-0.0112, glen=457, tlen=617, kl=0.000711, act_lr=1e-7, ent=3.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:27,  1.07it/s, pg=0.0817, ret=-0.00135, glen=128, tlen=288, kl=0.000916, act_lr=1e-7, ent=1.94]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:25,  1.10it/s, pg=0.0817, ret=-0.00135, glen=128, tlen=288, kl=0.000916, act_lr=1e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.10it/s, pg=0.052, ret=-0.000152, glen=127, tlen=288, kl=0.00091, act_lr=1e-7, ent=1.72] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.12it/s, pg=0.052, ret=-0.000152, glen=127, tlen=288, kl=0.00091, act_lr=1e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.12it/s, pg=0.0436, ret=-0.00116, glen=103, tlen=264, kl=0.000955, act_lr=1e-7, ent=1.63]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.13it/s, pg=0.0436, ret=-0.00116, glen=103, tlen=264, kl=0.000955, act_lr=1e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.13it/s, pg=0.174, ret=-0.000594, glen=117, tlen=277, kl=0.00093, act_lr=1e-7, ent=1.91] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.15it/s, pg=0.174, ret=-0.000594, glen=117, tlen=277, kl=0.00093, act_lr=1e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.15it/s, pg=-0.122, ret=0.000152, glen=100, tlen=260, kl=0.000945, act_lr=1e-7, ent=1.68]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=-0.122, ret=0.000152, glen=100, tlen=260, kl=0.000945, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=-0.12, ret=0.00174, glen=135, tlen=295, kl=0.000857, act_lr=1e-7, ent=1.98]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=-0.12, ret=0.00174, glen=135, tlen=295, kl=0.000857, act_lr=1e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=0.000488, ret=0.000735, glen=116, tlen=276, kl=0.00087, act_lr=1e-7, ent=1.88]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=0.000488, ret=0.000735, glen=116, tlen=276, kl=0.00087, act_lr=1e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=-0.182, ret=0.00127, glen=124, tlen=285, kl=0.000862, act_lr=1e-7, ent=1.62]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=-0.182, ret=0.00127, glen=124, tlen=285, kl=0.000862, act_lr=1e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.188, ret=0.000861, glen=107, tlen=267, kl=0.000903, act_lr=1e-7, ent=1.58]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.188, ret=0.000861, glen=107, tlen=267, kl=0.000903, act_lr=1e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=-0.0968, ret=0.00104, glen=96.5, tlen=257, kl=0.000886, act_lr=1e-7, ent=1.61]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=-0.0968, ret=0.00104, glen=96.5, tlen=257, kl=0.000886, act_lr=1e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=0.132, ret=-0.000902, glen=102, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.64] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=0.132, ret=-0.000902, glen=102, tlen=262, kl=0.000908, act_lr=1e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.204, ret=-0.00226, glen=108, tlen=269, kl=0.000894, act_lr=1e-7, ent=1.79] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.204, ret=-0.00226, glen=108, tlen=269, kl=0.000894, act_lr=1e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.132, ret=0.0013, glen=106, tlen=267, kl=0.000902, act_lr=1e-7, ent=1.66] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.132, ret=0.0013, glen=106, tlen=267, kl=0.000902, act_lr=1e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0173, ret=-0.000723, glen=110, tlen=271, kl=0.000862, act_lr=1e-7, ent=1.68]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:13,  1.15it/s, pg=0.0173, ret=-0.000723, glen=110, tlen=271, kl=0.000862, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:13,  1.15it/s, pg=0.0484, ret=0.00128, glen=109, tlen=269, kl=0.000883, act_lr=1e-7, ent=1.86]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.16it/s, pg=0.0484, ret=0.00128, glen=109, tlen=269, kl=0.000883, act_lr=1e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=0.0659, ret=-0.00124, glen=115, tlen=276, kl=0.000866, act_lr=1e-7, ent=1.74]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.0659, ret=-0.00124, glen=115, tlen=276, kl=0.000866, act_lr=1e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.16it/s, pg=0.17, ret=-0.00178, glen=134, tlen=295, kl=0.00088, act_lr=1e-7, ent=1.82]   Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.15it/s, pg=0.17, ret=-0.00178, glen=134, tlen=295, kl=0.00088, act_lr=1e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.15it/s, pg=0.021, ret=-0.000305, glen=104, tlen=265, kl=0.000911, act_lr=1e-7, ent=1.86]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.15it/s, pg=0.021, ret=-0.000305, glen=104, tlen=265, kl=0.000911, act_lr=1e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.15it/s, pg=0.0463, ret=-0.000431, glen=112, tlen=273, kl=0.000922, act_lr=1e-7, ent=1.73]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.16it/s, pg=0.0463, ret=-0.000431, glen=112, tlen=273, kl=0.000922, act_lr=1e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.16it/s, pg=0.00879, ret=-0.000657, glen=106, tlen=267, kl=0.0009, act_lr=1e-7, ent=1.83] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.00879, ret=-0.000657, glen=106, tlen=267, kl=0.0009, act_lr=1e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.105, ret=0.000524, glen=109, tlen=269, kl=0.000896, act_lr=1e-7, ent=1.88]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=-0.105, ret=0.000524, glen=109, tlen=269, kl=0.000896, act_lr=1e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=0.0397, ret=-0.000588, glen=119, tlen=280, kl=0.000926, act_lr=1e-7, ent=1.71]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.0397, ret=-0.000588, glen=119, tlen=280, kl=0.000926, act_lr=1e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.0233, ret=-0.0011, glen=135, tlen=295, kl=0.000856, act_lr=1e-7, ent=1.88] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.0233, ret=-0.0011, glen=135, tlen=295, kl=0.000856, act_lr=1e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.325, ret=0.00194, glen=109, tlen=269, kl=0.000875, act_lr=1e-7, ent=1.7]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.325, ret=0.00194, glen=109, tlen=269, kl=0.000875, act_lr=1e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0457, ret=-3.14e-5, glen=98.4, tlen=259, kl=0.000897, act_lr=1e-7, ent=1.68]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0457, ret=-3.14e-5, glen=98.4, tlen=259, kl=0.000897, act_lr=1e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.0144, ret=-0.00139, glen=112, tlen=272, kl=0.000852, act_lr=1e-7, ent=1.69]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.0144, ret=-0.00139, glen=112, tlen=272, kl=0.000852, act_lr=1e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0206, ret=0.00131, glen=123, tlen=284, kl=0.000875, act_lr=1e-7, ent=1.82]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0206, ret=0.00131, glen=123, tlen=284, kl=0.000875, act_lr=1e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.217, ret=-0.00255, glen=125, tlen=285, kl=0.000879, act_lr=1e-7, ent=1.91] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.07it/s, pg=0.217, ret=-0.00255, glen=125, tlen=285, kl=0.000879, act_lr=1e-7, ent=1.91]
2025-07-23 12:10:36.583 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.77s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.07it/s, pg=-0.118, ret=0.000387, glen=100, tlen=260, kl=0.00091, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.118, ret=0.000387, glen=100, tlen=260, kl=0.00091, act_lr=1.2e-7, ent=1.83]
2025-07-23 12:10:37.407 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-23 12:10:39.966 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-23 12:10:40.293 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.61s
2025-07-23 12:10:40.299 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.011487659655119243, 'actor_lr': 1.0035087826596729e-07, 'clip_ratio': 0.0, 'entropy': 1.7809384245621531, 'kl': 0.0008938856292189213, 'response_length': 119.40297645434998, 'total_length': 280.0177430270011, 'return': -0.00026617451019997693, 'policy_update_steps': 1.0}
Episode [1/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:44<25:43, 220.49s/it]2025-07-23 12:10:40.332 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:13:07.679 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:13:07.862 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:13:07.862 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 147.53s
2025-07-23 12:13:09.971 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0082,avg_pass_at_n: 1.0000,avg_num_tokens: 110.8486,std_num_tokens: 133.3345,avg_correct_num_tokens: 91.3896,std_correct_num_tokens: 80.4451,avg_incorrect_num_tokens: 120.2941,std_incorrect_num_tokens: 151.6357
2025-07-23 12:13:10.337 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.47s
2025-07-23 12:13:11.967 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.63s
2025-07-23 12:13:41.028 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 226
2025-07-23 12:13:41.029 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.06s
2025-07-23 12:13:42.002 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.97s
2025-07-23 12:13:42.003 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0006339073238329781, avg_kl: 0.0008905798988004701, avg_response_length: 113.06420652001304, avg_orm_score: 0.0, avg_custom_rewards: -0.0006339073238329781
2025-07-23 12:13:42.062 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter6_replay_buffer.jsonl
2025-07-23 12:13:43.728 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.67s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0319, ret=-0.000138, glen=115, tlen=275, kl=0.000913, act_lr=1.2e-7, ent=1.72]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.02s/it, pg=0.0319, ret=-0.000138, glen=115, tlen=275, kl=0.000913, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:57,  1.02s/it, pg=-0.0939, ret=0.00129, glen=94.7, tlen=255, kl=0.00088, act_lr=1.2e-7, ent=1.68] Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.08it/s, pg=-0.0939, ret=0.00129, glen=94.7, tlen=255, kl=0.00088, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.08it/s, pg=0.0497, ret=-0.000763, glen=107, tlen=268, kl=0.000828, act_lr=1.2e-7, ent=2.04]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.12it/s, pg=0.0497, ret=-0.000763, glen=107, tlen=268, kl=0.000828, act_lr=1.2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.12it/s, pg=0.0496, ret=-0.00123, glen=108, tlen=269, kl=0.000858, act_lr=1.2e-7, ent=1.66] Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.11it/s, pg=0.0496, ret=-0.00123, glen=108, tlen=269, kl=0.000858, act_lr=1.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.11it/s, pg=-0.116, ret=-0.000536, glen=99.3, tlen=260, kl=0.000865, act_lr=1.2e-7, ent=1.72]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.13it/s, pg=-0.116, ret=-0.000536, glen=99.3, tlen=260, kl=0.000865, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.13it/s, pg=0.0475, ret=0.000422, glen=121, tlen=282, kl=0.000953, act_lr=1.2e-7, ent=1.85]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.12it/s, pg=0.0475, ret=0.000422, glen=121, tlen=282, kl=0.000953, act_lr=1.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.12it/s, pg=-0.0439, ret=0.000186, glen=108, tlen=269, kl=0.000906, act_lr=1.2e-7, ent=1.68]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.14it/s, pg=-0.0439, ret=0.000186, glen=108, tlen=269, kl=0.000906, act_lr=1.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.14it/s, pg=0.225, ret=-0.0026, glen=111, tlen=272, kl=0.000887, act_lr=1.2e-7, ent=1.77]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.15it/s, pg=0.225, ret=-0.0026, glen=111, tlen=272, kl=0.000887, act_lr=1.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:42,  1.15it/s, pg=-0.1, ret=-7.01e-5, glen=109, tlen=270, kl=0.0009, act_lr=1.2e-7, ent=1.71]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.13it/s, pg=-0.1, ret=-7.01e-5, glen=109, tlen=270, kl=0.0009, act_lr=1.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.13it/s, pg=-0.00415, ret=-0.000665, glen=131, tlen=292, kl=0.00087, act_lr=1.2e-7, ent=2.16]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=-0.00415, ret=-0.000665, glen=131, tlen=292, kl=0.00087, act_lr=1.2e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.0647, ret=0.000188, glen=98, tlen=258, kl=0.000916, act_lr=1.2e-7, ent=1.65]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.15it/s, pg=-0.0647, ret=0.000188, glen=98, tlen=258, kl=0.000916, act_lr=1.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.15it/s, pg=-0.0502, ret=0.000724, glen=108, tlen=268, kl=0.000912, act_lr=1.2e-7, ent=1.7]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.0502, ret=0.000724, glen=108, tlen=268, kl=0.000912, act_lr=1.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.0854, ret=0.000511, glen=111, tlen=272, kl=0.000916, act_lr=1.2e-7, ent=1.8]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=-0.0854, ret=0.000511, glen=111, tlen=272, kl=0.000916, act_lr=1.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=-0.0159, ret=0.000917, glen=121, tlen=282, kl=0.000899, act_lr=1.2e-7, ent=1.76]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=-0.0159, ret=0.000917, glen=121, tlen=282, kl=0.000899, act_lr=1.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=0.0238, ret=-0.000549, glen=113, tlen=274, kl=0.000915, act_lr=1.2e-7, ent=1.8] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0238, ret=-0.000549, glen=113, tlen=274, kl=0.000915, act_lr=1.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:35,  1.17it/s, pg=0.0167, ret=0.00165, glen=121, tlen=281, kl=0.000863, act_lr=1.2e-7, ent=2.07] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.0167, ret=0.00165, glen=121, tlen=281, kl=0.000863, act_lr=1.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=0.0175, ret=0.000139, glen=106, tlen=267, kl=0.000912, act_lr=1.2e-7, ent=1.65]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.0175, ret=0.000139, glen=106, tlen=267, kl=0.000912, act_lr=1.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.134, ret=0.000613, glen=118, tlen=279, kl=0.000887, act_lr=1.2e-7, ent=1.95]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.134, ret=0.000613, glen=118, tlen=279, kl=0.000887, act_lr=1.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=0.122, ret=-0.00196, glen=103, tlen=264, kl=0.000848, act_lr=1.2e-7, ent=1.65] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=0.122, ret=-0.00196, glen=103, tlen=264, kl=0.000848, act_lr=1.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.00616, ret=0.000751, glen=95.6, tlen=256, kl=0.000915, act_lr=1.2e-7, ent=1.6]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:32,  1.15it/s, pg=-0.00616, ret=0.000751, glen=95.6, tlen=256, kl=0.000915, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:32,  1.15it/s, pg=0.0499, ret=0.000609, glen=113, tlen=273, kl=0.000913, act_lr=1.2e-7, ent=1.82]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:31,  1.16it/s, pg=0.0499, ret=0.000609, glen=113, tlen=273, kl=0.000913, act_lr=1.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:31,  1.16it/s, pg=0.0223, ret=-0.000893, glen=101, tlen=261, kl=0.000888, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=0.0223, ret=-0.000893, glen=101, tlen=261, kl=0.000888, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.16it/s, pg=0.0169, ret=0.000108, glen=118, tlen=279, kl=0.000861, act_lr=1.2e-7, ent=1.6]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0169, ret=0.000108, glen=118, tlen=279, kl=0.000861, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.102, ret=-0.00806, glen=144, tlen=303, kl=0.000931, act_lr=1.2e-7, ent=1.89]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.102, ret=-0.00806, glen=144, tlen=303, kl=0.000931, act_lr=1.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0139, ret=0.000773, glen=116, tlen=277, kl=0.000909, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0139, ret=0.000773, glen=116, tlen=277, kl=0.000909, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.176, ret=-0.00186, glen=126, tlen=287, kl=0.000873, act_lr=1.2e-7, ent=1.9]   Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.176, ret=-0.00186, glen=126, tlen=287, kl=0.000873, act_lr=1.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.156, ret=0.00154, glen=123, tlen=283, kl=0.000879, act_lr=1.2e-7, ent=1.69]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:27,  1.07it/s, pg=-0.156, ret=0.00154, glen=123, tlen=283, kl=0.000879, act_lr=1.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:27,  1.07it/s, pg=-0.135, ret=0.00128, glen=114, tlen=275, kl=0.000949, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.10it/s, pg=-0.135, ret=0.00128, glen=114, tlen=275, kl=0.000949, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.10it/s, pg=0.11, ret=-0.000757, glen=105, tlen=266, kl=0.0009, act_lr=1.2e-7, ent=1.74]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:24,  1.12it/s, pg=0.11, ret=-0.000757, glen=105, tlen=266, kl=0.0009, act_lr=1.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:24,  1.12it/s, pg=0.0123, ret=-0.000813, glen=129, tlen=290, kl=0.000893, act_lr=1.2e-7, ent=2.09]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=0.0123, ret=-0.000813, glen=129, tlen=290, kl=0.000893, act_lr=1.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.13it/s, pg=-0.0226, ret=-0.000447, glen=109, tlen=269, kl=0.000925, act_lr=1.2e-7, ent=1.93]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.0226, ret=-0.000447, glen=109, tlen=269, kl=0.000925, act_lr=1.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.0415, ret=4.51e-5, glen=99.8, tlen=261, kl=0.000906, act_lr=1.2e-7, ent=1.76] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.16it/s, pg=-0.0415, ret=4.51e-5, glen=99.8, tlen=261, kl=0.000906, act_lr=1.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.16it/s, pg=0.147, ret=-0.00186, glen=160, tlen=321, kl=0.000786, act_lr=1.2e-7, ent=1.61]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.15it/s, pg=0.147, ret=-0.00186, glen=160, tlen=321, kl=0.000786, act_lr=1.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=-0.00662, ret=0.000201, glen=91.3, tlen=252, kl=0.000895, act_lr=1.2e-7, ent=1.65]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=-0.00662, ret=0.000201, glen=91.3, tlen=252, kl=0.000895, act_lr=1.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=-0.0399, ret=-0.000405, glen=122, tlen=283, kl=0.000894, act_lr=1.2e-7, ent=1.65] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=-0.0399, ret=-0.000405, glen=122, tlen=283, kl=0.000894, act_lr=1.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=-0.0488, ret=-0.00078, glen=97.9, tlen=259, kl=0.000895, act_lr=1.2e-7, ent=1.66]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=-0.0488, ret=-0.00078, glen=97.9, tlen=259, kl=0.000895, act_lr=1.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.0128, ret=0.000148, glen=108, tlen=269, kl=0.000905, act_lr=1.2e-7, ent=1.72] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0128, ret=0.000148, glen=108, tlen=269, kl=0.000905, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=0.0786, ret=-0.00112, glen=107, tlen=267, kl=0.000897, act_lr=1.2e-7, ent=1.6]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.0786, ret=-0.00112, glen=107, tlen=267, kl=0.000897, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.117, ret=-0.000181, glen=103, tlen=263, kl=0.000944, act_lr=1.2e-7, ent=1.7]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.117, ret=-0.000181, glen=103, tlen=263, kl=0.000944, act_lr=1.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.235, ret=0.00161, glen=109, tlen=270, kl=0.000889, act_lr=1.2e-7, ent=1.6]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.235, ret=0.00161, glen=109, tlen=270, kl=0.000889, act_lr=1.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.142, ret=-0.0003, glen=108, tlen=268, kl=0.000912, act_lr=1.2e-7, ent=1.79]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.142, ret=-0.0003, glen=108, tlen=268, kl=0.000912, act_lr=1.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0817, ret=0.000937, glen=138, tlen=299, kl=0.00077, act_lr=1.2e-7, ent=2.44]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.0817, ret=0.000937, glen=138, tlen=299, kl=0.00077, act_lr=1.2e-7, ent=2.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.04, ret=0.000711, glen=94, tlen=255, kl=0.000895, act_lr=1.2e-7, ent=1.82]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.04, ret=0.000711, glen=94, tlen=255, kl=0.000895, act_lr=1.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=0.178, ret=-0.00132, glen=110, tlen=271, kl=0.000862, act_lr=1.2e-7, ent=1.65]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.178, ret=-0.00132, glen=110, tlen=271, kl=0.000862, act_lr=1.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=0.139, ret=-0.000446, glen=127, tlen=288, kl=0.000883, act_lr=1.2e-7, ent=1.84]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=0.139, ret=-0.000446, glen=127, tlen=288, kl=0.000883, act_lr=1.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=0.154, ret=-0.0017, glen=109, tlen=270, kl=0.000929, act_lr=1.2e-7, ent=1.77]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=0.154, ret=-0.0017, glen=109, tlen=270, kl=0.000929, act_lr=1.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.29, ret=0.00177, glen=100, tlen=261, kl=0.000896, act_lr=1.2e-7, ent=1.79]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.29, ret=0.00177, glen=100, tlen=261, kl=0.000896, act_lr=1.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=-0.0479, ret=0.000487, glen=112, tlen=273, kl=0.000896, act_lr=1.2e-7, ent=1.77]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=-0.0479, ret=0.000487, glen=112, tlen=273, kl=0.000896, act_lr=1.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=-0.0298, ret=0.000143, glen=112, tlen=273, kl=0.000869, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=-0.0298, ret=0.000143, glen=112, tlen=273, kl=0.000869, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=0.0284, ret=0.00259, glen=146, tlen=307, kl=0.000764, act_lr=1.2e-7, ent=1.51]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.17it/s, pg=0.0284, ret=0.00259, glen=146, tlen=307, kl=0.000764, act_lr=1.2e-7, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.17it/s, pg=0.136, ret=4.75e-5, glen=127, tlen=287, kl=0.000881, act_lr=1.2e-7, ent=1.72] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.136, ret=4.75e-5, glen=127, tlen=287, kl=0.000881, act_lr=1.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.152, ret=0.000726, glen=104, tlen=264, kl=0.000896, act_lr=1.2e-7, ent=1.75]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.152, ret=0.000726, glen=104, tlen=264, kl=0.000896, act_lr=1.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.2, ret=0.00195, glen=132, tlen=293, kl=0.000869, act_lr=1.2e-7, ent=2.01]   Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.2, ret=0.00195, glen=132, tlen=293, kl=0.000869, act_lr=1.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.025, ret=0.000921, glen=119, tlen=279, kl=0.000941, act_lr=1.2e-7, ent=1.83]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=-0.025, ret=0.000921, glen=119, tlen=279, kl=0.000941, act_lr=1.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=0.0427, ret=-0.000981, glen=109, tlen=270, kl=0.000884, act_lr=1.2e-7, ent=1.8]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=0.0427, ret=-0.000981, glen=109, tlen=270, kl=0.000884, act_lr=1.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0258, ret=0.000514, glen=101, tlen=261, kl=0.000901, act_lr=1.2e-7, ent=1.77]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.07it/s, pg=0.0258, ret=0.000514, glen=101, tlen=261, kl=0.000901, act_lr=1.2e-7, ent=1.77]
2025-07-23 12:14:33.577 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.68s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.07it/s, pg=0.0772, ret=-0.00116, glen=109, tlen=270, kl=0.000941, act_lr=1.4e-7, ent=1.8] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.0772, ret=-0.00116, glen=109, tlen=270, kl=0.000941, act_lr=1.4e-7, ent=1.8]
2025-07-23 12:14:34.272 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 12:14:36.559 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-23 12:14:36.860 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.08s
2025-07-23 12:14:36.867 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.00335726821631716, 'actor_lr': 1.2035087294000397e-07, 'clip_ratio': 0.0, 'entropy': 1.7758127911049022, 'kl': 0.0008905477691114994, 'response_length': 113.20463307698567, 'total_length': 273.88067921420986, 'return': -0.00012435225500284056, 'policy_update_steps': 1.0}
Episode [1/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [26:40<22:34, 225.74s/it]2025-07-23 12:14:36.900 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:16:38.931 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:16:39.115 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:16:39.115 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 122.22s
2025-07-23 12:16:41.063 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0101,avg_pass_at_n: 1.0000,avg_num_tokens: 109.4235,std_num_tokens: 127.7889,avg_correct_num_tokens: 94.1186,std_correct_num_tokens: 82.9188,avg_incorrect_num_tokens: 117.3280,std_incorrect_num_tokens: 145.0156
2025-07-23 12:16:41.518 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.40s
2025-07-23 12:16:43.156 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.63s
2025-07-23 12:17:11.607 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 224
2025-07-23 12:17:11.607 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.45s
2025-07-23 12:17:12.644 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 1.03s
2025-07-23 12:17:12.645 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0009962947868708788, avg_kl: 0.0009091581617082868, avg_response_length: 111.59739817891803, avg_orm_score: 0.0, avg_custom_rewards: -0.0009962947868708788
2025-07-23 12:17:12.707 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter7_replay_buffer.jsonl
2025-07-23 12:17:14.356 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.65s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.0367, ret=-0.000821, glen=117, tlen=278, kl=0.000916, act_lr=1.4e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.02s/it, pg=-0.0367, ret=-0.000821, glen=117, tlen=278, kl=0.000916, act_lr=1.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:56,  1.02s/it, pg=-0.241, ret=0.00222, glen=100, tlen=261, kl=0.00095, act_lr=1.4e-7, ent=1.71]    Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:50,  1.08it/s, pg=-0.241, ret=0.00222, glen=100, tlen=261, kl=0.00095, act_lr=1.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:50,  1.08it/s, pg=-0.0342, ret=7.21e-6, glen=108, tlen=269, kl=0.000922, act_lr=1.4e-7, ent=1.78]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:47,  1.12it/s, pg=-0.0342, ret=7.21e-6, glen=108, tlen=269, kl=0.000922, act_lr=1.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:47,  1.12it/s, pg=-0.112, ret=0.00055, glen=93.8, tlen=254, kl=0.000898, act_lr=1.4e-7, ent=1.62]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:45,  1.14it/s, pg=-0.112, ret=0.00055, glen=93.8, tlen=254, kl=0.000898, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:45,  1.14it/s, pg=-0.0527, ret=0.000454, glen=101, tlen=261, kl=0.000902, act_lr=1.4e-7, ent=1.76]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:44,  1.15it/s, pg=-0.0527, ret=0.000454, glen=101, tlen=261, kl=0.000902, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:44,  1.15it/s, pg=0.0887, ret=-2.67e-5, glen=108, tlen=268, kl=0.000924, act_lr=1.4e-7, ent=1.67] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.16it/s, pg=0.0887, ret=-2.67e-5, glen=108, tlen=268, kl=0.000924, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.16it/s, pg=0.00952, ret=0.000244, glen=107, tlen=267, kl=0.00091, act_lr=1.4e-7, ent=1.73]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.16it/s, pg=0.00952, ret=0.000244, glen=107, tlen=267, kl=0.00091, act_lr=1.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.16it/s, pg=0.102, ret=-0.000904, glen=106, tlen=266, kl=0.000888, act_lr=1.4e-7, ent=1.67]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=0.102, ret=-0.000904, glen=106, tlen=266, kl=0.000888, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.16it/s, pg=-0.138, ret=-2.48e-5, glen=100, tlen=260, kl=0.000908, act_lr=1.4e-7, ent=1.72]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.17it/s, pg=-0.138, ret=-2.48e-5, glen=100, tlen=260, kl=0.000908, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.17it/s, pg=-0.0624, ret=0.00143, glen=107, tlen=267, kl=0.000911, act_lr=1.4e-7, ent=1.7] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0624, ret=0.00143, glen=107, tlen=267, kl=0.000911, act_lr=1.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=0.0467, ret=-0.0013, glen=114, tlen=274, kl=0.000907, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.14it/s, pg=0.0467, ret=-0.0013, glen=114, tlen=274, kl=0.000907, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.14it/s, pg=-0.0276, ret=-2.25e-5, glen=103, tlen=263, kl=0.000871, act_lr=1.4e-7, ent=1.81]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.15it/s, pg=-0.0276, ret=-2.25e-5, glen=103, tlen=263, kl=0.000871, act_lr=1.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.15it/s, pg=0.106, ret=-0.000592, glen=116, tlen=277, kl=0.000907, act_lr=1.4e-7, ent=1.92] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.16it/s, pg=0.106, ret=-0.000592, glen=116, tlen=277, kl=0.000907, act_lr=1.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.16it/s, pg=0.102, ret=-0.000161, glen=124, tlen=284, kl=0.0009, act_lr=1.4e-7, ent=1.76]  Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.16it/s, pg=0.102, ret=-0.000161, glen=124, tlen=284, kl=0.0009, act_lr=1.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.16it/s, pg=-0.0878, ret=0.00158, glen=115, tlen=276, kl=0.000853, act_lr=1.4e-7, ent=1.73]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.0878, ret=0.00158, glen=115, tlen=276, kl=0.000853, act_lr=1.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.167, ret=0.00113, glen=106, tlen=267, kl=0.000903, act_lr=1.4e-7, ent=1.71] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:13<00:34,  1.17it/s, pg=-0.167, ret=0.00113, glen=106, tlen=267, kl=0.000903, act_lr=1.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=0.159, ret=-0.00194, glen=116, tlen=276, kl=0.000901, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=0.159, ret=-0.00194, glen=116, tlen=276, kl=0.000901, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=-0.00388, ret=-7.38e-5, glen=111, tlen=271, kl=0.000921, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=-0.00388, ret=-7.38e-5, glen=111, tlen=271, kl=0.000921, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.164, ret=0.000168, glen=110, tlen=271, kl=0.000933, act_lr=1.4e-7, ent=1.8]   Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.164, ret=0.000168, glen=110, tlen=271, kl=0.000933, act_lr=1.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.0564, ret=-0.000707, glen=109, tlen=269, kl=0.000925, act_lr=1.4e-7, ent=1.77]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.0564, ret=-0.000707, glen=109, tlen=269, kl=0.000925, act_lr=1.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=-0.0596, ret=0.00133, glen=109, tlen=270, kl=0.000925, act_lr=1.4e-7, ent=1.9]  Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=-0.0596, ret=0.00133, glen=109, tlen=270, kl=0.000925, act_lr=1.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=0.087, ret=-0.000981, glen=111, tlen=272, kl=0.000916, act_lr=1.4e-7, ent=1.67]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=0.087, ret=-0.000981, glen=111, tlen=272, kl=0.000916, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.0229, ret=-0.000748, glen=115, tlen=276, kl=0.000889, act_lr=1.4e-7, ent=2.04]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.16it/s, pg=-0.0229, ret=-0.000748, glen=115, tlen=276, kl=0.000889, act_lr=1.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.00698, ret=0.000477, glen=99.6, tlen=260, kl=0.000929, act_lr=1.4e-7, ent=1.65]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.16it/s, pg=-0.00698, ret=0.000477, glen=99.6, tlen=260, kl=0.000929, act_lr=1.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.16it/s, pg=-0.0469, ret=6.59e-5, glen=131, tlen=291, kl=0.000879, act_lr=1.4e-7, ent=1.8]    Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.0469, ret=6.59e-5, glen=131, tlen=291, kl=0.000879, act_lr=1.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=0.125, ret=-0.00212, glen=109, tlen=269, kl=0.000925, act_lr=1.4e-7, ent=1.79]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=0.125, ret=-0.00212, glen=109, tlen=269, kl=0.000925, act_lr=1.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=0.0439, ret=-0.000416, glen=110, tlen=270, kl=0.000986, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:27,  1.07it/s, pg=0.0439, ret=-0.000416, glen=110, tlen=270, kl=0.000986, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:27,  1.07it/s, pg=-0.0558, ret=-0.000664, glen=142, tlen=302, kl=0.000871, act_lr=1.4e-7, ent=2.18]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:25,  1.09it/s, pg=-0.0558, ret=-0.000664, glen=142, tlen=302, kl=0.000871, act_lr=1.4e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:25,  1.09it/s, pg=-0.00314, ret=0.000462, glen=119, tlen=279, kl=0.000861, act_lr=1.4e-7, ent=1.86]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:24,  1.11it/s, pg=-0.00314, ret=0.000462, glen=119, tlen=279, kl=0.000861, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:24,  1.11it/s, pg=-0.146, ret=0.00106, glen=111, tlen=271, kl=0.00092, act_lr=1.4e-7, ent=1.95]    Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:22,  1.13it/s, pg=-0.146, ret=0.00106, glen=111, tlen=271, kl=0.00092, act_lr=1.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:22,  1.13it/s, pg=-0.195, ret=0.00102, glen=114, tlen=275, kl=0.00091, act_lr=1.4e-7, ent=1.66]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:21,  1.15it/s, pg=-0.195, ret=0.00102, glen=114, tlen=275, kl=0.00091, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:21,  1.15it/s, pg=0.0212, ret=-0.000422, glen=123, tlen=283, kl=0.000863, act_lr=1.4e-7, ent=1.96]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:20,  1.15it/s, pg=0.0212, ret=-0.000422, glen=123, tlen=283, kl=0.000863, act_lr=1.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:20,  1.15it/s, pg=0.0793, ret=-0.000547, glen=98, tlen=258, kl=0.000906, act_lr=1.4e-7, ent=1.74] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:19,  1.16it/s, pg=0.0793, ret=-0.000547, glen=98, tlen=258, kl=0.000906, act_lr=1.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:19,  1.16it/s, pg=0.0486, ret=-0.000107, glen=104, tlen=265, kl=0.00092, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:18,  1.16it/s, pg=0.0486, ret=-0.000107, glen=104, tlen=265, kl=0.00092, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:18,  1.16it/s, pg=0.00114, ret=-0.00124, glen=109, tlen=270, kl=0.000926, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:17,  1.17it/s, pg=0.00114, ret=-0.00124, glen=109, tlen=270, kl=0.000926, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:17,  1.17it/s, pg=0.0775, ret=0.000422, glen=101, tlen=261, kl=0.000961, act_lr=1.4e-7, ent=1.75] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.17it/s, pg=0.0775, ret=0.000422, glen=101, tlen=261, kl=0.000961, act_lr=1.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.17it/s, pg=0.0422, ret=-0.000567, glen=120, tlen=281, kl=0.000935, act_lr=1.4e-7, ent=1.68]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=0.0422, ret=-0.000567, glen=120, tlen=281, kl=0.000935, act_lr=1.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=0.0477, ret=-0.000266, glen=120, tlen=280, kl=0.000881, act_lr=1.4e-7, ent=1.81]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:32<00:15,  1.17it/s, pg=0.0477, ret=-0.000266, glen=120, tlen=280, kl=0.000881, act_lr=1.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.234, ret=0.00239, glen=97.6, tlen=258, kl=0.000931, act_lr=1.4e-7, ent=1.66] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.17it/s, pg=-0.234, ret=0.00239, glen=97.6, tlen=258, kl=0.000931, act_lr=1.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.119, ret=0.00147, glen=100, tlen=261, kl=0.000908, act_lr=1.4e-7, ent=1.63] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.119, ret=0.00147, glen=100, tlen=261, kl=0.000908, act_lr=1.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0299, ret=0.000429, glen=106, tlen=266, kl=0.000939, act_lr=1.4e-7, ent=1.67]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.18it/s, pg=-0.0299, ret=0.000429, glen=106, tlen=266, kl=0.000939, act_lr=1.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.18it/s, pg=0.00873, ret=-0.000954, glen=120, tlen=281, kl=0.000897, act_lr=1.4e-7, ent=1.92]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.18it/s, pg=0.00873, ret=-0.000954, glen=120, tlen=281, kl=0.000897, act_lr=1.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.18it/s, pg=0.169, ret=-0.00126, glen=114, tlen=275, kl=0.000936, act_lr=1.4e-7, ent=1.84]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=0.169, ret=-0.00126, glen=114, tlen=275, kl=0.000936, act_lr=1.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0615, ret=-0.000967, glen=138, tlen=298, kl=0.000886, act_lr=1.4e-7, ent=2.04]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0615, ret=-0.000967, glen=138, tlen=298, kl=0.000886, act_lr=1.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.121, ret=-0.00175, glen=124, tlen=285, kl=0.000839, act_lr=1.4e-7, ent=1.86]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:38<00:09,  1.17it/s, pg=0.121, ret=-0.00175, glen=124, tlen=285, kl=0.000839, act_lr=1.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0674, ret=-0.000705, glen=108, tlen=268, kl=0.000882, act_lr=1.4e-7, ent=1.78]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.0674, ret=-0.000705, glen=108, tlen=268, kl=0.000882, act_lr=1.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=0.0773, ret=-0.00194, glen=112, tlen=272, kl=0.000922, act_lr=1.4e-7, ent=1.72]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=0.0773, ret=-0.00194, glen=112, tlen=272, kl=0.000922, act_lr=1.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.063, ret=-0.00121, glen=164, tlen=323, kl=0.000841, act_lr=1.4e-7, ent=2.35] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.16it/s, pg=0.063, ret=-0.00121, glen=164, tlen=323, kl=0.000841, act_lr=1.4e-7, ent=2.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.16it/s, pg=-0.0861, ret=0.000418, glen=102, tlen=263, kl=0.000905, act_lr=1.4e-7, ent=1.89]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:06,  1.16it/s, pg=-0.0861, ret=0.000418, glen=102, tlen=263, kl=0.000905, act_lr=1.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:06,  1.16it/s, pg=0.0433, ret=-0.000411, glen=107, tlen=267, kl=0.000909, act_lr=1.4e-7, ent=1.9] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=0.0433, ret=-0.000411, glen=107, tlen=267, kl=0.000909, act_lr=1.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=-0.0232, ret=0.000598, glen=105, tlen=265, kl=0.000914, act_lr=1.4e-7, ent=1.62]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=-0.0232, ret=0.000598, glen=105, tlen=265, kl=0.000914, act_lr=1.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0455, ret=-0.000269, glen=117, tlen=278, kl=0.000928, act_lr=1.4e-7, ent=1.8] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:44<00:03,  1.17it/s, pg=0.0455, ret=-0.000269, glen=117, tlen=278, kl=0.000928, act_lr=1.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.0111, ret=0.000817, glen=107, tlen=267, kl=0.00089, act_lr=1.4e-7, ent=1.61] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=0.0111, ret=0.000817, glen=107, tlen=267, kl=0.00089, act_lr=1.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.032, ret=0.000452, glen=104, tlen=264, kl=0.00092, act_lr=1.4e-7, ent=1.59]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.032, ret=0.000452, glen=104, tlen=264, kl=0.00092, act_lr=1.4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0832, ret=-0.00158, glen=102, tlen=262, kl=0.000965, act_lr=1.4e-7, ent=1.8]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.18it/s, pg=0.0832, ret=-0.00158, glen=102, tlen=262, kl=0.000965, act_lr=1.4e-7, ent=1.8]
2025-07-23 12:18:03.223 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 48.68s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.18it/s, pg=-0.182, ret=0.00271, glen=104, tlen=264, kl=0.000946, act_lr=1.6e-7, ent=1.88]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=-0.182, ret=0.00271, glen=104, tlen=264, kl=0.000946, act_lr=1.6e-7, ent=1.88]
2025-07-23 12:18:04.062 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 12:18:06.639 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-23 12:18:06.967 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 52.54s
2025-07-23 12:18:06.989 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.009074415479387556, 'actor_lr': 1.403571402594902e-07, 'clip_ratio': 0.0, 'entropy': 1.7895862460136414, 'kl': 0.0009091581617082868, 'response_length': 111.59739821297782, 'total_length': 271.9539130074637, 'return': -6.743561867291905e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [30:10<18:23, 220.77s/it]2025-07-23 12:18:07.024 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:20:26.434 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:20:26.623 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:20:26.623 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 139.60s
2025-07-23 12:20:28.678 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0104,avg_pass_at_n: 1.0000,avg_num_tokens: 110.6039,std_num_tokens: 169.4630,avg_correct_num_tokens: 91.2931,std_correct_num_tokens: 81.7613,avg_incorrect_num_tokens: 120.5991,std_incorrect_num_tokens: 199.5719
2025-07-23 12:20:29.150 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.53s
2025-07-23 12:20:30.768 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.61s
2025-07-23 12:20:59.734 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 225
2025-07-23 12:20:59.734 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.97s
2025-07-23 12:21:00.806 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 1.07s
2025-07-23 12:21:00.806 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00013858285072880486, avg_kl: 0.00089874267578125, avg_response_length: 115.2205391438802, avg_orm_score: 0.0, avg_custom_rewards: -0.00013858285072880486
2025-07-23 12:21:00.860 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter8_replay_buffer.jsonl
2025-07-23 12:21:02.535 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.68s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.0224, ret=-9.18e-5, glen=108, tlen=267, kl=0.000895, act_lr=1.6e-7, ent=1.74]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:59,  1.06s/it, pg=0.0224, ret=-9.18e-5, glen=108, tlen=267, kl=0.000895, act_lr=1.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:59,  1.06s/it, pg=-0.0845, ret=-0.000611, glen=105, tlen=265, kl=0.000907, act_lr=1.6e-7, ent=1.77]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.06it/s, pg=-0.0845, ret=-0.000611, glen=105, tlen=265, kl=0.000907, act_lr=1.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.06it/s, pg=-0.0261, ret=-0.000793, glen=106, tlen=266, kl=0.00095, act_lr=1.6e-7, ent=1.76] Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=-0.0261, ret=-0.000793, glen=106, tlen=266, kl=0.00095, act_lr=1.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=-0.0809, ret=-0.000167, glen=123, tlen=283, kl=0.000886, act_lr=1.6e-7, ent=1.84]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.13it/s, pg=-0.0809, ret=-0.000167, glen=123, tlen=283, kl=0.000886, act_lr=1.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.13it/s, pg=0.149, ret=-0.00223, glen=105, tlen=265, kl=0.000888, act_lr=1.6e-7, ent=1.66]   Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.15it/s, pg=0.149, ret=-0.00223, glen=105, tlen=265, kl=0.000888, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.15it/s, pg=0.0757, ret=-0.00121, glen=111, tlen=272, kl=0.000952, act_lr=1.6e-7, ent=1.9]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.0757, ret=-0.00121, glen=111, tlen=272, kl=0.000952, act_lr=1.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=0.0357, ret=-0.000377, glen=106, tlen=266, kl=0.000903, act_lr=1.6e-7, ent=1.81]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=0.0357, ret=-0.000377, glen=106, tlen=266, kl=0.000903, act_lr=1.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=-0.113, ret=0.000474, glen=104, tlen=264, kl=0.000915, act_lr=1.6e-7, ent=1.69] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=-0.113, ret=0.000474, glen=104, tlen=264, kl=0.000915, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.16it/s, pg=0.125, ret=-0.000836, glen=99.9, tlen=259, kl=0.000977, act_lr=1.6e-7, ent=1.71]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.17it/s, pg=0.125, ret=-0.000836, glen=99.9, tlen=259, kl=0.000977, act_lr=1.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.17it/s, pg=-0.0249, ret=0.00119, glen=139, tlen=299, kl=0.000851, act_lr=1.6e-7, ent=1.91] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=-0.0249, ret=0.00119, glen=139, tlen=299, kl=0.000851, act_lr=1.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=0.123, ret=-0.0019, glen=224, tlen=384, kl=0.000877, act_lr=1.6e-7, ent=1.91]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.14it/s, pg=0.123, ret=-0.0019, glen=224, tlen=384, kl=0.000877, act_lr=1.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.14it/s, pg=-0.0294, ret=9.39e-5, glen=104, tlen=264, kl=0.000914, act_lr=1.6e-7, ent=1.64]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.15it/s, pg=-0.0294, ret=9.39e-5, glen=104, tlen=264, kl=0.000914, act_lr=1.6e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.15it/s, pg=-0.127, ret=0.00158, glen=118, tlen=278, kl=0.000888, act_lr=1.6e-7, ent=1.66] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.127, ret=0.00158, glen=118, tlen=278, kl=0.000888, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.0131, ret=0.00215, glen=126, tlen=286, kl=0.000845, act_lr=1.6e-7, ent=1.92]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=-0.0131, ret=0.00215, glen=126, tlen=286, kl=0.000845, act_lr=1.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0535, ret=-0.000602, glen=108, tlen=268, kl=0.000929, act_lr=1.6e-7, ent=1.69]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=0.0535, ret=-0.000602, glen=108, tlen=268, kl=0.000929, act_lr=1.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=-0.0918, ret=0.00124, glen=108, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.79] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.15it/s, pg=-0.0918, ret=0.00124, glen=108, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.15it/s, pg=0.0577, ret=6.02e-5, glen=114, tlen=275, kl=0.000902, act_lr=1.6e-7, ent=1.87] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.16it/s, pg=0.0577, ret=6.02e-5, glen=114, tlen=275, kl=0.000902, act_lr=1.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.16it/s, pg=-0.196, ret=0.00228, glen=109, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.74]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=-0.196, ret=0.00228, glen=109, tlen=269, kl=0.000896, act_lr=1.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=-0.138, ret=0.00107, glen=96.8, tlen=257, kl=0.000933, act_lr=1.6e-7, ent=1.67]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=-0.138, ret=0.00107, glen=96.8, tlen=257, kl=0.000933, act_lr=1.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.0956, ret=0.00124, glen=140, tlen=301, kl=0.000814, act_lr=1.6e-7, ent=1.82] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.0956, ret=0.00124, glen=140, tlen=301, kl=0.000814, act_lr=1.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=0.0269, ret=-0.00109, glen=115, tlen=275, kl=0.000951, act_lr=1.6e-7, ent=1.76]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.0269, ret=-0.00109, glen=115, tlen=275, kl=0.000951, act_lr=1.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=0.115, ret=-0.000384, glen=132, tlen=292, kl=0.000919, act_lr=1.6e-7, ent=1.85]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.115, ret=-0.000384, glen=132, tlen=292, kl=0.000919, act_lr=1.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.117, ret=-0.00102, glen=108, tlen=269, kl=0.00092, act_lr=1.6e-7, ent=1.65]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.17it/s, pg=0.117, ret=-0.00102, glen=108, tlen=269, kl=0.00092, act_lr=1.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.17it/s, pg=-0.0358, ret=-0.000463, glen=96.8, tlen=257, kl=0.000888, act_lr=1.6e-7, ent=1.66]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.0358, ret=-0.000463, glen=96.8, tlen=257, kl=0.000888, act_lr=1.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.0567, ret=0.000115, glen=104, tlen=263, kl=0.000898, act_lr=1.6e-7, ent=1.85]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.0567, ret=0.000115, glen=104, tlen=263, kl=0.000898, act_lr=1.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.108, ret=0.00107, glen=126, tlen=286, kl=0.000879, act_lr=1.6e-7, ent=1.59]   Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.108, ret=0.00107, glen=126, tlen=286, kl=0.000879, act_lr=1.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.116, ret=0.000385, glen=109, tlen=269, kl=0.000905, act_lr=1.6e-7, ent=1.65]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:28,  1.06it/s, pg=-0.116, ret=0.000385, glen=109, tlen=269, kl=0.000905, act_lr=1.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.06it/s, pg=0.089, ret=-0.0007, glen=110, tlen=269, kl=0.000912, act_lr=1.6e-7, ent=1.78]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.09it/s, pg=0.089, ret=-0.0007, glen=110, tlen=269, kl=0.000912, act_lr=1.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.09it/s, pg=-0.203, ret=0.00146, glen=124, tlen=284, kl=0.000898, act_lr=1.6e-7, ent=1.8]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:25,  1.11it/s, pg=-0.203, ret=0.00146, glen=124, tlen=284, kl=0.000898, act_lr=1.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.11it/s, pg=-0.0636, ret=-0.000846, glen=105, tlen=265, kl=0.000893, act_lr=1.6e-7, ent=1.83]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=-0.0636, ret=-0.000846, glen=105, tlen=265, kl=0.000893, act_lr=1.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.13it/s, pg=-0.0257, ret=-0.000775, glen=100, tlen=260, kl=0.000957, act_lr=1.6e-7, ent=1.85]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.14it/s, pg=-0.0257, ret=-0.000775, glen=100, tlen=260, kl=0.000957, act_lr=1.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.14it/s, pg=-0.0415, ret=-0.000919, glen=103, tlen=263, kl=0.000948, act_lr=1.6e-7, ent=1.63]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.15it/s, pg=-0.0415, ret=-0.000919, glen=103, tlen=263, kl=0.000948, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.15it/s, pg=0.0531, ret=-0.00057, glen=105, tlen=265, kl=0.000858, act_lr=1.6e-7, ent=1.63]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.16it/s, pg=0.0531, ret=-0.00057, glen=105, tlen=265, kl=0.000858, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.16it/s, pg=0.115, ret=-0.0016, glen=111, tlen=271, kl=0.000896, act_lr=1.6e-7, ent=2.05]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=0.115, ret=-0.0016, glen=111, tlen=271, kl=0.000896, act_lr=1.6e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=0.043, ret=-0.000966, glen=102, tlen=262, kl=0.000891, act_lr=1.6e-7, ent=1.63]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.17it/s, pg=0.043, ret=-0.000966, glen=102, tlen=262, kl=0.000891, act_lr=1.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.17it/s, pg=0.0234, ret=-0.000414, glen=103, tlen=263, kl=0.000887, act_lr=1.6e-7, ent=1.7]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:17,  1.17it/s, pg=0.0234, ret=-0.000414, glen=103, tlen=263, kl=0.000887, act_lr=1.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:17,  1.17it/s, pg=-0.0479, ret=0.000816, glen=100, tlen=261, kl=0.000933, act_lr=1.6e-7, ent=1.58]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0479, ret=0.000816, glen=100, tlen=261, kl=0.000933, act_lr=1.6e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.0644, ret=0.000419, glen=93, tlen=252, kl=0.000908, act_lr=1.6e-7, ent=1.68] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0644, ret=0.000419, glen=93, tlen=252, kl=0.000908, act_lr=1.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.0186, ret=8.57e-5, glen=122, tlen=282, kl=0.00089, act_lr=1.6e-7, ent=1.98] Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.0186, ret=8.57e-5, glen=122, tlen=282, kl=0.00089, act_lr=1.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.153, ret=0.000542, glen=104, tlen=265, kl=0.00091, act_lr=1.6e-7, ent=1.67]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=-0.153, ret=0.000542, glen=104, tlen=265, kl=0.00091, act_lr=1.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.122, ret=-0.000714, glen=119, tlen=280, kl=0.000896, act_lr=1.6e-7, ent=1.8]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.122, ret=-0.000714, glen=119, tlen=280, kl=0.000896, act_lr=1.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0674, ret=-0.000847, glen=115, tlen=274, kl=0.000892, act_lr=1.6e-7, ent=1.89]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.0674, ret=-0.000847, glen=115, tlen=274, kl=0.000892, act_lr=1.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=0.261, ret=-0.00144, glen=146, tlen=305, kl=0.000857, act_lr=1.6e-7, ent=1.85]   Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.16it/s, pg=0.261, ret=-0.00144, glen=146, tlen=305, kl=0.000857, act_lr=1.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=-0.118, ret=0.00423, glen=157, tlen=317, kl=0.000823, act_lr=1.6e-7, ent=2.11]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.15it/s, pg=-0.118, ret=0.00423, glen=157, tlen=317, kl=0.000823, act_lr=1.6e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.15it/s, pg=-0.0436, ret=-0.00101, glen=105, tlen=265, kl=0.000894, act_lr=1.6e-7, ent=1.75]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.16it/s, pg=-0.0436, ret=-0.00101, glen=105, tlen=265, kl=0.000894, act_lr=1.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.16it/s, pg=-0.0214, ret=0.000468, glen=115, tlen=275, kl=0.000911, act_lr=1.6e-7, ent=1.72]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.0214, ret=0.000468, glen=115, tlen=275, kl=0.000911, act_lr=1.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0289, ret=-0.00127, glen=99.4, tlen=259, kl=0.000916, act_lr=1.6e-7, ent=1.74]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=0.0289, ret=-0.00127, glen=99.4, tlen=259, kl=0.000916, act_lr=1.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.0956, ret=0.00165, glen=125, tlen=285, kl=0.000819, act_lr=1.6e-7, ent=2.16] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.16it/s, pg=-0.0956, ret=0.00165, glen=125, tlen=285, kl=0.000819, act_lr=1.6e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.16it/s, pg=0.0614, ret=-0.000113, glen=129, tlen=290, kl=0.000883, act_lr=1.6e-7, ent=1.78]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.0614, ret=-0.000113, glen=129, tlen=290, kl=0.000883, act_lr=1.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=0.0431, ret=-0.00218, glen=96.6, tlen=256, kl=0.000918, act_lr=1.6e-7, ent=1.86]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.0431, ret=-0.00218, glen=96.6, tlen=256, kl=0.000918, act_lr=1.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.206, ret=0.00134, glen=105, tlen=265, kl=0.000896, act_lr=1.6e-7, ent=1.53]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.206, ret=0.00134, glen=105, tlen=265, kl=0.000896, act_lr=1.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.152, ret=-0.000577, glen=113, tlen=274, kl=0.000854, act_lr=1.6e-7, ent=1.61]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.152, ret=-0.000577, glen=113, tlen=274, kl=0.000854, act_lr=1.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.211, ret=0.00207, glen=98.4, tlen=259, kl=0.000925, act_lr=1.6e-7, ent=1.75] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.211, ret=0.00207, glen=98.4, tlen=259, kl=0.000925, act_lr=1.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.221, ret=0.00232, glen=96.2, tlen=256, kl=0.000907, act_lr=1.6e-7, ent=1.6] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=-0.221, ret=0.00232, glen=96.2, tlen=256, kl=0.000907, act_lr=1.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.104, ret=-0.000193, glen=104, tlen=264, kl=0.000938, act_lr=1.6e-7, ent=1.73]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.104, ret=-0.000193, glen=104, tlen=264, kl=0.000938, act_lr=1.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0266, ret=6.12e-5, glen=112, tlen=272, kl=0.00089, act_lr=1.6e-7, ent=1.74]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=0.0266, ret=6.12e-5, glen=112, tlen=272, kl=0.00089, act_lr=1.6e-7, ent=1.74]
2025-07-23 12:21:52.380 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.69s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=0.169, ret=-0.00317, glen=182, tlen=343, kl=0.000834, act_lr=1.8e-7, ent=2.27]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.169, ret=-0.00317, glen=182, tlen=343, kl=0.000834, act_lr=1.8e-7, ent=2.27]
2025-07-23 12:21:53.223 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 12:21:55.800 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-23 12:21:56.111 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.53s
2025-07-23 12:21:56.117 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.014968554178873697, 'actor_lr': 1.6035087624961881e-07, 'clip_ratio': 0.0, 'entropy': 1.7753764892879285, 'kl': 0.0008984448616964775, 'response_length': 114.8577502066629, 'total_length': 274.88451131184894, 'return': -2.9418908525258303e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [34:00<14:53, 223.38s/it]2025-07-23 12:21:56.149 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:23:53.844 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:23:54.032 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:23:54.032 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 117.88s
2025-07-23 12:23:56.142 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0122,avg_pass_at_n: 1.0000,avg_num_tokens: 109.4648,std_num_tokens: 138.9052,avg_correct_num_tokens: 91.3289,std_correct_num_tokens: 69.4536,avg_incorrect_num_tokens: 118.9133,std_incorrect_num_tokens: 163.0125
2025-07-23 12:23:56.585 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.55s
2025-07-23 12:23:58.189 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.60s
2025-07-23 12:24:27.180 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 225
2025-07-23 12:24:27.180 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.99s
2025-07-23 12:24:28.241 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 1.06s
2025-07-23 12:24:28.242 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0007542970517857207, avg_kl: 0.0009120602077907986, avg_response_length: 112.29234866672093, avg_orm_score: 0.0, avg_custom_rewards: -0.0007542970517857207
2025-07-23 12:24:28.308 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter9_replay_buffer.jsonl
2025-07-23 12:24:29.995 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.69s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=0.108, ret=-0.000737, glen=120, tlen=280, kl=0.000923, act_lr=1.8e-7, ent=1.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.108, ret=-0.000737, glen=120, tlen=280, kl=0.000923, act_lr=1.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:58,  1.04s/it, pg=0.0909, ret=-0.0011, glen=113, tlen=274, kl=0.000913, act_lr=1.8e-7, ent=1.63] Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:51,  1.07it/s, pg=0.0909, ret=-0.0011, glen=113, tlen=274, kl=0.000913, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:51,  1.07it/s, pg=0.035, ret=-0.00113, glen=105, tlen=266, kl=0.000935, act_lr=1.8e-7, ent=1.71]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=0.035, ret=-0.00113, glen=105, tlen=266, kl=0.000935, act_lr=1.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=-0.0783, ret=0.000415, glen=90.8, tlen=251, kl=0.000918, act_lr=1.8e-7, ent=1.73]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:47,  1.11it/s, pg=-0.0783, ret=0.000415, glen=90.8, tlen=251, kl=0.000918, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:47,  1.11it/s, pg=0.0927, ret=-0.00184, glen=111, tlen=271, kl=0.000908, act_lr=1.8e-7, ent=1.84]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:47,  1.09it/s, pg=0.0927, ret=-0.00184, glen=111, tlen=271, kl=0.000908, act_lr=1.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:47,  1.09it/s, pg=0.055, ret=-0.000665, glen=99.4, tlen=260, kl=0.000961, act_lr=1.8e-7, ent=1.68]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:46,  1.09it/s, pg=0.055, ret=-0.000665, glen=99.4, tlen=260, kl=0.000961, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:46,  1.09it/s, pg=-0.0698, ret=0.00137, glen=102, tlen=262, kl=0.000917, act_lr=1.8e-7, ent=1.78] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:45,  1.10it/s, pg=-0.0698, ret=0.00137, glen=102, tlen=262, kl=0.000917, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:45,  1.10it/s, pg=0.0522, ret=-0.000669, glen=106, tlen=266, kl=0.00092, act_lr=1.8e-7, ent=1.69]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.12it/s, pg=0.0522, ret=-0.000669, glen=106, tlen=266, kl=0.00092, act_lr=1.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.12it/s, pg=0.0448, ret=-0.000372, glen=106, tlen=266, kl=0.000924, act_lr=1.8e-7, ent=1.64]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=0.0448, ret=-0.000372, glen=106, tlen=266, kl=0.000924, act_lr=1.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=0.0199, ret=-0.000233, glen=112, tlen=273, kl=0.000925, act_lr=1.8e-7, ent=1.86]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=0.0199, ret=-0.000233, glen=112, tlen=273, kl=0.000925, act_lr=1.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.22, ret=-0.00269, glen=112, tlen=272, kl=0.000914, act_lr=1.8e-7, ent=1.81]   Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.13it/s, pg=0.22, ret=-0.00269, glen=112, tlen=272, kl=0.000914, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.13it/s, pg=0.0946, ret=-0.000678, glen=113, tlen=273, kl=0.000921, act_lr=1.8e-7, ent=1.87]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=0.0946, ret=-0.000678, glen=113, tlen=273, kl=0.000921, act_lr=1.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=0.013, ret=-0.000298, glen=95.5, tlen=256, kl=0.000919, act_lr=1.8e-7, ent=1.68]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=0.013, ret=-0.000298, glen=95.5, tlen=256, kl=0.000919, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=0.177, ret=0.00221, glen=209, tlen=371, kl=0.000802, act_lr=1.8e-7, ent=2.53]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=0.177, ret=0.00221, glen=209, tlen=371, kl=0.000802, act_lr=1.8e-7, ent=2.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=0.15, ret=0.000358, glen=161, tlen=322, kl=0.000835, act_lr=1.8e-7, ent=1.55]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.14it/s, pg=0.15, ret=0.000358, glen=161, tlen=322, kl=0.000835, act_lr=1.8e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.14it/s, pg=-0.047, ret=-0.000403, glen=104, tlen=264, kl=0.000932, act_lr=1.8e-7, ent=1.7]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.15it/s, pg=-0.047, ret=-0.000403, glen=104, tlen=264, kl=0.000932, act_lr=1.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:15<00:35,  1.15it/s, pg=0.0253, ret=-0.0014, glen=104, tlen=264, kl=0.000905, act_lr=1.8e-7, ent=1.67] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:35,  1.13it/s, pg=0.0253, ret=-0.0014, glen=104, tlen=264, kl=0.000905, act_lr=1.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:16<00:35,  1.13it/s, pg=-0.113, ret=-0.000361, glen=111, tlen=271, kl=0.000906, act_lr=1.8e-7, ent=1.66]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:34,  1.13it/s, pg=-0.113, ret=-0.000361, glen=111, tlen=271, kl=0.000906, act_lr=1.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:34,  1.13it/s, pg=-0.141, ret=0.00203, glen=115, tlen=275, kl=0.000915, act_lr=1.8e-7, ent=1.85]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:33,  1.14it/s, pg=-0.141, ret=0.00203, glen=115, tlen=275, kl=0.000915, act_lr=1.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:33,  1.14it/s, pg=0.0621, ret=-0.000484, glen=114, tlen=274, kl=0.00094, act_lr=1.8e-7, ent=1.72]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:32,  1.15it/s, pg=0.0621, ret=-0.000484, glen=114, tlen=274, kl=0.00094, act_lr=1.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:32,  1.15it/s, pg=0.0859, ret=-0.00144, glen=109, tlen=270, kl=0.000869, act_lr=1.8e-7, ent=1.56]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:31,  1.16it/s, pg=0.0859, ret=-0.00144, glen=109, tlen=270, kl=0.000869, act_lr=1.8e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:31,  1.16it/s, pg=-0.00462, ret=0.000788, glen=106, tlen=266, kl=0.00092, act_lr=1.8e-7, ent=1.68]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=-0.00462, ret=0.000788, glen=106, tlen=266, kl=0.00092, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.16it/s, pg=0.00757, ret=-0.000744, glen=113, tlen=273, kl=0.000937, act_lr=1.8e-7, ent=1.73]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.00757, ret=-0.000744, glen=113, tlen=273, kl=0.000937, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:21<00:29,  1.17it/s, pg=-0.051, ret=0.000509, glen=117, tlen=277, kl=0.000941, act_lr=1.8e-7, ent=1.73]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.051, ret=0.000509, glen=117, tlen=277, kl=0.000941, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:22<00:28,  1.17it/s, pg=-0.0179, ret=-0.000316, glen=109, tlen=269, kl=0.00092, act_lr=1.8e-7, ent=1.63]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:28,  1.14it/s, pg=-0.0179, ret=-0.000316, glen=109, tlen=269, kl=0.00092, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:28,  1.14it/s, pg=0.129, ret=0.000696, glen=123, tlen=283, kl=0.000871, act_lr=1.8e-7, ent=1.99]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.15it/s, pg=0.129, ret=0.000696, glen=123, tlen=283, kl=0.000871, act_lr=1.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:24<00:26,  1.15it/s, pg=-0.118, ret=0.00097, glen=98.6, tlen=259, kl=0.000895, act_lr=1.8e-7, ent=1.54]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.05it/s, pg=-0.118, ret=0.00097, glen=98.6, tlen=259, kl=0.000895, act_lr=1.8e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.05it/s, pg=-0.116, ret=0.00228, glen=109, tlen=269, kl=0.000915, act_lr=1.8e-7, ent=1.89] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.09it/s, pg=-0.116, ret=0.00228, glen=109, tlen=269, kl=0.000915, act_lr=1.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.09it/s, pg=-0.251, ret=0.00283, glen=99.9, tlen=260, kl=0.000943, act_lr=1.8e-7, ent=1.6]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:25,  1.11it/s, pg=-0.251, ret=0.00283, glen=99.9, tlen=260, kl=0.000943, act_lr=1.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.11it/s, pg=-0.0137, ret=-0.000257, glen=91.5, tlen=252, kl=0.000924, act_lr=1.8e-7, ent=1.58]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:24,  1.11it/s, pg=-0.0137, ret=-0.000257, glen=91.5, tlen=252, kl=0.000924, act_lr=1.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:24,  1.11it/s, pg=-0.13, ret=0.000325, glen=96.7, tlen=257, kl=0.000966, act_lr=1.8e-7, ent=1.7]    Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:23,  1.12it/s, pg=-0.13, ret=0.000325, glen=96.7, tlen=257, kl=0.000966, act_lr=1.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:23,  1.12it/s, pg=0.131, ret=-0.00162, glen=119, tlen=279, kl=0.000905, act_lr=1.8e-7, ent=1.81]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.14it/s, pg=0.131, ret=-0.00162, glen=119, tlen=279, kl=0.000905, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:29<00:21,  1.14it/s, pg=0.0386, ret=-8.3e-5, glen=102, tlen=262, kl=0.00091, act_lr=1.8e-7, ent=1.78] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.15it/s, pg=0.0386, ret=-8.3e-5, glen=102, tlen=262, kl=0.00091, act_lr=1.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:30<00:20,  1.15it/s, pg=-0.237, ret=0.00143, glen=102, tlen=261, kl=0.000912, act_lr=1.8e-7, ent=1.66]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=-0.237, ret=0.00143, glen=102, tlen=261, kl=0.000912, act_lr=1.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=0.0464, ret=-0.000788, glen=98.2, tlen=259, kl=0.000912, act_lr=1.8e-7, ent=1.56]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.0464, ret=-0.000788, glen=98.2, tlen=259, kl=0.000912, act_lr=1.8e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.0913, ret=-0.000451, glen=132, tlen=293, kl=0.000875, act_lr=1.8e-7, ent=1.98] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=0.0913, ret=-0.000451, glen=132, tlen=293, kl=0.000875, act_lr=1.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.0861, ret=-0.000336, glen=113, tlen=273, kl=0.000906, act_lr=1.8e-7, ent=1.7]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0861, ret=-0.000336, glen=113, tlen=273, kl=0.000906, act_lr=1.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=0.0175, ret=0.00026, glen=111, tlen=271, kl=0.000916, act_lr=1.8e-7, ent=1.71]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.0175, ret=0.00026, glen=111, tlen=271, kl=0.000916, act_lr=1.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=-0.214, ret=0.00262, glen=108, tlen=268, kl=0.000934, act_lr=1.8e-7, ent=1.65]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=-0.214, ret=0.00262, glen=108, tlen=268, kl=0.000934, act_lr=1.8e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:35<00:15,  1.17it/s, pg=-0.166, ret=0.000836, glen=120, tlen=280, kl=0.000901, act_lr=1.8e-7, ent=1.62]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.166, ret=0.000836, glen=120, tlen=280, kl=0.000901, act_lr=1.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:36<00:14,  1.17it/s, pg=-0.0505, ret=-0.00171, glen=108, tlen=268, kl=0.000956, act_lr=1.8e-7, ent=1.81]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0505, ret=-0.00171, glen=108, tlen=268, kl=0.000956, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0305, ret=0.000564, glen=109, tlen=269, kl=0.000911, act_lr=1.8e-7, ent=1.79]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.18it/s, pg=-0.0305, ret=0.000564, glen=109, tlen=269, kl=0.000911, act_lr=1.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.18it/s, pg=-0.144, ret=0.000712, glen=113, tlen=273, kl=0.000919, act_lr=1.8e-7, ent=1.74] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.18it/s, pg=-0.144, ret=0.000712, glen=113, tlen=273, kl=0.000919, act_lr=1.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.18it/s, pg=-0.0184, ret=-0.000528, glen=110, tlen=271, kl=0.000912, act_lr=1.8e-7, ent=1.87]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.0184, ret=-0.000528, glen=110, tlen=271, kl=0.000912, act_lr=1.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.0788, ret=0.000574, glen=99.3, tlen=259, kl=0.000926, act_lr=1.8e-7, ent=1.62]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.18it/s, pg=-0.0788, ret=0.000574, glen=99.3, tlen=259, kl=0.000926, act_lr=1.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.18it/s, pg=0.0149, ret=-0.000512, glen=129, tlen=289, kl=0.000942, act_lr=1.8e-7, ent=1.76] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.0149, ret=-0.000512, glen=129, tlen=289, kl=0.000942, act_lr=1.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:41<00:09,  1.17it/s, pg=-0.133, ret=0.000174, glen=99.2, tlen=260, kl=0.000938, act_lr=1.8e-7, ent=1.72]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=-0.133, ret=0.000174, glen=99.2, tlen=260, kl=0.000938, act_lr=1.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=-0.0288, ret=0.000553, glen=114, tlen=274, kl=0.000866, act_lr=1.8e-7, ent=1.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=-0.0288, ret=0.000553, glen=114, tlen=274, kl=0.000866, act_lr=1.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=0.00122, ret=-0.000106, glen=109, tlen=270, kl=0.00091, act_lr=1.8e-7, ent=1.98]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.00122, ret=-0.000106, glen=109, tlen=270, kl=0.00091, act_lr=1.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.0212, ret=9.73e-5, glen=120, tlen=280, kl=0.000898, act_lr=1.8e-7, ent=1.73] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.0212, ret=9.73e-5, glen=120, tlen=280, kl=0.000898, act_lr=1.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=0.0571, ret=-0.000548, glen=105, tlen=266, kl=0.000937, act_lr=1.8e-7, ent=1.68]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.0571, ret=-0.000548, glen=105, tlen=266, kl=0.000937, act_lr=1.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.0144, ret=-0.000359, glen=105, tlen=265, kl=0.000904, act_lr=1.8e-7, ent=1.59]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.18it/s, pg=-0.0144, ret=-0.000359, glen=105, tlen=265, kl=0.000904, act_lr=1.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.18it/s, pg=0.0612, ret=-0.00126, glen=108, tlen=268, kl=0.000896, act_lr=1.8e-7, ent=1.58]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=0.0612, ret=-0.00126, glen=108, tlen=268, kl=0.000896, act_lr=1.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:47<00:03,  1.18it/s, pg=-0.0563, ret=0.000129, glen=99.9, tlen=260, kl=0.000938, act_lr=1.8e-7, ent=1.61]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=-0.0563, ret=0.000129, glen=99.9, tlen=260, kl=0.000938, act_lr=1.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=0.05, ret=-0.000348, glen=120, tlen=280, kl=0.00092, act_lr=1.8e-7, ent=1.81]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=0.05, ret=-0.000348, glen=120, tlen=280, kl=0.00092, act_lr=1.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:49<00:01,  1.17it/s, pg=0.164, ret=-0.000624, glen=154, tlen=314, kl=0.000808, act_lr=1.8e-7, ent=2.32]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.06it/s, pg=0.164, ret=-0.000624, glen=154, tlen=314, kl=0.000808, act_lr=1.8e-7, ent=2.32]
2025-07-23 12:25:20.274 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 50.09s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:50<00:00,  1.06it/s, pg=0.0234, ret=0.000301, glen=125, tlen=285, kl=0.000881, act_lr=2e-7, ent=1.89]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:50<00:00,  1.12it/s, pg=0.0234, ret=0.000301, glen=125, tlen=285, kl=0.000881, act_lr=2e-7, ent=1.89]
2025-07-23 12:25:21.116 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 12:25:23.699 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-23 12:25:24.006 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.94s
2025-07-23 12:25:24.019 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.004758148862604509, 'actor_lr': 1.8035087790442623e-07, 'clip_ratio': 0.0, 'entropy': 1.7501339431394611, 'kl': 0.000911737743176912, 'response_length': 112.43038860120271, 'total_length': 272.71384363007127, 'return': -3.584998385600844e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [37:27<10:55, 218.60s/it]2025-07-23 12:25:24.025 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:   1%|          | 1/172 [00:00<00:44,  3.84it/s, est. speed input: 703.31 toks/s, output: 19.22 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:04<00:01, 13.19it/s, est. speed input: 6070.02 toks/s, output: 2490.03 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:01, 15.93it/s, est. speed input: 5481.20 toks/s, output: 2697.29 toks/s][32m [repeated 118x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00,  3.98it/s, est. speed input: 3340.49 toks/s, output: 1845.36 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:09<00:00, 18.43it/s, est. speed input: 3340.49 toks/s, output: 1845.36 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:10<00:00,  2.16it/s, est. speed input: 3073.17 toks/s, output: 1891.95 toks/s][32m [repeated 33x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:05<00:01, 13.75it/s, est. speed input: 5325.36 toks/s, output: 2543.30 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00,  1.09s/it, est. speed input: 2145.47 toks/s, output: 1233.30 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00, 11.81it/s, est. speed input: 2145.47 toks/s, output: 1233.30 toks/s][32m [repeated 2x across cluster][0m
2025-07-23 12:25:40.495 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 501.8501,strategyqa_test/accuracy: 0.2838,eval_accuracy: 0.2838
2025-07-23 12:25:40.765 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:28:13.938 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:28:14.117 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:28:14.117 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 153.35s
2025-07-23 12:28:16.083 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0004,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0111,avg_pass_at_n: 1.0000,avg_num_tokens: 114.3750,std_num_tokens: 196.3909,avg_correct_num_tokens: 94.0475,std_correct_num_tokens: 76.1266,avg_incorrect_num_tokens: 124.6752,std_incorrect_num_tokens: 234.2243
2025-07-23 12:28:16.526 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.41s
2025-07-23 12:28:18.146 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.62s
2025-07-23 12:28:47.670 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 229
2025-07-23 12:28:47.670 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.52s
2025-07-23 12:28:48.548 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.87s
2025-07-23 12:28:48.548 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0003422054392997354, avg_kl: 0.0008931805473227688, avg_response_length: 125.65933823897849, avg_orm_score: 0.0, avg_custom_rewards: 0.0003422054392997354
2025-07-23 12:28:48.587 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter10_replay_buffer.jsonl
2025-07-23 12:28:50.273 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.69s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:00<?, ?it/s]
[36m(LLMActor pid=884796)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:13<00:01,  1.19s/it, est. speed input: 2234.95 toks/s, output: 1225.00 toks/s][32m [repeated 2x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00,  1.38s/it, est. speed input: 2097.45 toks/s, output: 1381.81 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:14<00:00, 11.58it/s, est. speed input: 2097.45 toks/s, output: 1381.81 toks/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/58 [00:01<?, ?it/s, pg=-0.0975, ret=-2.5e-5, glen=94.7, tlen=255, kl=0.00092, act_lr=2e-7, ent=1.7]Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:57,  1.00s/it, pg=-0.0975, ret=-2.5e-5, glen=94.7, tlen=255, kl=0.00092, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/58 [00:01<00:57,  1.00s/it, pg=-0.167, ret=0.000894, glen=95.1, tlen=255, kl=0.000902, act_lr=2e-7, ent=1.69]Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:01<00:51,  1.09it/s, pg=-0.167, ret=0.000894, glen=95.1, tlen=255, kl=0.000902, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 2/58 [00:02<00:51,  1.09it/s, pg=-0.119, ret=-0.000241, glen=118, tlen=278, kl=0.000841, act_lr=2e-7, ent=1.93]Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:02<00:49,  1.12it/s, pg=-0.119, ret=-0.000241, glen=118, tlen=278, kl=0.000841, act_lr=2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/58 [00:03<00:49,  1.12it/s, pg=-0.0877, ret=-0.000589, glen=114, tlen=274, kl=0.000957, act_lr=2e-7, ent=1.86]Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:03<00:47,  1.14it/s, pg=-0.0877, ret=-0.000589, glen=114, tlen=274, kl=0.000957, act_lr=2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/58 [00:04<00:47,  1.14it/s, pg=-0.0804, ret=-0.000115, glen=98.9, tlen=259, kl=0.000942, act_lr=2e-7, ent=1.74]Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:04<00:46,  1.15it/s, pg=-0.0804, ret=-0.000115, glen=98.9, tlen=259, kl=0.000942, act_lr=2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñä         | 5/58 [00:05<00:46,  1.15it/s, pg=-0.225, ret=0.00136, glen=108, tlen=269, kl=0.000903, act_lr=2e-7, ent=1.74]    Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:05<00:46,  1.13it/s, pg=-0.225, ret=0.00136, glen=108, tlen=269, kl=0.000903, act_lr=2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 6/58 [00:06<00:46,  1.13it/s, pg=0.0691, ret=0.000472, glen=125, tlen=286, kl=0.000938, act_lr=2e-7, ent=1.94]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:06<00:44,  1.13it/s, pg=0.0691, ret=0.000472, glen=125, tlen=286, kl=0.000938, act_lr=2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/58 [00:07<00:44,  1.13it/s, pg=-0.117, ret=0.000265, glen=105, tlen=266, kl=0.000906, act_lr=2e-7, ent=1.7] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.15it/s, pg=-0.117, ret=0.000265, glen=105, tlen=266, kl=0.000906, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/58 [00:07<00:43,  1.15it/s, pg=-0.0873, ret=-1.78e-5, glen=105, tlen=266, kl=0.000899, act_lr=2e-7, ent=1.61]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:07<00:42,  1.16it/s, pg=-0.0873, ret=-1.78e-5, glen=105, tlen=266, kl=0.000899, act_lr=2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/58 [00:08<00:42,  1.16it/s, pg=0.0453, ret=0.000315, glen=102, tlen=262, kl=0.000885, act_lr=2e-7, ent=1.64] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:08<00:41,  1.16it/s, pg=0.0453, ret=0.000315, glen=102, tlen=262, kl=0.000885, act_lr=2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 10/58 [00:09<00:41,  1.16it/s, pg=0.0825, ret=-0.0015, glen=135, tlen=296, kl=0.000896, act_lr=2e-7, ent=2.09] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:09<00:41,  1.14it/s, pg=0.0825, ret=-0.0015, glen=135, tlen=296, kl=0.000896, act_lr=2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/58 [00:10<00:41,  1.14it/s, pg=-0.13, ret=0.000412, glen=107, tlen=267, kl=0.00091, act_lr=2e-7, ent=1.65] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:10<00:40,  1.15it/s, pg=-0.13, ret=0.000412, glen=107, tlen=267, kl=0.00091, act_lr=2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/58 [00:11<00:40,  1.15it/s, pg=-0.0952, ret=0.00199, glen=127, tlen=288, kl=0.000943, act_lr=2e-7, ent=1.96]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:11<00:38,  1.16it/s, pg=-0.0952, ret=0.00199, glen=127, tlen=288, kl=0.000943, act_lr=2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 13/58 [00:12<00:38,  1.16it/s, pg=-0.0405, ret=-0.000226, glen=115, tlen=275, kl=0.000872, act_lr=2e-7, ent=1.98]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:12<00:37,  1.16it/s, pg=-0.0405, ret=-0.000226, glen=115, tlen=275, kl=0.000872, act_lr=2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 14/58 [00:13<00:37,  1.16it/s, pg=0.0498, ret=9.77e-5, glen=143, tlen=304, kl=0.000885, act_lr=2e-7, ent=1.77]   Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.16it/s, pg=0.0498, ret=9.77e-5, glen=143, tlen=304, kl=0.000885, act_lr=2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 15/58 [00:13<00:36,  1.16it/s, pg=-0.032, ret=0.000606, glen=137, tlen=298, kl=0.000792, act_lr=2e-7, ent=1.49]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:13<00:36,  1.16it/s, pg=-0.032, ret=0.000606, glen=137, tlen=298, kl=0.000792, act_lr=2e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/58 [00:14<00:36,  1.16it/s, pg=0.197, ret=-0.000811, glen=120, tlen=280, kl=0.000854, act_lr=2e-7, ent=2.04]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:14<00:35,  1.16it/s, pg=0.197, ret=-0.000811, glen=120, tlen=280, kl=0.000854, act_lr=2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 17/58 [00:15<00:35,  1.16it/s, pg=0.0128, ret=5e-5, glen=137, tlen=298, kl=0.000782, act_lr=2e-7, ent=1.54]    Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:15<00:34,  1.16it/s, pg=0.0128, ret=5e-5, glen=137, tlen=298, kl=0.000782, act_lr=2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 18/58 [00:16<00:34,  1.16it/s, pg=0.314, ret=0.00501, glen=345, tlen=506, kl=0.000742, act_lr=2e-7, ent=2.93]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:16<00:34,  1.13it/s, pg=0.314, ret=0.00501, glen=345, tlen=506, kl=0.000742, act_lr=2e-7, ent=2.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/58 [00:17<00:34,  1.13it/s, pg=0.0515, ret=-0.00238, glen=103, tlen=263, kl=0.000911, act_lr=2e-7, ent=1.72]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:17<00:33,  1.15it/s, pg=0.0515, ret=-0.00238, glen=103, tlen=263, kl=0.000911, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 20/58 [00:18<00:33,  1.15it/s, pg=-0.186, ret=0.00134, glen=106, tlen=266, kl=0.000941, act_lr=2e-7, ent=1.79] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:18<00:32,  1.15it/s, pg=-0.186, ret=0.00134, glen=106, tlen=266, kl=0.000941, act_lr=2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 21/58 [00:19<00:32,  1.15it/s, pg=-0.0494, ret=-0.000131, glen=105, tlen=266, kl=0.000857, act_lr=2e-7, ent=1.64]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:19<00:31,  1.16it/s, pg=-0.0494, ret=-0.000131, glen=105, tlen=266, kl=0.000857, act_lr=2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 22/58 [00:20<00:31,  1.16it/s, pg=-0.062, ret=0.000865, glen=121, tlen=282, kl=0.000891, act_lr=2e-7, ent=2.06]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=-0.062, ret=0.000865, glen=121, tlen=282, kl=0.000891, act_lr=2e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 23/58 [00:20<00:29,  1.17it/s, pg=0.0483, ret=-0.00126, glen=118, tlen=279, kl=0.000883, act_lr=2e-7, ent=1.7] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:20<00:29,  1.17it/s, pg=0.0483, ret=-0.00126, glen=118, tlen=279, kl=0.000883, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/58 [00:21<00:29,  1.17it/s, pg=0.0882, ret=-0.00141, glen=96.3, tlen=257, kl=0.00092, act_lr=2e-7, ent=1.75]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:21<00:28,  1.15it/s, pg=0.0882, ret=-0.00141, glen=96.3, tlen=257, kl=0.00092, act_lr=2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 25/58 [00:22<00:28,  1.15it/s, pg=-0.0688, ret=-0.000415, glen=123, tlen=284, kl=0.000848, act_lr=2e-7, ent=1.84]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:22<00:27,  1.15it/s, pg=-0.0688, ret=-0.000415, glen=123, tlen=284, kl=0.000848, act_lr=2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 26/58 [00:23<00:27,  1.15it/s, pg=0.0311, ret=0.00136, glen=124, tlen=285, kl=0.000955, act_lr=2e-7, ent=2]      Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:23<00:29,  1.06it/s, pg=0.0311, ret=0.00136, glen=124, tlen=285, kl=0.000955, act_lr=2e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/58 [00:24<00:29,  1.06it/s, pg=0.1, ret=7.42e-5, glen=130, tlen=290, kl=0.000911, act_lr=2e-7, ent=1.72]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:24<00:27,  1.10it/s, pg=0.1, ret=7.42e-5, glen=130, tlen=290, kl=0.000911, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 28/58 [00:25<00:27,  1.10it/s, pg=-0.0728, ret=-0.00105, glen=123, tlen=284, kl=0.000899, act_lr=2e-7, ent=1.78]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:25<00:25,  1.12it/s, pg=-0.0728, ret=-0.00105, glen=123, tlen=284, kl=0.000899, act_lr=2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/58 [00:26<00:25,  1.12it/s, pg=-0.117, ret=0.000634, glen=119, tlen=279, kl=0.000981, act_lr=2e-7, ent=1.87] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:26<00:24,  1.13it/s, pg=-0.117, ret=0.000634, glen=119, tlen=279, kl=0.000981, act_lr=2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 30/58 [00:27<00:24,  1.13it/s, pg=-0.0786, ret=0.000638, glen=102, tlen=263, kl=0.000883, act_lr=2e-7, ent=1.59]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:27<00:23,  1.15it/s, pg=-0.0786, ret=0.000638, glen=102, tlen=263, kl=0.000883, act_lr=2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 31/58 [00:28<00:23,  1.15it/s, pg=0.134, ret=-0.0013, glen=122, tlen=283, kl=0.000878, act_lr=2e-7, ent=2.03]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.15it/s, pg=0.134, ret=-0.0013, glen=122, tlen=283, kl=0.000878, act_lr=2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/58 [00:28<00:22,  1.15it/s, pg=-0.0637, ret=0.000307, glen=104, tlen=265, kl=0.000927, act_lr=2e-7, ent=1.66]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:28<00:21,  1.16it/s, pg=-0.0637, ret=0.000307, glen=104, tlen=265, kl=0.000927, act_lr=2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 33/58 [00:29<00:21,  1.16it/s, pg=-0.134, ret=0.00101, glen=107, tlen=267, kl=0.000928, act_lr=2e-7, ent=1.72]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:29<00:20,  1.17it/s, pg=-0.134, ret=0.00101, glen=107, tlen=267, kl=0.000928, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 34/58 [00:30<00:20,  1.17it/s, pg=0.288, ret=-0.000932, glen=142, tlen=303, kl=0.000821, act_lr=2e-7, ent=2.37]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:30<00:19,  1.16it/s, pg=0.288, ret=-0.000932, glen=142, tlen=303, kl=0.000821, act_lr=2e-7, ent=2.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 35/58 [00:31<00:19,  1.16it/s, pg=0.0572, ret=-0.000914, glen=109, tlen=270, kl=0.000879, act_lr=2e-7, ent=1.96]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:31<00:18,  1.17it/s, pg=0.0572, ret=-0.000914, glen=109, tlen=270, kl=0.000879, act_lr=2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 36/58 [00:32<00:18,  1.17it/s, pg=-0.0153, ret=-0.000363, glen=112, tlen=273, kl=0.000896, act_lr=2e-7, ent=1.87]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:32<00:17,  1.17it/s, pg=-0.0153, ret=-0.000363, glen=112, tlen=273, kl=0.000896, act_lr=2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/58 [00:33<00:17,  1.17it/s, pg=0.142, ret=-0.0024, glen=106, tlen=267, kl=0.00095, act_lr=2e-7, ent=1.65]     Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.142, ret=-0.0024, glen=106, tlen=267, kl=0.00095, act_lr=2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 38/58 [00:33<00:17,  1.17it/s, pg=0.0178, ret=-0.0008, glen=99.6, tlen=260, kl=0.000918, act_lr=2e-7, ent=1.63]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:33<00:16,  1.17it/s, pg=0.0178, ret=-0.0008, glen=99.6, tlen=260, kl=0.000918, act_lr=2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 39/58 [00:34<00:16,  1.17it/s, pg=0.112, ret=-0.000552, glen=119, tlen=280, kl=0.000887, act_lr=2e-7, ent=1.99]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:34<00:15,  1.17it/s, pg=0.112, ret=-0.000552, glen=119, tlen=280, kl=0.000887, act_lr=2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 40/58 [00:35<00:15,  1.17it/s, pg=0.0178, ret=0.000375, glen=113, tlen=274, kl=0.000912, act_lr=2e-7, ent=1.66]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:35<00:14,  1.17it/s, pg=0.0178, ret=0.000375, glen=113, tlen=274, kl=0.000912, act_lr=2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 41/58 [00:36<00:14,  1.17it/s, pg=-0.0417, ret=-0.000337, glen=125, tlen=285, kl=0.000902, act_lr=2e-7, ent=1.76]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:36<00:13,  1.17it/s, pg=-0.0417, ret=-0.000337, glen=125, tlen=285, kl=0.000902, act_lr=2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 42/58 [00:37<00:13,  1.17it/s, pg=-0.131, ret=-9.45e-5, glen=112, tlen=272, kl=0.000936, act_lr=2e-7, ent=1.76]  Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:37<00:13,  1.15it/s, pg=-0.131, ret=-9.45e-5, glen=112, tlen=272, kl=0.000936, act_lr=2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 43/58 [00:38<00:13,  1.15it/s, pg=-0.156, ret=0.00122, glen=101, tlen=262, kl=0.000892, act_lr=2e-7, ent=1.72] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:38<00:12,  1.15it/s, pg=-0.156, ret=0.00122, glen=101, tlen=262, kl=0.000892, act_lr=2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 44/58 [00:39<00:12,  1.15it/s, pg=0.00836, ret=8.27e-5, glen=122, tlen=283, kl=0.000915, act_lr=2e-7, ent=1.8]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:39<00:11,  1.16it/s, pg=0.00836, ret=8.27e-5, glen=122, tlen=283, kl=0.000915, act_lr=2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 45/58 [00:40<00:11,  1.16it/s, pg=0.157, ret=-0.000953, glen=112, tlen=273, kl=0.000941, act_lr=2e-7, ent=1.77]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.16it/s, pg=0.157, ret=-0.000953, glen=112, tlen=273, kl=0.000941, act_lr=2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 46/58 [00:40<00:10,  1.16it/s, pg=-0.144, ret=0.000135, glen=103, tlen=263, kl=0.000908, act_lr=2e-7, ent=1.7] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:40<00:09,  1.17it/s, pg=-0.144, ret=0.000135, glen=103, tlen=263, kl=0.000908, act_lr=2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 47/58 [00:41<00:09,  1.17it/s, pg=0.255, ret=-0.00451, glen=321, tlen=482, kl=0.000762, act_lr=2e-7, ent=2.49]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:41<00:08,  1.14it/s, pg=0.255, ret=-0.00451, glen=321, tlen=482, kl=0.000762, act_lr=2e-7, ent=2.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 48/58 [00:42<00:08,  1.14it/s, pg=0.00171, ret=-0.00108, glen=113, tlen=274, kl=0.000875, act_lr=2e-7, ent=1.65]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:42<00:07,  1.15it/s, pg=0.00171, ret=-0.00108, glen=113, tlen=274, kl=0.000875, act_lr=2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 49/58 [00:43<00:07,  1.15it/s, pg=-0.189, ret=0.000832, glen=110, tlen=271, kl=0.000843, act_lr=2e-7, ent=1.75] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:43<00:06,  1.16it/s, pg=-0.189, ret=0.000832, glen=110, tlen=271, kl=0.000843, act_lr=2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 50/58 [00:44<00:06,  1.16it/s, pg=0.105, ret=0.00498, glen=342, tlen=503, kl=0.000782, act_lr=2e-7, ent=2.93]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:44<00:06,  1.13it/s, pg=0.105, ret=0.00498, glen=342, tlen=503, kl=0.000782, act_lr=2e-7, ent=2.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 51/58 [00:45<00:06,  1.13it/s, pg=-0.0138, ret=0.000722, glen=109, tlen=269, kl=0.000916, act_lr=2e-7, ent=1.86]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:45<00:05,  1.14it/s, pg=-0.0138, ret=0.000722, glen=109, tlen=269, kl=0.000916, act_lr=2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 52/58 [00:46<00:05,  1.14it/s, pg=0.0625, ret=-0.00182, glen=102, tlen=263, kl=0.000937, act_lr=2e-7, ent=1.69] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.15it/s, pg=0.0625, ret=-0.00182, glen=102, tlen=263, kl=0.000937, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/58 [00:46<00:04,  1.15it/s, pg=-0.101, ret=0.000337, glen=112, tlen=273, kl=0.000891, act_lr=2e-7, ent=1.81]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:46<00:03,  1.16it/s, pg=-0.101, ret=0.000337, glen=112, tlen=273, kl=0.000891, act_lr=2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 54/58 [00:47<00:03,  1.16it/s, pg=-0.0834, ret=0.00233, glen=124, tlen=284, kl=0.000892, act_lr=2e-7, ent=2.01]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:47<00:02,  1.16it/s, pg=-0.0834, ret=0.00233, glen=124, tlen=284, kl=0.000892, act_lr=2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/58 [00:48<00:02,  1.16it/s, pg=-0.294, ret=0.00196, glen=106, tlen=266, kl=0.000885, act_lr=2e-7, ent=1.69] Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:48<00:01,  1.07it/s, pg=-0.294, ret=0.00196, glen=106, tlen=266, kl=0.000885, act_lr=2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 56/58 [00:49<00:01,  1.07it/s, pg=-0.114, ret=0.000898, glen=98.5, tlen=259, kl=0.000974, act_lr=2e-7, ent=1.63]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:49<00:00,  1.10it/s, pg=-0.114, ret=0.000898, glen=98.5, tlen=259, kl=0.000974, act_lr=2e-7, ent=1.63]
2025-07-23 12:29:41.204 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 50.76s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.10it/s, pg=-0.0718, ret=0.000684, glen=119, tlen=280, kl=0.000915, act_lr=2.2e-7, ent=1.75]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 57/58 [00:50<00:00,  1.12it/s, pg=-0.0718, ret=0.000684, glen=119, tlen=280, kl=0.000915, act_lr=2.2e-7, ent=1.75]
2025-07-23 12:29:41.906 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.63s
2025-07-23 12:29:44.201 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-23 12:29:44.508 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 54.18s
2025-07-23 12:29:44.514 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.017583962144522833, 'actor_lr': 2.0034482995195754e-07, 'clip_ratio': 0.0, 'entropy': 1.8412527421425129, 'kl': 0.0008932886452510439, 'response_length': 125.28243847551018, 'total_length': 285.96053498366786, 'return': 0.00010418919868996078, 'policy_update_steps': 1.0}
Episode [1/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [41:48<07:42, 231.42s/it]2025-07-23 12:29:44.547 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:32:19.326 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:32:19.514 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:32:19.515 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 154.97s
2025-07-23 12:32:21.523 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0122,avg_pass_at_n: 1.0000,avg_num_tokens: 113.7134,std_num_tokens: 157.9692,avg_correct_num_tokens: 95.9693,std_correct_num_tokens: 84.5275,avg_incorrect_num_tokens: 122.9177,std_incorrect_num_tokens: 184.2400
2025-07-23 12:32:21.972 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.46s
2025-07-23 12:32:23.720 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.74s
2025-07-23 12:32:52.649 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 228
2025-07-23 12:32:52.649 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.93s
2025-07-23 12:32:53.486 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.83s
2025-07-23 12:32:53.487 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0004731604026266978, avg_kl: 0.0009302193658393726, avg_response_length: 118.88012939586974, avg_orm_score: 0.0, avg_custom_rewards: -0.0004731604026266978
2025-07-23 12:32:53.519 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter11_replay_buffer.jsonl
2025-07-23 12:32:55.191 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.67s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s, pg=-0.111, ret=0.00162, glen=110, tlen=271, kl=0.000882, act_lr=2.2e-7, ent=1.71]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:00<00:54,  1.03it/s, pg=-0.111, ret=0.00162, glen=110, tlen=271, kl=0.000882, act_lr=2.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:54,  1.03it/s, pg=-0.177, ret=0.000455, glen=99.1, tlen=260, kl=0.000929, act_lr=2.2e-7, ent=1.63]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:49,  1.10it/s, pg=-0.177, ret=0.000455, glen=99.1, tlen=260, kl=0.000929, act_lr=2.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:49,  1.10it/s, pg=0.155, ret=-0.00124, glen=99.7, tlen=260, kl=0.000957, act_lr=2.2e-7, ent=1.74] Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:47,  1.14it/s, pg=0.155, ret=-0.00124, glen=99.7, tlen=260, kl=0.000957, act_lr=2.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:47,  1.14it/s, pg=0.0181, ret=-0.000144, glen=118, tlen=278, kl=0.000962, act_lr=2.2e-7, ent=1.72]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.15it/s, pg=0.0181, ret=-0.000144, glen=118, tlen=278, kl=0.000962, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.15it/s, pg=-0.0962, ret=0.000371, glen=99.2, tlen=260, kl=0.000954, act_lr=2.2e-7, ent=1.73]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:44,  1.16it/s, pg=-0.0962, ret=0.000371, glen=99.2, tlen=260, kl=0.000954, act_lr=2.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:44,  1.16it/s, pg=0.0364, ret=-0.000421, glen=114, tlen=274, kl=0.000918, act_lr=2.2e-7, ent=2.05] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:43,  1.16it/s, pg=0.0364, ret=-0.000421, glen=114, tlen=274, kl=0.000918, act_lr=2.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:43,  1.16it/s, pg=-0.0785, ret=-0.00207, glen=125, tlen=285, kl=0.000898, act_lr=2.2e-7, ent=1.71]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:42,  1.16it/s, pg=-0.0785, ret=-0.00207, glen=125, tlen=285, kl=0.000898, act_lr=2.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:42,  1.16it/s, pg=0.11, ret=-0.00179, glen=109, tlen=270, kl=0.000971, act_lr=2.2e-7, ent=1.75]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:06<00:42,  1.17it/s, pg=0.11, ret=-0.00179, glen=109, tlen=270, kl=0.000971, act_lr=2.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:42,  1.17it/s, pg=0.0883, ret=-0.000972, glen=112, tlen=273, kl=0.000924, act_lr=2.2e-7, ent=1.72]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.17it/s, pg=0.0883, ret=-0.000972, glen=112, tlen=273, kl=0.000924, act_lr=2.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.17it/s, pg=0.019, ret=0.000783, glen=116, tlen=277, kl=0.000954, act_lr=2.2e-7, ent=1.83]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:41,  1.14it/s, pg=0.019, ret=0.000783, glen=116, tlen=277, kl=0.000954, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=0.0414, ret=7.43e-5, glen=116, tlen=277, kl=0.000953, act_lr=2.2e-7, ent=1.85]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.15it/s, pg=0.0414, ret=7.43e-5, glen=116, tlen=277, kl=0.000953, act_lr=2.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.15it/s, pg=-0.0156, ret=0.00109, glen=123, tlen=284, kl=0.00093, act_lr=2.2e-7, ent=1.84]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.14it/s, pg=-0.0156, ret=0.00109, glen=123, tlen=284, kl=0.00093, act_lr=2.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.14it/s, pg=-0.139, ret=0.000625, glen=117, tlen=278, kl=0.000938, act_lr=2.2e-7, ent=1.68]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.15it/s, pg=-0.139, ret=0.000625, glen=117, tlen=278, kl=0.000938, act_lr=2.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.15it/s, pg=-0.154, ret=0.00257, glen=99.6, tlen=260, kl=0.00094, act_lr=2.2e-7, ent=1.79] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.16it/s, pg=-0.154, ret=0.00257, glen=99.6, tlen=260, kl=0.00094, act_lr=2.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.16it/s, pg=0.0305, ret=0.000247, glen=118, tlen=278, kl=0.000921, act_lr=2.2e-7, ent=2.18]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=0.0305, ret=0.000247, glen=118, tlen=278, kl=0.000921, act_lr=2.2e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=-0.125, ret=0.000508, glen=117, tlen=278, kl=0.000897, act_lr=2.2e-7, ent=1.9] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:35,  1.16it/s, pg=-0.125, ret=0.000508, glen=117, tlen=278, kl=0.000897, act_lr=2.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=0.264, ret=-0.00165, glen=113, tlen=274, kl=0.000925, act_lr=2.2e-7, ent=2.07]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=0.264, ret=-0.00165, glen=113, tlen=274, kl=0.000925, act_lr=2.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.0155, ret=0.000199, glen=146, tlen=306, kl=0.000846, act_lr=2.2e-7, ent=2.21]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=0.0155, ret=0.000199, glen=146, tlen=306, kl=0.000846, act_lr=2.2e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=0.0692, ret=-0.000477, glen=114, tlen=275, kl=0.000969, act_lr=2.2e-7, ent=1.93]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=0.0692, ret=-0.000477, glen=114, tlen=275, kl=0.000969, act_lr=2.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=0.15, ret=-0.000287, glen=121, tlen=281, kl=0.000971, act_lr=2.2e-7, ent=1.7]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=0.15, ret=-0.000287, glen=121, tlen=281, kl=0.000971, act_lr=2.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=0.00433, ret=0.000862, glen=114, tlen=275, kl=0.000924, act_lr=2.2e-7, ent=1.85]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.17it/s, pg=0.00433, ret=0.000862, glen=114, tlen=275, kl=0.000924, act_lr=2.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.17it/s, pg=-0.303, ret=0.000743, glen=103, tlen=264, kl=0.000915, act_lr=2.2e-7, ent=2.01] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.303, ret=0.000743, glen=103, tlen=264, kl=0.000915, act_lr=2.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=0.0851, ret=-0.001, glen=117, tlen=278, kl=0.000925, act_lr=2.2e-7, ent=1.85]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:19<00:28,  1.17it/s, pg=0.0851, ret=-0.001, glen=117, tlen=278, kl=0.000925, act_lr=2.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:28,  1.17it/s, pg=-0.102, ret=0.00126, glen=106, tlen=266, kl=0.000962, act_lr=2.2e-7, ent=1.8]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=-0.102, ret=0.00126, glen=106, tlen=266, kl=0.000962, act_lr=2.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=-0.152, ret=0.000478, glen=108, tlen=268, kl=0.00093, act_lr=2.2e-7, ent=1.85]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=-0.152, ret=0.000478, glen=108, tlen=268, kl=0.00093, act_lr=2.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=-0.036, ret=-0.000269, glen=116, tlen=277, kl=0.000916, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.036, ret=-0.000269, glen=116, tlen=277, kl=0.000916, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0891, ret=0.0014, glen=161, tlen=322, kl=0.000819, act_lr=2.2e-7, ent=2.53]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:28,  1.06it/s, pg=0.0891, ret=0.0014, glen=161, tlen=322, kl=0.000819, act_lr=2.2e-7, ent=2.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.06it/s, pg=0.149, ret=-0.000982, glen=127, tlen=287, kl=0.000937, act_lr=2.2e-7, ent=1.85]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.09it/s, pg=0.149, ret=-0.000982, glen=127, tlen=287, kl=0.000937, act_lr=2.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.09it/s, pg=0.0122, ret=0.000674, glen=117, tlen=278, kl=0.000957, act_lr=2.2e-7, ent=1.78]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:25,  1.11it/s, pg=0.0122, ret=0.000674, glen=117, tlen=278, kl=0.000957, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.11it/s, pg=0.115, ret=-0.0014, glen=114, tlen=274, kl=0.000909, act_lr=2.2e-7, ent=1.78]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=0.115, ret=-0.0014, glen=114, tlen=274, kl=0.000909, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=0.156, ret=-0.00209, glen=127, tlen=289, kl=0.000896, act_lr=2.2e-7, ent=1.81]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:26<00:22,  1.15it/s, pg=0.156, ret=-0.00209, glen=127, tlen=289, kl=0.000896, act_lr=2.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.0691, ret=-0.000226, glen=93.4, tlen=254, kl=0.000982, act_lr=2.2e-7, ent=1.69]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.16it/s, pg=-0.0691, ret=-0.000226, glen=93.4, tlen=254, kl=0.000982, act_lr=2.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.16it/s, pg=0.185, ret=0.000703, glen=345, tlen=506, kl=0.000739, act_lr=2.2e-7, ent=1.42]    Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.185, ret=0.000703, glen=345, tlen=506, kl=0.000739, act_lr=2.2e-7, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=-0.0649, ret=1.05e-5, glen=106, tlen=267, kl=0.000963, act_lr=2.2e-7, ent=1.75]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.14it/s, pg=-0.0649, ret=1.05e-5, glen=106, tlen=267, kl=0.000963, act_lr=2.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.14it/s, pg=-0.0341, ret=0.000754, glen=101, tlen=262, kl=0.000978, act_lr=2.2e-7, ent=1.71]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.15it/s, pg=-0.0341, ret=0.000754, glen=101, tlen=262, kl=0.000978, act_lr=2.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.15it/s, pg=0.0205, ret=-0.000172, glen=106, tlen=267, kl=0.000896, act_lr=2.2e-7, ent=1.83]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.16it/s, pg=0.0205, ret=-0.000172, glen=106, tlen=267, kl=0.000896, act_lr=2.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.16it/s, pg=-0.0931, ret=-3.18e-6, glen=111, tlen=271, kl=0.000961, act_lr=2.2e-7, ent=1.88]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0931, ret=-3.18e-6, glen=111, tlen=271, kl=0.000961, act_lr=2.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=0.0249, ret=-0.000403, glen=123, tlen=283, kl=0.000939, act_lr=2.2e-7, ent=1.99]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=0.0249, ret=-0.000403, glen=123, tlen=283, kl=0.000939, act_lr=2.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.184, ret=0.00091, glen=112, tlen=273, kl=0.000964, act_lr=2.2e-7, ent=1.7]   Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:33<00:15,  1.17it/s, pg=-0.184, ret=0.00091, glen=112, tlen=273, kl=0.000964, act_lr=2.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0386, ret=-0.0011, glen=153, tlen=314, kl=0.000889, act_lr=2.2e-7, ent=2.58]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.16it/s, pg=0.0386, ret=-0.0011, glen=153, tlen=314, kl=0.000889, act_lr=2.2e-7, ent=2.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.16it/s, pg=0.0701, ret=-0.00166, glen=127, tlen=288, kl=0.000949, act_lr=2.2e-7, ent=2.08]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0701, ret=-0.00166, glen=127, tlen=288, kl=0.000949, act_lr=2.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0468, ret=0.000657, glen=115, tlen=276, kl=0.000945, act_lr=2.2e-7, ent=1.67]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.0468, ret=0.000657, glen=115, tlen=276, kl=0.000945, act_lr=2.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=0.0259, ret=-0.0017, glen=120, tlen=281, kl=0.000959, act_lr=2.2e-7, ent=1.87]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=0.0259, ret=-0.0017, glen=120, tlen=281, kl=0.000959, act_lr=2.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.0714, ret=-6.72e-5, glen=110, tlen=270, kl=0.00097, act_lr=2.2e-7, ent=2.03]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.0714, ret=-6.72e-5, glen=110, tlen=270, kl=0.00097, act_lr=2.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.0929, ret=0.000884, glen=111, tlen=273, kl=0.000957, act_lr=2.2e-7, ent=1.79]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:38<00:10,  1.17it/s, pg=-0.0929, ret=0.000884, glen=111, tlen=273, kl=0.000957, act_lr=2.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.108, ret=0.0021, glen=94.8, tlen=256, kl=0.000937, act_lr=2.2e-7, ent=1.79]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.108, ret=0.0021, glen=94.8, tlen=256, kl=0.000937, act_lr=2.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.00166, ret=0.000508, glen=112, tlen=272, kl=0.000916, act_lr=2.2e-7, ent=1.66]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.00166, ret=0.000508, glen=112, tlen=272, kl=0.000916, act_lr=2.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.00253, ret=3.96e-5, glen=104, tlen=265, kl=0.000925, act_lr=2.2e-7, ent=1.71] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=-0.00253, ret=3.96e-5, glen=104, tlen=265, kl=0.000925, act_lr=2.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=-0.169, ret=0.00122, glen=110, tlen=271, kl=0.000938, act_lr=2.2e-7, ent=1.8]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.18it/s, pg=-0.169, ret=0.00122, glen=110, tlen=271, kl=0.000938, act_lr=2.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.18it/s, pg=-0.185, ret=0.00142, glen=115, tlen=276, kl=0.000907, act_lr=2.2e-7, ent=1.62]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.18it/s, pg=-0.185, ret=0.00142, glen=115, tlen=276, kl=0.000907, act_lr=2.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.18it/s, pg=0.062, ret=-0.000329, glen=112, tlen=273, kl=0.000965, act_lr=2.2e-7, ent=1.7]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=0.062, ret=-0.000329, glen=112, tlen=273, kl=0.000965, act_lr=2.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=0.0325, ret=-0.00035, glen=103, tlen=263, kl=0.000942, act_lr=2.2e-7, ent=1.78]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:44<00:04,  1.16it/s, pg=0.0325, ret=-0.00035, glen=103, tlen=263, kl=0.000942, act_lr=2.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=-0.0881, ret=-0.000515, glen=106, tlen=266, kl=0.000899, act_lr=2.2e-7, ent=1.89]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0881, ret=-0.000515, glen=106, tlen=266, kl=0.000899, act_lr=2.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.145, ret=-0.00278, glen=127, tlen=288, kl=0.00098, act_lr=2.2e-7, ent=1.81]    Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.145, ret=-0.00278, glen=127, tlen=288, kl=0.00098, act_lr=2.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0877, ret=0.000293, glen=110, tlen=270, kl=0.000933, act_lr=2.2e-7, ent=1.79]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0877, ret=0.000293, glen=110, tlen=270, kl=0.000933, act_lr=2.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0808, ret=0.000412, glen=144, tlen=305, kl=0.000902, act_lr=2.2e-7, ent=2.13] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=0.0808, ret=0.000412, glen=144, tlen=305, kl=0.000902, act_lr=2.2e-7, ent=2.13]
2025-07-23 12:33:44.923 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.57s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=0.0582, ret=-0.000577, glen=110, tlen=271, kl=0.000954, act_lr=2.4e-7, ent=2.05]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.0582, ret=-0.000577, glen=110, tlen=271, kl=0.000954, act_lr=2.4e-7, ent=2.05]
2025-07-23 12:33:45.604 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 12:33:47.850 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.24s
2025-07-23 12:33:48.164 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 52.93s
2025-07-23 12:33:48.170 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.007621062429327714, 'actor_lr': 2.2035088096472783e-07, 'clip_ratio': 0.0, 'entropy': 1.8497176295832585, 'kl': 0.0009302193658393726, 'response_length': 118.88013029935067, 'total_length': 279.5767383240817, 'return': -1.3931673030336306e-05, 'policy_update_steps': 1.0}
Episode [1/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [45:52<03:55, 235.15s/it]2025-07-23 12:33:48.175 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 1/172 [00:00<00:33,  5.08it/s, est. speed input: 969.68 toks/s, output: 10.15 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/172 [00:04<00:00, 18.80it/s, est. speed input: 6355.34 toks/s, output: 2662.02 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/172 [00:04<00:01, 16.76it/s, est. speed input: 5405.38 toks/s, output: 2464.41 toks/s][32m [repeated 111x across cluster][0m
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:08<00:00,  2.87it/s, est. speed input: 3589.82 toks/s, output: 1973.74 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:08<00:00, 19.76it/s, est. speed input: 3589.82 toks/s, output: 1973.74 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:09<00:00,  2.31it/s, est. speed input: 3192.03 toks/s, output: 1876.47 toks/s][32m [repeated 26x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:05<00:01, 12.27it/s, est. speed input: 4852.66 toks/s, output: 2442.76 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:15<00:02,  1.28s/it, est. speed input: 1970.68 toks/s, output: 1314.08 toks/s][32m [repeated 4x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:20<00:00,  2.17s/it, est. speed input: 1484.50 toks/s, output: 965.42 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:20<00:00,  8.19it/s, est. speed input: 1484.50 toks/s, output: 965.42 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:19<00:01,  1.89s/it, est. speed input: 1592.31 toks/s, output: 1119.91 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:26<00:04,  4.38s/it, est. speed input: 1186.84 toks/s, output: 688.72 toks/s] 
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:26<00:00,  3.28s/it, est. speed input: 1163.28 toks/s, output: 878.37 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:26<00:00,  6.42it/s, est. speed input: 1163.28 toks/s, output: 878.37 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:32<00:00,  4.75s/it, est. speed input: 975.38 toks/s, output: 632.23 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:32<00:00,  5.37it/s, est. speed input: 975.38 toks/s, output: 632.23 toks/s]
2025-07-23 12:34:22.080 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 551.1601,strategyqa_test/accuracy: 0.3100,eval_accuracy: 0.3100
2025-07-23 12:34:22.337 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:35:38.846 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:35:39.028 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:35:39.029 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 76.69s
2025-07-23 12:35:40.173 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0098,avg_pass_at_n: 1.0000,avg_num_tokens: 111.2136,std_num_tokens: 138.5985,avg_correct_num_tokens: 93.2574,std_correct_num_tokens: 77.2859,avg_incorrect_num_tokens: 120.6777,std_incorrect_num_tokens: 161.0170
2025-07-23 12:35:40.480 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 1.45s
2025-07-23 12:35:41.171 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 0.69s
2025-07-23 12:35:56.680 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 119
2025-07-23 12:35:56.681 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 15.51s
2025-07-23 12:35:57.405 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.72s
2025-07-23 12:35:57.406 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0008252734174568425, avg_kl: 0.000918588718446363, avg_response_length: 112.7792503292821, avg_orm_score: 0.0, avg_custom_rewards: -0.0008252734174568425
2025-07-23 12:35:57.435 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter12_replay_buffer.jsonl
2025-07-23 12:35:58.301 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 0.87s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/30 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/30 [00:00<?, ?it/s, pg=-0.0751, ret=0.00046, glen=103, tlen=263, kl=0.000967, act_lr=2.4e-7, ent=1.72]Actor Train epoch [1/1]:   3%|‚ñé         | 1/30 [00:00<00:26,  1.07it/s, pg=-0.0751, ret=0.00046, glen=103, tlen=263, kl=0.000967, act_lr=2.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 1/30 [00:01<00:26,  1.07it/s, pg=0.011, ret=0.000384, glen=96.5, tlen=257, kl=0.000933, act_lr=2.4e-7, ent=1.73]Actor Train epoch [1/1]:   7%|‚ñã         | 2/30 [00:01<00:24,  1.12it/s, pg=0.011, ret=0.000384, glen=96.5, tlen=257, kl=0.000933, act_lr=2.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/30 [00:02<00:24,  1.12it/s, pg=0.193, ret=-0.000736, glen=122, tlen=282, kl=0.000869, act_lr=2.4e-7, ent=2.09]Actor Train epoch [1/1]:  10%|‚ñà         | 3/30 [00:02<00:23,  1.14it/s, pg=0.193, ret=-0.000736, glen=122, tlen=282, kl=0.000869, act_lr=2.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 3/30 [00:03<00:23,  1.14it/s, pg=-0.14, ret=0.000129, glen=113, tlen=273, kl=0.000927, act_lr=2.4e-7, ent=1.67] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 4/30 [00:03<00:22,  1.16it/s, pg=-0.14, ret=0.000129, glen=113, tlen=273, kl=0.000927, act_lr=2.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 4/30 [00:04<00:22,  1.16it/s, pg=-0.0706, ret=0.0008, glen=125, tlen=285, kl=0.000918, act_lr=2.4e-7, ent=1.72]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/30 [00:04<00:21,  1.16it/s, pg=-0.0706, ret=0.0008, glen=125, tlen=285, kl=0.000918, act_lr=2.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/30 [00:05<00:21,  1.16it/s, pg=0.0654, ret=-1.53e-5, glen=115, tlen=275, kl=0.000881, act_lr=2.4e-7, ent=2.02]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 6/30 [00:05<00:20,  1.16it/s, pg=0.0654, ret=-1.53e-5, glen=115, tlen=275, kl=0.000881, act_lr=2.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 6/30 [00:06<00:20,  1.16it/s, pg=-0.322, ret=0.00195, glen=103, tlen=263, kl=0.000967, act_lr=2.4e-7, ent=1.76] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 7/30 [00:06<00:19,  1.16it/s, pg=-0.322, ret=0.00195, glen=103, tlen=263, kl=0.000967, act_lr=2.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 7/30 [00:06<00:19,  1.16it/s, pg=-0.0477, ret=0.000533, glen=113, tlen=273, kl=0.000889, act_lr=2.4e-7, ent=1.61]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 8/30 [00:06<00:18,  1.17it/s, pg=-0.0477, ret=0.000533, glen=113, tlen=273, kl=0.000889, act_lr=2.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 8/30 [00:07<00:18,  1.17it/s, pg=0.149, ret=-0.000657, glen=176, tlen=336, kl=0.00085, act_lr=2.4e-7, ent=2.49]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 9/30 [00:07<00:18,  1.16it/s, pg=0.149, ret=-0.000657, glen=176, tlen=336, kl=0.00085, act_lr=2.4e-7, ent=2.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 9/30 [00:08<00:18,  1.16it/s, pg=0.0628, ret=0.000618, glen=128, tlen=288, kl=0.000933, act_lr=2.4e-7, ent=1.87]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:08<00:17,  1.16it/s, pg=0.0628, ret=0.000618, glen=128, tlen=288, kl=0.000933, act_lr=2.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:09<00:17,  1.16it/s, pg=-0.172, ret=0.000401, glen=94.1, tlen=255, kl=0.000923, act_lr=2.4e-7, ent=1.61]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:09<00:16,  1.17it/s, pg=-0.172, ret=0.000401, glen=94.1, tlen=255, kl=0.000923, act_lr=2.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:10<00:16,  1.17it/s, pg=0.0604, ret=-0.000599, glen=110, tlen=270, kl=0.000918, act_lr=2.4e-7, ent=1.87]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:10<00:15,  1.16it/s, pg=0.0604, ret=-0.000599, glen=110, tlen=270, kl=0.000918, act_lr=2.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:11<00:15,  1.16it/s, pg=-0.145, ret=0.00232, glen=108, tlen=268, kl=0.000945, act_lr=2.4e-7, ent=1.9]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:11<00:14,  1.16it/s, pg=-0.145, ret=0.00232, glen=108, tlen=268, kl=0.000945, act_lr=2.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:12<00:14,  1.16it/s, pg=-0.279, ret=0.000596, glen=105, tlen=265, kl=0.000933, act_lr=2.4e-7, ent=1.61]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:12<00:14,  1.14it/s, pg=-0.279, ret=0.000596, glen=105, tlen=265, kl=0.000933, act_lr=2.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:13<00:14,  1.14it/s, pg=0.103, ret=-0.00149, glen=103, tlen=263, kl=0.000976, act_lr=2.4e-7, ent=1.86] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:13<00:13,  1.15it/s, pg=0.103, ret=-0.00149, glen=103, tlen=263, kl=0.000976, act_lr=2.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:13<00:13,  1.15it/s, pg=-0.167, ret=0.000554, glen=109, tlen=270, kl=0.000965, act_lr=2.4e-7, ent=1.69]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:13<00:12,  1.15it/s, pg=-0.167, ret=0.000554, glen=109, tlen=270, kl=0.000965, act_lr=2.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:14<00:12,  1.15it/s, pg=0.0332, ret=-0.000696, glen=116, tlen=276, kl=0.000972, act_lr=2.4e-7, ent=1.88]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:14<00:11,  1.16it/s, pg=0.0332, ret=-0.000696, glen=116, tlen=276, kl=0.000972, act_lr=2.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:15<00:11,  1.16it/s, pg=0.201, ret=0.000253, glen=156, tlen=316, kl=0.000784, act_lr=2.4e-7, ent=2.29]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:15<00:10,  1.15it/s, pg=0.201, ret=0.000253, glen=156, tlen=316, kl=0.000784, act_lr=2.4e-7, ent=2.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:16<00:10,  1.15it/s, pg=0.146, ret=-0.0027, glen=106, tlen=267, kl=0.000887, act_lr=2.4e-7, ent=1.59] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:16<00:09,  1.16it/s, pg=0.146, ret=-0.0027, glen=106, tlen=267, kl=0.000887, act_lr=2.4e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:17<00:09,  1.16it/s, pg=-0.093, ret=0.000413, glen=108, tlen=267, kl=0.000827, act_lr=2.4e-7, ent=1.49]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:17<00:08,  1.16it/s, pg=-0.093, ret=0.000413, glen=108, tlen=267, kl=0.000827, act_lr=2.4e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:18<00:08,  1.16it/s, pg=0.0349, ret=-0.00457, glen=104, tlen=264, kl=0.000897, act_lr=2.4e-7, ent=1.81]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:18<00:07,  1.16it/s, pg=0.0349, ret=-0.00457, glen=104, tlen=264, kl=0.000897, act_lr=2.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:19<00:07,  1.16it/s, pg=0.129, ret=-0.00105, glen=102, tlen=262, kl=0.000955, act_lr=2.4e-7, ent=1.69] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [00:19<00:06,  1.17it/s, pg=0.129, ret=-0.00105, glen=102, tlen=262, kl=0.000955, act_lr=2.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [00:19<00:06,  1.17it/s, pg=0.143, ret=-0.000664, glen=108, tlen=268, kl=0.000938, act_lr=2.4e-7, ent=1.8]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [00:19<00:05,  1.17it/s, pg=0.143, ret=-0.000664, glen=108, tlen=268, kl=0.000938, act_lr=2.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [00:20<00:05,  1.17it/s, pg=0.157, ret=-0.0013, glen=116, tlen=276, kl=0.000931, act_lr=2.4e-7, ent=1.89] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:20<00:05,  1.17it/s, pg=0.157, ret=-0.0013, glen=116, tlen=276, kl=0.000931, act_lr=2.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:21<00:05,  1.17it/s, pg=-0.0386, ret=-0.000401, glen=101, tlen=262, kl=0.000932, act_lr=2.4e-7, ent=1.72]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [00:21<00:04,  1.17it/s, pg=-0.0386, ret=-0.000401, glen=101, tlen=262, kl=0.000932, act_lr=2.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [00:22<00:04,  1.17it/s, pg=-0.151, ret=0.00198, glen=115, tlen=275, kl=0.000904, act_lr=2.4e-7, ent=1.8]    Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:22<00:03,  1.17it/s, pg=-0.151, ret=0.00198, glen=115, tlen=275, kl=0.000904, act_lr=2.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:23<00:03,  1.17it/s, pg=0.128, ret=-0.00115, glen=113, tlen=273, kl=0.000899, act_lr=2.4e-7, ent=1.71]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:23<00:02,  1.17it/s, pg=0.128, ret=-0.00115, glen=113, tlen=273, kl=0.000899, act_lr=2.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:24<00:02,  1.17it/s, pg=0.0968, ret=-0.00148, glen=112, tlen=273, kl=0.000955, act_lr=2.4e-7, ent=1.73]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [00:24<00:01,  1.08it/s, pg=0.0968, ret=-0.00148, glen=112, tlen=273, kl=0.000955, act_lr=2.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [00:25<00:01,  1.08it/s, pg=-0.0464, ret=-0.000192, glen=111, tlen=271, kl=0.000935, act_lr=2.4e-7, ent=1.81]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:25<00:00,  1.10it/s, pg=-0.0464, ret=-0.000192, glen=111, tlen=271, kl=0.000935, act_lr=2.4e-7, ent=1.81]
2025-07-23 12:36:24.596 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 26.14s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:26<00:00,  1.10it/s, pg=-0.18, ret=0.0018, glen=93.3, tlen=254, kl=0.000949, act_lr=2.6e-7, ent=1.66]    Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:26<00:00,  1.11it/s, pg=-0.18, ret=0.0018, glen=93.3, tlen=254, kl=0.000949, act_lr=2.6e-7, ent=1.66]
2025-07-23 12:36:25.465 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 12:36:27.980 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-23 12:36:28.284 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 29.95s
2025-07-23 12:36:28.287 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0070718129475911455, 'actor_lr': 2.406666586315017e-07, 'clip_ratio': 0.0, 'entropy': 1.803765590985616, 'kl': 0.0009185949961344401, 'response_length': 112.73919143676758, 'total_length': 272.9371337890625, 'return': -0.00014988578235109647, 'policy_update_steps': 1.0}
Episode [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [48:32<00:00, 212.42s/it]2025-07-23 12:36:37.327 | INFO     | orz.ppo.trainer:train:193 - Successfully update ref model with policy model, training continue.
[36m(pid=889771)[0m [2025-07-23 11:50:55,024] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[36m(pid=889771)[0m [2025-07-23 11:50:55,029] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[36m(LLMActor pid=884799)[0m init_process_group: master_address=10.224.3.59, master_port=59053,  rank=4, world_size=5, group_name=openrlhf[32m [repeated 3x across cluster][0m
[36m(get_reflection_pattern_score pid=889771)[0m math_verify is not installed in this environment
[36m(pid=890486)[0m [2025-07-23 11:50:58,724] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 14x across cluster][0m
[36m(pid=890486)[0m [2025-07-23 11:50:58,727] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 14x across cluster][0m
[36m(get_reflection_pattern_score pid=890486)[0m math_verify is not installed in this environment[32m [repeated 14x across cluster][0m
[36m(pid=891094)[0m [2025-07-23 11:51:05,744] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[36m(pid=891094)[0m [2025-07-23 11:51:05,749] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[36m(pid=891005)[0m [2025-07-23 11:51:05,839] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[36m(pid=891005)[0m [2025-07-23 11:51:05,844] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[36m(PolicyRayActorBase pid=885360)[0m Invalidate trace cache @ step 370 and module 0: cache has only 370 modules
[36m(pid=891770)[0m math_verify is not installed in this environment[32m [repeated 23x across cluster][0m
[36m(pid=891251)[0m [2025-07-23 11:51:06,015] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 27x across cluster][0m
[36m(pid=891770)[0m [2025-07-23 11:51:06,017] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 27x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:52:36,247] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(extract_final_answers_batch pid=891094)[0m math_verify is not installed in this environment
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 11:56:35,250] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(get_reflection_pattern_score pid=891868)[0m math_verify is not installed in this environment[32m [repeated 5x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:00:26,646] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:10:36,576] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:14:33,570] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:18:03,216] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:21:52,373] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:25:20,264] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:29:41,197] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:33:44,916] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:34,939] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:35,458] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 678, num_elems = 3.55B
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:36,961] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:36,962] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:36,969] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:36,971] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,177] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,178] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 6.45 GB         CA 2.51 GB         Max_CA 38 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,178] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 106.02 GB, percent = 21.1%
[36m(RefRayActorBase pid=885532)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [2/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [48:41<00:00, 224.72s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,321] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,322] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 2.51 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,322] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 106.01 GB, percent = 21.1%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=885532)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=885532)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=885532)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=885532)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=885532)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=885532)[0m     "profile": false
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "start_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "end_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=885532)[0m     "model_info": null, 
[36m(RefRayActorBase pid=885532)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=885532)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=885532)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=885532)[0m     "fast": true, 
[36m(RefRayActorBase pid=885532)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=885532)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=885532)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=885532)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x764c3e1dbef0>
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layerhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=885532)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=885532)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=885532)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=885532)[0m     "detailed": true, 
[36m(RefRayActorBase pid=885532)[0m     "output_file": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=885532)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=885532)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=885532)[0m     "load_path": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,324] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=885532)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=885532)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=885532)[0m         "stage": 3, 
[36m(RefRayActorBase pid=885532)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=885532)[0m         "offload_param": {
[36m(RefRayActorBase pid=885532)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=885532)[0m             "pin_memory": true
[36m(RefRayActorBase pid=885532)[0m         }
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "bf16": {
[36m(RefRayActorBase pid=885532)[0m         "enabled": true
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=885532)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=885532)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=885532)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=885532)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-23 12:36:37.557 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:39:14.861 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:39:15.047 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:39:15.048 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 157.49s
2025-07-23 12:39:17.502 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0114,avg_pass_at_n: 1.0000,avg_num_tokens: 112.7085,std_num_tokens: 171.3859,avg_correct_num_tokens: 97.4690,std_correct_num_tokens: 94.9379,avg_incorrect_num_tokens: 120.6652,std_incorrect_num_tokens: 199.5462
2025-07-23 12:39:17.896 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.85s
2025-07-23 12:39:19.259 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.36s
2025-07-23 12:39:48.687 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 227
2025-07-23 12:39:48.687 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 29.43s
2025-07-23 12:39:49.756 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 1.06s
2025-07-23 12:39:49.757 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0020386607443610215, avg_kl: 0.0, avg_response_length: 121.21357152325466, avg_orm_score: 0.0, avg_custom_rewards: -0.0020386607443610215
2025-07-23 12:39:49.818 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter13_replay_buffer.jsonl
2025-07-23 12:39:51.504 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.69s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:01<?, ?it/s, pg=-0.00525, ret=-0.000115, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.81]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:59,  1.07s/it, pg=-0.00525, ret=-0.000115, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:59,  1.07s/it, pg=-0.0928, ret=-6.83e-5, glen=112, tlen=272, kl=0, act_lr=2.6e-7, ent=1.65]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:52,  1.06it/s, pg=-0.0928, ret=-6.83e-5, glen=112, tlen=272, kl=0, act_lr=2.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:52,  1.06it/s, pg=-0.0303, ret=-0.000254, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.56]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=-0.0303, ret=-0.000254, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=0.13, ret=-0.001, glen=108, tlen=269, kl=0, act_lr=2.6e-7, ent=1.71]      Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.13it/s, pg=0.13, ret=-0.001, glen=108, tlen=269, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.13it/s, pg=-0.182, ret=0.00196, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.77]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.15it/s, pg=-0.182, ret=0.00196, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.15it/s, pg=-0.0162, ret=-0.000635, glen=90.4, tlen=251, kl=0, act_lr=2.6e-7, ent=1.6]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.16it/s, pg=-0.0162, ret=-0.000635, glen=90.4, tlen=251, kl=0, act_lr=2.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.16it/s, pg=-0.156, ret=-0.014, glen=461, tlen=622, kl=0, act_lr=2.6e-7, ent=1.94]    Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.12it/s, pg=-0.156, ret=-0.014, glen=461, tlen=622, kl=0, act_lr=2.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.12it/s, pg=-0.0217, ret=-0.000843, glen=106, tlen=266, kl=0, act_lr=2.6e-7, ent=1.7]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.13it/s, pg=-0.0217, ret=-0.000843, glen=106, tlen=266, kl=0, act_lr=2.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.13it/s, pg=0.119, ret=-0.00166, glen=144, tlen=305, kl=0, act_lr=2.6e-7, ent=1.98]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=0.119, ret=-0.00166, glen=144, tlen=305, kl=0, act_lr=2.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=0.159, ret=-0.00113, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.62]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=0.159, ret=-0.00113, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=-0.0346, ret=0.00131, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.9]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=-0.0346, ret=0.00131, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=0.139, ret=-0.000689, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=2.04]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=0.139, ret=-0.000689, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.159, ret=-0.00309, glen=228, tlen=389, kl=0, act_lr=2.6e-7, ent=1.87] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:39,  1.12it/s, pg=0.159, ret=-0.00309, glen=228, tlen=389, kl=0, act_lr=2.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:39,  1.12it/s, pg=0.0353, ret=0.000372, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.78]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.14it/s, pg=0.0353, ret=0.000372, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.14it/s, pg=-0.0354, ret=0.0002, glen=114, tlen=275, kl=0, act_lr=2.6e-7, ent=1.8]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.15it/s, pg=-0.0354, ret=0.0002, glen=114, tlen=275, kl=0, act_lr=2.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.15it/s, pg=-0.0625, ret=0.000972, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.7]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.15it/s, pg=-0.0625, ret=0.000972, glen=105, tlen=266, kl=0, act_lr=2.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.15it/s, pg=0.0022, ret=0.000316, glen=109, tlen=269, kl=0, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.16it/s, pg=0.0022, ret=0.000316, glen=109, tlen=269, kl=0, act_lr=2.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.16it/s, pg=-0.094, ret=0.00142, glen=104, tlen=265, kl=0, act_lr=2.6e-7, ent=1.62] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.16it/s, pg=-0.094, ret=0.00142, glen=104, tlen=265, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.16it/s, pg=0.0372, ret=-0.00104, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.62]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.17it/s, pg=0.0372, ret=-0.00104, glen=113, tlen=274, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.17it/s, pg=-0.0279, ret=0.000255, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.62]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.17it/s, pg=-0.0279, ret=0.000255, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.17it/s, pg=-0.0616, ret=0.000897, glen=95.1, tlen=256, kl=0, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:31,  1.15it/s, pg=-0.0616, ret=0.000897, glen=95.1, tlen=256, kl=0, act_lr=2.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:31,  1.15it/s, pg=0.0349, ret=8.27e-5, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.79]   Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=0.0349, ret=8.27e-5, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.16it/s, pg=0.0979, ret=-0.000348, glen=153, tlen=314, kl=0, act_lr=2.6e-7, ent=1.6]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.15it/s, pg=0.0979, ret=-0.000348, glen=153, tlen=314, kl=0, act_lr=2.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.15it/s, pg=-0.123, ret=0.00123, glen=123, tlen=284, kl=0, act_lr=2.6e-7, ent=1.85] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.16it/s, pg=-0.123, ret=0.00123, glen=123, tlen=284, kl=0, act_lr=2.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.16it/s, pg=-0.146, ret=0.000688, glen=102, tlen=263, kl=0, act_lr=2.6e-7, ent=1.72]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.16it/s, pg=-0.146, ret=0.000688, glen=102, tlen=263, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.16it/s, pg=0.031, ret=0.000109, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.74] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.031, ret=0.000109, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=0.0785, ret=-0.00194, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.83]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:27,  1.07it/s, pg=0.0785, ret=-0.00194, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:27,  1.07it/s, pg=0.142, ret=-1.54e-5, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.96] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.10it/s, pg=0.142, ret=-1.54e-5, glen=128, tlen=289, kl=0, act_lr=2.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.10it/s, pg=0.125, ret=-0.00129, glen=113, tlen=273, kl=0, act_lr=2.6e-7, ent=1.71]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:24,  1.12it/s, pg=0.125, ret=-0.00129, glen=113, tlen=273, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:24,  1.12it/s, pg=0.0864, ret=-0.000632, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.83]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.14it/s, pg=0.0864, ret=-0.000632, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.14it/s, pg=-0.128, ret=-0.000397, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.8] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.128, ret=-0.000397, glen=104, tlen=264, kl=0, act_lr=2.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:22,  1.15it/s, pg=-0.015, ret=-0.00062, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.79]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.15it/s, pg=-0.015, ret=-0.00062, glen=111, tlen=271, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.15it/s, pg=0.0925, ret=-0.000128, glen=116, tlen=276, kl=0, act_lr=2.6e-7, ent=1.75]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.16it/s, pg=0.0925, ret=-0.000128, glen=116, tlen=276, kl=0, act_lr=2.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.16it/s, pg=-0.0417, ret=-0.000762, glen=108, tlen=269, kl=0, act_lr=2.6e-7, ent=1.75]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=-0.0417, ret=-0.000762, glen=108, tlen=269, kl=0, act_lr=2.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=0.0989, ret=-0.000633, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=2.15] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.0989, ret=-0.000633, glen=111, tlen=272, kl=0, act_lr=2.6e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.00739, ret=-0.00109, glen=105, tlen=265, kl=0, act_lr=2.6e-7, ent=1.68]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:17,  1.17it/s, pg=0.00739, ret=-0.00109, glen=105, tlen=265, kl=0, act_lr=2.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:17,  1.17it/s, pg=-0.0833, ret=-6.69e-5, glen=93.5, tlen=255, kl=0, act_lr=2.6e-7, ent=1.72]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.17it/s, pg=-0.0833, ret=-6.69e-5, glen=93.5, tlen=255, kl=0, act_lr=2.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.17it/s, pg=-0.169, ret=0.00108, glen=116, tlen=277, kl=0, act_lr=2.6e-7, ent=1.8]    Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.17it/s, pg=-0.169, ret=0.00108, glen=116, tlen=277, kl=0, act_lr=2.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.17it/s, pg=0.0382, ret=0.000111, glen=109, tlen=269, kl=0, act_lr=2.6e-7, ent=1.78]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0382, ret=0.000111, glen=109, tlen=269, kl=0, act_lr=2.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0418, ret=-0.000237, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.66]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.0418, ret=-0.000237, glen=101, tlen=262, kl=0, act_lr=2.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.0881, ret=0.000544, glen=148, tlen=308, kl=0, act_lr=2.6e-7, ent=2.55] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.0881, ret=0.000544, glen=148, tlen=308, kl=0, act_lr=2.6e-7, ent=2.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=-0.0833, ret=0.000547, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.69]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=-0.0833, ret=0.000547, glen=106, tlen=267, kl=0, act_lr=2.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=-0.174, ret=9.04e-5, glen=136, tlen=296, kl=0, act_lr=2.6e-7, ent=1.86]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=-0.174, ret=9.04e-5, glen=136, tlen=296, kl=0, act_lr=2.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.141, ret=0.000735, glen=102, tlen=263, kl=0, act_lr=2.6e-7, ent=1.75]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.141, ret=0.000735, glen=102, tlen=263, kl=0, act_lr=2.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.146, ret=0.00339, glen=105, tlen=265, kl=0, act_lr=2.6e-7, ent=1.97] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.146, ret=0.00339, glen=105, tlen=265, kl=0, act_lr=2.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.17it/s, pg=-0.172, ret=8.94e-5, glen=101, tlen=261, kl=0, act_lr=2.6e-7, ent=1.71]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.172, ret=8.94e-5, glen=101, tlen=261, kl=0, act_lr=2.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=-0.0332, ret=-0.00124, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.82]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.0332, ret=-0.00124, glen=115, tlen=276, kl=0, act_lr=2.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=-0.0956, ret=0.00132, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.68] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=-0.0956, ret=0.00132, glen=109, tlen=270, kl=0, act_lr=2.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=0.0811, ret=-0.00118, glen=136, tlen=297, kl=0, act_lr=2.6e-7, ent=1.73]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.0811, ret=-0.00118, glen=136, tlen=297, kl=0, act_lr=2.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.0642, ret=0.000256, glen=134, tlen=294, kl=0, act_lr=2.6e-7, ent=1.67]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.0642, ret=0.000256, glen=134, tlen=294, kl=0, act_lr=2.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=0.128, ret=-0.00128, glen=121, tlen=282, kl=0, act_lr=2.6e-7, ent=2.07]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=0.128, ret=-0.00128, glen=121, tlen=282, kl=0, act_lr=2.6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=-0.198, ret=0.00154, glen=117, tlen=278, kl=0, act_lr=2.6e-7, ent=1.68]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.198, ret=0.00154, glen=117, tlen=278, kl=0, act_lr=2.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=-0.0493, ret=-0.00113, glen=155, tlen=316, kl=0, act_lr=2.6e-7, ent=2.47]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:45<00:03,  1.17it/s, pg=-0.0493, ret=-0.00113, glen=155, tlen=316, kl=0, act_lr=2.6e-7, ent=2.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.1, ret=1.67e-5, glen=99.6, tlen=261, kl=0, act_lr=2.6e-7, ent=1.8]    Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=-0.1, ret=1.67e-5, glen=99.6, tlen=261, kl=0, act_lr=2.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.0975, ret=0.000506, glen=107, tlen=267, kl=0, act_lr=2.6e-7, ent=1.79]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.0975, ret=0.000506, glen=107, tlen=267, kl=0, act_lr=2.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0872, ret=-0.000479, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.78]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=0.0872, ret=-0.000479, glen=108, tlen=268, kl=0, act_lr=2.6e-7, ent=1.78]
2025-07-23 12:40:41.534 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.77s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=0.0145, ret=-0.000726, glen=99.5, tlen=259, kl=0, act_lr=2.8e-7, ent=1.56]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.0145, ret=-0.000726, glen=99.5, tlen=259, kl=0, act_lr=2.8e-7, ent=1.56]
2025-07-23 12:40:42.384 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 12:40:44.958 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-23 12:40:45.265 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.61s
2025-07-23 12:40:45.271 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.014476207264682702, 'actor_lr': 2.6035088427434265e-07, 'clip_ratio': 0.0, 'entropy': 1.792012003430149, 'kl': 0.0, 'response_length': 121.12445349442332, 'total_length': 281.694226448996, 'return': -0.00032863166594771637, 'policy_update_steps': 1.0}

Episode [2/20]:   8%|‚ñä         | 1/13 [04:07<49:35, 247.94s/it][A2025-07-23 12:40:45.304 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:42:17.784 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:42:17.973 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:42:17.973 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 92.67s
[36m(get pid=889770)[0m _repeat_score
2025-07-23 12:42:19.949 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0148,avg_reflection_pattern_score: 0.0103,avg_pass_at_n: 1.0000,avg_num_tokens: 110.3052,std_num_tokens: 118.4850,avg_correct_num_tokens: 94.0969,std_correct_num_tokens: 77.2423,avg_incorrect_num_tokens: 118.3398,std_incorrect_num_tokens: 133.5868
2025-07-23 12:42:20.325 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.35s
2025-07-23 12:42:21.934 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.61s
2025-07-23 12:42:50.842 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 225
2025-07-23 12:42:50.843 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.91s
2025-07-23 12:42:51.674 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.83s
2025-07-23 12:42:51.675 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0004989435805732177, avg_kl: 0.0009128146701388889, avg_response_length: 112.09894961886936, avg_orm_score: 0.0, avg_custom_rewards: -0.0004989435805732177
2025-07-23 12:42:51.709 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter14_replay_buffer.jsonl
2025-07-23 12:42:53.336 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.63s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s, pg=-0.00754, ret=-0.0011, glen=100, tlen=261, kl=0.000907, act_lr=2.8e-7, ent=1.75]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:00<00:53,  1.04it/s, pg=-0.00754, ret=-0.0011, glen=100, tlen=261, kl=0.000907, act_lr=2.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:53,  1.04it/s, pg=0.0598, ret=-0.0014, glen=116, tlen=276, kl=0.000921, act_lr=2.8e-7, ent=1.88]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:49,  1.11it/s, pg=0.0598, ret=-0.0014, glen=116, tlen=276, kl=0.000921, act_lr=2.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:49,  1.11it/s, pg=0.0191, ret=-0.000787, glen=114, tlen=275, kl=0.00091, act_lr=2.8e-7, ent=1.6]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:48,  1.11it/s, pg=0.0191, ret=-0.000787, glen=114, tlen=275, kl=0.00091, act_lr=2.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:48,  1.11it/s, pg=0.148, ret=-0.00045, glen=113, tlen=274, kl=0.000867, act_lr=2.8e-7, ent=2.24]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:46,  1.13it/s, pg=0.148, ret=-0.00045, glen=113, tlen=274, kl=0.000867, act_lr=2.8e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:46,  1.13it/s, pg=-0.0707, ret=0.000296, glen=109, tlen=270, kl=0.000886, act_lr=2.8e-7, ent=1.82]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:45,  1.14it/s, pg=-0.0707, ret=0.000296, glen=109, tlen=270, kl=0.000886, act_lr=2.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:45,  1.14it/s, pg=0.165, ret=-0.00105, glen=148, tlen=309, kl=0.000881, act_lr=2.8e-7, ent=2.42]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.15it/s, pg=0.165, ret=-0.00105, glen=148, tlen=309, kl=0.000881, act_lr=2.8e-7, ent=2.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.15it/s, pg=-0.0674, ret=0.000963, glen=112, tlen=273, kl=0.000912, act_lr=2.8e-7, ent=2.01]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.16it/s, pg=-0.0674, ret=0.000963, glen=112, tlen=273, kl=0.000912, act_lr=2.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.16it/s, pg=0.034, ret=-0.00081, glen=101, tlen=261, kl=0.000941, act_lr=2.8e-7, ent=1.73]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.14it/s, pg=0.034, ret=-0.00081, glen=101, tlen=261, kl=0.000941, act_lr=2.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.14it/s, pg=-0.141, ret=0.00061, glen=102, tlen=262, kl=0.000926, act_lr=2.8e-7, ent=1.74]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:07<00:41,  1.15it/s, pg=-0.141, ret=0.00061, glen=102, tlen=262, kl=0.000926, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.0955, ret=5.11e-5, glen=99.1, tlen=260, kl=0.000941, act_lr=2.8e-7, ent=1.7]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.16it/s, pg=-0.0955, ret=5.11e-5, glen=99.1, tlen=260, kl=0.000941, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.16it/s, pg=-0.0458, ret=-0.000256, glen=102, tlen=263, kl=0.000945, act_lr=2.8e-7, ent=1.7]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:41,  1.12it/s, pg=-0.0458, ret=-0.000256, glen=102, tlen=263, kl=0.000945, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:41,  1.12it/s, pg=-0.0831, ret=0.000387, glen=110, tlen=270, kl=0.000907, act_lr=2.8e-7, ent=1.75]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:39,  1.13it/s, pg=-0.0831, ret=0.000387, glen=110, tlen=270, kl=0.000907, act_lr=2.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:39,  1.13it/s, pg=0.0507, ret=-0.000793, glen=115, tlen=276, kl=0.000866, act_lr=2.8e-7, ent=2.11]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:38,  1.14it/s, pg=0.0507, ret=-0.000793, glen=115, tlen=276, kl=0.000866, act_lr=2.8e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:38,  1.14it/s, pg=0.0444, ret=-0.00067, glen=114, tlen=274, kl=0.000862, act_lr=2.8e-7, ent=1.89] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:37,  1.15it/s, pg=0.0444, ret=-0.00067, glen=114, tlen=274, kl=0.000862, act_lr=2.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:37,  1.15it/s, pg=-0.0165, ret=0.000122, glen=97.8, tlen=259, kl=0.00096, act_lr=2.8e-7, ent=1.86]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:36,  1.16it/s, pg=-0.0165, ret=0.000122, glen=97.8, tlen=259, kl=0.00096, act_lr=2.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:36,  1.16it/s, pg=-0.0632, ret=0.00106, glen=99.8, tlen=260, kl=0.000931, act_lr=2.8e-7, ent=1.69]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=-0.0632, ret=0.00106, glen=99.8, tlen=260, kl=0.000931, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.16it/s, pg=-0.202, ret=0.00167, glen=104, tlen=265, kl=0.000897, act_lr=2.8e-7, ent=1.86]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.202, ret=0.00167, glen=104, tlen=265, kl=0.000897, act_lr=2.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=0.0264, ret=-0.000314, glen=102, tlen=263, kl=0.000975, act_lr=2.8e-7, ent=1.67]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=0.0264, ret=-0.000314, glen=102, tlen=263, kl=0.000975, act_lr=2.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=-0.0451, ret=0.000184, glen=106, tlen=267, kl=0.00089, act_lr=2.8e-7, ent=1.63] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:32,  1.16it/s, pg=-0.0451, ret=0.000184, glen=106, tlen=267, kl=0.00089, act_lr=2.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:32,  1.16it/s, pg=-0.0175, ret=0.000687, glen=107, tlen=268, kl=0.000883, act_lr=2.8e-7, ent=1.74]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:31,  1.16it/s, pg=-0.0175, ret=0.000687, glen=107, tlen=268, kl=0.000883, act_lr=2.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:31,  1.16it/s, pg=-0.143, ret=0.00102, glen=118, tlen=279, kl=0.000916, act_lr=2.8e-7, ent=1.78]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:30,  1.16it/s, pg=-0.143, ret=0.00102, glen=118, tlen=279, kl=0.000916, act_lr=2.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:30,  1.16it/s, pg=-0.21, ret=0.00133, glen=111, tlen=271, kl=0.000905, act_lr=2.8e-7, ent=1.72] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:29,  1.17it/s, pg=-0.21, ret=0.00133, glen=111, tlen=271, kl=0.000905, act_lr=2.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:29,  1.17it/s, pg=-0.0226, ret=6.24e-5, glen=124, tlen=284, kl=0.000891, act_lr=2.8e-7, ent=1.79]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=-0.0226, ret=6.24e-5, glen=124, tlen=284, kl=0.000891, act_lr=2.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.17it/s, pg=0.0524, ret=-0.000542, glen=107, tlen=268, kl=0.000918, act_lr=2.8e-7, ent=1.68]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.0524, ret=-0.000542, glen=107, tlen=268, kl=0.000918, act_lr=2.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0343, ret=-0.000497, glen=112, tlen=273, kl=0.000911, act_lr=2.8e-7, ent=1.86]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0343, ret=-0.000497, glen=112, tlen=273, kl=0.000911, act_lr=2.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.033, ret=-0.000482, glen=106, tlen=266, kl=0.000881, act_lr=2.8e-7, ent=1.75] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.033, ret=-0.000482, glen=106, tlen=266, kl=0.000881, act_lr=2.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.000488, ret=0.000292, glen=99.5, tlen=260, kl=0.000939, act_lr=2.8e-7, ent=1.72]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:27,  1.07it/s, pg=-0.000488, ret=0.000292, glen=99.5, tlen=260, kl=0.000939, act_lr=2.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:27,  1.07it/s, pg=-0.139, ret=0.00241, glen=96.8, tlen=258, kl=0.000894, act_lr=2.8e-7, ent=1.7]     Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.10it/s, pg=-0.139, ret=0.00241, glen=96.8, tlen=258, kl=0.000894, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.10it/s, pg=0.147, ret=-0.00106, glen=102, tlen=262, kl=0.000891, act_lr=2.8e-7, ent=1.72]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:24,  1.12it/s, pg=0.147, ret=-0.00106, glen=102, tlen=262, kl=0.000891, act_lr=2.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:24,  1.12it/s, pg=-0.0288, ret=7.02e-5, glen=107, tlen=268, kl=0.000902, act_lr=2.8e-7, ent=1.79]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.14it/s, pg=-0.0288, ret=7.02e-5, glen=107, tlen=268, kl=0.000902, act_lr=2.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.14it/s, pg=-0.108, ret=0.000565, glen=105, tlen=266, kl=0.000886, act_lr=2.8e-7, ent=1.85]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.108, ret=0.000565, glen=105, tlen=266, kl=0.000886, act_lr=2.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=0.0389, ret=-1.41e-5, glen=128, tlen=288, kl=0.000914, act_lr=2.8e-7, ent=2]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:27<00:21,  1.15it/s, pg=0.0389, ret=-1.41e-5, glen=128, tlen=288, kl=0.000914, act_lr=2.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.15it/s, pg=-0.00916, ret=0.000154, glen=117, tlen=278, kl=0.000936, act_lr=2.8e-7, ent=1.85]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.16it/s, pg=-0.00916, ret=0.000154, glen=117, tlen=278, kl=0.000936, act_lr=2.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.16it/s, pg=-0.0818, ret=-0.000818, glen=116, tlen=276, kl=0.000913, act_lr=2.8e-7, ent=1.64]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.14it/s, pg=-0.0818, ret=-0.000818, glen=116, tlen=276, kl=0.000913, act_lr=2.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.14it/s, pg=0.114, ret=-0.000224, glen=117, tlen=278, kl=0.000946, act_lr=2.8e-7, ent=1.7]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.13it/s, pg=0.114, ret=-0.000224, glen=117, tlen=278, kl=0.000946, act_lr=2.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.13it/s, pg=0.116, ret=-0.00219, glen=115, tlen=276, kl=0.00093, act_lr=2.8e-7, ent=1.61] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=0.116, ret=-0.00219, glen=115, tlen=276, kl=0.00093, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.0157, ret=0.000286, glen=115, tlen=276, kl=0.00089, act_lr=2.8e-7, ent=1.72]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.15it/s, pg=-0.0157, ret=0.000286, glen=115, tlen=276, kl=0.00089, act_lr=2.8e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.15it/s, pg=-0.1, ret=0.000987, glen=113, tlen=273, kl=0.000949, act_lr=2.8e-7, ent=1.81]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=-0.1, ret=0.000987, glen=113, tlen=273, kl=0.000949, act_lr=2.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.16it/s, pg=0.212, ret=-0.00135, glen=132, tlen=293, kl=0.000881, act_lr=2.8e-7, ent=1.8]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.16it/s, pg=0.212, ret=-0.00135, glen=132, tlen=293, kl=0.000881, act_lr=2.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.16it/s, pg=0.0234, ret=-0.000855, glen=114, tlen=275, kl=0.000947, act_lr=2.8e-7, ent=1.78]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.0234, ret=-0.000855, glen=114, tlen=275, kl=0.000947, act_lr=2.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=-0.0177, ret=0.00101, glen=104, tlen=265, kl=0.000926, act_lr=2.8e-7, ent=1.79] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.0177, ret=0.00101, glen=104, tlen=265, kl=0.000926, act_lr=2.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0747, ret=-0.000119, glen=146, tlen=307, kl=0.000817, act_lr=2.8e-7, ent=1.69]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.16it/s, pg=0.0747, ret=-0.000119, glen=146, tlen=307, kl=0.000817, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.16it/s, pg=0.122, ret=-0.000541, glen=100, tlen=261, kl=0.000906, act_lr=2.8e-7, ent=1.77] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.16it/s, pg=0.122, ret=-0.000541, glen=100, tlen=261, kl=0.000906, act_lr=2.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.16it/s, pg=0.0855, ret=-0.00197, glen=103, tlen=264, kl=0.000927, act_lr=2.8e-7, ent=1.68]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=0.0855, ret=-0.00197, glen=103, tlen=264, kl=0.000927, act_lr=2.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.00314, ret=0.000385, glen=118, tlen=279, kl=0.000952, act_lr=2.8e-7, ent=1.8]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.00314, ret=0.000385, glen=118, tlen=279, kl=0.000952, act_lr=2.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.042, ret=0.00056, glen=106, tlen=267, kl=0.000923, act_lr=2.8e-7, ent=1.86]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:39<00:09,  1.17it/s, pg=-0.042, ret=0.00056, glen=106, tlen=267, kl=0.000923, act_lr=2.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.17it/s, pg=0.13, ret=-0.00136, glen=115, tlen=276, kl=0.000901, act_lr=2.8e-7, ent=1.8]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.16it/s, pg=0.13, ret=-0.00136, glen=115, tlen=276, kl=0.000901, act_lr=2.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.16it/s, pg=0.144, ret=-0.00118, glen=116, tlen=276, kl=0.000937, act_lr=2.8e-7, ent=1.87]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.16it/s, pg=0.144, ret=-0.00118, glen=116, tlen=276, kl=0.000937, act_lr=2.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.16it/s, pg=0.0873, ret=-0.000717, glen=103, tlen=263, kl=0.000916, act_lr=2.8e-7, ent=1.71]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.15it/s, pg=0.0873, ret=-0.000717, glen=103, tlen=263, kl=0.000916, act_lr=2.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.15it/s, pg=-0.025, ret=-0.000403, glen=129, tlen=290, kl=0.000954, act_lr=2.8e-7, ent=1.8] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:06,  1.16it/s, pg=-0.025, ret=-0.000403, glen=129, tlen=290, kl=0.000954, act_lr=2.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:06,  1.16it/s, pg=-0.186, ret=0.0021, glen=104, tlen=265, kl=0.000926, act_lr=2.8e-7, ent=1.69]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.16it/s, pg=-0.186, ret=0.0021, glen=104, tlen=265, kl=0.000926, act_lr=2.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.16it/s, pg=-0.0696, ret=0.00217, glen=161, tlen=321, kl=0.000904, act_lr=2.8e-7, ent=2.2]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.16it/s, pg=-0.0696, ret=0.00217, glen=161, tlen=321, kl=0.000904, act_lr=2.8e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.16it/s, pg=-0.0432, ret=3.95e-5, glen=114, tlen=275, kl=0.000886, act_lr=2.8e-7, ent=1.61]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.0432, ret=3.95e-5, glen=114, tlen=275, kl=0.000886, act_lr=2.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.0375, ret=-0.00102, glen=106, tlen=266, kl=0.000924, act_lr=2.8e-7, ent=1.66]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.0375, ret=-0.00102, glen=106, tlen=266, kl=0.000924, act_lr=2.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.00479, ret=0.000673, glen=110, tlen=271, kl=0.000951, act_lr=2.8e-7, ent=1.95]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.00479, ret=0.000673, glen=110, tlen=271, kl=0.000951, act_lr=2.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=-0.174, ret=0.002, glen=104, tlen=264, kl=0.00096, act_lr=2.8e-7, ent=1.74]      Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=-0.174, ret=0.002, glen=104, tlen=264, kl=0.00096, act_lr=2.8e-7, ent=1.74]
2025-07-23 12:43:43.294 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.80s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=-0.171, ret=0.000327, glen=121, tlen=281, kl=0.000888, act_lr=3e-7, ent=1.89]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=-0.171, ret=0.000327, glen=121, tlen=281, kl=0.000888, act_lr=3e-7, ent=1.89]
2025-07-23 12:43:44.177 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-23 12:43:46.700 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-23 12:43:47.020 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.64s
2025-07-23 12:43:47.026 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.007938719632332785, 'actor_lr': 2.8035087221692183e-07, 'clip_ratio': 0.0, 'entropy': 1.7996321142765515, 'kl': 0.0009130511367530154, 'response_length': 112.06025160404674, 'total_length': 272.7323340700384, 'return': -8.793330236681198e-06, 'policy_update_steps': 1.0}

Episode [2/20]:  15%|‚ñà‚ñå        | 2/13 [07:09<38:19, 209.01s/it][A2025-07-23 12:43:47.058 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:46:24.772 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:46:24.957 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:46:24.958 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 157.90s
2025-07-23 12:46:27.041 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0082,avg_pass_at_n: 1.0000,avg_num_tokens: 112.7194,std_num_tokens: 195.4070,avg_correct_num_tokens: 91.9465,std_correct_num_tokens: 105.8674,avg_incorrect_num_tokens: 124.3108,std_incorrect_num_tokens: 229.9164
2025-07-23 12:46:27.510 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.55s
2025-07-23 12:46:29.087 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.58s
2025-07-23 12:46:58.082 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 228
2025-07-23 12:46:58.082 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.99s
2025-07-23 12:46:58.969 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.88s
2025-07-23 12:46:58.970 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0006712520583900378, avg_kl: 0.0009731200703403406, avg_response_length: 121.28374886094478, avg_orm_score: 0.0, avg_custom_rewards: 0.0006712520583900378
2025-07-23 12:46:59.011 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter15_replay_buffer.jsonl
2025-07-23 12:47:00.703 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.69s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s, pg=0.0365, ret=-0.00121, glen=99.5, tlen=260, kl=0.000976, act_lr=3e-7, ent=1.71]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:00<00:55,  1.02it/s, pg=0.0365, ret=-0.00121, glen=99.5, tlen=260, kl=0.000976, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:55,  1.02it/s, pg=-0.191, ret=-0.000426, glen=153, tlen=314, kl=0.000937, act_lr=3e-7, ent=2.15]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:52,  1.04it/s, pg=-0.191, ret=-0.000426, glen=153, tlen=314, kl=0.000937, act_lr=3e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:52,  1.04it/s, pg=0.0865, ret=-0.00198, glen=375, tlen=535, kl=0.000792, act_lr=3e-7, ent=3.02] Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:52,  1.03it/s, pg=0.0865, ret=-0.00198, glen=375, tlen=535, kl=0.000792, act_lr=3e-7, ent=3.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:52,  1.03it/s, pg=-0.0899, ret=0.00111, glen=122, tlen=282, kl=0.000942, act_lr=3e-7, ent=1.54]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:49,  1.08it/s, pg=-0.0899, ret=0.00111, glen=122, tlen=282, kl=0.000942, act_lr=3e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:49,  1.08it/s, pg=0.00626, ret=-0.000959, glen=124, tlen=285, kl=0.000976, act_lr=3e-7, ent=1.83]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.11it/s, pg=0.00626, ret=-0.000959, glen=124, tlen=285, kl=0.000976, act_lr=3e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.11it/s, pg=-0.025, ret=-0.000451, glen=112, tlen=272, kl=0.000952, act_lr=3e-7, ent=1.79] Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:45,  1.12it/s, pg=-0.025, ret=-0.000451, glen=112, tlen=272, kl=0.000952, act_lr=3e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:45,  1.12it/s, pg=0.0264, ret=-0.000234, glen=140, tlen=301, kl=0.000958, act_lr=3e-7, ent=1.96]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:44,  1.12it/s, pg=0.0264, ret=-0.000234, glen=140, tlen=301, kl=0.000958, act_lr=3e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:44,  1.12it/s, pg=0.085, ret=-0.000997, glen=102, tlen=262, kl=0.00102, act_lr=3e-7, ent=1.77]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.14it/s, pg=0.085, ret=-0.000997, glen=102, tlen=262, kl=0.00102, act_lr=3e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.14it/s, pg=-0.121, ret=0.00149, glen=109, tlen=270, kl=0.000999, act_lr=3e-7, ent=1.68]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:42,  1.14it/s, pg=-0.121, ret=0.00149, glen=109, tlen=270, kl=0.000999, act_lr=3e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:09<00:42,  1.14it/s, pg=-0.104, ret=0.002, glen=144, tlen=304, kl=0.000897, act_lr=3e-7, ent=1.59]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.104, ret=0.002, glen=144, tlen=304, kl=0.000897, act_lr=3e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:41,  1.14it/s, pg=-0.181, ret=0.00139, glen=101, tlen=261, kl=0.000958, act_lr=3e-7, ent=1.8]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:40,  1.15it/s, pg=-0.181, ret=0.00139, glen=101, tlen=261, kl=0.000958, act_lr=3e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:40,  1.15it/s, pg=0.146, ret=-0.00131, glen=106, tlen=266, kl=0.00101, act_lr=3e-7, ent=1.83]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=0.146, ret=-0.00131, glen=106, tlen=266, kl=0.00101, act_lr=3e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=-0.00433, ret=-0.000253, glen=115, tlen=276, kl=0.000975, act_lr=3e-7, ent=2.07]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.16it/s, pg=-0.00433, ret=-0.000253, glen=115, tlen=276, kl=0.000975, act_lr=3e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.16it/s, pg=-0.0258, ret=-0.000111, glen=127, tlen=287, kl=0.000973, act_lr=3e-7, ent=2.15] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.16it/s, pg=-0.0258, ret=-0.000111, glen=127, tlen=287, kl=0.000973, act_lr=3e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.16it/s, pg=0.0605, ret=-0.00054, glen=110, tlen=270, kl=0.000993, act_lr=3e-7, ent=1.75]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0605, ret=-0.00054, glen=110, tlen=270, kl=0.000993, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:14<00:35,  1.17it/s, pg=-0.0134, ret=0.00144, glen=123, tlen=283, kl=0.000971, act_lr=3e-7, ent=1.99]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.0134, ret=0.00144, glen=123, tlen=283, kl=0.000971, act_lr=3e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:35,  1.17it/s, pg=-0.212, ret=0.00158, glen=104, tlen=264, kl=0.00101, act_lr=3e-7, ent=1.87]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.17it/s, pg=-0.212, ret=0.00158, glen=104, tlen=264, kl=0.00101, act_lr=3e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.17it/s, pg=-0.166, ret=0.00101, glen=98.2, tlen=258, kl=0.000999, act_lr=3e-7, ent=1.92]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.17it/s, pg=-0.166, ret=0.00101, glen=98.2, tlen=258, kl=0.000999, act_lr=3e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.17it/s, pg=0.13, ret=0.000334, glen=177, tlen=337, kl=0.000801, act_lr=3e-7, ent=1.61]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:33,  1.14it/s, pg=0.13, ret=0.000334, glen=177, tlen=337, kl=0.000801, act_lr=3e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:33,  1.14it/s, pg=0.0312, ret=-7.62e-6, glen=98.5, tlen=259, kl=0.000954, act_lr=3e-7, ent=1.92]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:32,  1.15it/s, pg=0.0312, ret=-7.62e-6, glen=98.5, tlen=259, kl=0.000954, act_lr=3e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:32,  1.15it/s, pg=-0.065, ret=0.00112, glen=107, tlen=267, kl=0.000931, act_lr=3e-7, ent=1.68]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:31,  1.16it/s, pg=-0.065, ret=0.00112, glen=107, tlen=267, kl=0.000931, act_lr=3e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:31,  1.16it/s, pg=-0.000244, ret=9.65e-5, glen=109, tlen=270, kl=0.000992, act_lr=3e-7, ent=1.8]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=-0.000244, ret=9.65e-5, glen=109, tlen=270, kl=0.000992, act_lr=3e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.16it/s, pg=0.0865, ret=-0.00123, glen=118, tlen=278, kl=0.000984, act_lr=3e-7, ent=1.77] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.15it/s, pg=0.0865, ret=-0.00123, glen=118, tlen=278, kl=0.000984, act_lr=3e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:21<00:29,  1.15it/s, pg=-0.0128, ret=0.000316, glen=110, tlen=270, kl=0.00106, act_lr=3e-7, ent=1.79]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.16it/s, pg=-0.0128, ret=0.000316, glen=110, tlen=270, kl=0.00106, act_lr=3e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.16it/s, pg=-0.0781, ret=-0.000814, glen=119, tlen=280, kl=0.000992, act_lr=3e-7, ent=1.84]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.16it/s, pg=-0.0781, ret=-0.000814, glen=119, tlen=280, kl=0.000992, act_lr=3e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.16it/s, pg=-0.106, ret=0.000212, glen=97.2, tlen=257, kl=0.00107, act_lr=3e-7, ent=1.77]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=-0.106, ret=0.000212, glen=97.2, tlen=257, kl=0.00107, act_lr=3e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.0511, ret=-0.000205, glen=103, tlen=263, kl=0.001, act_lr=3e-7, ent=1.75] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:28,  1.07it/s, pg=-0.0511, ret=-0.000205, glen=103, tlen=263, kl=0.001, act_lr=3e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.07it/s, pg=0.00403, ret=-0.000713, glen=117, tlen=277, kl=0.000973, act_lr=3e-7, ent=2.01]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.09it/s, pg=0.00403, ret=-0.000713, glen=117, tlen=277, kl=0.000973, act_lr=3e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.09it/s, pg=0.0299, ret=-0.000382, glen=113, tlen=274, kl=0.000996, act_lr=3e-7, ent=1.62] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:25,  1.12it/s, pg=0.0299, ret=-0.000382, glen=113, tlen=274, kl=0.000996, act_lr=3e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:25,  1.12it/s, pg=-0.161, ret=0.0015, glen=108, tlen=269, kl=0.000976, act_lr=3e-7, ent=1.72]   Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.13it/s, pg=-0.161, ret=0.0015, glen=108, tlen=269, kl=0.000976, act_lr=3e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.13it/s, pg=0.249, ret=-0.00167, glen=126, tlen=287, kl=0.000947, act_lr=3e-7, ent=1.83]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=0.249, ret=-0.00167, glen=126, tlen=287, kl=0.000947, act_lr=3e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:22,  1.15it/s, pg=-0.149, ret=0.00251, glen=102, tlen=263, kl=0.00105, act_lr=3e-7, ent=1.74] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.16it/s, pg=-0.149, ret=0.00251, glen=102, tlen=263, kl=0.00105, act_lr=3e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:21,  1.16it/s, pg=0.126, ret=-0.000957, glen=107, tlen=268, kl=0.000956, act_lr=3e-7, ent=2.12]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:20,  1.16it/s, pg=0.126, ret=-0.000957, glen=107, tlen=268, kl=0.000956, act_lr=3e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:20,  1.16it/s, pg=0.215, ret=-0.00138, glen=142, tlen=303, kl=0.000927, act_lr=3e-7, ent=1.76] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:19,  1.16it/s, pg=0.215, ret=-0.00138, glen=142, tlen=303, kl=0.000927, act_lr=3e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:19,  1.16it/s, pg=0.062, ret=-0.000263, glen=105, tlen=265, kl=0.00101, act_lr=3e-7, ent=1.73]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:18,  1.16it/s, pg=0.062, ret=-0.000263, glen=105, tlen=265, kl=0.00101, act_lr=3e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:18,  1.16it/s, pg=0.139, ret=-0.00256, glen=165, tlen=325, kl=0.000868, act_lr=3e-7, ent=1.38]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=0.139, ret=-0.00256, glen=165, tlen=325, kl=0.000868, act_lr=3e-7, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.0917, ret=0.000345, glen=95.9, tlen=256, kl=0.000969, act_lr=3e-7, ent=1.77]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.0917, ret=0.000345, glen=95.9, tlen=256, kl=0.000969, act_lr=3e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=0.277, ret=-0.00236, glen=164, tlen=325, kl=0.000838, act_lr=3e-7, ent=1.66]   Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.15it/s, pg=0.277, ret=-0.00236, glen=164, tlen=325, kl=0.000838, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.15it/s, pg=0.0743, ret=-0.000139, glen=121, tlen=282, kl=0.000886, act_lr=3e-7, ent=1.49]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.16it/s, pg=0.0743, ret=-0.000139, glen=121, tlen=282, kl=0.000886, act_lr=3e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:35<00:15,  1.16it/s, pg=-0.185, ret=0.000729, glen=105, tlen=266, kl=0.000998, act_lr=3e-7, ent=1.62] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.16it/s, pg=-0.185, ret=0.000729, glen=105, tlen=266, kl=0.000998, act_lr=3e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.16it/s, pg=-0.155, ret=0.00196, glen=108, tlen=268, kl=0.000996, act_lr=3e-7, ent=1.67] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=-0.155, ret=0.00196, glen=108, tlen=268, kl=0.000996, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0693, ret=-0.00106, glen=102, tlen=262, kl=0.00101, act_lr=3e-7, ent=1.71]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:12,  1.17it/s, pg=0.0693, ret=-0.00106, glen=102, tlen=262, kl=0.00101, act_lr=3e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:12,  1.17it/s, pg=0.131, ret=-0.00072, glen=138, tlen=299, kl=0.000979, act_lr=3e-7, ent=1.94]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:11,  1.17it/s, pg=0.131, ret=-0.00072, glen=138, tlen=299, kl=0.000979, act_lr=3e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:11,  1.17it/s, pg=-0.221, ret=0.00318, glen=102, tlen=262, kl=0.00101, act_lr=3e-7, ent=1.67] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.17it/s, pg=-0.221, ret=0.00318, glen=102, tlen=262, kl=0.00101, act_lr=3e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.17it/s, pg=-0.101, ret=0.000437, glen=104, tlen=265, kl=0.00103, act_lr=3e-7, ent=1.66]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.17it/s, pg=-0.101, ret=0.000437, glen=104, tlen=265, kl=0.00103, act_lr=3e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.17it/s, pg=-0.016, ret=0.000379, glen=98.1, tlen=259, kl=0.00102, act_lr=3e-7, ent=1.62]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.016, ret=0.000379, glen=98.1, tlen=259, kl=0.00102, act_lr=3e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.18it/s, pg=-0.153, ret=0.0017, glen=104, tlen=265, kl=0.00105, act_lr=3e-7, ent=1.72]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.18it/s, pg=-0.153, ret=0.0017, glen=104, tlen=265, kl=0.00105, act_lr=3e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.18it/s, pg=0.01, ret=-0.000342, glen=110, tlen=271, kl=0.00105, act_lr=3e-7, ent=1.79]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.18it/s, pg=0.01, ret=-0.000342, glen=110, tlen=271, kl=0.00105, act_lr=3e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.18it/s, pg=-0.127, ret=0.000948, glen=98, tlen=259, kl=0.00101, act_lr=3e-7, ent=1.63]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=-0.127, ret=0.000948, glen=98, tlen=259, kl=0.00101, act_lr=3e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=0.183, ret=-0.00146, glen=107, tlen=268, kl=0.000977, act_lr=3e-7, ent=1.64]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=0.183, ret=-0.00146, glen=107, tlen=268, kl=0.000977, act_lr=3e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.147, ret=0.000431, glen=109, tlen=269, kl=0.000976, act_lr=3e-7, ent=1.63]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.18it/s, pg=-0.147, ret=0.000431, glen=109, tlen=269, kl=0.000976, act_lr=3e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.18it/s, pg=-0.0334, ret=0.000181, glen=106, tlen=266, kl=0.001, act_lr=3e-7, ent=1.85]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.18it/s, pg=-0.0334, ret=0.000181, glen=106, tlen=266, kl=0.001, act_lr=3e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.18it/s, pg=-0.178, ret=-6.05e-5, glen=98.6, tlen=259, kl=0.00101, act_lr=3e-7, ent=1.61]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=-0.178, ret=-6.05e-5, glen=98.6, tlen=259, kl=0.00101, act_lr=3e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.18it/s, pg=0.0319, ret=-0.000263, glen=213, tlen=373, kl=0.000817, act_lr=3e-7, ent=2.22]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.16it/s, pg=0.0319, ret=-0.000263, glen=213, tlen=373, kl=0.000817, act_lr=3e-7, ent=2.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.16it/s, pg=0.222, ret=-0.00178, glen=110, tlen=271, kl=0.00103, act_lr=3e-7, ent=1.7]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.15it/s, pg=0.222, ret=-0.00178, glen=110, tlen=271, kl=0.00103, act_lr=3e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.15it/s, pg=-0.133, ret=0.00055, glen=111, tlen=271, kl=0.000977, act_lr=3e-7, ent=1.81]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.06it/s, pg=-0.133, ret=0.00055, glen=111, tlen=271, kl=0.000977, act_lr=3e-7, ent=1.81]
2025-07-23 12:47:50.786 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.93s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.06it/s, pg=-0.000275, ret=0.00175, glen=118, tlen=278, kl=0.000978, act_lr=3.2e-7, ent=2.17]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.12it/s, pg=-0.000275, ret=0.00175, glen=118, tlen=278, kl=0.000978, act_lr=3.2e-7, ent=2.17]
2025-07-23 12:47:51.649 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-23 12:47:54.164 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-23 12:47:54.476 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.73s
2025-07-23 12:47:54.482 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.013730249906841078, 'actor_lr': 3.003508875839575e-07, 'clip_ratio': 0.0, 'entropy': 1.8044050492738422, 'kl': 0.0009731200703403406, 'response_length': 121.28374949672767, 'total_length': 281.68362587376646, 'return': 3.266448804520463e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:17<37:45, 226.56s/it][A2025-07-23 12:47:54.516 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:50:34.475 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:50:34.662 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:50:34.663 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 160.15s
2025-07-23 12:50:36.623 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0116,avg_pass_at_n: 1.0000,avg_num_tokens: 106.4906,std_num_tokens: 145.0967,avg_correct_num_tokens: 90.4660,std_correct_num_tokens: 83.4086,avg_incorrect_num_tokens: 116.4248,std_incorrect_num_tokens: 171.8540
2025-07-23 12:50:37.061 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.40s
2025-07-23 12:50:38.602 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.54s
2025-07-23 12:51:07.132 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 222
2025-07-23 12:51:07.133 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.53s
2025-07-23 12:51:07.959 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.82s
2025-07-23 12:51:07.960 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00039985112747311724, avg_kl: 0.0010422285612639007, avg_response_length: 110.55806371327994, avg_orm_score: 0.0, avg_custom_rewards: -0.00039985112747311724
2025-07-23 12:51:08.019 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter16_replay_buffer.jsonl
2025-07-23 12:51:09.600 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.58s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.00732, ret=7.73e-5, glen=97.9, tlen=258, kl=0.00101, act_lr=3.2e-7, ent=1.67]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=-0.00732, ret=7.73e-5, glen=97.9, tlen=258, kl=0.00101, act_lr=3.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.01s/it, pg=0.174, ret=-0.000976, glen=106, tlen=266, kl=0.000973, act_lr=3.2e-7, ent=1.8]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.09it/s, pg=0.174, ret=-0.000976, glen=106, tlen=266, kl=0.000973, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.09it/s, pg=-0.0477, ret=0.000575, glen=101, tlen=262, kl=0.00101, act_lr=3.2e-7, ent=1.61]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:48,  1.10it/s, pg=-0.0477, ret=0.000575, glen=101, tlen=262, kl=0.00101, act_lr=3.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:48,  1.10it/s, pg=-0.0232, ret=0.000596, glen=104, tlen=265, kl=0.00108, act_lr=3.2e-7, ent=1.72]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:47,  1.11it/s, pg=-0.0232, ret=0.000596, glen=104, tlen=265, kl=0.00108, act_lr=3.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:47,  1.11it/s, pg=0.0134, ret=0.000579, glen=113, tlen=272, kl=0.00104, act_lr=3.2e-7, ent=1.95] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.13it/s, pg=0.0134, ret=0.000579, glen=113, tlen=272, kl=0.00104, act_lr=3.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.13it/s, pg=0.152, ret=-0.00062, glen=127, tlen=287, kl=0.000958, act_lr=3.2e-7, ent=2.15]Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:43,  1.14it/s, pg=0.152, ret=-0.00062, glen=127, tlen=287, kl=0.000958, act_lr=3.2e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:43,  1.14it/s, pg=0.0527, ret=7.94e-5, glen=101, tlen=261, kl=0.00109, act_lr=3.2e-7, ent=1.71] Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=0.0527, ret=7.94e-5, glen=101, tlen=261, kl=0.00109, act_lr=3.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=0.0292, ret=-0.000174, glen=107, tlen=267, kl=0.00108, act_lr=3.2e-7, ent=1.75]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.14it/s, pg=0.0292, ret=-0.000174, glen=107, tlen=267, kl=0.00108, act_lr=3.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.14it/s, pg=-0.134, ret=0.0012, glen=125, tlen=285, kl=0.000972, act_lr=3.2e-7, ent=2.3]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:07<00:40,  1.15it/s, pg=-0.134, ret=0.0012, glen=125, tlen=285, kl=0.000972, act_lr=3.2e-7, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.15it/s, pg=-0.19, ret=0.00288, glen=96.3, tlen=256, kl=0.00104, act_lr=3.2e-7, ent=1.54]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:40,  1.14it/s, pg=-0.19, ret=0.00288, glen=96.3, tlen=256, kl=0.00104, act_lr=3.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:40,  1.14it/s, pg=0.134, ret=-0.00139, glen=131, tlen=291, kl=0.001, act_lr=3.2e-7, ent=2.05]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:39,  1.15it/s, pg=0.134, ret=-0.00139, glen=131, tlen=291, kl=0.001, act_lr=3.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:39,  1.15it/s, pg=0.188, ret=-0.000722, glen=130, tlen=289, kl=0.00104, act_lr=3.2e-7, ent=2]Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:38,  1.15it/s, pg=0.188, ret=-0.000722, glen=130, tlen=289, kl=0.00104, act_lr=3.2e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:38,  1.15it/s, pg=-0.0415, ret=0.000924, glen=108, tlen=268, kl=0.00101, act_lr=3.2e-7, ent=1.82]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:37,  1.16it/s, pg=-0.0415, ret=0.000924, glen=108, tlen=268, kl=0.00101, act_lr=3.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:37,  1.16it/s, pg=-0.0458, ret=-0.00118, glen=121, tlen=281, kl=0.00105, act_lr=3.2e-7, ent=1.83]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:36,  1.16it/s, pg=-0.0458, ret=-0.00118, glen=121, tlen=281, kl=0.00105, act_lr=3.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:36,  1.16it/s, pg=0.0244, ret=-0.000146, glen=110, tlen=270, kl=0.00105, act_lr=3.2e-7, ent=1.76]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.16it/s, pg=0.0244, ret=-0.000146, glen=110, tlen=270, kl=0.00105, act_lr=3.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.16it/s, pg=-0.116, ret=-0.000151, glen=107, tlen=267, kl=0.00101, act_lr=3.2e-7, ent=1.75]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=-0.116, ret=-0.000151, glen=107, tlen=267, kl=0.00101, act_lr=3.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.17it/s, pg=0.0703, ret=-0.000526, glen=93, tlen=254, kl=0.00108, act_lr=3.2e-7, ent=1.58] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.17it/s, pg=0.0703, ret=-0.000526, glen=93, tlen=254, kl=0.00108, act_lr=3.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.17it/s, pg=0.157, ret=-0.00184, glen=109, tlen=269, kl=0.00102, act_lr=3.2e-7, ent=1.76] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.17it/s, pg=0.157, ret=-0.00184, glen=109, tlen=269, kl=0.00102, act_lr=3.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.17it/s, pg=-0.0172, ret=-0.00055, glen=107, tlen=267, kl=0.00108, act_lr=3.2e-7, ent=1.72]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0172, ret=-0.00055, glen=107, tlen=267, kl=0.00108, act_lr=3.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.105, ret=-0.00165, glen=105, tlen=265, kl=0.0011, act_lr=3.2e-7, ent=1.71]   Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:30,  1.17it/s, pg=0.105, ret=-0.00165, glen=105, tlen=265, kl=0.0011, act_lr=3.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:30,  1.17it/s, pg=0.0617, ret=-0.00103, glen=91, tlen=251, kl=0.00106, act_lr=3.2e-7, ent=1.6]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:29,  1.17it/s, pg=0.0617, ret=-0.00103, glen=91, tlen=251, kl=0.00106, act_lr=3.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:29,  1.17it/s, pg=-0.0878, ret=0.000915, glen=98.8, tlen=259, kl=0.00104, act_lr=3.2e-7, ent=1.73]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.0878, ret=0.000915, glen=98.8, tlen=259, kl=0.00104, act_lr=3.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.17it/s, pg=-0.0977, ret=0.000161, glen=104, tlen=263, kl=0.00102, act_lr=3.2e-7, ent=1.68] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:19<00:28,  1.17it/s, pg=-0.0977, ret=0.000161, glen=104, tlen=263, kl=0.00102, act_lr=3.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.17it/s, pg=-0.223, ret=0.00177, glen=106, tlen=266, kl=0.00105, act_lr=3.2e-7, ent=1.7]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.17it/s, pg=-0.223, ret=0.00177, glen=106, tlen=266, kl=0.00105, act_lr=3.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.17it/s, pg=0.14, ret=-0.000967, glen=109, tlen=270, kl=0.00104, act_lr=3.2e-7, ent=1.76]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.15it/s, pg=0.14, ret=-0.000967, glen=109, tlen=270, kl=0.00104, act_lr=3.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.15it/s, pg=0.0104, ret=-0.000831, glen=111, tlen=271, kl=0.00106, act_lr=3.2e-7, ent=1.79]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.16it/s, pg=0.0104, ret=-0.000831, glen=111, tlen=271, kl=0.00106, act_lr=3.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.16it/s, pg=-0.0992, ret=-0.00053, glen=108, tlen=267, kl=0.00109, act_lr=3.2e-7, ent=1.71]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:27,  1.07it/s, pg=-0.0992, ret=-0.00053, glen=108, tlen=267, kl=0.00109, act_lr=3.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:27,  1.07it/s, pg=-0.05, ret=1.39e-5, glen=114, tlen=274, kl=0.00107, act_lr=3.2e-7, ent=1.84]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:25,  1.10it/s, pg=-0.05, ret=1.39e-5, glen=114, tlen=274, kl=0.00107, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:25,  1.10it/s, pg=-0.142, ret=-0.00872, glen=91.2, tlen=251, kl=0.00113, act_lr=3.2e-7, ent=1.7]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:24,  1.12it/s, pg=-0.142, ret=-0.00872, glen=91.2, tlen=251, kl=0.00113, act_lr=3.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:24,  1.12it/s, pg=-0.0205, ret=-0.000849, glen=99.1, tlen=259, kl=0.00102, act_lr=3.2e-7, ent=1.66]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:22,  1.13it/s, pg=-0.0205, ret=-0.000849, glen=99.1, tlen=259, kl=0.00102, act_lr=3.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:22,  1.13it/s, pg=-0.186, ret=0.00129, glen=94.2, tlen=255, kl=0.00104, act_lr=3.2e-7, ent=1.52]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:21,  1.15it/s, pg=-0.186, ret=0.00129, glen=94.2, tlen=255, kl=0.00104, act_lr=3.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:21,  1.15it/s, pg=0.117, ret=-0.00231, glen=102, tlen=263, kl=0.00106, act_lr=3.2e-7, ent=1.62] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:27<00:20,  1.16it/s, pg=0.117, ret=-0.00231, glen=102, tlen=263, kl=0.00106, act_lr=3.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:20,  1.16it/s, pg=-0.131, ret=0.000829, glen=101, tlen=261, kl=0.00108, act_lr=3.2e-7, ent=1.75]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:19,  1.16it/s, pg=-0.131, ret=0.000829, glen=101, tlen=261, kl=0.00108, act_lr=3.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:19,  1.16it/s, pg=0.0115, ret=0.000166, glen=114, tlen=275, kl=0.00104, act_lr=3.2e-7, ent=1.7] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:18,  1.16it/s, pg=0.0115, ret=0.000166, glen=114, tlen=275, kl=0.00104, act_lr=3.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:18,  1.16it/s, pg=0.0399, ret=0.000245, glen=123, tlen=283, kl=0.00103, act_lr=3.2e-7, ent=2.01]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=0.0399, ret=0.000245, glen=123, tlen=283, kl=0.00103, act_lr=3.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.0542, ret=-0.000451, glen=120, tlen=281, kl=0.00103, act_lr=3.2e-7, ent=2.01]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.0542, ret=-0.000451, glen=120, tlen=281, kl=0.00103, act_lr=3.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.0394, ret=1.57e-5, glen=114, tlen=274, kl=0.00102, act_lr=3.2e-7, ent=1.89] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.0394, ret=1.57e-5, glen=114, tlen=274, kl=0.00102, act_lr=3.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=-0.0747, ret=-0.00111, glen=116, tlen=276, kl=0.00106, act_lr=3.2e-7, ent=1.85]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.0747, ret=-0.00111, glen=116, tlen=276, kl=0.00106, act_lr=3.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=0.0997, ret=-0.000986, glen=117, tlen=277, kl=0.00105, act_lr=3.2e-7, ent=2.11]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:33<00:14,  1.16it/s, pg=0.0997, ret=-0.000986, glen=117, tlen=277, kl=0.00105, act_lr=3.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.16it/s, pg=-0.0459, ret=-0.000548, glen=99.7, tlen=260, kl=0.00106, act_lr=3.2e-7, ent=1.63]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=-0.0459, ret=-0.000548, glen=99.7, tlen=260, kl=0.00106, act_lr=3.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.000488, ret=0.00036, glen=95.8, tlen=256, kl=0.0011, act_lr=3.2e-7, ent=1.89] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.000488, ret=0.00036, glen=95.8, tlen=256, kl=0.0011, act_lr=3.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.078, ret=0.000764, glen=112, tlen=272, kl=0.00106, act_lr=3.2e-7, ent=1.82]   Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.078, ret=0.000764, glen=112, tlen=272, kl=0.00106, act_lr=3.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.00385, ret=-0.000611, glen=115, tlen=275, kl=0.00102, act_lr=3.2e-7, ent=1.72]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.00385, ret=-0.000611, glen=115, tlen=275, kl=0.00102, act_lr=3.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=0.0137, ret=0.00026, glen=98.1, tlen=258, kl=0.00111, act_lr=3.2e-7, ent=1.69]   Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=0.0137, ret=0.00026, glen=98.1, tlen=258, kl=0.00111, act_lr=3.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0814, ret=0.00122, glen=113, tlen=273, kl=0.00106, act_lr=3.2e-7, ent=1.73]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0814, ret=0.00122, glen=113, tlen=273, kl=0.00106, act_lr=3.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0742, ret=0.000604, glen=90.7, tlen=251, kl=0.00105, act_lr=3.2e-7, ent=1.76]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:39<00:08,  1.17it/s, pg=-0.0742, ret=0.000604, glen=90.7, tlen=251, kl=0.00105, act_lr=3.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.0436, ret=0.000342, glen=115, tlen=275, kl=0.00104, act_lr=3.2e-7, ent=1.8]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=-0.0436, ret=0.000342, glen=115, tlen=275, kl=0.00104, act_lr=3.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.108, ret=0.000282, glen=95.6, tlen=256, kl=0.00104, act_lr=3.2e-7, ent=1.68]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.17it/s, pg=-0.108, ret=0.000282, glen=95.6, tlen=256, kl=0.00104, act_lr=3.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.17it/s, pg=-0.0773, ret=0.000288, glen=98.9, tlen=259, kl=0.00104, act_lr=3.2e-7, ent=1.81]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.17it/s, pg=-0.0773, ret=0.000288, glen=98.9, tlen=259, kl=0.00104, act_lr=3.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.17it/s, pg=-0.00839, ret=0.000465, glen=107, tlen=267, kl=0.00097, act_lr=3.2e-7, ent=1.69]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.17it/s, pg=-0.00839, ret=0.000465, glen=107, tlen=267, kl=0.00097, act_lr=3.2e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.17it/s, pg=0.0437, ret=0.000634, glen=108, tlen=269, kl=0.00107, act_lr=3.2e-7, ent=1.77]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.17it/s, pg=0.0437, ret=0.000634, glen=108, tlen=269, kl=0.00107, act_lr=3.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.17it/s, pg=0.192, ret=-0.00115, glen=106, tlen=267, kl=0.00103, act_lr=3.2e-7, ent=1.61] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=0.192, ret=-0.00115, glen=106, tlen=267, kl=0.00103, act_lr=3.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.262, ret=0.00244, glen=90.2, tlen=250, kl=0.00104, act_lr=3.2e-7, ent=1.74]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:45<00:02,  1.17it/s, pg=-0.262, ret=0.00244, glen=90.2, tlen=250, kl=0.00104, act_lr=3.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0551, ret=-0.000348, glen=105, tlen=265, kl=0.0011, act_lr=3.2e-7, ent=1.84]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0551, ret=-0.000348, glen=105, tlen=265, kl=0.0011, act_lr=3.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=0.0603, ret=0.00235, glen=305, tlen=464, kl=0.000888, act_lr=3.2e-7, ent=1.34] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.14it/s, pg=0.0603, ret=0.00235, glen=305, tlen=464, kl=0.000888, act_lr=3.2e-7, ent=1.34]
2025-07-23 12:51:58.651 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 48.89s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.14it/s, pg=0.125, ret=-0.00162, glen=102, tlen=263, kl=0.00105, act_lr=3.4e-7, ent=1.62] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.13it/s, pg=0.125, ret=-0.00162, glen=102, tlen=263, kl=0.00105, act_lr=3.4e-7, ent=1.62]
2025-07-23 12:51:59.509 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 12:52:02.069 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-23 12:52:02.374 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 52.73s
2025-07-23 12:52:02.380 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.006891131401062012, 'actor_lr': 3.203571411956675e-07, 'clip_ratio': 0.0, 'entropy': 1.7667705097368784, 'kl': 0.0010427321706499373, 'response_length': 110.50852080753872, 'total_length': 270.6878828321184, 'return': -0.00017275203884180103, 'policy_update_steps': 1.0}

Episode [2/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [15:25<35:14, 234.99s/it][A2025-07-23 12:52:02.412 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:54:40.773 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:54:40.962 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 12:54:40.962 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 158.55s
2025-07-23 12:54:43.127 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0112,avg_pass_at_n: 1.0000,avg_num_tokens: 110.8601,std_num_tokens: 185.4644,avg_correct_num_tokens: 93.9926,std_correct_num_tokens: 84.1977,avg_incorrect_num_tokens: 120.3727,std_incorrect_num_tokens: 222.5904
2025-07-23 12:54:43.555 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.59s
2025-07-23 12:54:45.119 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.56s
2025-07-23 12:55:14.115 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 226
2025-07-23 12:55:14.115 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.99s
2025-07-23 12:55:14.962 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.84s
2025-07-23 12:55:14.962 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0004469473092647813, avg_kl: 0.0010184123452785796, avg_response_length: 121.46447645879425, avg_orm_score: 0.0, avg_custom_rewards: 0.0004469473092647813
2025-07-23 12:55:14.997 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter17_replay_buffer.jsonl
2025-07-23 12:55:16.648 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.65s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/57 [00:00<?, ?it/s, pg=-0.139, ret=0.000268, glen=110, tlen=270, kl=0.001, act_lr=3.4e-7, ent=1.78]Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:00<00:55,  1.01it/s, pg=-0.139, ret=0.000268, glen=110, tlen=270, kl=0.001, act_lr=3.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/57 [00:01<00:55,  1.01it/s, pg=-0.0771, ret=0.00131, glen=134, tlen=294, kl=0.000891, act_lr=3.4e-7, ent=1.71]Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:01<00:50,  1.09it/s, pg=-0.0771, ret=0.00131, glen=134, tlen=294, kl=0.000891, act_lr=3.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/57 [00:02<00:50,  1.09it/s, pg=0.0439, ret=-0.00026, glen=99.7, tlen=261, kl=0.00107, act_lr=3.4e-7, ent=1.73]Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:02<00:49,  1.09it/s, pg=0.0439, ret=-0.00026, glen=99.7, tlen=261, kl=0.00107, act_lr=3.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/57 [00:03<00:49,  1.09it/s, pg=-0.0386, ret=1.32e-5, glen=95.3, tlen=256, kl=0.00109, act_lr=3.4e-7, ent=1.87]Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:03<00:48,  1.09it/s, pg=-0.0386, ret=1.32e-5, glen=95.3, tlen=256, kl=0.00109, act_lr=3.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/57 [00:04<00:48,  1.09it/s, pg=-0.134, ret=0.000649, glen=103, tlen=263, kl=0.00105, act_lr=3.4e-7, ent=1.95] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:04<00:46,  1.12it/s, pg=-0.134, ret=0.000649, glen=103, tlen=263, kl=0.00105, act_lr=3.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/57 [00:05<00:46,  1.12it/s, pg=-0.143, ret=-7.23e-5, glen=112, tlen=272, kl=0.00103, act_lr=3.4e-7, ent=1.85]Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:05<00:44,  1.13it/s, pg=-0.143, ret=-7.23e-5, glen=112, tlen=272, kl=0.00103, act_lr=3.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/57 [00:06<00:44,  1.13it/s, pg=-0.149, ret=0.000379, glen=110, tlen=271, kl=0.00103, act_lr=3.4e-7, ent=1.73]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:06<00:43,  1.15it/s, pg=-0.149, ret=0.000379, glen=110, tlen=271, kl=0.00103, act_lr=3.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 7/57 [00:07<00:43,  1.15it/s, pg=-0.105, ret=0.00072, glen=101, tlen=262, kl=0.00105, act_lr=3.4e-7, ent=1.77] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:07<00:43,  1.14it/s, pg=-0.105, ret=0.00072, glen=101, tlen=262, kl=0.00105, act_lr=3.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/57 [00:08<00:43,  1.14it/s, pg=-0.152, ret=-1.36e-5, glen=108, tlen=269, kl=0.00103, act_lr=3.4e-7, ent=1.83]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=-0.152, ret=-1.36e-5, glen=108, tlen=269, kl=0.00103, act_lr=3.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/57 [00:08<00:41,  1.15it/s, pg=0.0371, ret=-0.000505, glen=92.3, tlen=253, kl=0.00105, act_lr=3.4e-7, ent=1.67]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:08<00:40,  1.15it/s, pg=0.0371, ret=-0.000505, glen=92.3, tlen=253, kl=0.00105, act_lr=3.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/57 [00:09<00:40,  1.15it/s, pg=0.0818, ret=-0.00126, glen=111, tlen=272, kl=0.00106, act_lr=3.4e-7, ent=1.73]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:09<00:39,  1.16it/s, pg=0.0818, ret=-0.00126, glen=111, tlen=272, kl=0.00106, act_lr=3.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 11/57 [00:10<00:39,  1.16it/s, pg=-0.0369, ret=-0.000386, glen=96.3, tlen=257, kl=0.00104, act_lr=3.4e-7, ent=1.72]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:10<00:38,  1.16it/s, pg=-0.0369, ret=-0.000386, glen=96.3, tlen=257, kl=0.00104, act_lr=3.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 12/57 [00:11<00:38,  1.16it/s, pg=0.0435, ret=-0.00097, glen=92.8, tlen=253, kl=0.00109, act_lr=3.4e-7, ent=1.69]  Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:11<00:37,  1.17it/s, pg=0.0435, ret=-0.00097, glen=92.8, tlen=253, kl=0.00109, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/57 [00:12<00:37,  1.17it/s, pg=-0.0981, ret=0.00242, glen=110, tlen=271, kl=0.00101, act_lr=3.4e-7, ent=2.09] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:12<00:36,  1.17it/s, pg=-0.0981, ret=0.00242, glen=110, tlen=271, kl=0.00101, act_lr=3.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 14/57 [00:13<00:36,  1.17it/s, pg=0.0985, ret=1.95e-5, glen=99.5, tlen=260, kl=0.00102, act_lr=3.4e-7, ent=1.86]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.0985, ret=1.95e-5, glen=99.5, tlen=260, kl=0.00102, act_lr=3.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 15/57 [00:13<00:35,  1.17it/s, pg=0.138, ret=-0.000989, glen=91.6, tlen=252, kl=0.00104, act_lr=3.4e-7, ent=1.62]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:13<00:34,  1.17it/s, pg=0.138, ret=-0.000989, glen=91.6, tlen=252, kl=0.00104, act_lr=3.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 16/57 [00:14<00:34,  1.17it/s, pg=0.104, ret=-0.000388, glen=189, tlen=350, kl=0.000846, act_lr=3.4e-7, ent=1.53]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:14<00:34,  1.15it/s, pg=0.104, ret=-0.000388, glen=189, tlen=350, kl=0.000846, act_lr=3.4e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 17/57 [00:15<00:34,  1.15it/s, pg=-0.0487, ret=-0.00053, glen=106, tlen=266, kl=0.00105, act_lr=3.4e-7, ent=1.78]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:15<00:33,  1.15it/s, pg=-0.0487, ret=-0.00053, glen=106, tlen=266, kl=0.00105, act_lr=3.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/57 [00:16<00:33,  1.15it/s, pg=-0.0787, ret=0.000895, glen=103, tlen=263, kl=0.00107, act_lr=3.4e-7, ent=1.68]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:16<00:33,  1.15it/s, pg=-0.0787, ret=0.000895, glen=103, tlen=263, kl=0.00107, act_lr=3.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 19/57 [00:17<00:33,  1.15it/s, pg=0.138, ret=-0.00253, glen=111, tlen=271, kl=0.00102, act_lr=3.4e-7, ent=1.85]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:17<00:32,  1.15it/s, pg=0.138, ret=-0.00253, glen=111, tlen=271, kl=0.00102, act_lr=3.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 20/57 [00:18<00:32,  1.15it/s, pg=-0.17, ret=0.0012, glen=116, tlen=276, kl=0.00101, act_lr=3.4e-7, ent=1.85]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:18<00:31,  1.16it/s, pg=-0.17, ret=0.0012, glen=116, tlen=276, kl=0.00101, act_lr=3.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 21/57 [00:19<00:31,  1.16it/s, pg=-0.132, ret=0.000166, glen=115, tlen=276, kl=0.00107, act_lr=3.4e-7, ent=1.9]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:19<00:30,  1.16it/s, pg=-0.132, ret=0.000166, glen=115, tlen=276, kl=0.00107, act_lr=3.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñä      | 22/57 [00:20<00:30,  1.16it/s, pg=0.0644, ret=-0.0006, glen=113, tlen=273, kl=0.00104, act_lr=3.4e-7, ent=1.9] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.16it/s, pg=0.0644, ret=-0.0006, glen=113, tlen=273, kl=0.00104, act_lr=3.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 23/57 [00:20<00:29,  1.16it/s, pg=0.24, ret=-0.000626, glen=135, tlen=296, kl=0.000994, act_lr=3.4e-7, ent=2.17]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:20<00:28,  1.17it/s, pg=0.24, ret=-0.000626, glen=135, tlen=296, kl=0.000994, act_lr=3.4e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 24/57 [00:21<00:28,  1.17it/s, pg=0.0225, ret=-0.000464, glen=102, tlen=262, kl=0.00107, act_lr=3.4e-7, ent=1.81]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:21<00:27,  1.17it/s, pg=0.0225, ret=-0.000464, glen=102, tlen=262, kl=0.00107, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/57 [00:22<00:27,  1.17it/s, pg=0.111, ret=-0.00147, glen=152, tlen=313, kl=0.000927, act_lr=3.4e-7, ent=1.49] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:22<00:26,  1.17it/s, pg=0.111, ret=-0.00147, glen=152, tlen=313, kl=0.000927, act_lr=3.4e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 26/57 [00:23<00:26,  1.17it/s, pg=-0.113, ret=0.000484, glen=133, tlen=293, kl=0.000986, act_lr=3.4e-7, ent=1.81]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:23<00:28,  1.07it/s, pg=-0.113, ret=0.000484, glen=133, tlen=293, kl=0.000986, act_lr=3.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 27/57 [00:24<00:28,  1.07it/s, pg=0.073, ret=-0.00117, glen=104, tlen=264, kl=0.00101, act_lr=3.4e-7, ent=1.91]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:24<00:26,  1.10it/s, pg=0.073, ret=-0.00117, glen=104, tlen=264, kl=0.00101, act_lr=3.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 28/57 [00:25<00:26,  1.10it/s, pg=-0.111, ret=0.000673, glen=115, tlen=275, kl=0.00105, act_lr=3.4e-7, ent=1.85]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:25<00:24,  1.12it/s, pg=-0.111, ret=0.000673, glen=115, tlen=275, kl=0.00105, act_lr=3.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 29/57 [00:26<00:24,  1.12it/s, pg=0.0985, ret=-0.00168, glen=113, tlen=274, kl=0.00103, act_lr=3.4e-7, ent=1.9] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:26<00:23,  1.14it/s, pg=0.0985, ret=-0.00168, glen=113, tlen=274, kl=0.00103, act_lr=3.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/57 [00:27<00:23,  1.14it/s, pg=-0.0403, ret=-0.000885, glen=126, tlen=287, kl=0.00102, act_lr=3.4e-7, ent=1.98]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:27<00:22,  1.15it/s, pg=-0.0403, ret=-0.000885, glen=126, tlen=287, kl=0.00102, act_lr=3.4e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 31/57 [00:28<00:22,  1.15it/s, pg=0.0304, ret=0.00577, glen=373, tlen=533, kl=0.000809, act_lr=3.4e-7, ent=1.41]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=0.0304, ret=0.00577, glen=373, tlen=533, kl=0.000809, act_lr=3.4e-7, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 32/57 [00:28<00:22,  1.13it/s, pg=0.0247, ret=0.000773, glen=162, tlen=323, kl=0.000954, act_lr=3.4e-7, ent=2.37]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:28<00:21,  1.13it/s, pg=0.0247, ret=0.000773, glen=162, tlen=323, kl=0.000954, act_lr=3.4e-7, ent=2.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 33/57 [00:29<00:21,  1.13it/s, pg=-0.072, ret=0.000731, glen=102, tlen=262, kl=0.00103, act_lr=3.4e-7, ent=1.72] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:29<00:20,  1.15it/s, pg=-0.072, ret=0.000731, glen=102, tlen=262, kl=0.00103, act_lr=3.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 34/57 [00:30<00:20,  1.15it/s, pg=0.0222, ret=0.000584, glen=133, tlen=293, kl=0.000964, act_lr=3.4e-7, ent=2.06]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:30<00:19,  1.14it/s, pg=0.0222, ret=0.000584, glen=133, tlen=293, kl=0.000964, act_lr=3.4e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 35/57 [00:31<00:19,  1.14it/s, pg=-0.00645, ret=0.000321, glen=102, tlen=263, kl=0.00105, act_lr=3.4e-7, ent=1.74]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:31<00:18,  1.15it/s, pg=-0.00645, ret=0.000321, glen=102, tlen=263, kl=0.00105, act_lr=3.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 36/57 [00:32<00:18,  1.15it/s, pg=-0.117, ret=0.00137, glen=105, tlen=265, kl=0.00106, act_lr=3.4e-7, ent=1.7]    Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:32<00:17,  1.16it/s, pg=-0.117, ret=0.00137, glen=105, tlen=265, kl=0.00106, act_lr=3.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 37/57 [00:33<00:17,  1.16it/s, pg=0.00616, ret=-0.00109, glen=91.5, tlen=252, kl=0.00105, act_lr=3.4e-7, ent=1.69]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:33<00:16,  1.16it/s, pg=0.00616, ret=-0.00109, glen=91.5, tlen=252, kl=0.00105, act_lr=3.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 38/57 [00:34<00:16,  1.16it/s, pg=0.0871, ret=-0.000458, glen=117, tlen=277, kl=0.000998, act_lr=3.4e-7, ent=1.76]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0871, ret=-0.000458, glen=117, tlen=277, kl=0.000998, act_lr=3.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 39/57 [00:34<00:15,  1.17it/s, pg=0.0668, ret=-0.00111, glen=107, tlen=268, kl=0.00104, act_lr=3.4e-7, ent=1.78]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:34<00:14,  1.17it/s, pg=0.0668, ret=-0.00111, glen=107, tlen=268, kl=0.00104, act_lr=3.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 40/57 [00:35<00:14,  1.17it/s, pg=0.219, ret=-0.00309, glen=122, tlen=283, kl=0.00101, act_lr=3.4e-7, ent=1.89] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:35<00:13,  1.17it/s, pg=0.219, ret=-0.00309, glen=122, tlen=283, kl=0.00101, act_lr=3.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 41/57 [00:36<00:13,  1.17it/s, pg=0.0166, ret=0.00437, glen=374, tlen=534, kl=0.000866, act_lr=3.4e-7, ent=2.8]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:36<00:13,  1.14it/s, pg=0.0166, ret=0.00437, glen=374, tlen=534, kl=0.000866, act_lr=3.4e-7, ent=2.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 42/57 [00:37<00:13,  1.14it/s, pg=-0.299, ret=0.00209, glen=107, tlen=267, kl=0.00099, act_lr=3.4e-7, ent=1.82]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:37<00:12,  1.15it/s, pg=-0.299, ret=0.00209, glen=107, tlen=267, kl=0.00099, act_lr=3.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 43/57 [00:38<00:12,  1.15it/s, pg=0.00378, ret=-0.000338, glen=100, tlen=261, kl=0.00104, act_lr=3.4e-7, ent=1.84]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:38<00:11,  1.16it/s, pg=0.00378, ret=-0.000338, glen=100, tlen=261, kl=0.00104, act_lr=3.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 44/57 [00:39<00:11,  1.16it/s, pg=-0.197, ret=0.00141, glen=94, tlen=254, kl=0.00104, act_lr=3.4e-7, ent=1.63]    Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:39<00:10,  1.16it/s, pg=-0.197, ret=0.00141, glen=94, tlen=254, kl=0.00104, act_lr=3.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 45/57 [00:40<00:10,  1.16it/s, pg=0.0487, ret=0.00046, glen=123, tlen=284, kl=0.000995, act_lr=3.4e-7, ent=1.9]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.16it/s, pg=0.0487, ret=0.00046, glen=123, tlen=284, kl=0.000995, act_lr=3.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 46/57 [00:40<00:09,  1.16it/s, pg=-0.0348, ret=-0.00025, glen=112, tlen=273, kl=0.00103, act_lr=3.4e-7, ent=1.89]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:40<00:08,  1.17it/s, pg=-0.0348, ret=-0.00025, glen=112, tlen=273, kl=0.00103, act_lr=3.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 47/57 [00:41<00:08,  1.17it/s, pg=0.0132, ret=0.000368, glen=103, tlen=263, kl=0.00106, act_lr=3.4e-7, ent=1.9]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:41<00:07,  1.17it/s, pg=0.0132, ret=0.000368, glen=103, tlen=263, kl=0.00106, act_lr=3.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 48/57 [00:42<00:07,  1.17it/s, pg=0.0678, ret=-0.00108, glen=117, tlen=277, kl=0.00103, act_lr=3.4e-7, ent=1.7]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:42<00:06,  1.17it/s, pg=0.0678, ret=-0.00108, glen=117, tlen=277, kl=0.00103, act_lr=3.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 49/57 [00:43<00:06,  1.17it/s, pg=-0.169, ret=0.00118, glen=109, tlen=269, kl=0.00099, act_lr=3.4e-7, ent=1.78]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:43<00:05,  1.17it/s, pg=-0.169, ret=0.00118, glen=109, tlen=269, kl=0.00099, act_lr=3.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 50/57 [00:44<00:05,  1.17it/s, pg=-0.0715, ret=-0.000552, glen=99.6, tlen=260, kl=0.00106, act_lr=3.4e-7, ent=1.75]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:44<00:05,  1.17it/s, pg=-0.0715, ret=-0.000552, glen=99.6, tlen=260, kl=0.00106, act_lr=3.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 51/57 [00:45<00:05,  1.17it/s, pg=0.0977, ret=-0.000345, glen=111, tlen=271, kl=0.00101, act_lr=3.4e-7, ent=1.88]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:45<00:04,  1.17it/s, pg=0.0977, ret=-0.000345, glen=111, tlen=271, kl=0.00101, act_lr=3.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 52/57 [00:46<00:04,  1.17it/s, pg=-0.1, ret=-0.000752, glen=117, tlen=278, kl=0.00105, act_lr=3.4e-7, ent=1.83]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=-0.1, ret=-0.000752, glen=117, tlen=278, kl=0.00105, act_lr=3.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 53/57 [00:46<00:03,  1.17it/s, pg=0.0428, ret=-0.000281, glen=101, tlen=261, kl=0.00106, act_lr=3.4e-7, ent=1.92]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:46<00:02,  1.17it/s, pg=0.0428, ret=-0.000281, glen=101, tlen=261, kl=0.00106, act_lr=3.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 54/57 [00:47<00:02,  1.17it/s, pg=-0.105, ret=0.000738, glen=105, tlen=266, kl=0.00107, act_lr=3.4e-7, ent=1.83] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:47<00:01,  1.17it/s, pg=-0.105, ret=0.000738, glen=105, tlen=266, kl=0.00107, act_lr=3.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 55/57 [00:48<00:01,  1.17it/s, pg=0.0507, ret=-2.97e-5, glen=113, tlen=274, kl=0.000988, act_lr=3.4e-7, ent=1.9]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:48<00:00,  1.08it/s, pg=0.0507, ret=-2.97e-5, glen=113, tlen=274, kl=0.000988, act_lr=3.4e-7, ent=1.9]
2025-07-23 12:56:06.647 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.84s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.08it/s, pg=0.0238, ret=-0.00011, glen=115, tlen=276, kl=0.00106, act_lr=3.6e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 56/57 [00:49<00:00,  1.13it/s, pg=0.0238, ret=-0.00011, glen=115, tlen=276, kl=0.00106, act_lr=3.6e-7, ent=1.82]
2025-07-23 12:56:07.531 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-23 12:56:10.052 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.52s
2025-07-23 12:56:10.396 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 53.70s
2025-07-23 12:56:10.402 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.014424742313853482, 'actor_lr': 3.403508908935723e-07, 'clip_ratio': 0.0, 'entropy': 1.8303940358914828, 'kl': 0.0010186140997367993, 'response_length': 121.29304343775699, 'total_length': 281.79762589304073, 'return': 8.904338800787452e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [19:33<31:57, 239.69s/it][A2025-07-23 12:56:10.433 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 12:58:50.932 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 12:58:51.113 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 12:58:51.114 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 160.68s
2025-07-23 12:58:53.149 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0101,avg_pass_at_n: 1.0000,avg_num_tokens: 108.5009,std_num_tokens: 179.5736,avg_correct_num_tokens: 92.0676,std_correct_num_tokens: 123.8731,avg_incorrect_num_tokens: 118.6201,std_incorrect_num_tokens: 205.8785
2025-07-23 12:58:53.586 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.47s
2025-07-23 12:58:55.169 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.58s
2025-07-23 12:59:23.651 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 224
2025-07-23 12:59:23.651 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.48s
2025-07-23 12:59:24.546 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.89s
2025-07-23 12:59:24.547 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0006044466211148704, avg_kl: 0.001115309340613229, avg_response_length: 115.52549074377332, avg_orm_score: 0.0, avg_custom_rewards: -0.0006044466211148704
2025-07-23 12:59:24.590 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter18_replay_buffer.jsonl
2025-07-23 12:59:26.204 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.62s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=0.0464, ret=-0.00115, glen=106, tlen=266, kl=0.00111, act_lr=3.6e-7, ent=1.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:58,  1.06s/it, pg=0.0464, ret=-0.00115, glen=106, tlen=266, kl=0.00111, act_lr=3.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:58,  1.06s/it, pg=0.0895, ret=-0.0024, glen=102, tlen=262, kl=0.00116, act_lr=3.6e-7, ent=1.7]  Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:51,  1.04it/s, pg=0.0895, ret=-0.0024, glen=102, tlen=262, kl=0.00116, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:51,  1.04it/s, pg=-0.0687, ret=-0.000577, glen=120, tlen=281, kl=0.00112, act_lr=3.6e-7, ent=1.89]Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:49,  1.07it/s, pg=-0.0687, ret=-0.000577, glen=120, tlen=281, kl=0.00112, act_lr=3.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:49,  1.07it/s, pg=0.0825, ret=-0.000174, glen=113, tlen=273, kl=0.00112, act_lr=3.6e-7, ent=1.79] Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.11it/s, pg=0.0825, ret=-0.000174, glen=113, tlen=273, kl=0.00112, act_lr=3.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.11it/s, pg=-0.0495, ret=0.000794, glen=93.6, tlen=254, kl=0.00118, act_lr=3.6e-7, ent=1.87]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:45,  1.11it/s, pg=-0.0495, ret=0.000794, glen=93.6, tlen=254, kl=0.00118, act_lr=3.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:45,  1.11it/s, pg=-0.0417, ret=0.00101, glen=93.1, tlen=253, kl=0.00115, act_lr=3.6e-7, ent=1.55] Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:44,  1.13it/s, pg=-0.0417, ret=0.00101, glen=93.1, tlen=253, kl=0.00115, act_lr=3.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:44,  1.13it/s, pg=-0.061, ret=-0.000574, glen=106, tlen=266, kl=0.00113, act_lr=3.6e-7, ent=1.76]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:42,  1.15it/s, pg=-0.061, ret=-0.000574, glen=106, tlen=266, kl=0.00113, act_lr=3.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:42,  1.15it/s, pg=0.0551, ret=0.000343, glen=123, tlen=283, kl=0.00108, act_lr=3.6e-7, ent=1.73] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:41,  1.15it/s, pg=0.0551, ret=0.000343, glen=123, tlen=283, kl=0.00108, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:41,  1.15it/s, pg=-0.198, ret=0.000275, glen=95.5, tlen=256, kl=0.0012, act_lr=3.6e-7, ent=1.69]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=-0.198, ret=0.000275, glen=95.5, tlen=256, kl=0.0012, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:40,  1.16it/s, pg=-0.0975, ret=0.00147, glen=119, tlen=280, kl=0.0011, act_lr=3.6e-7, ent=2.01] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.17it/s, pg=-0.0975, ret=0.00147, glen=119, tlen=280, kl=0.0011, act_lr=3.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.17it/s, pg=-0.132, ret=-0.000167, glen=121, tlen=282, kl=0.00109, act_lr=3.6e-7, ent=1.73]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.17it/s, pg=-0.132, ret=-0.000167, glen=121, tlen=282, kl=0.00109, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.17it/s, pg=-0.23, ret=0.00263, glen=92.1, tlen=253, kl=0.00116, act_lr=3.6e-7, ent=1.7]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=-0.23, ret=0.00263, glen=92.1, tlen=253, kl=0.00116, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0754, ret=-0.00126, glen=99.5, tlen=259, kl=0.00113, act_lr=3.6e-7, ent=1.89]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0754, ret=-0.00126, glen=99.5, tlen=259, kl=0.00113, act_lr=3.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=0.234, ret=-0.000816, glen=151, tlen=311, kl=0.000992, act_lr=3.6e-7, ent=2.09]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=0.234, ret=-0.000816, glen=151, tlen=311, kl=0.000992, act_lr=3.6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=-0.0767, ret=-0.000907, glen=102, tlen=262, kl=0.00112, act_lr=3.6e-7, ent=1.87]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.17it/s, pg=-0.0767, ret=-0.000907, glen=102, tlen=262, kl=0.00112, act_lr=3.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.17it/s, pg=0.067, ret=-0.000612, glen=113, tlen=274, kl=0.00105, act_lr=3.6e-7, ent=1.87]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.15it/s, pg=0.067, ret=-0.000612, glen=113, tlen=274, kl=0.00105, act_lr=3.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.15it/s, pg=-0.0927, ret=0.000451, glen=99.8, tlen=260, kl=0.00112, act_lr=3.6e-7, ent=1.8]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.16it/s, pg=-0.0927, ret=0.000451, glen=99.8, tlen=260, kl=0.00112, act_lr=3.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.181, ret=0.000688, glen=94.7, tlen=255, kl=0.00114, act_lr=3.6e-7, ent=1.76]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.16it/s, pg=-0.181, ret=0.000688, glen=94.7, tlen=255, kl=0.00114, act_lr=3.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.16it/s, pg=-0.0556, ret=0.000492, glen=95.7, tlen=256, kl=0.00117, act_lr=3.6e-7, ent=1.66]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=-0.0556, ret=0.000492, glen=95.7, tlen=256, kl=0.00117, act_lr=3.6e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=0.0691, ret=-0.000933, glen=90.8, tlen=251, kl=0.00115, act_lr=3.6e-7, ent=1.71]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.15it/s, pg=0.0691, ret=-0.000933, glen=90.8, tlen=251, kl=0.00115, act_lr=3.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.15it/s, pg=-0.0187, ret=0.00161, glen=118, tlen=279, kl=0.00109, act_lr=3.6e-7, ent=1.72]  Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.16it/s, pg=-0.0187, ret=0.00161, glen=118, tlen=279, kl=0.00109, act_lr=3.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.16it/s, pg=-0.0418, ret=0.000692, glen=107, tlen=267, kl=0.00111, act_lr=3.6e-7, ent=1.94]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.0418, ret=0.000692, glen=107, tlen=267, kl=0.00111, act_lr=3.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.16it/s, pg=0.0502, ret=0.00123, glen=186, tlen=346, kl=0.00106, act_lr=3.6e-7, ent=2.2]   Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.15it/s, pg=0.0502, ret=0.00123, glen=186, tlen=346, kl=0.00106, act_lr=3.6e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.15it/s, pg=-0.23, ret=0.00161, glen=109, tlen=269, kl=0.0011, act_lr=3.6e-7, ent=1.76] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:28,  1.13it/s, pg=-0.23, ret=0.00161, glen=109, tlen=269, kl=0.0011, act_lr=3.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:28,  1.13it/s, pg=0.0603, ret=-0.000426, glen=108, tlen=269, kl=0.0011, act_lr=3.6e-7, ent=1.68]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:27,  1.15it/s, pg=0.0603, ret=-0.000426, glen=108, tlen=269, kl=0.0011, act_lr=3.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:27,  1.15it/s, pg=0.127, ret=-0.000759, glen=348, tlen=508, kl=0.000903, act_lr=3.6e-7, ent=1.24]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:26,  1.12it/s, pg=0.127, ret=-0.000759, glen=348, tlen=508, kl=0.000903, act_lr=3.6e-7, ent=1.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:26,  1.12it/s, pg=0.18, ret=-0.00138, glen=118, tlen=278, kl=0.00109, act_lr=3.6e-7, ent=2.02]   Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:27,  1.04it/s, pg=0.18, ret=-0.00138, glen=118, tlen=278, kl=0.00109, act_lr=3.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:27,  1.04it/s, pg=-0.0529, ret=-0.000219, glen=101, tlen=260, kl=0.00112, act_lr=3.6e-7, ent=1.82]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:25,  1.08it/s, pg=-0.0529, ret=-0.000219, glen=101, tlen=260, kl=0.00112, act_lr=3.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:25,  1.08it/s, pg=-0.0468, ret=8.64e-5, glen=95.8, tlen=255, kl=0.00117, act_lr=3.6e-7, ent=1.77] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:24,  1.11it/s, pg=-0.0468, ret=8.64e-5, glen=95.8, tlen=255, kl=0.00117, act_lr=3.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:24,  1.11it/s, pg=0.0562, ret=-0.00108, glen=113, tlen=273, kl=0.00106, act_lr=3.6e-7, ent=1.93] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:23,  1.13it/s, pg=0.0562, ret=-0.00108, glen=113, tlen=273, kl=0.00106, act_lr=3.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:23,  1.13it/s, pg=0.135, ret=-0.00165, glen=107, tlen=268, kl=0.00114, act_lr=3.6e-7, ent=1.89] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:21,  1.14it/s, pg=0.135, ret=-0.00165, glen=107, tlen=268, kl=0.00114, act_lr=3.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:21,  1.14it/s, pg=-0.159, ret=-0.00208, glen=165, tlen=325, kl=0.000998, act_lr=3.6e-7, ent=2.08]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:21,  1.14it/s, pg=-0.159, ret=-0.00208, glen=165, tlen=325, kl=0.000998, act_lr=3.6e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:29<00:21,  1.14it/s, pg=0.135, ret=0.00279, glen=195, tlen=355, kl=0.000926, act_lr=3.6e-7, ent=1.36]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.13it/s, pg=0.135, ret=0.00279, glen=195, tlen=355, kl=0.000926, act_lr=3.6e-7, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:20,  1.13it/s, pg=0.128, ret=-0.00194, glen=102, tlen=262, kl=0.00121, act_lr=3.6e-7, ent=1.85]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:19,  1.14it/s, pg=0.128, ret=-0.00194, glen=102, tlen=262, kl=0.00121, act_lr=3.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:19,  1.14it/s, pg=0.265, ret=-0.00165, glen=144, tlen=305, kl=0.00101, act_lr=3.6e-7, ent=1.83]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.15it/s, pg=0.265, ret=-0.00165, glen=144, tlen=305, kl=0.00101, act_lr=3.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.15it/s, pg=0.00183, ret=0.000358, glen=97.4, tlen=257, kl=0.00121, act_lr=3.6e-7, ent=1.81]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.15it/s, pg=0.00183, ret=0.000358, glen=97.4, tlen=257, kl=0.00121, act_lr=3.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.15it/s, pg=-0.034, ret=-6.94e-5, glen=91.1, tlen=251, kl=0.00119, act_lr=3.6e-7, ent=1.73] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.16it/s, pg=-0.034, ret=-6.94e-5, glen=91.1, tlen=251, kl=0.00119, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.16it/s, pg=0.0221, ret=-0.0007, glen=108, tlen=268, kl=0.00113, act_lr=3.6e-7, ent=1.88]  Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.16it/s, pg=0.0221, ret=-0.0007, glen=108, tlen=268, kl=0.00113, act_lr=3.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.16it/s, pg=0.022, ret=-0.00121, glen=116, tlen=275, kl=0.00106, act_lr=3.6e-7, ent=2.04]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=0.022, ret=-0.00121, glen=116, tlen=275, kl=0.00106, act_lr=3.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:35<00:14,  1.17it/s, pg=-0.0659, ret=0.00188, glen=123, tlen=283, kl=0.00108, act_lr=3.6e-7, ent=2.06]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.0659, ret=0.00188, glen=123, tlen=283, kl=0.00108, act_lr=3.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=0.0223, ret=0.00113, glen=97.8, tlen=258, kl=0.00118, act_lr=3.6e-7, ent=1.7] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=0.0223, ret=0.00113, glen=97.8, tlen=258, kl=0.00118, act_lr=3.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=-0.000366, ret=0.000295, glen=99.4, tlen=259, kl=0.00118, act_lr=3.6e-7, ent=1.72]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=-0.000366, ret=0.000295, glen=99.4, tlen=259, kl=0.00118, act_lr=3.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.0242, ret=0.000611, glen=110, tlen=270, kl=0.00118, act_lr=3.6e-7, ent=1.73]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.0242, ret=0.000611, glen=110, tlen=270, kl=0.00118, act_lr=3.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.0923, ret=0.00132, glen=117, tlen=277, kl=0.0011, act_lr=3.6e-7, ent=1.93]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.0923, ret=0.00132, glen=117, tlen=277, kl=0.0011, act_lr=3.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.0795, ret=0.000636, glen=100, tlen=260, kl=0.00111, act_lr=3.6e-7, ent=1.61]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.0795, ret=0.000636, glen=100, tlen=260, kl=0.00111, act_lr=3.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=-0.141, ret=0.000589, glen=103, tlen=263, kl=0.00113, act_lr=3.6e-7, ent=1.56] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=-0.141, ret=0.000589, glen=103, tlen=263, kl=0.00113, act_lr=3.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:41<00:08,  1.17it/s, pg=0.0491, ret=-0.00813, glen=97.4, tlen=258, kl=0.00118, act_lr=3.6e-7, ent=1.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=0.0491, ret=-0.00813, glen=97.4, tlen=258, kl=0.00118, act_lr=3.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.219, ret=0.0012, glen=108, tlen=269, kl=0.00109, act_lr=3.6e-7, ent=1.76]   Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.18it/s, pg=-0.219, ret=0.0012, glen=108, tlen=269, kl=0.00109, act_lr=3.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.18it/s, pg=-0.073, ret=0.000147, glen=108, tlen=268, kl=0.00109, act_lr=3.6e-7, ent=2.01]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:05,  1.18it/s, pg=-0.073, ret=0.000147, glen=108, tlen=268, kl=0.00109, act_lr=3.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:05,  1.18it/s, pg=0.0439, ret=-0.000229, glen=105, tlen=265, kl=0.00112, act_lr=3.6e-7, ent=1.69]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.18it/s, pg=0.0439, ret=-0.000229, glen=105, tlen=265, kl=0.00112, act_lr=3.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.18it/s, pg=-0.191, ret=0.000639, glen=108, tlen=268, kl=0.00107, act_lr=3.6e-7, ent=1.9]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.18it/s, pg=-0.191, ret=0.000639, glen=108, tlen=268, kl=0.00107, act_lr=3.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.18it/s, pg=-0.00769, ret=0.000243, glen=102, tlen=262, kl=0.00118, act_lr=3.6e-7, ent=1.78]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.17it/s, pg=-0.00769, ret=0.000243, glen=102, tlen=262, kl=0.00118, act_lr=3.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.17it/s, pg=0.0876, ret=-0.000472, glen=101, tlen=261, kl=0.0012, act_lr=3.6e-7, ent=1.79]  Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=0.0876, ret=-0.000472, glen=101, tlen=261, kl=0.0012, act_lr=3.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.17it/s, pg=-0.0879, ret=-0.000233, glen=92.3, tlen=253, kl=0.00117, act_lr=3.6e-7, ent=1.56]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:46<00:01,  1.17it/s, pg=-0.0879, ret=-0.000233, glen=92.3, tlen=253, kl=0.00117, act_lr=3.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.17it/s, pg=-0.0082, ret=-0.000805, glen=108, tlen=269, kl=0.00115, act_lr=3.6e-7, ent=1.69] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.17it/s, pg=-0.0082, ret=-0.000805, glen=108, tlen=269, kl=0.00115, act_lr=3.6e-7, ent=1.69]
2025-07-23 13:00:15.423 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.05s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.17it/s, pg=0.02, ret=-0.0013, glen=125, tlen=285, kl=0.00109, act_lr=3.8e-7, ent=2]        Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:48<00:00,  1.12it/s, pg=0.02, ret=-0.0013, glen=125, tlen=285, kl=0.00109, act_lr=3.8e-7, ent=2]
2025-07-23 13:00:16.295 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 13:00:18.874 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-23 13:00:19.198 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 52.94s
2025-07-23 13:00:19.204 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.013135739735194616, 'actor_lr': 3.6035714399775186e-07, 'clip_ratio': 0.0, 'entropy': 1.7969443308455604, 'kl': 0.001115309340613229, 'response_length': 115.52549130575997, 'total_length': 275.6471800122942, 'return': -0.00015486337230998158, 'policy_update_steps': 1.0}

Episode [2/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [23:41<28:19, 242.79s/it][A2025-07-23 13:00:19.237 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:03:03.476 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:03:03.653 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:03:03.654 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 164.42s
2025-07-23 13:03:05.822 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0151,avg_reflection_pattern_score: 0.0098,avg_pass_at_n: 1.0000,avg_num_tokens: 104.7455,std_num_tokens: 160.3215,avg_correct_num_tokens: 91.2642,std_correct_num_tokens: 81.9365,avg_incorrect_num_tokens: 112.0703,std_incorrect_num_tokens: 189.3888
2025-07-23 13:03:06.260 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.61s
2025-07-23 13:03:07.827 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.56s
2025-07-23 13:03:35.770 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 220
2025-07-23 13:03:35.771 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.94s
2025-07-23 13:03:36.532 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.76s
2025-07-23 13:03:36.532 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.002435568437265994, avg_kl: 0.0011560071598399769, avg_response_length: 112.89906943928112, avg_orm_score: 0.0, avg_custom_rewards: -0.002435568437265994
2025-07-23 13:03:36.564 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter19_replay_buffer.jsonl
2025-07-23 13:03:38.111 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.55s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s, pg=-0.0628, ret=0.000493, glen=110, tlen=270, kl=0.00118, act_lr=3.8e-7, ent=1.79]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:00<00:53,  1.00it/s, pg=-0.0628, ret=0.000493, glen=110, tlen=270, kl=0.00118, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:53,  1.00it/s, pg=0.104, ret=-0.000996, glen=94.9, tlen=255, kl=0.00117, act_lr=3.8e-7, ent=1.67]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.09it/s, pg=0.104, ret=-0.000996, glen=94.9, tlen=255, kl=0.00117, act_lr=3.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.09it/s, pg=-0.0836, ret=-0.000227, glen=97.3, tlen=258, kl=0.00119, act_lr=3.8e-7, ent=1.76]Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:46,  1.11it/s, pg=-0.0836, ret=-0.000227, glen=97.3, tlen=258, kl=0.00119, act_lr=3.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:46,  1.11it/s, pg=-0.165, ret=0.000205, glen=94.9, tlen=256, kl=0.00111, act_lr=3.8e-7, ent=1.74]  Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.13it/s, pg=-0.165, ret=0.000205, glen=94.9, tlen=256, kl=0.00111, act_lr=3.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.13it/s, pg=-0.00842, ret=0.000171, glen=97.7, tlen=258, kl=0.0012, act_lr=3.8e-7, ent=1.85]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:43,  1.15it/s, pg=-0.00842, ret=0.000171, glen=97.7, tlen=258, kl=0.0012, act_lr=3.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:43,  1.15it/s, pg=0.0723, ret=-0.000612, glen=99.3, tlen=260, kl=0.00115, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:42,  1.15it/s, pg=0.0723, ret=-0.000612, glen=99.3, tlen=260, kl=0.00115, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:42,  1.15it/s, pg=-0.0142, ret=0.000644, glen=109, tlen=269, kl=0.00115, act_lr=3.8e-7, ent=1.85] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.15it/s, pg=-0.0142, ret=0.000644, glen=109, tlen=269, kl=0.00115, act_lr=3.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.15it/s, pg=0.0594, ret=-0.000233, glen=104, tlen=265, kl=0.00109, act_lr=3.8e-7, ent=1.78]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=0.0594, ret=-0.000233, glen=104, tlen=265, kl=0.00109, act_lr=3.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.129, ret=0.000829, glen=103, tlen=264, kl=0.0012, act_lr=3.8e-7, ent=1.82]  Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:39,  1.16it/s, pg=-0.129, ret=0.000829, glen=103, tlen=264, kl=0.0012, act_lr=3.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.16it/s, pg=-0.15, ret=7.57e-5, glen=99.2, tlen=260, kl=0.00119, act_lr=3.8e-7, ent=1.94]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:38,  1.17it/s, pg=-0.15, ret=7.57e-5, glen=99.2, tlen=260, kl=0.00119, act_lr=3.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:38,  1.17it/s, pg=0.195, ret=-0.00208, glen=107, tlen=267, kl=0.00111, act_lr=3.8e-7, ent=1.7] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:37,  1.17it/s, pg=0.195, ret=-0.00208, glen=107, tlen=267, kl=0.00111, act_lr=3.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:37,  1.17it/s, pg=0.0369, ret=-0.000576, glen=99, tlen=259, kl=0.00129, act_lr=3.8e-7, ent=1.66]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:36,  1.17it/s, pg=0.0369, ret=-0.000576, glen=99, tlen=259, kl=0.00129, act_lr=3.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:36,  1.17it/s, pg=0.0947, ret=-0.00607, glen=358, tlen=518, kl=0.00102, act_lr=3.8e-7, ent=2.43]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.14it/s, pg=0.0947, ret=-0.00607, glen=358, tlen=518, kl=0.00102, act_lr=3.8e-7, ent=2.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.14it/s, pg=-0.0907, ret=0.000436, glen=103, tlen=263, kl=0.00116, act_lr=3.8e-7, ent=1.78]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.15it/s, pg=-0.0907, ret=0.000436, glen=103, tlen=263, kl=0.00116, act_lr=3.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.15it/s, pg=-0.0958, ret=0.00174, glen=98.2, tlen=259, kl=0.00124, act_lr=3.8e-7, ent=1.77]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.15it/s, pg=-0.0958, ret=0.00174, glen=98.2, tlen=259, kl=0.00124, act_lr=3.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.15it/s, pg=0.0256, ret=-0.000655, glen=91.1, tlen=252, kl=0.0012, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.16it/s, pg=0.0256, ret=-0.000655, glen=91.1, tlen=252, kl=0.0012, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.16it/s, pg=-0.156, ret=0.000986, glen=107, tlen=268, kl=0.00116, act_lr=3.8e-7, ent=2.01] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.16it/s, pg=-0.156, ret=0.000986, glen=107, tlen=268, kl=0.00116, act_lr=3.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.16it/s, pg=-0.209, ret=0.00111, glen=107, tlen=267, kl=0.0012, act_lr=3.8e-7, ent=1.87]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.209, ret=0.00111, glen=107, tlen=267, kl=0.0012, act_lr=3.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=0.176, ret=-0.000904, glen=113, tlen=273, kl=0.00116, act_lr=3.8e-7, ent=1.92]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=0.176, ret=-0.000904, glen=113, tlen=273, kl=0.00116, act_lr=3.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=0.0123, ret=-6.98e-5, glen=105, tlen=266, kl=0.00116, act_lr=3.8e-7, ent=1.78]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=0.0123, ret=-6.98e-5, glen=105, tlen=266, kl=0.00116, act_lr=3.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=-0.127, ret=0.000515, glen=99.7, tlen=260, kl=0.00112, act_lr=3.8e-7, ent=1.84]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.17it/s, pg=-0.127, ret=0.000515, glen=99.7, tlen=260, kl=0.00112, act_lr=3.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.17it/s, pg=-0.0712, ret=-0.000946, glen=98.3, tlen=258, kl=0.00119, act_lr=3.8e-7, ent=1.76]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.0712, ret=-0.000946, glen=98.3, tlen=258, kl=0.00119, act_lr=3.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.129, ret=0.00191, glen=106, tlen=266, kl=0.00112, act_lr=3.8e-7, ent=1.76]    Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:19<00:27,  1.17it/s, pg=-0.129, ret=0.00191, glen=106, tlen=266, kl=0.00112, act_lr=3.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.0058, ret=9.18e-5, glen=92.2, tlen=253, kl=0.00118, act_lr=3.8e-7, ent=1.81]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.0058, ret=9.18e-5, glen=92.2, tlen=253, kl=0.00118, act_lr=3.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=0.0967, ret=-0.00161, glen=108, tlen=268, kl=0.00115, act_lr=3.8e-7, ent=1.78] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=0.0967, ret=-0.00161, glen=108, tlen=268, kl=0.00115, act_lr=3.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.0602, ret=0.000816, glen=114, tlen=274, kl=0.00114, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.0602, ret=0.000816, glen=114, tlen=274, kl=0.00114, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.00244, ret=0.00102, glen=106, tlen=266, kl=0.00116, act_lr=3.8e-7, ent=1.73]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:26,  1.07it/s, pg=-0.00244, ret=0.00102, glen=106, tlen=266, kl=0.00116, act_lr=3.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:26,  1.07it/s, pg=0.0261, ret=0.000105, glen=113, tlen=273, kl=0.00114, act_lr=3.8e-7, ent=1.89] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:24,  1.10it/s, pg=0.0261, ret=0.000105, glen=113, tlen=273, kl=0.00114, act_lr=3.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:24,  1.10it/s, pg=-0.0279, ret=7.85e-5, glen=111, tlen=271, kl=0.00116, act_lr=3.8e-7, ent=1.92]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:23,  1.12it/s, pg=-0.0279, ret=7.85e-5, glen=111, tlen=271, kl=0.00116, act_lr=3.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:23,  1.12it/s, pg=0.125, ret=-0.000519, glen=106, tlen=267, kl=0.00114, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.13it/s, pg=0.125, ret=-0.000519, glen=106, tlen=267, kl=0.00114, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.13it/s, pg=0.104, ret=-0.001, glen=107, tlen=267, kl=0.00114, act_lr=3.8e-7, ent=1.67]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:20,  1.15it/s, pg=0.104, ret=-0.001, glen=107, tlen=267, kl=0.00114, act_lr=3.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:20,  1.15it/s, pg=0.033, ret=0.00084, glen=102, tlen=262, kl=0.00121, act_lr=3.8e-7, ent=1.83]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:19,  1.15it/s, pg=0.033, ret=0.00084, glen=102, tlen=262, kl=0.00121, act_lr=3.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:19,  1.15it/s, pg=-0.113, ret=0.00124, glen=91.5, tlen=252, kl=0.00121, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:18,  1.16it/s, pg=-0.113, ret=0.00124, glen=91.5, tlen=252, kl=0.00121, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:18,  1.16it/s, pg=-0.0194, ret=0.00109, glen=104, tlen=264, kl=0.00126, act_lr=3.8e-7, ent=1.67]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.16it/s, pg=-0.0194, ret=0.00109, glen=104, tlen=264, kl=0.00126, act_lr=3.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.16it/s, pg=0.0562, ret=-0.000976, glen=94.8, tlen=255, kl=0.00121, act_lr=3.8e-7, ent=1.64]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.17it/s, pg=0.0562, ret=-0.000976, glen=94.8, tlen=255, kl=0.00121, act_lr=3.8e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.17it/s, pg=-0.0342, ret=0.00071, glen=109, tlen=269, kl=0.00115, act_lr=3.8e-7, ent=1.75]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=-0.0342, ret=0.00071, glen=109, tlen=269, kl=0.00115, act_lr=3.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=-0.0317, ret=-0.000337, glen=104, tlen=264, kl=0.00114, act_lr=3.8e-7, ent=1.76]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.0317, ret=-0.000337, glen=104, tlen=264, kl=0.00114, act_lr=3.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=0.101, ret=-0.00107, glen=102, tlen=263, kl=0.00116, act_lr=3.8e-7, ent=1.94]   Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:32<00:14,  1.17it/s, pg=0.101, ret=-0.00107, glen=102, tlen=263, kl=0.00116, act_lr=3.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.276, ret=-0.0145, glen=368, tlen=528, kl=0.000849, act_lr=3.8e-7, ent=1.77]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:14,  1.14it/s, pg=0.276, ret=-0.0145, glen=368, tlen=528, kl=0.000849, act_lr=3.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:14,  1.14it/s, pg=0.096, ret=-0.000711, glen=111, tlen=271, kl=0.00116, act_lr=3.8e-7, ent=1.74]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:13,  1.14it/s, pg=0.096, ret=-0.000711, glen=111, tlen=271, kl=0.00116, act_lr=3.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:13,  1.14it/s, pg=0.0677, ret=-0.000701, glen=98.1, tlen=258, kl=0.00112, act_lr=3.8e-7, ent=1.59]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:12,  1.15it/s, pg=0.0677, ret=-0.000701, glen=98.1, tlen=258, kl=0.00112, act_lr=3.8e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:12,  1.15it/s, pg=-0.00229, ret=0.000773, glen=94.6, tlen=255, kl=0.00119, act_lr=3.8e-7, ent=1.6]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.16it/s, pg=-0.00229, ret=0.000773, glen=94.6, tlen=255, kl=0.00119, act_lr=3.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.16it/s, pg=0.0565, ret=6.11e-6, glen=122, tlen=282, kl=0.00111, act_lr=3.8e-7, ent=1.89]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.16it/s, pg=0.0565, ret=6.11e-6, glen=122, tlen=282, kl=0.00111, act_lr=3.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.16it/s, pg=-0.284, ret=0.00149, glen=82.6, tlen=243, kl=0.00123, act_lr=3.8e-7, ent=1.68]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=-0.284, ret=0.00149, glen=82.6, tlen=243, kl=0.00123, act_lr=3.8e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.17it/s, pg=0.00146, ret=0.000173, glen=104, tlen=265, kl=0.00114, act_lr=3.8e-7, ent=1.77]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.00146, ret=0.000173, glen=104, tlen=265, kl=0.00114, act_lr=3.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.0236, ret=-0.000794, glen=98.5, tlen=259, kl=0.00115, act_lr=3.8e-7, ent=1.79]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.17it/s, pg=0.0236, ret=-0.000794, glen=98.5, tlen=259, kl=0.00115, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.126, ret=0.00145, glen=109, tlen=269, kl=0.00112, act_lr=3.8e-7, ent=1.61]   Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=-0.126, ret=0.00145, glen=109, tlen=269, kl=0.00112, act_lr=3.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=0.0264, ret=0.000316, glen=90.9, tlen=251, kl=0.00119, act_lr=3.8e-7, ent=1.69]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=0.0264, ret=0.000316, glen=90.9, tlen=251, kl=0.00119, act_lr=3.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=-0.127, ret=-0.000434, glen=117, tlen=277, kl=0.00111, act_lr=3.8e-7, ent=1.96]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=-0.127, ret=-0.000434, glen=117, tlen=277, kl=0.00111, act_lr=3.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=0.0414, ret=-0.000719, glen=110, tlen=270, kl=0.00118, act_lr=3.8e-7, ent=1.98]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=0.0414, ret=-0.000719, glen=110, tlen=270, kl=0.00118, act_lr=3.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=-0.171, ret=0.000664, glen=100, tlen=261, kl=0.00113, act_lr=3.8e-7, ent=1.58] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.171, ret=0.000664, glen=100, tlen=261, kl=0.00113, act_lr=3.8e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=0.0792, ret=-0.00109, glen=113, tlen=274, kl=0.00114, act_lr=3.8e-7, ent=1.78]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.15it/s, pg=0.0792, ret=-0.00109, glen=113, tlen=274, kl=0.00114, act_lr=3.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.15it/s, pg=-0.0275, ret=-0.00115, glen=112, tlen=273, kl=0.00111, act_lr=3.8e-7, ent=1.79]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.16it/s, pg=-0.0275, ret=-0.00115, glen=112, tlen=273, kl=0.00111, act_lr=3.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.16it/s, pg=-0.0272, ret=0.000363, glen=98.6, tlen=259, kl=0.00119, act_lr=3.8e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.16it/s, pg=-0.0272, ret=0.000363, glen=98.6, tlen=259, kl=0.00119, act_lr=3.8e-7, ent=1.82]
2025-07-23 13:04:25.997 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.72s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.16it/s, pg=0.0279, ret=-0.000298, glen=114, tlen=274, kl=0.00115, act_lr=4e-7, ent=1.81]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=0.0279, ret=-0.000298, glen=114, tlen=274, kl=0.00115, act_lr=4e-7, ent=1.81]
2025-07-23 13:04:26.675 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 13:04:29.043 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.37s
2025-07-23 13:04:29.349 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 51.19s
2025-07-23 13:04:29.355 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.009754406322132458, 'actor_lr': 3.803636254608798e-07, 'clip_ratio': 0.0, 'entropy': 1.7925825920971956, 'kl': 0.0011560071598399769, 'response_length': 112.89906949129971, 'total_length': 273.1838731245561, 'return': -0.0003449184267083183, 'policy_update_steps': 1.0}

Episode [2/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [27:52<24:31, 245.19s/it][A2025-07-23 13:04:29.361 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 1/172 [00:00<00:33,  5.12it/s, est. speed input: 942.93 toks/s, output: 10.25 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 2/172 [00:00<00:34,  4.92it/s, est. speed input: 928.89 toks/s, output: 32.20 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   5%|‚ñç         | 8/172 [00:00<00:15, 10.39it/s, est. speed input: 1717.92 toks/s, output: 146.00 toks/s]Processed prompts:   6%|‚ñå         | 10/172 [00:00<00:12, 12.56it/s, est. speed input: 1922.61 toks/s, output: 195.62 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:04<00:00, 23.27it/s, est. speed input: 6233.61 toks/s, output: 2769.60 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:04<00:00, 18.47it/s, est. speed input: 5972.95 toks/s, output: 2744.26 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:01, 14.29it/s, est. speed input: 5615.43 toks/s, output: 2527.34 toks/s]Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:05<00:00, 15.02it/s, est. speed input: 5572.81 toks/s, output: 2570.21 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:05<00:00, 19.04it/s, est. speed input: 5444.08 toks/s, output: 2598.89 toks/s][32m [repeated 109x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:03<00:01, 47.08it/s, est. speed input: 6898.17 toks/s, output: 2205.74 toks/s]Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:03<00:00, 62.60it/s, est. speed input: 7365.60 toks/s, output: 2505.70 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:07<00:00,  4.09it/s, est. speed input: 3979.18 toks/s, output: 2076.11 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:07<00:00, 21.95it/s, est. speed input: 3979.18 toks/s, output: 2076.11 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:10<00:00,  1.89it/s, est. speed input: 3027.73 toks/s, output: 1772.59 toks/s][32m [repeated 27x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/172 [00:05<00:00, 16.22it/s, est. speed input: 5410.32 toks/s, output: 2600.36 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:16<00:00,  1.82s/it, est. speed input: 1876.05 toks/s, output: 1158.02 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:16<00:00, 10.32it/s, est. speed input: 1876.05 toks/s, output: 1158.02 toks/s][32m [repeated 3x across cluster][0m
2025-07-23 13:04:47.374 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 495.0102,strategyqa_test/accuracy: 0.3581,eval_accuracy: 0.3581
2025-07-23 13:04:47.662 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:07:19.928 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:07:20.122 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 13:07:20.122 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 152.46s
2025-07-23 13:07:22.260 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0105,avg_pass_at_n: 1.0000,avg_num_tokens: 108.1624,std_num_tokens: 166.8744,avg_correct_num_tokens: 91.2586,std_correct_num_tokens: 78.0276,avg_incorrect_num_tokens: 118.7339,std_incorrect_num_tokens: 202.8903
2025-07-23 13:07:22.741 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.62s
2025-07-23 13:07:24.345 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.60s
2025-07-23 13:07:52.749 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 223
2025-07-23 13:07:52.749 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.40s
2025-07-23 13:07:53.621 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.87s
2025-07-23 13:07:53.622 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.001854734851314682, avg_kl: 0.0011491476153044423, avg_response_length: 115.58623937427195, avg_orm_score: 0.0, avg_custom_rewards: -0.001854734851314682
2025-07-23 13:07:53.657 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter20_replay_buffer.jsonl
2025-07-23 13:07:55.259 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.60s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/56 [00:01<?, ?it/s, pg=-0.00171, ret=0.000586, glen=116, tlen=276, kl=0.00116, act_lr=4e-7, ent=1.85]Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.02s/it, pg=-0.00171, ret=0.000586, glen=116, tlen=276, kl=0.00116, act_lr=4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/56 [00:01<00:55,  1.02s/it, pg=-0.000366, ret=-1.76e-5, glen=107, tlen=266, kl=0.00118, act_lr=4e-7, ent=1.74]Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:01<00:49,  1.08it/s, pg=-0.000366, ret=-1.76e-5, glen=107, tlen=266, kl=0.00118, act_lr=4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/56 [00:02<00:49,  1.08it/s, pg=0.113, ret=-0.00438, glen=395, tlen=554, kl=0.000994, act_lr=4e-7, ent=2.56]   Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:02<00:49,  1.08it/s, pg=0.113, ret=-0.00438, glen=395, tlen=554, kl=0.000994, act_lr=4e-7, ent=2.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/56 [00:03<00:49,  1.08it/s, pg=0.151, ret=-0.000798, glen=114, tlen=274, kl=0.00112, act_lr=4e-7, ent=1.75]Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:03<00:46,  1.11it/s, pg=0.151, ret=-0.000798, glen=114, tlen=274, kl=0.00112, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/56 [00:04<00:46,  1.11it/s, pg=-0.0605, ret=8.55e-5, glen=99.5, tlen=260, kl=0.00122, act_lr=4e-7, ent=1.68]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:04<00:46,  1.11it/s, pg=-0.0605, ret=8.55e-5, glen=99.5, tlen=260, kl=0.00122, act_lr=4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/56 [00:05<00:46,  1.11it/s, pg=0.137, ret=-0.00135, glen=115, tlen=275, kl=0.00105, act_lr=4e-7, ent=1.61]  Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:05<00:45,  1.11it/s, pg=0.137, ret=-0.00135, glen=115, tlen=275, kl=0.00105, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/56 [00:06<00:45,  1.11it/s, pg=0.00525, ret=0.000595, glen=124, tlen=285, kl=0.00119, act_lr=4e-7, ent=1.61]Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:06<00:43,  1.12it/s, pg=0.00525, ret=0.000595, glen=124, tlen=285, kl=0.00119, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 7/56 [00:07<00:43,  1.12it/s, pg=0.1, ret=-0.000125, glen=104, tlen=264, kl=0.00118, act_lr=4e-7, ent=1.85]   Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:07<00:42,  1.13it/s, pg=0.1, ret=-0.000125, glen=104, tlen=264, kl=0.00118, act_lr=4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 8/56 [00:08<00:42,  1.13it/s, pg=-0.255, ret=0.000631, glen=112, tlen=272, kl=0.00117, act_lr=4e-7, ent=1.83]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.15it/s, pg=-0.255, ret=0.000631, glen=112, tlen=272, kl=0.00117, act_lr=4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 9/56 [00:08<00:41,  1.15it/s, pg=-0.301, ret=0.00323, glen=106, tlen=266, kl=0.00114, act_lr=4e-7, ent=1.61] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:08<00:39,  1.15it/s, pg=-0.301, ret=0.00323, glen=106, tlen=266, kl=0.00114, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/56 [00:09<00:39,  1.15it/s, pg=-0.0718, ret=-0.000284, glen=119, tlen=280, kl=0.00107, act_lr=4e-7, ent=1.96]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:09<00:38,  1.16it/s, pg=-0.0718, ret=-0.000284, glen=119, tlen=280, kl=0.00107, act_lr=4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 11/56 [00:10<00:38,  1.16it/s, pg=0.0166, ret=-0.000716, glen=105, tlen=265, kl=0.00124, act_lr=4e-7, ent=1.74] Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:10<00:37,  1.17it/s, pg=0.0166, ret=-0.000716, glen=105, tlen=265, kl=0.00124, act_lr=4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà‚ñè       | 12/56 [00:11<00:37,  1.17it/s, pg=0.0516, ret=-0.000504, glen=98.9, tlen=259, kl=0.00115, act_lr=4e-7, ent=1.72]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:11<00:36,  1.17it/s, pg=0.0516, ret=-0.000504, glen=98.9, tlen=259, kl=0.00115, act_lr=4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 13/56 [00:12<00:36,  1.17it/s, pg=0.0473, ret=-0.00132, glen=133, tlen=293, kl=0.00115, act_lr=4e-7, ent=2.07]  Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:12<00:35,  1.17it/s, pg=0.0473, ret=-0.00132, glen=133, tlen=293, kl=0.00115, act_lr=4e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/56 [00:13<00:35,  1.17it/s, pg=0.0999, ret=-0.000455, glen=104, tlen=264, kl=0.00117, act_lr=4e-7, ent=1.87]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:13<00:35,  1.15it/s, pg=0.0999, ret=-0.000455, glen=104, tlen=264, kl=0.00117, act_lr=4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/56 [00:14<00:35,  1.15it/s, pg=0.283, ret=-0.00247, glen=123, tlen=283, kl=0.0011, act_lr=4e-7, ent=1.92]   Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=0.283, ret=-0.00247, glen=123, tlen=283, kl=0.0011, act_lr=4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 16/56 [00:14<00:34,  1.16it/s, pg=-0.121, ret=-0.000206, glen=87.9, tlen=248, kl=0.00126, act_lr=4e-7, ent=1.7]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:14<00:33,  1.16it/s, pg=-0.121, ret=-0.000206, glen=87.9, tlen=248, kl=0.00126, act_lr=4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 17/56 [00:15<00:33,  1.16it/s, pg=-0.0767, ret=0.000843, glen=112, tlen=272, kl=0.00111, act_lr=4e-7, ent=1.77]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:15<00:32,  1.16it/s, pg=-0.0767, ret=0.000843, glen=112, tlen=272, kl=0.00111, act_lr=4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 18/56 [00:16<00:32,  1.16it/s, pg=0.0331, ret=-0.000534, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.8] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:16<00:31,  1.17it/s, pg=0.0331, ret=-0.000534, glen=104, tlen=264, kl=0.00113, act_lr=4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 19/56 [00:17<00:31,  1.17it/s, pg=-0.114, ret=4.04e-5, glen=102, tlen=262, kl=0.00117, act_lr=4e-7, ent=1.6]  Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:17<00:31,  1.14it/s, pg=-0.114, ret=4.04e-5, glen=102, tlen=262, kl=0.00117, act_lr=4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 20/56 [00:18<00:31,  1.14it/s, pg=-0.0641, ret=0.000113, glen=123, tlen=284, kl=0.00114, act_lr=4e-7, ent=1.83]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:18<00:30,  1.15it/s, pg=-0.0641, ret=0.000113, glen=123, tlen=284, kl=0.00114, act_lr=4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/56 [00:19<00:30,  1.15it/s, pg=-0.014, ret=0.000506, glen=112, tlen=272, kl=0.00111, act_lr=4e-7, ent=1.75] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:19<00:29,  1.16it/s, pg=-0.014, ret=0.000506, glen=112, tlen=272, kl=0.00111, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 22/56 [00:20<00:29,  1.16it/s, pg=-0.0179, ret=-0.000283, glen=93.2, tlen=253, kl=0.00121, act_lr=4e-7, ent=1.61]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.0179, ret=-0.000283, glen=93.2, tlen=253, kl=0.00121, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 23/56 [00:20<00:28,  1.16it/s, pg=-0.183, ret=0.00118, glen=95.3, tlen=256, kl=0.00117, act_lr=4e-7, ent=1.7]    Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:20<00:27,  1.16it/s, pg=-0.183, ret=0.00118, glen=95.3, tlen=256, kl=0.00117, act_lr=4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/56 [00:21<00:27,  1.16it/s, pg=-0.035, ret=-0.000135, glen=99.7, tlen=260, kl=0.00123, act_lr=4e-7, ent=1.85]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:21<00:26,  1.17it/s, pg=-0.035, ret=-0.000135, glen=99.7, tlen=260, kl=0.00123, act_lr=4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 25/56 [00:22<00:26,  1.17it/s, pg=-0.151, ret=0.000254, glen=102, tlen=262, kl=0.00121, act_lr=4e-7, ent=1.67]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:22<00:25,  1.17it/s, pg=-0.151, ret=0.000254, glen=102, tlen=262, kl=0.00121, act_lr=4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/56 [00:23<00:25,  1.17it/s, pg=-0.11, ret=0.000598, glen=108, tlen=268, kl=0.00117, act_lr=4e-7, ent=1.83] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:23<00:27,  1.07it/s, pg=-0.11, ret=0.000598, glen=108, tlen=268, kl=0.00117, act_lr=4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 27/56 [00:24<00:27,  1.07it/s, pg=0.176, ret=-0.00148, glen=146, tlen=305, kl=0.000996, act_lr=4e-7, ent=2.4]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:24<00:25,  1.09it/s, pg=0.176, ret=-0.00148, glen=146, tlen=305, kl=0.000996, act_lr=4e-7, ent=2.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/56 [00:25<00:25,  1.09it/s, pg=0.0656, ret=-0.00061, glen=112, tlen=272, kl=0.0011, act_lr=4e-7, ent=1.82]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:25<00:24,  1.11it/s, pg=0.0656, ret=-0.00061, glen=112, tlen=272, kl=0.0011, act_lr=4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 29/56 [00:26<00:24,  1.11it/s, pg=0.127, ret=2.74e-6, glen=96, tlen=256, kl=0.00121, act_lr=4e-7, ent=1.78]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:26<00:22,  1.13it/s, pg=0.127, ret=2.74e-6, glen=96, tlen=256, kl=0.00121, act_lr=4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 30/56 [00:27<00:22,  1.13it/s, pg=-0.112, ret=-0.000242, glen=116, tlen=276, kl=0.00116, act_lr=4e-7, ent=1.88]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:27<00:21,  1.14it/s, pg=-0.112, ret=-0.000242, glen=116, tlen=276, kl=0.00116, act_lr=4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 31/56 [00:28<00:21,  1.14it/s, pg=-0.0644, ret=0.000655, glen=105, tlen=266, kl=0.00116, act_lr=4e-7, ent=1.65]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:20,  1.15it/s, pg=-0.0644, ret=0.000655, glen=105, tlen=266, kl=0.00116, act_lr=4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 32/56 [00:28<00:20,  1.15it/s, pg=-0.0696, ret=0.000875, glen=98.9, tlen=259, kl=0.00114, act_lr=4e-7, ent=1.61]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:28<00:19,  1.16it/s, pg=-0.0696, ret=0.000875, glen=98.9, tlen=259, kl=0.00114, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 33/56 [00:29<00:19,  1.16it/s, pg=-0.149, ret=-0.000244, glen=97.1, tlen=258, kl=0.00119, act_lr=4e-7, ent=1.61]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:29<00:18,  1.16it/s, pg=-0.149, ret=-0.000244, glen=97.1, tlen=258, kl=0.00119, act_lr=4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 34/56 [00:30<00:18,  1.16it/s, pg=0.078, ret=-0.000495, glen=142, tlen=302, kl=0.00094, act_lr=4e-7, ent=1.55]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:30<00:18,  1.16it/s, pg=0.078, ret=-0.000495, glen=142, tlen=302, kl=0.00094, act_lr=4e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/56 [00:31<00:18,  1.16it/s, pg=0.124, ret=-0.0022, glen=129, tlen=289, kl=0.00113, act_lr=4e-7, ent=1.94]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:31<00:17,  1.16it/s, pg=0.124, ret=-0.0022, glen=129, tlen=289, kl=0.00113, act_lr=4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 36/56 [00:32<00:17,  1.16it/s, pg=-0.185, ret=0.00104, glen=101, tlen=261, kl=0.0012, act_lr=4e-7, ent=1.75]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:32<00:16,  1.17it/s, pg=-0.185, ret=0.00104, glen=101, tlen=261, kl=0.0012, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 37/56 [00:33<00:16,  1.17it/s, pg=-0.167, ret=0.00029, glen=104, tlen=264, kl=0.00115, act_lr=4e-7, ent=1.71]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:33<00:15,  1.17it/s, pg=-0.167, ret=0.00029, glen=104, tlen=264, kl=0.00115, act_lr=4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 38/56 [00:34<00:15,  1.17it/s, pg=-0.233, ret=0.00658, glen=92, tlen=252, kl=0.00125, act_lr=4e-7, ent=1.79] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=-0.233, ret=0.00658, glen=92, tlen=252, kl=0.00125, act_lr=4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 39/56 [00:34<00:14,  1.17it/s, pg=0.00981, ret=-0.000865, glen=103, tlen=263, kl=0.0012, act_lr=4e-7, ent=1.76]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:34<00:13,  1.17it/s, pg=0.00981, ret=-0.000865, glen=103, tlen=263, kl=0.0012, act_lr=4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40/56 [00:35<00:13,  1.17it/s, pg=-0.11, ret=0.00117, glen=105, tlen=265, kl=0.00107, act_lr=4e-7, ent=1.75]   Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:35<00:12,  1.17it/s, pg=-0.11, ret=0.00117, glen=105, tlen=265, kl=0.00107, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 41/56 [00:36<00:12,  1.17it/s, pg=0.199, ret=-0.000752, glen=116, tlen=276, kl=0.00115, act_lr=4e-7, ent=1.92]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:36<00:11,  1.17it/s, pg=0.199, ret=-0.000752, glen=116, tlen=276, kl=0.00115, act_lr=4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 42/56 [00:37<00:11,  1.17it/s, pg=-0.0719, ret=0.000873, glen=108, tlen=268, kl=0.00116, act_lr=4e-7, ent=1.74]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:37<00:11,  1.17it/s, pg=-0.0719, ret=0.000873, glen=108, tlen=268, kl=0.00116, act_lr=4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 43/56 [00:38<00:11,  1.17it/s, pg=-0.118, ret=0.000776, glen=101, tlen=261, kl=0.00116, act_lr=4e-7, ent=1.75] Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:38<00:10,  1.17it/s, pg=-0.118, ret=0.000776, glen=101, tlen=261, kl=0.00116, act_lr=4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 44/56 [00:39<00:10,  1.17it/s, pg=-0.014, ret=0.000578, glen=93.6, tlen=254, kl=0.00117, act_lr=4e-7, ent=1.69]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:39<00:09,  1.17it/s, pg=-0.014, ret=0.000578, glen=93.6, tlen=254, kl=0.00117, act_lr=4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 45/56 [00:40<00:09,  1.17it/s, pg=0.0394, ret=-0.000509, glen=108, tlen=269, kl=0.00111, act_lr=4e-7, ent=1.76]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=0.0394, ret=-0.000509, glen=108, tlen=269, kl=0.00111, act_lr=4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 46/56 [00:40<00:08,  1.17it/s, pg=0.102, ret=-0.00152, glen=98.1, tlen=258, kl=0.00117, act_lr=4e-7, ent=1.57] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:40<00:07,  1.17it/s, pg=0.102, ret=-0.00152, glen=98.1, tlen=258, kl=0.00117, act_lr=4e-7, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 47/56 [00:41<00:07,  1.17it/s, pg=-0.154, ret=-0.000154, glen=97.2, tlen=257, kl=0.00118, act_lr=4e-7, ent=1.76]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:41<00:06,  1.15it/s, pg=-0.154, ret=-0.000154, glen=97.2, tlen=257, kl=0.00118, act_lr=4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 48/56 [00:42<00:06,  1.15it/s, pg=-0.0459, ret=-0.00135, glen=102, tlen=262, kl=0.00116, act_lr=4e-7, ent=1.62] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:42<00:06,  1.16it/s, pg=-0.0459, ret=-0.00135, glen=102, tlen=262, kl=0.00116, act_lr=4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 49/56 [00:43<00:06,  1.16it/s, pg=-0.0958, ret=0.000502, glen=101, tlen=261, kl=0.00116, act_lr=4e-7, ent=1.96]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:43<00:05,  1.16it/s, pg=-0.0958, ret=0.000502, glen=101, tlen=261, kl=0.00116, act_lr=4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 50/56 [00:44<00:05,  1.16it/s, pg=-0.118, ret=-0.000632, glen=102, tlen=262, kl=0.0012, act_lr=4e-7, ent=1.7]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:44<00:04,  1.16it/s, pg=-0.118, ret=-0.000632, glen=102, tlen=262, kl=0.0012, act_lr=4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 51/56 [00:45<00:04,  1.16it/s, pg=0.0812, ret=-3.23e-5, glen=124, tlen=284, kl=0.0011, act_lr=4e-7, ent=1.95]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:45<00:03,  1.15it/s, pg=0.0812, ret=-3.23e-5, glen=124, tlen=284, kl=0.0011, act_lr=4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 52/56 [00:46<00:03,  1.15it/s, pg=-0.225, ret=0.00137, glen=96.6, tlen=257, kl=0.00119, act_lr=4e-7, ent=1.63]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:46<00:02,  1.15it/s, pg=-0.225, ret=0.00137, glen=96.6, tlen=257, kl=0.00119, act_lr=4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 53/56 [00:47<00:02,  1.15it/s, pg=0.396, ret=-0.00718, glen=262, tlen=423, kl=0.00101, act_lr=4e-7, ent=2.41] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.13it/s, pg=0.396, ret=-0.00718, glen=262, tlen=423, kl=0.00101, act_lr=4e-7, ent=2.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 54/56 [00:47<00:01,  1.13it/s, pg=0.0272, ret=-0.000277, glen=94.6, tlen=255, kl=0.00122, act_lr=4e-7, ent=1.69]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:47<00:00,  1.14it/s, pg=0.0272, ret=-0.000277, glen=94.6, tlen=255, kl=0.00122, act_lr=4e-7, ent=1.69]
2025-07-23 13:08:44.497 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 49.08s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:49<00:00,  1.14it/s, pg=-0.0349, ret=0.000156, glen=105, tlen=265, kl=0.00114, act_lr=4.2e-7, ent=1.98]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 55/56 [00:49<00:00,  1.12it/s, pg=-0.0349, ret=0.000156, glen=105, tlen=265, kl=0.00114, act_lr=4.2e-7, ent=1.98]
2025-07-23 13:08:45.340 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 13:08:47.926 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-23 13:08:48.252 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 52.95s
2025-07-23 13:08:48.257 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.019328159945351735, 'actor_lr': 4.003571473073667e-07, 'clip_ratio': 0.0, 'entropy': 1.797073551586696, 'kl': 0.0011489221027919225, 'response_length': 115.53240053994315, 'total_length': 275.64521081107006, 'return': -0.00015357584730476707, 'policy_update_steps': 1.0}

Episode [2/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [32:10<20:47, 249.56s/it][A2025-07-23 13:08:48.290 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:11:10.694 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:11:10.883 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 13:11:10.883 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 142.59s
2025-07-23 13:11:12.904 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 103.7808,std_num_tokens: 160.4559,avg_correct_num_tokens: 89.6364,std_correct_num_tokens: 100.5558,avg_incorrect_num_tokens: 114.4960,std_incorrect_num_tokens: 193.1940
2025-07-23 13:11:13.350 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.47s
2025-07-23 13:11:14.872 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.52s
2025-07-23 13:11:42.823 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 220
2025-07-23 13:11:42.824 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.95s
2025-07-23 13:11:43.629 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.80s
2025-07-23 13:11:43.630 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0019132053474260663, avg_kl: 0.0032974763350053266, avg_response_length: 109.50961851640182, avg_orm_score: 0.0, avg_custom_rewards: -0.0019132053474260663
2025-07-23 13:11:43.670 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter21_replay_buffer.jsonl
2025-07-23 13:11:45.234 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.57s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s, pg=0.0585, ret=-0.000823, glen=111, tlen=271, kl=0.00319, act_lr=4.2e-7, ent=1.78]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:00<00:53,  1.00it/s, pg=0.0585, ret=-0.000823, glen=111, tlen=271, kl=0.00319, act_lr=4.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:53,  1.00it/s, pg=-0.0634, ret=-0.000668, glen=131, tlen=291, kl=0.00296, act_lr=4.2e-7, ent=2.61]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.08it/s, pg=-0.0634, ret=-0.000668, glen=131, tlen=291, kl=0.00296, act_lr=4.2e-7, ent=2.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.08it/s, pg=-0.179, ret=-0.00103, glen=134, tlen=295, kl=0.00314, act_lr=4.2e-7, ent=1.89]  Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:47,  1.10it/s, pg=-0.179, ret=-0.00103, glen=134, tlen=295, kl=0.00314, act_lr=4.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:47,  1.10it/s, pg=0.132, ret=-0.00147, glen=117, tlen=278, kl=0.00314, act_lr=4.2e-7, ent=2.09] Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:46,  1.10it/s, pg=0.132, ret=-0.00147, glen=117, tlen=278, kl=0.00314, act_lr=4.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:46,  1.10it/s, pg=0.04, ret=0.000597, glen=108, tlen=269, kl=0.00334, act_lr=4.2e-7, ent=1.92] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:44,  1.12it/s, pg=0.04, ret=0.000597, glen=108, tlen=269, kl=0.00334, act_lr=4.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:44,  1.12it/s, pg=-0.173, ret=0.00112, glen=97.3, tlen=258, kl=0.00336, act_lr=4.2e-7, ent=1.74]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.12it/s, pg=-0.173, ret=0.00112, glen=97.3, tlen=258, kl=0.00336, act_lr=4.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.12it/s, pg=-0.225, ret=0.00144, glen=101, tlen=261, kl=0.00346, act_lr=4.2e-7, ent=1.83] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:42,  1.13it/s, pg=-0.225, ret=0.00144, glen=101, tlen=261, kl=0.00346, act_lr=4.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:42,  1.13it/s, pg=0.272, ret=-0.00288, glen=352, tlen=512, kl=0.00254, act_lr=4.2e-7, ent=1.47]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:42,  1.11it/s, pg=0.272, ret=-0.00288, glen=352, tlen=512, kl=0.00254, act_lr=4.2e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:08<00:42,  1.11it/s, pg=0.0768, ret=-0.000141, glen=139, tlen=300, kl=0.00294, act_lr=4.2e-7, ent=1.93]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:41,  1.12it/s, pg=0.0768, ret=-0.000141, glen=139, tlen=300, kl=0.00294, act_lr=4.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:41,  1.12it/s, pg=-0.144, ret=-0.000692, glen=99.7, tlen=261, kl=0.00323, act_lr=4.2e-7, ent=1.57]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.13it/s, pg=-0.144, ret=-0.000692, glen=99.7, tlen=261, kl=0.00323, act_lr=4.2e-7, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.13it/s, pg=0.103, ret=-0.000478, glen=118, tlen=279, kl=0.00293, act_lr=4.2e-7, ent=1.86]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.15it/s, pg=0.103, ret=-0.000478, glen=118, tlen=279, kl=0.00293, act_lr=4.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.15it/s, pg=-0.0275, ret=-0.0011, glen=86.3, tlen=247, kl=0.00384, act_lr=4.2e-7, ent=1.87]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.15it/s, pg=-0.0275, ret=-0.0011, glen=86.3, tlen=247, kl=0.00384, act_lr=4.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.15it/s, pg=0.0859, ret=7.07e-5, glen=120, tlen=281, kl=0.00315, act_lr=4.2e-7, ent=2.17]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.16it/s, pg=0.0859, ret=7.07e-5, glen=120, tlen=281, kl=0.00315, act_lr=4.2e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.16it/s, pg=-0.132, ret=0.00112, glen=99.5, tlen=260, kl=0.00327, act_lr=4.2e-7, ent=1.7]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.16it/s, pg=-0.132, ret=0.00112, glen=99.5, tlen=260, kl=0.00327, act_lr=4.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.16it/s, pg=-0.054, ret=0.00169, glen=103, tlen=264, kl=0.00291, act_lr=4.2e-7, ent=2.12]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.054, ret=0.00169, glen=103, tlen=264, kl=0.00291, act_lr=4.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:14<00:34,  1.17it/s, pg=-0.0257, ret=0.000391, glen=103, tlen=265, kl=0.00322, act_lr=4.2e-7, ent=1.9]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.0257, ret=0.000391, glen=103, tlen=265, kl=0.00322, act_lr=4.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=0.189, ret=-0.00164, glen=111, tlen=272, kl=0.00323, act_lr=4.2e-7, ent=2.12] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=0.189, ret=-0.00164, glen=111, tlen=272, kl=0.00323, act_lr=4.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=-0.274, ret=0.00181, glen=104, tlen=265, kl=0.0032, act_lr=4.2e-7, ent=1.74] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.274, ret=0.00181, glen=104, tlen=265, kl=0.0032, act_lr=4.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=-0.221, ret=0.00259, glen=95.6, tlen=256, kl=0.00368, act_lr=4.2e-7, ent=1.71]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.221, ret=0.00259, glen=95.6, tlen=256, kl=0.00368, act_lr=4.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=0.0487, ret=-0.000735, glen=94.7, tlen=256, kl=0.00362, act_lr=4.2e-7, ent=1.76]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=0.0487, ret=-0.000735, glen=94.7, tlen=256, kl=0.00362, act_lr=4.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=-0.0875, ret=0.000129, glen=101, tlen=262, kl=0.00338, act_lr=4.2e-7, ent=1.64] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:28,  1.17it/s, pg=-0.0875, ret=0.000129, glen=101, tlen=262, kl=0.00338, act_lr=4.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:28,  1.17it/s, pg=0.162, ret=-0.000682, glen=98.7, tlen=259, kl=0.00339, act_lr=4.2e-7, ent=1.99]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.16it/s, pg=0.162, ret=-0.000682, glen=98.7, tlen=259, kl=0.00339, act_lr=4.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:20<00:28,  1.16it/s, pg=-0.0403, ret=-0.00144, glen=104, tlen=265, kl=0.00339, act_lr=4.2e-7, ent=1.72]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.0403, ret=-0.00144, glen=104, tlen=265, kl=0.00339, act_lr=4.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.0114, ret=0.000105, glen=91.8, tlen=253, kl=0.00367, act_lr=4.2e-7, ent=1.86]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.16it/s, pg=-0.0114, ret=0.000105, glen=91.8, tlen=253, kl=0.00367, act_lr=4.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.16it/s, pg=0.0535, ret=-0.00023, glen=99.4, tlen=260, kl=0.00332, act_lr=4.2e-7, ent=1.8]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=0.0535, ret=-0.00023, glen=99.4, tlen=260, kl=0.00332, act_lr=4.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.0546, ret=0.000167, glen=93.3, tlen=254, kl=0.00366, act_lr=4.2e-7, ent=1.8]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.0546, ret=0.000167, glen=93.3, tlen=254, kl=0.00366, act_lr=4.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.0103, ret=-0.00112, glen=99.8, tlen=260, kl=0.00329, act_lr=4.2e-7, ent=1.7]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:26,  1.07it/s, pg=-0.0103, ret=-0.00112, glen=99.8, tlen=260, kl=0.00329, act_lr=4.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:26,  1.07it/s, pg=-0.0386, ret=0.00015, glen=105, tlen=265, kl=0.00335, act_lr=4.2e-7, ent=1.82] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:24,  1.10it/s, pg=-0.0386, ret=0.00015, glen=105, tlen=265, kl=0.00335, act_lr=4.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:24,  1.10it/s, pg=0.0171, ret=-0.00095, glen=102, tlen=262, kl=0.00322, act_lr=4.2e-7, ent=1.73]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:23,  1.12it/s, pg=0.0171, ret=-0.00095, glen=102, tlen=262, kl=0.00322, act_lr=4.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:23,  1.12it/s, pg=-0.176, ret=0.000685, glen=98.1, tlen=259, kl=0.00356, act_lr=4.2e-7, ent=1.87]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.14it/s, pg=-0.176, ret=0.000685, glen=98.1, tlen=259, kl=0.00356, act_lr=4.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.14it/s, pg=-0.0688, ret=0.00119, glen=95.3, tlen=256, kl=0.00313, act_lr=4.2e-7, ent=1.73]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.14it/s, pg=-0.0688, ret=0.00119, glen=95.3, tlen=256, kl=0.00313, act_lr=4.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:28<00:21,  1.14it/s, pg=0.0228, ret=-0.000211, glen=112, tlen=273, kl=0.0033, act_lr=4.2e-7, ent=1.99] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:19,  1.15it/s, pg=0.0228, ret=-0.000211, glen=112, tlen=273, kl=0.0033, act_lr=4.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:19,  1.15it/s, pg=-0.074, ret=-0.00035, glen=94.6, tlen=255, kl=0.0036, act_lr=4.2e-7, ent=1.67]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:18,  1.16it/s, pg=-0.074, ret=-0.00035, glen=94.6, tlen=255, kl=0.0036, act_lr=4.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:18,  1.16it/s, pg=0.211, ret=0.000318, glen=145, tlen=306, kl=0.00293, act_lr=4.2e-7, ent=2.5]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.16it/s, pg=0.211, ret=0.000318, glen=145, tlen=306, kl=0.00293, act_lr=4.2e-7, ent=2.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.16it/s, pg=-0.168, ret=-0.000197, glen=95.1, tlen=256, kl=0.00338, act_lr=4.2e-7, ent=1.9]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.168, ret=-0.000197, glen=95.1, tlen=256, kl=0.00338, act_lr=4.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=0.0638, ret=-0.000853, glen=103, tlen=264, kl=0.0032, act_lr=4.2e-7, ent=1.89] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=0.0638, ret=-0.000853, glen=103, tlen=264, kl=0.0032, act_lr=4.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=-0.212, ret=0.000987, glen=102, tlen=263, kl=0.00313, act_lr=4.2e-7, ent=1.96]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.212, ret=0.000987, glen=102, tlen=263, kl=0.00313, act_lr=4.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.17it/s, pg=0.0261, ret=0.000101, glen=118, tlen=278, kl=0.0032, act_lr=4.2e-7, ent=1.99] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.0261, ret=0.000101, glen=118, tlen=278, kl=0.0032, act_lr=4.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:34<00:14,  1.17it/s, pg=-0.0223, ret=-0.00103, glen=108, tlen=269, kl=0.00303, act_lr=4.2e-7, ent=1.81]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.0223, ret=-0.00103, glen=108, tlen=269, kl=0.00303, act_lr=4.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.173, ret=0.00164, glen=99.3, tlen=260, kl=0.00316, act_lr=4.2e-7, ent=1.78] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.173, ret=0.00164, glen=99.3, tlen=260, kl=0.00316, act_lr=4.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=-0.0146, ret=-0.000585, glen=89.9, tlen=251, kl=0.0034, act_lr=4.2e-7, ent=1.8]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=-0.0146, ret=-0.000585, glen=89.9, tlen=251, kl=0.0034, act_lr=4.2e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=0.0416, ret=0.000233, glen=96.3, tlen=256, kl=0.00343, act_lr=4.2e-7, ent=1.72]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.17it/s, pg=0.0416, ret=0.000233, glen=96.3, tlen=256, kl=0.00343, act_lr=4.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.17it/s, pg=0.0364, ret=-0.00444, glen=99.1, tlen=260, kl=0.00375, act_lr=4.2e-7, ent=1.81]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.17it/s, pg=0.0364, ret=-0.00444, glen=99.1, tlen=260, kl=0.00375, act_lr=4.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.17it/s, pg=-0.0193, ret=-3.23e-5, glen=100, tlen=260, kl=0.00322, act_lr=4.2e-7, ent=1.79]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=-0.0193, ret=-3.23e-5, glen=100, tlen=260, kl=0.00322, act_lr=4.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.17it/s, pg=0.098, ret=-0.00105, glen=99.9, tlen=260, kl=0.00304, act_lr=4.2e-7, ent=1.52] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=0.098, ret=-0.00105, glen=99.9, tlen=260, kl=0.00304, act_lr=4.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.17it/s, pg=-0.0393, ret=-0.000116, glen=94.2, tlen=255, kl=0.00359, act_lr=4.2e-7, ent=1.75]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.17it/s, pg=-0.0393, ret=-0.000116, glen=94.2, tlen=255, kl=0.00359, act_lr=4.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.222, ret=0.00137, glen=93.3, tlen=254, kl=0.00394, act_lr=4.2e-7, ent=1.82]   Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=-0.222, ret=0.00137, glen=93.3, tlen=254, kl=0.00394, act_lr=4.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=0.179, ret=-0.00229, glen=107, tlen=268, kl=0.00331, act_lr=4.2e-7, ent=1.96] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=0.179, ret=-0.00229, glen=107, tlen=268, kl=0.00331, act_lr=4.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=-0.0352, ret=0.000678, glen=116, tlen=276, kl=0.00325, act_lr=4.2e-7, ent=2.09]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=-0.0352, ret=0.000678, glen=116, tlen=276, kl=0.00325, act_lr=4.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=0.118, ret=-0.00151, glen=102, tlen=262, kl=0.00342, act_lr=4.2e-7, ent=2.03]  Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=0.118, ret=-0.00151, glen=102, tlen=262, kl=0.00342, act_lr=4.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=0.0683, ret=0.00045, glen=116, tlen=277, kl=0.00308, act_lr=4.2e-7, ent=1.94]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=0.0683, ret=0.00045, glen=116, tlen=277, kl=0.00308, act_lr=4.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=0.0368, ret=-0.000291, glen=108, tlen=268, kl=0.00328, act_lr=4.2e-7, ent=2] Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.0368, ret=-0.000291, glen=108, tlen=268, kl=0.00328, act_lr=4.2e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.198, ret=0.00149, glen=93.8, tlen=254, kl=0.00343, act_lr=4.2e-7, ent=1.79]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.17it/s, pg=-0.198, ret=0.00149, glen=93.8, tlen=254, kl=0.00343, act_lr=4.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=0.159, ret=-0.000765, glen=111, tlen=272, kl=0.00288, act_lr=4.2e-7, ent=2.05]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.17it/s, pg=0.159, ret=-0.000765, glen=111, tlen=272, kl=0.00288, act_lr=4.2e-7, ent=2.05]
2025-07-23 13:12:33.139 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.75s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.17it/s, pg=0.054, ret=-1.86e-5, glen=104, tlen=264, kl=0.00368, act_lr=4.4e-7, ent=1.9]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=0.054, ret=-1.86e-5, glen=104, tlen=264, kl=0.00368, act_lr=4.4e-7, ent=1.9]
2025-07-23 13:12:33.821 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 13:12:36.135 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.31s
2025-07-23 13:12:36.440 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 51.16s
2025-07-23 13:12:36.446 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.01508976329456676, 'actor_lr': 4.2036362877049465e-07, 'clip_ratio': 0.0, 'entropy': 1.8710681265050715, 'kl': 0.0032974763350053266, 'response_length': 109.50961900190873, 'total_length': 270.0976412686435, 'return': -0.00016908347064269368, 'policy_update_steps': 1.0}

Episode [2/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [35:59<16:11, 242.88s/it][A2025-07-23 13:12:36.480 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:14:27.899 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:14:28.083 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:14:28.084 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 111.60s
2025-07-23 13:14:30.063 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0085,avg_pass_at_n: 1.0000,avg_num_tokens: 99.7579,std_num_tokens: 122.7023,avg_correct_num_tokens: 89.4720,std_correct_num_tokens: 80.5087,avg_incorrect_num_tokens: 107.6281,std_incorrect_num_tokens: 146.5380
2025-07-23 13:14:30.520 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.44s
2025-07-23 13:14:32.004 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.48s
2025-07-23 13:14:59.386 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 216
2025-07-23 13:14:59.386 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.38s
2025-07-23 13:15:00.188 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.80s
2025-07-23 13:15:00.189 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0006531438473892329, avg_kl: 0.0037127953988534434, avg_response_length: 101.53834056854248, avg_orm_score: 0.0, avg_custom_rewards: 0.0006531438473892329
2025-07-23 13:15:00.220 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter22_replay_buffer.jsonl
2025-07-23 13:15:01.719 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.50s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:00<?, ?it/s, pg=-0.199, ret=0.00106, glen=95.5, tlen=256, kl=0.00402, act_lr=4.4e-7, ent=1.74]Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:00<00:50,  1.04it/s, pg=-0.199, ret=0.00106, glen=95.5, tlen=256, kl=0.00402, act_lr=4.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:01<00:50,  1.04it/s, pg=0.149, ret=-0.000314, glen=92.2, tlen=253, kl=0.00369, act_lr=4.4e-7, ent=1.69]Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:01<00:48,  1.07it/s, pg=0.149, ret=-0.000314, glen=92.2, tlen=253, kl=0.00369, act_lr=4.4e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:02<00:48,  1.07it/s, pg=0.0947, ret=-0.000701, glen=102, tlen=262, kl=0.00417, act_lr=4.4e-7, ent=1.85]Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:02<00:45,  1.11it/s, pg=0.0947, ret=-0.000701, glen=102, tlen=262, kl=0.00417, act_lr=4.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:03<00:45,  1.11it/s, pg=-0.0704, ret=0.000851, glen=98.4, tlen=259, kl=0.00351, act_lr=4.4e-7, ent=1.62]Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:03<00:43,  1.14it/s, pg=-0.0704, ret=0.000851, glen=98.4, tlen=259, kl=0.00351, act_lr=4.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:04<00:43,  1.14it/s, pg=0.0779, ret=-0.000308, glen=90.8, tlen=251, kl=0.00379, act_lr=4.4e-7, ent=1.65]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:04<00:42,  1.15it/s, pg=0.0779, ret=-0.000308, glen=90.8, tlen=251, kl=0.00379, act_lr=4.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:05<00:42,  1.15it/s, pg=-0.00769, ret=-0.00034, glen=105, tlen=266, kl=0.0036, act_lr=4.4e-7, ent=1.64] Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:05<00:41,  1.16it/s, pg=-0.00769, ret=-0.00034, glen=105, tlen=266, kl=0.0036, act_lr=4.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:06<00:41,  1.16it/s, pg=-0.13, ret=0.000911, glen=94.1, tlen=255, kl=0.00354, act_lr=4.4e-7, ent=1.76] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:06<00:40,  1.16it/s, pg=-0.13, ret=0.000911, glen=94.1, tlen=255, kl=0.00354, act_lr=4.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:06<00:40,  1.16it/s, pg=-0.108, ret=-0.000178, glen=93.8, tlen=254, kl=0.0037, act_lr=4.4e-7, ent=1.82]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:06<00:39,  1.17it/s, pg=-0.108, ret=-0.000178, glen=93.8, tlen=254, kl=0.0037, act_lr=4.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:07<00:39,  1.17it/s, pg=0.0194, ret=-0.00107, glen=111, tlen=271, kl=0.0034, act_lr=4.4e-7, ent=2.04]  Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:07<00:39,  1.14it/s, pg=0.0194, ret=-0.00107, glen=111, tlen=271, kl=0.0034, act_lr=4.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:08<00:39,  1.14it/s, pg=-0.0259, ret=-0.000191, glen=93, tlen=253, kl=0.00404, act_lr=4.4e-7, ent=1.8]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:08<00:38,  1.15it/s, pg=-0.0259, ret=-0.000191, glen=93, tlen=253, kl=0.00404, act_lr=4.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:09<00:38,  1.15it/s, pg=-0.0257, ret=5.08e-5, glen=105, tlen=266, kl=0.00345, act_lr=4.4e-7, ent=1.82]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:09<00:37,  1.16it/s, pg=-0.0257, ret=5.08e-5, glen=105, tlen=266, kl=0.00345, act_lr=4.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:10<00:37,  1.16it/s, pg=-0.055, ret=0.000979, glen=99.9, tlen=260, kl=0.00355, act_lr=4.4e-7, ent=1.94]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:10<00:36,  1.16it/s, pg=-0.055, ret=0.000979, glen=99.9, tlen=260, kl=0.00355, act_lr=4.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:11<00:36,  1.16it/s, pg=-0.109, ret=0.000135, glen=93.2, tlen=254, kl=0.00378, act_lr=4.4e-7, ent=1.66]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:11<00:35,  1.17it/s, pg=-0.109, ret=0.000135, glen=93.2, tlen=254, kl=0.00378, act_lr=4.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:12<00:35,  1.17it/s, pg=0.161, ret=-0.00136, glen=99.8, tlen=260, kl=0.00348, act_lr=4.4e-7, ent=1.86] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:12<00:34,  1.17it/s, pg=0.161, ret=-0.00136, glen=99.8, tlen=260, kl=0.00348, act_lr=4.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:13<00:34,  1.17it/s, pg=-0.0851, ret=0.00146, glen=96.9, tlen=257, kl=0.00374, act_lr=4.4e-7, ent=1.87]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:13<00:33,  1.17it/s, pg=-0.0851, ret=0.00146, glen=96.9, tlen=257, kl=0.00374, act_lr=4.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:13<00:33,  1.17it/s, pg=0.145, ret=-0.000903, glen=104, tlen=265, kl=0.0039, act_lr=4.4e-7, ent=1.81]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:13<00:32,  1.17it/s, pg=0.145, ret=-0.000903, glen=104, tlen=265, kl=0.0039, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:14<00:32,  1.17it/s, pg=-0.19, ret=0.00212, glen=110, tlen=271, kl=0.00341, act_lr=4.4e-7, ent=2.01] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:14<00:31,  1.17it/s, pg=-0.19, ret=0.00212, glen=110, tlen=271, kl=0.00341, act_lr=4.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:15<00:31,  1.17it/s, pg=-0.014, ret=0.000416, glen=102, tlen=262, kl=0.00362, act_lr=4.4e-7, ent=1.9]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:15<00:30,  1.18it/s, pg=-0.014, ret=0.000416, glen=102, tlen=262, kl=0.00362, act_lr=4.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:16<00:30,  1.18it/s, pg=0.126, ret=-0.00048, glen=116, tlen=277, kl=0.0034, act_lr=4.4e-7, ent=1.91] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:16<00:29,  1.17it/s, pg=0.126, ret=-0.00048, glen=116, tlen=277, kl=0.0034, act_lr=4.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:17<00:29,  1.17it/s, pg=0.000534, ret=0.000232, glen=107, tlen=267, kl=0.00371, act_lr=4.4e-7, ent=1.78]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:17<00:28,  1.17it/s, pg=0.000534, ret=0.000232, glen=107, tlen=267, kl=0.00371, act_lr=4.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:18<00:28,  1.17it/s, pg=-0.0125, ret=-0.000489, glen=95.1, tlen=255, kl=0.004, act_lr=4.4e-7, ent=1.78] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:18<00:28,  1.17it/s, pg=-0.0125, ret=-0.000489, glen=95.1, tlen=255, kl=0.004, act_lr=4.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:18<00:28,  1.17it/s, pg=-0.0466, ret=-3.38e-5, glen=112, tlen=274, kl=0.00333, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:18<00:27,  1.17it/s, pg=-0.0466, ret=-3.38e-5, glen=112, tlen=274, kl=0.00333, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:19<00:27,  1.17it/s, pg=0.041, ret=-0.00166, glen=88.1, tlen=249, kl=0.00422, act_lr=4.4e-7, ent=1.72] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:19<00:26,  1.17it/s, pg=0.041, ret=-0.00166, glen=88.1, tlen=249, kl=0.00422, act_lr=4.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:20<00:26,  1.17it/s, pg=0.0466, ret=0.000297, glen=92.3, tlen=252, kl=0.0041, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:20<00:25,  1.17it/s, pg=0.0466, ret=0.000297, glen=92.3, tlen=252, kl=0.0041, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:21<00:25,  1.17it/s, pg=0.112, ret=-0.000339, glen=108, tlen=269, kl=0.00364, act_lr=4.4e-7, ent=1.9] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:21<00:25,  1.16it/s, pg=0.112, ret=-0.000339, glen=108, tlen=269, kl=0.00364, act_lr=4.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:22<00:25,  1.16it/s, pg=-0.137, ret=0.000887, glen=91.8, tlen=252, kl=0.00359, act_lr=4.4e-7, ent=1.93]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:22<00:24,  1.16it/s, pg=-0.137, ret=0.000887, glen=91.8, tlen=252, kl=0.00359, act_lr=4.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:23<00:24,  1.16it/s, pg=-0.104, ret=0.000779, glen=95.1, tlen=255, kl=0.00396, act_lr=4.4e-7, ent=1.73]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:23<00:25,  1.06it/s, pg=-0.104, ret=0.000779, glen=95.1, tlen=255, kl=0.00396, act_lr=4.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:24<00:25,  1.06it/s, pg=0.0398, ret=-0.000105, glen=104, tlen=264, kl=0.00358, act_lr=4.4e-7, ent=2.28]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:24<00:23,  1.09it/s, pg=0.0398, ret=-0.000105, glen=104, tlen=264, kl=0.00358, act_lr=4.4e-7, ent=2.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:25<00:23,  1.09it/s, pg=-0.0384, ret=-0.00074, glen=93.5, tlen=253, kl=0.00379, act_lr=4.4e-7, ent=1.82]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:25<00:22,  1.12it/s, pg=-0.0384, ret=-0.00074, glen=93.5, tlen=253, kl=0.00379, act_lr=4.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:26<00:22,  1.12it/s, pg=-0.0978, ret=0.00093, glen=102, tlen=262, kl=0.00362, act_lr=4.4e-7, ent=1.94]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:26<00:21,  1.13it/s, pg=-0.0978, ret=0.00093, glen=102, tlen=262, kl=0.00362, act_lr=4.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:26<00:21,  1.13it/s, pg=-0.0939, ret=-0.0005, glen=93.8, tlen=254, kl=0.00397, act_lr=4.4e-7, ent=1.87]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:26<00:20,  1.15it/s, pg=-0.0939, ret=-0.0005, glen=93.8, tlen=254, kl=0.00397, act_lr=4.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:27<00:20,  1.15it/s, pg=-0.0459, ret=-0.000891, glen=92.1, tlen=253, kl=0.00401, act_lr=4.4e-7, ent=1.68]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:27<00:19,  1.16it/s, pg=-0.0459, ret=-0.000891, glen=92.1, tlen=253, kl=0.00401, act_lr=4.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:28<00:19,  1.16it/s, pg=0.0924, ret=-0.00124, glen=111, tlen=272, kl=0.00373, act_lr=4.4e-7, ent=2.02]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:28<00:18,  1.16it/s, pg=0.0924, ret=-0.00124, glen=111, tlen=272, kl=0.00373, act_lr=4.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:29<00:18,  1.16it/s, pg=0.0569, ret=0.00064, glen=112, tlen=273, kl=0.00346, act_lr=4.4e-7, ent=2.09] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:29<00:17,  1.16it/s, pg=0.0569, ret=0.00064, glen=112, tlen=273, kl=0.00346, act_lr=4.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:30<00:17,  1.16it/s, pg=-0.183, ret=0.000632, glen=92.6, tlen=253, kl=0.0041, act_lr=4.4e-7, ent=1.71]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:30<00:16,  1.17it/s, pg=-0.183, ret=0.000632, glen=92.6, tlen=253, kl=0.0041, act_lr=4.4e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:31<00:16,  1.17it/s, pg=-0.0459, ret=0.000473, glen=102, tlen=263, kl=0.00354, act_lr=4.4e-7, ent=1.82]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:31<00:15,  1.17it/s, pg=-0.0459, ret=0.000473, glen=102, tlen=263, kl=0.00354, act_lr=4.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:32<00:15,  1.17it/s, pg=-0.269, ret=0.00241, glen=90.4, tlen=251, kl=0.00373, act_lr=4.4e-7, ent=1.74] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:32<00:14,  1.17it/s, pg=-0.269, ret=0.00241, glen=90.4, tlen=251, kl=0.00373, act_lr=4.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:32<00:14,  1.17it/s, pg=-0.133, ret=0.000472, glen=92.6, tlen=253, kl=0.00401, act_lr=4.4e-7, ent=1.87]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:32<00:13,  1.17it/s, pg=-0.133, ret=0.000472, glen=92.6, tlen=253, kl=0.00401, act_lr=4.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:33<00:13,  1.17it/s, pg=0.111, ret=-0.000634, glen=109, tlen=269, kl=0.00357, act_lr=4.4e-7, ent=1.94] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:33<00:12,  1.17it/s, pg=0.111, ret=-0.000634, glen=109, tlen=269, kl=0.00357, act_lr=4.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:34<00:12,  1.17it/s, pg=-0.0261, ret=0.000553, glen=105, tlen=265, kl=0.00345, act_lr=4.4e-7, ent=1.93]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:34<00:11,  1.17it/s, pg=-0.0261, ret=0.000553, glen=105, tlen=265, kl=0.00345, act_lr=4.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:35<00:11,  1.17it/s, pg=-0.04, ret=-0.000522, glen=94.5, tlen=255, kl=0.00347, act_lr=4.4e-7, ent=1.81]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:35<00:11,  1.17it/s, pg=-0.04, ret=-0.000522, glen=94.5, tlen=255, kl=0.00347, act_lr=4.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:36<00:11,  1.17it/s, pg=-0.0324, ret=0.000912, glen=98.8, tlen=259, kl=0.00388, act_lr=4.4e-7, ent=1.79]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:36<00:10,  1.17it/s, pg=-0.0324, ret=0.000912, glen=98.8, tlen=259, kl=0.00388, act_lr=4.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:37<00:10,  1.17it/s, pg=0.105, ret=-0.000298, glen=91.6, tlen=252, kl=0.00442, act_lr=4.4e-7, ent=1.78] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:37<00:09,  1.17it/s, pg=0.105, ret=-0.000298, glen=91.6, tlen=252, kl=0.00442, act_lr=4.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:38<00:09,  1.17it/s, pg=0.122, ret=-0.000127, glen=103, tlen=264, kl=0.00394, act_lr=4.4e-7, ent=1.78] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:38<00:08,  1.17it/s, pg=0.122, ret=-0.000127, glen=103, tlen=264, kl=0.00394, act_lr=4.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:38<00:08,  1.17it/s, pg=0.117, ret=-0.000998, glen=94.5, tlen=255, kl=0.00384, act_lr=4.4e-7, ent=1.66]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:38<00:07,  1.18it/s, pg=0.117, ret=-0.000998, glen=94.5, tlen=255, kl=0.00384, act_lr=4.4e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:39<00:07,  1.18it/s, pg=0.143, ret=-0.00121, glen=116, tlen=276, kl=0.00369, act_lr=4.4e-7, ent=2.17]  Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:39<00:06,  1.18it/s, pg=0.143, ret=-0.00121, glen=116, tlen=276, kl=0.00369, act_lr=4.4e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:40<00:06,  1.18it/s, pg=0.0898, ret=-0.000625, glen=82.1, tlen=242, kl=0.00478, act_lr=4.4e-7, ent=1.83]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:40<00:05,  1.18it/s, pg=0.0898, ret=-0.000625, glen=82.1, tlen=242, kl=0.00478, act_lr=4.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:41<00:05,  1.18it/s, pg=0.124, ret=-0.000596, glen=130, tlen=290, kl=0.00308, act_lr=4.4e-7, ent=2.39]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:41<00:05,  1.17it/s, pg=0.124, ret=-0.000596, glen=130, tlen=290, kl=0.00308, act_lr=4.4e-7, ent=2.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:42<00:05,  1.17it/s, pg=0.0394, ret=-0.000113, glen=99.3, tlen=260, kl=0.00372, act_lr=4.4e-7, ent=1.64]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:42<00:04,  1.17it/s, pg=0.0394, ret=-0.000113, glen=99.3, tlen=260, kl=0.00372, act_lr=4.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:43<00:04,  1.17it/s, pg=0.0255, ret=-0.000361, glen=88.2, tlen=249, kl=0.00375, act_lr=4.4e-7, ent=1.68]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:43<00:03,  1.17it/s, pg=0.0255, ret=-0.000361, glen=88.2, tlen=249, kl=0.00375, act_lr=4.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:44<00:03,  1.17it/s, pg=-0.159, ret=0.00149, glen=106, tlen=266, kl=0.00357, act_lr=4.4e-7, ent=1.94]   Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:44<00:02,  1.17it/s, pg=-0.159, ret=0.00149, glen=106, tlen=266, kl=0.00357, act_lr=4.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:44<00:02,  1.17it/s, pg=0.0823, ret=0.000138, glen=112, tlen=272, kl=0.00323, act_lr=4.4e-7, ent=2.2]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:44<00:01,  1.17it/s, pg=0.0823, ret=0.000138, glen=112, tlen=272, kl=0.00323, act_lr=4.4e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:45<00:01,  1.17it/s, pg=0.0638, ret=-0.000847, glen=110, tlen=270, kl=0.00344, act_lr=4.4e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:45<00:00,  1.17it/s, pg=0.0638, ret=-0.000847, glen=110, tlen=270, kl=0.00344, act_lr=4.4e-7, ent=1.82]
2025-07-23 13:15:48.567 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 46.68s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:46<00:00,  1.17it/s, pg=0.0443, ret=0.00129, glen=165, tlen=326, kl=0.0028, act_lr=4.6e-7, ent=1.93]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:46<00:00,  1.14it/s, pg=0.0443, ret=0.00129, glen=165, tlen=326, kl=0.0028, act_lr=4.6e-7, ent=1.93]
2025-07-23 13:15:49.259 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 13:15:51.465 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.21s
2025-07-23 13:15:51.770 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 50.00s
2025-07-23 13:15:51.775 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.004685719807942708, 'actor_lr': 4.403703781219048e-07, 'clip_ratio': 0.0, 'entropy': 1.8511281057640359, 'kl': 0.0037127953988534434, 'response_length': 101.53834123964663, 'total_length': 261.9098507916486, 'return': 3.580883397565534e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [39:14<11:24, 228.20s/it][A2025-07-23 13:15:51.807 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:18:26.119 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:18:26.296 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:18:26.297 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 154.49s
2025-07-23 13:18:28.168 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0157,avg_reflection_pattern_score: 0.0110,avg_pass_at_n: 1.0000,avg_num_tokens: 100.6855,std_num_tokens: 152.3755,avg_correct_num_tokens: 91.0217,std_correct_num_tokens: 91.0334,avg_incorrect_num_tokens: 107.2676,std_incorrect_num_tokens: 182.4309
2025-07-23 13:18:28.603 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.31s
2025-07-23 13:18:30.128 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.52s
2025-07-23 13:18:58.165 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 217
2025-07-23 13:18:58.166 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.04s
2025-07-23 13:18:58.996 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.83s
2025-07-23 13:18:58.997 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0008305942225012465, avg_kl: 0.003994954895863335, avg_response_length: 105.97095264487552, avg_orm_score: 0.0, avg_custom_rewards: -0.0008305942225012465
2025-07-23 13:18:59.024 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter23_replay_buffer.jsonl
2025-07-23 13:19:00.543 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.52s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s, pg=0.0676, ret=-0.0014, glen=99.7, tlen=260, kl=0.00378, act_lr=4.6e-7, ent=1.87]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:00<00:52,  1.03it/s, pg=0.0676, ret=-0.0014, glen=99.7, tlen=260, kl=0.00378, act_lr=4.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:52,  1.03it/s, pg=0.0127, ret=0.000158, glen=101, tlen=261, kl=0.00379, act_lr=4.6e-7, ent=2.06]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:47,  1.11it/s, pg=0.0127, ret=0.000158, glen=101, tlen=261, kl=0.00379, act_lr=4.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:47,  1.11it/s, pg=-0.0464, ret=0.000202, glen=93.6, tlen=254, kl=0.00431, act_lr=4.6e-7, ent=1.76]Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:47,  1.11it/s, pg=-0.0464, ret=0.000202, glen=93.6, tlen=254, kl=0.00431, act_lr=4.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:47,  1.11it/s, pg=-0.00247, ret=-0.00122, glen=92.4, tlen=253, kl=0.00476, act_lr=4.6e-7, ent=1.68]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.13it/s, pg=-0.00247, ret=-0.00122, glen=92.4, tlen=253, kl=0.00476, act_lr=4.6e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.13it/s, pg=0.144, ret=-0.00085, glen=187, tlen=348, kl=0.00361, act_lr=4.6e-7, ent=1.88]    Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:45,  1.10it/s, pg=0.144, ret=-0.00085, glen=187, tlen=348, kl=0.00361, act_lr=4.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:45,  1.10it/s, pg=0.0104, ret=0.000105, glen=87.7, tlen=248, kl=0.00436, act_lr=4.6e-7, ent=1.72]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:43,  1.13it/s, pg=0.0104, ret=0.000105, glen=87.7, tlen=248, kl=0.00436, act_lr=4.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:43,  1.13it/s, pg=-0.115, ret=-9.61e-7, glen=99.2, tlen=260, kl=0.00393, act_lr=4.6e-7, ent=1.61]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:42,  1.12it/s, pg=-0.115, ret=-9.61e-7, glen=99.2, tlen=260, kl=0.00393, act_lr=4.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:42,  1.12it/s, pg=-0.0413, ret=5.76e-5, glen=86.2, tlen=247, kl=0.00428, act_lr=4.6e-7, ent=1.69]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.13it/s, pg=-0.0413, ret=5.76e-5, glen=86.2, tlen=247, kl=0.00428, act_lr=4.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.13it/s, pg=-0.0452, ret=0.00023, glen=88.9, tlen=249, kl=0.0042, act_lr=4.6e-7, ent=1.8]  Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:40,  1.15it/s, pg=-0.0452, ret=0.00023, glen=88.9, tlen=249, kl=0.0042, act_lr=4.6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:40,  1.15it/s, pg=0.0917, ret=-0.000484, glen=92.2, tlen=253, kl=0.00418, act_lr=4.6e-7, ent=1.79]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.15it/s, pg=0.0917, ret=-0.000484, glen=92.2, tlen=253, kl=0.00418, act_lr=4.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.15it/s, pg=-0.149, ret=0.00143, glen=99, tlen=260, kl=0.00396, act_lr=4.6e-7, ent=1.77]    Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:37,  1.16it/s, pg=-0.149, ret=0.00143, glen=99, tlen=260, kl=0.00396, act_lr=4.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:37,  1.16it/s, pg=0.152, ret=5.96e-5, glen=105, tlen=265, kl=0.00362, act_lr=4.6e-7, ent=1.69]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:36,  1.16it/s, pg=0.152, ret=5.96e-5, glen=105, tlen=265, kl=0.00362, act_lr=4.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:36,  1.16it/s, pg=0.0174, ret=-0.00112, glen=89.9, tlen=250, kl=0.00426, act_lr=4.6e-7, ent=1.77]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.17it/s, pg=0.0174, ret=-0.00112, glen=89.9, tlen=250, kl=0.00426, act_lr=4.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.17it/s, pg=-0.134, ret=0.00132, glen=88.9, tlen=249, kl=0.00467, act_lr=4.6e-7, ent=1.73] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.17it/s, pg=-0.134, ret=0.00132, glen=88.9, tlen=249, kl=0.00467, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.17it/s, pg=-0.0788, ret=6.98e-5, glen=100, tlen=261, kl=0.00372, act_lr=4.6e-7, ent=1.71]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.0788, ret=6.98e-5, glen=100, tlen=261, kl=0.00372, act_lr=4.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=0.281, ret=-0.00105, glen=109, tlen=269, kl=0.0037, act_lr=4.6e-7, ent=1.86]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=0.281, ret=-0.00105, glen=109, tlen=269, kl=0.0037, act_lr=4.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=-0.0558, ret=0.000606, glen=108, tlen=269, kl=0.00352, act_lr=4.6e-7, ent=1.78]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:32,  1.17it/s, pg=-0.0558, ret=0.000606, glen=108, tlen=269, kl=0.00352, act_lr=4.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:32,  1.17it/s, pg=-0.00397, ret=0.000347, glen=113, tlen=273, kl=0.00379, act_lr=4.6e-7, ent=2.01]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:31,  1.17it/s, pg=-0.00397, ret=0.000347, glen=113, tlen=273, kl=0.00379, act_lr=4.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:31,  1.17it/s, pg=-0.0812, ret=-0.000567, glen=95, tlen=255, kl=0.00422, act_lr=4.6e-7, ent=1.69] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:30,  1.17it/s, pg=-0.0812, ret=-0.000567, glen=95, tlen=255, kl=0.00422, act_lr=4.6e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:30,  1.17it/s, pg=-0.097, ret=9.81e-5, glen=93.9, tlen=254, kl=0.00456, act_lr=4.6e-7, ent=1.85] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:29,  1.17it/s, pg=-0.097, ret=9.81e-5, glen=93.9, tlen=254, kl=0.00456, act_lr=4.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:29,  1.17it/s, pg=0.172, ret=-0.000595, glen=347, tlen=508, kl=0.00261, act_lr=4.6e-7, ent=1.77]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.14it/s, pg=0.172, ret=-0.000595, glen=347, tlen=508, kl=0.00261, act_lr=4.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.14it/s, pg=-0.017, ret=-0.000322, glen=93.3, tlen=254, kl=0.00406, act_lr=4.6e-7, ent=1.74]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.15it/s, pg=-0.017, ret=-0.000322, glen=93.3, tlen=254, kl=0.00406, act_lr=4.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:20<00:28,  1.15it/s, pg=0.111, ret=-0.000734, glen=99.8, tlen=260, kl=0.00405, act_lr=4.6e-7, ent=1.83] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=0.111, ret=-0.000734, glen=99.8, tlen=260, kl=0.00405, act_lr=4.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.16it/s, pg=-0.0131, ret=-0.000356, glen=110, tlen=271, kl=0.00356, act_lr=4.6e-7, ent=2.12]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.16it/s, pg=-0.0131, ret=-0.000356, glen=110, tlen=271, kl=0.00356, act_lr=4.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.16it/s, pg=-0.106, ret=0.0013, glen=109, tlen=269, kl=0.00373, act_lr=4.6e-7, ent=2]       Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.16it/s, pg=-0.106, ret=0.0013, glen=109, tlen=269, kl=0.00373, act_lr=4.6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.16it/s, pg=-0.134, ret=0.000546, glen=103, tlen=264, kl=0.00388, act_lr=4.6e-7, ent=1.87]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.134, ret=0.000546, glen=103, tlen=264, kl=0.00388, act_lr=4.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=0.0011, ret=-0.00096, glen=106, tlen=266, kl=0.00378, act_lr=4.6e-7, ent=2.09]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:26,  1.06it/s, pg=0.0011, ret=-0.00096, glen=106, tlen=266, kl=0.00378, act_lr=4.6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:26,  1.06it/s, pg=0.127, ret=0.000306, glen=91.3, tlen=252, kl=0.00404, act_lr=4.6e-7, ent=1.72]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:25,  1.07it/s, pg=0.127, ret=0.000306, glen=91.3, tlen=252, kl=0.00404, act_lr=4.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:25,  1.07it/s, pg=-0.0963, ret=0.000612, glen=109, tlen=270, kl=0.00361, act_lr=4.6e-7, ent=1.84]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:23,  1.10it/s, pg=-0.0963, ret=0.000612, glen=109, tlen=270, kl=0.00361, act_lr=4.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:23,  1.10it/s, pg=-0.0208, ret=-0.000969, glen=115, tlen=275, kl=0.00331, act_lr=4.6e-7, ent=1.97]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.12it/s, pg=-0.0208, ret=-0.000969, glen=115, tlen=275, kl=0.00331, act_lr=4.6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.12it/s, pg=0.0468, ret=-0.00118, glen=90.1, tlen=250, kl=0.00426, act_lr=4.6e-7, ent=1.75] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.13it/s, pg=0.0468, ret=-0.00118, glen=90.1, tlen=250, kl=0.00426, act_lr=4.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:28<00:21,  1.13it/s, pg=-0.101, ret=0.00137, glen=109, tlen=269, kl=0.00369, act_lr=4.6e-7, ent=1.83]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.14it/s, pg=-0.101, ret=0.00137, glen=109, tlen=269, kl=0.00369, act_lr=4.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.14it/s, pg=-0.0458, ret=-0.000198, glen=102, tlen=263, kl=0.00393, act_lr=4.6e-7, ent=1.74]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=-0.0458, ret=-0.000198, glen=102, tlen=263, kl=0.00393, act_lr=4.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=-0.0685, ret=0.000496, glen=87.8, tlen=248, kl=0.004, act_lr=4.6e-7, ent=1.67]  Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.16it/s, pg=-0.0685, ret=0.000496, glen=87.8, tlen=248, kl=0.004, act_lr=4.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.16it/s, pg=0.132, ret=-0.000739, glen=95.2, tlen=256, kl=0.00404, act_lr=4.6e-7, ent=1.85]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=0.132, ret=-0.000739, glen=95.2, tlen=256, kl=0.00404, act_lr=4.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=-0.0813, ret=0.000429, glen=105, tlen=265, kl=0.00414, act_lr=4.6e-7, ent=1.87]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=-0.0813, ret=0.000429, glen=105, tlen=265, kl=0.00414, act_lr=4.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=0.158, ret=-0.00113, glen=99.5, tlen=259, kl=0.00452, act_lr=4.6e-7, ent=1.89] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=0.158, ret=-0.00113, glen=99.5, tlen=259, kl=0.00452, act_lr=4.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.17it/s, pg=0.0385, ret=-0.000378, glen=97.6, tlen=258, kl=0.00437, act_lr=4.6e-7, ent=1.75]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.0385, ret=-0.000378, glen=97.6, tlen=258, kl=0.00437, act_lr=4.6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:34<00:14,  1.17it/s, pg=-0.268, ret=0.000916, glen=102, tlen=262, kl=0.00391, act_lr=4.6e-7, ent=1.7]   Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.268, ret=0.000916, glen=102, tlen=262, kl=0.00391, act_lr=4.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=-0.169, ret=-0.00211, glen=124, tlen=284, kl=0.00343, act_lr=4.6e-7, ent=1.52]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.17it/s, pg=-0.169, ret=-0.00211, glen=124, tlen=284, kl=0.00343, act_lr=4.6e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.17it/s, pg=0.00235, ret=1.46e-5, glen=99.7, tlen=260, kl=0.00406, act_lr=4.6e-7, ent=1.83]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:11,  1.17it/s, pg=0.00235, ret=1.46e-5, glen=99.7, tlen=260, kl=0.00406, act_lr=4.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:11,  1.17it/s, pg=0.153, ret=0.000818, glen=123, tlen=283, kl=0.00388, act_lr=4.6e-7, ent=1.94]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.14it/s, pg=0.153, ret=0.000818, glen=123, tlen=283, kl=0.00388, act_lr=4.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.14it/s, pg=-0.092, ret=0.000774, glen=102, tlen=262, kl=0.00393, act_lr=4.6e-7, ent=1.73]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.15it/s, pg=-0.092, ret=0.000774, glen=102, tlen=262, kl=0.00393, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.15it/s, pg=0.167, ret=-0.000673, glen=91.9, tlen=252, kl=0.00437, act_lr=4.6e-7, ent=1.88]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.16it/s, pg=0.167, ret=-0.000673, glen=91.9, tlen=252, kl=0.00437, act_lr=4.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.16it/s, pg=0.098, ret=-0.00035, glen=93, tlen=253, kl=0.00457, act_lr=4.6e-7, ent=1.74]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.16it/s, pg=0.098, ret=-0.00035, glen=93, tlen=253, kl=0.00457, act_lr=4.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:40<00:08,  1.16it/s, pg=-0.084, ret=0.000232, glen=94.8, tlen=255, kl=0.00405, act_lr=4.6e-7, ent=1.9]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.084, ret=0.000232, glen=94.8, tlen=255, kl=0.00405, act_lr=4.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.15, ret=0.000277, glen=106, tlen=267, kl=0.00351, act_lr=4.6e-7, ent=1.82] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=-0.15, ret=0.000277, glen=106, tlen=267, kl=0.00351, act_lr=4.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=0.1, ret=0.000129, glen=91.4, tlen=252, kl=0.00438, act_lr=4.6e-7, ent=1.71] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=0.1, ret=0.000129, glen=91.4, tlen=252, kl=0.00438, act_lr=4.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=-0.13, ret=0.000843, glen=99.5, tlen=260, kl=0.00384, act_lr=4.6e-7, ent=1.76]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=-0.13, ret=0.000843, glen=99.5, tlen=260, kl=0.00384, act_lr=4.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=-0.138, ret=0.00185, glen=91.5, tlen=252, kl=0.00411, act_lr=4.6e-7, ent=1.73]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=-0.138, ret=0.00185, glen=91.5, tlen=252, kl=0.00411, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=0.0283, ret=-0.00047, glen=97, tlen=257, kl=0.00415, act_lr=4.6e-7, ent=1.73] Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=0.0283, ret=-0.00047, glen=97, tlen=257, kl=0.00415, act_lr=4.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=-0.0623, ret=-0.00022, glen=101, tlen=261, kl=0.00401, act_lr=4.6e-7, ent=1.67]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.0623, ret=-0.00022, glen=101, tlen=261, kl=0.00401, act_lr=4.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:46<00:02,  1.17it/s, pg=-0.12, ret=0.000466, glen=83.2, tlen=243, kl=0.00445, act_lr=4.6e-7, ent=1.87] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=-0.12, ret=0.000466, glen=83.2, tlen=243, kl=0.00445, act_lr=4.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=-0.0529, ret=0.000482, glen=98.2, tlen=259, kl=0.00443, act_lr=4.6e-7, ent=1.8]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.17it/s, pg=-0.0529, ret=0.000482, glen=98.2, tlen=259, kl=0.00443, act_lr=4.6e-7, ent=1.8]
2025-07-23 13:19:48.572 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.84s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.17it/s, pg=0.0425, ret=0.000144, glen=114, tlen=275, kl=0.00363, act_lr=4.8e-7, ent=1.82] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=0.0425, ret=0.000144, glen=114, tlen=275, kl=0.00363, act_lr=4.8e-7, ent=1.82]
2025-07-23 13:19:49.242 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 13:19:51.436 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.19s
2025-07-23 13:19:51.741 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 51.13s
2025-07-23 13:19:51.746 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.011768965287642045, 'actor_lr': 4.603636315633511e-07, 'clip_ratio': 0.0, 'entropy': 1.8032783768393776, 'kl': 0.003990901600230824, 'response_length': 105.7318897594105, 'total_length': 266.0056776566939, 'return': -2.5320033124775033e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [43:14<07:43, 231.80s/it][A2025-07-23 13:19:51.778 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:22:30.615 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:22:30.806 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 13:22:30.806 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 159.03s
2025-07-23 13:22:32.719 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0105,avg_pass_at_n: 1.0000,avg_num_tokens: 100.8563,std_num_tokens: 172.3146,avg_correct_num_tokens: 88.6217,std_correct_num_tokens: 69.7146,avg_incorrect_num_tokens: 109.9600,std_incorrect_num_tokens: 219.0332
2025-07-23 13:22:33.152 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.35s
2025-07-23 13:22:34.688 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.53s
2025-07-23 13:23:02.954 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 218
2025-07-23 13:23:02.955 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.27s
2025-07-23 13:23:03.771 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.81s
2025-07-23 13:23:03.772 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 5.044418863990609e-05, avg_kl: 0.004018293608219252, avg_response_length: 109.02375387489249, avg_orm_score: 0.0, avg_custom_rewards: 5.044418863990609e-05
2025-07-23 13:23:03.802 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter24_replay_buffer.jsonl
2025-07-23 13:23:05.315 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.51s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s, pg=-0.0692, ret=0.00103, glen=104, tlen=265, kl=0.00399, act_lr=4.8e-7, ent=1.84]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:00<00:53,  1.00it/s, pg=-0.0692, ret=0.00103, glen=104, tlen=265, kl=0.00399, act_lr=4.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:53,  1.00it/s, pg=-0.0146, ret=-0.00032, glen=99.8, tlen=261, kl=0.00387, act_lr=4.8e-7, ent=1.95]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.09it/s, pg=-0.0146, ret=-0.00032, glen=99.8, tlen=261, kl=0.00387, act_lr=4.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.09it/s, pg=0.0392, ret=0.00504, glen=345, tlen=506, kl=0.00301, act_lr=4.8e-7, ent=2.6]    Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:48,  1.08it/s, pg=0.0392, ret=0.00504, glen=345, tlen=506, kl=0.00301, act_lr=4.8e-7, ent=2.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:48,  1.08it/s, pg=0.203, ret=-0.00183, glen=100, tlen=260, kl=0.00372, act_lr=4.8e-7, ent=1.86]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:46,  1.11it/s, pg=0.203, ret=-0.00183, glen=100, tlen=260, kl=0.00372, act_lr=4.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:46,  1.11it/s, pg=0.0457, ret=-0.00037, glen=86.9, tlen=247, kl=0.0046, act_lr=4.8e-7, ent=1.71]Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:44,  1.13it/s, pg=0.0457, ret=-0.00037, glen=86.9, tlen=247, kl=0.0046, act_lr=4.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:44,  1.13it/s, pg=0.142, ret=-0.000764, glen=107, tlen=267, kl=0.00381, act_lr=4.8e-7, ent=1.66]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:42,  1.14it/s, pg=0.142, ret=-0.000764, glen=107, tlen=267, kl=0.00381, act_lr=4.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:42,  1.14it/s, pg=0.0735, ret=-0.00145, glen=92.9, tlen=254, kl=0.00382, act_lr=4.8e-7, ent=1.66]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.15it/s, pg=0.0735, ret=-0.00145, glen=92.9, tlen=254, kl=0.00382, act_lr=4.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.15it/s, pg=-0.313, ret=0.00199, glen=95.1, tlen=256, kl=0.00391, act_lr=4.8e-7, ent=1.81] Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.313, ret=0.00199, glen=95.1, tlen=256, kl=0.00391, act_lr=4.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:40,  1.16it/s, pg=-0.0459, ret=2.7e-5, glen=97.7, tlen=258, kl=0.00434, act_lr=4.8e-7, ent=1.92]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:39,  1.16it/s, pg=-0.0459, ret=2.7e-5, glen=97.7, tlen=258, kl=0.00434, act_lr=4.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.16it/s, pg=0.00317, ret=-0.000153, glen=110, tlen=271, kl=0.00376, act_lr=4.8e-7, ent=1.97]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.14it/s, pg=0.00317, ret=-0.000153, glen=110, tlen=271, kl=0.00376, act_lr=4.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.14it/s, pg=-0.0523, ret=0.00177, glen=115, tlen=276, kl=0.00369, act_lr=4.8e-7, ent=2.06]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.15it/s, pg=-0.0523, ret=0.00177, glen=115, tlen=276, kl=0.00369, act_lr=4.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.15it/s, pg=0.0731, ret=-0.000588, glen=105, tlen=266, kl=0.00374, act_lr=4.8e-7, ent=1.77]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.16it/s, pg=0.0731, ret=-0.000588, glen=105, tlen=266, kl=0.00374, act_lr=4.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.16it/s, pg=0.11, ret=-0.000699, glen=112, tlen=273, kl=0.00383, act_lr=4.8e-7, ent=1.77]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.16it/s, pg=0.11, ret=-0.000699, glen=112, tlen=273, kl=0.00383, act_lr=4.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.16it/s, pg=-0.0667, ret=-0.000308, glen=98, tlen=259, kl=0.00372, act_lr=4.8e-7, ent=1.94]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.17it/s, pg=-0.0667, ret=-0.000308, glen=98, tlen=259, kl=0.00372, act_lr=4.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.17it/s, pg=-0.151, ret=0.000262, glen=97.1, tlen=258, kl=0.00416, act_lr=4.8e-7, ent=1.77]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.151, ret=0.000262, glen=97.1, tlen=258, kl=0.00416, act_lr=4.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.041, ret=-0.000965, glen=104, tlen=265, kl=0.00405, act_lr=4.8e-7, ent=1.82]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=-0.041, ret=-0.000965, glen=104, tlen=265, kl=0.00405, act_lr=4.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=0.165, ret=0.00274, glen=340, tlen=501, kl=0.00332, act_lr=4.8e-7, ent=2.39]   Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:33,  1.14it/s, pg=0.165, ret=0.00274, glen=340, tlen=501, kl=0.00332, act_lr=4.8e-7, ent=2.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:33,  1.14it/s, pg=-0.0381, ret=0.000165, glen=99.4, tlen=260, kl=0.00405, act_lr=4.8e-7, ent=1.88]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:32,  1.14it/s, pg=-0.0381, ret=0.000165, glen=99.4, tlen=260, kl=0.00405, act_lr=4.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:32,  1.14it/s, pg=0.0341, ret=-0.000607, glen=86.4, tlen=247, kl=0.00433, act_lr=4.8e-7, ent=1.82]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:31,  1.15it/s, pg=0.0341, ret=-0.000607, glen=86.4, tlen=247, kl=0.00433, act_lr=4.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:31,  1.15it/s, pg=-0.082, ret=-0.000272, glen=105, tlen=266, kl=0.00393, act_lr=4.8e-7, ent=1.99] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:30,  1.16it/s, pg=-0.082, ret=-0.000272, glen=105, tlen=266, kl=0.00393, act_lr=4.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:30,  1.16it/s, pg=-0.08, ret=0.000633, glen=94.7, tlen=255, kl=0.00414, act_lr=4.8e-7, ent=2.06] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.16it/s, pg=-0.08, ret=0.000633, glen=94.7, tlen=255, kl=0.00414, act_lr=4.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.16it/s, pg=-0.0636, ret=-0.00046, glen=99.4, tlen=260, kl=0.00391, act_lr=4.8e-7, ent=1.99]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.0636, ret=-0.00046, glen=99.4, tlen=260, kl=0.00391, act_lr=4.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:20<00:28,  1.17it/s, pg=-0.131, ret=-0.000237, glen=89.7, tlen=250, kl=0.00403, act_lr=4.8e-7, ent=1.8] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.131, ret=-0.000237, glen=89.7, tlen=250, kl=0.00403, act_lr=4.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.0383, ret=-0.000618, glen=114, tlen=274, kl=0.00388, act_lr=4.8e-7, ent=2.07]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.0383, ret=-0.000618, glen=114, tlen=274, kl=0.00388, act_lr=4.8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=-0.109, ret=0.000773, glen=82.3, tlen=243, kl=0.00459, act_lr=4.8e-7, ent=1.75] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=-0.109, ret=0.000773, glen=82.3, tlen=243, kl=0.00459, act_lr=4.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.0674, ret=0.000503, glen=82.8, tlen=243, kl=0.00414, act_lr=4.8e-7, ent=1.76]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.0674, ret=0.000503, glen=82.8, tlen=243, kl=0.00414, act_lr=4.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=-0.00891, ret=0.00154, glen=102, tlen=262, kl=0.00385, act_lr=4.8e-7, ent=2.01] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:26,  1.07it/s, pg=-0.00891, ret=0.00154, glen=102, tlen=262, kl=0.00385, act_lr=4.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:26,  1.07it/s, pg=0.114, ret=-0.000405, glen=92.3, tlen=253, kl=0.00423, act_lr=4.8e-7, ent=1.66]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:24,  1.10it/s, pg=0.114, ret=-0.000405, glen=92.3, tlen=253, kl=0.00423, act_lr=4.8e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:24,  1.10it/s, pg=-0.018, ret=-0.000231, glen=98.5, tlen=259, kl=0.00405, act_lr=4.8e-7, ent=1.89]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:23,  1.12it/s, pg=-0.018, ret=-0.000231, glen=98.5, tlen=259, kl=0.00405, act_lr=4.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:23,  1.12it/s, pg=-0.0981, ret=-0.0011, glen=95.7, tlen=256, kl=0.0041, act_lr=4.8e-7, ent=1.95]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:21,  1.14it/s, pg=-0.0981, ret=-0.0011, glen=95.7, tlen=256, kl=0.0041, act_lr=4.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:21,  1.14it/s, pg=-0.117, ret=0.000104, glen=86.7, tlen=248, kl=0.00504, act_lr=4.8e-7, ent=1.71]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:20,  1.15it/s, pg=-0.117, ret=0.000104, glen=86.7, tlen=248, kl=0.00504, act_lr=4.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:20,  1.15it/s, pg=0.181, ret=-0.00163, glen=93.2, tlen=254, kl=0.0039, act_lr=4.8e-7, ent=1.63]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:19,  1.16it/s, pg=0.181, ret=-0.00163, glen=93.2, tlen=254, kl=0.0039, act_lr=4.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:19,  1.16it/s, pg=0.132, ret=-0.00335, glen=299, tlen=459, kl=0.00337, act_lr=4.8e-7, ent=3.09]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.13it/s, pg=0.132, ret=-0.00335, glen=299, tlen=459, kl=0.00337, act_lr=4.8e-7, ent=3.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.13it/s, pg=0.0712, ret=0.000818, glen=116, tlen=277, kl=0.00366, act_lr=4.8e-7, ent=2.31]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.14it/s, pg=0.0712, ret=0.000818, glen=116, tlen=277, kl=0.00366, act_lr=4.8e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.14it/s, pg=0.058, ret=-0.000481, glen=109, tlen=270, kl=0.00381, act_lr=4.8e-7, ent=2.29]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.15it/s, pg=0.058, ret=-0.000481, glen=109, tlen=270, kl=0.00381, act_lr=4.8e-7, ent=2.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.15it/s, pg=-0.0645, ret=-0.00033, glen=82.8, tlen=244, kl=0.00492, act_lr=4.8e-7, ent=1.92]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.16it/s, pg=-0.0645, ret=-0.00033, glen=82.8, tlen=244, kl=0.00492, act_lr=4.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.16it/s, pg=-0.133, ret=0.00056, glen=97, tlen=258, kl=0.00417, act_lr=4.8e-7, ent=1.75]    Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.14it/s, pg=-0.133, ret=0.00056, glen=97, tlen=258, kl=0.00417, act_lr=4.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.14it/s, pg=-0.111, ret=-0.000116, glen=106, tlen=266, kl=0.00412, act_lr=4.8e-7, ent=1.91]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.15it/s, pg=-0.111, ret=-0.000116, glen=106, tlen=266, kl=0.00412, act_lr=4.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:34<00:14,  1.15it/s, pg=-0.0533, ret=0.000428, glen=113, tlen=273, kl=0.00368, act_lr=4.8e-7, ent=1.9] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.16it/s, pg=-0.0533, ret=0.000428, glen=113, tlen=273, kl=0.00368, act_lr=4.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.16it/s, pg=-0.203, ret=0.00275, glen=110, tlen=270, kl=0.00392, act_lr=4.8e-7, ent=1.92] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:12,  1.16it/s, pg=-0.203, ret=0.00275, glen=110, tlen=270, kl=0.00392, act_lr=4.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:12,  1.16it/s, pg=0.0193, ret=0.000796, glen=98.5, tlen=259, kl=0.00417, act_lr=4.8e-7, ent=2.31]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:12,  1.14it/s, pg=0.0193, ret=0.000796, glen=98.5, tlen=259, kl=0.00417, act_lr=4.8e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:12,  1.14it/s, pg=0.0812, ret=-0.00144, glen=90.3, tlen=251, kl=0.00431, act_lr=4.8e-7, ent=1.73]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.15it/s, pg=0.0812, ret=-0.00144, glen=90.3, tlen=251, kl=0.00431, act_lr=4.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.15it/s, pg=-0.0299, ret=0.000634, glen=91.6, tlen=252, kl=0.00421, act_lr=4.8e-7, ent=1.73]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.16it/s, pg=-0.0299, ret=0.000634, glen=91.6, tlen=252, kl=0.00421, act_lr=4.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.16it/s, pg=0.0862, ret=-0.00065, glen=105, tlen=266, kl=0.00401, act_lr=4.8e-7, ent=1.98]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.16it/s, pg=0.0862, ret=-0.00065, glen=105, tlen=266, kl=0.00401, act_lr=4.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.16it/s, pg=0.123, ret=-0.000601, glen=105, tlen=265, kl=0.00435, act_lr=4.8e-7, ent=2.12]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.16it/s, pg=0.123, ret=-0.000601, glen=105, tlen=265, kl=0.00435, act_lr=4.8e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:40<00:08,  1.16it/s, pg=-0.133, ret=1.69e-5, glen=102, tlen=263, kl=0.00414, act_lr=4.8e-7, ent=1.94] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.133, ret=1.69e-5, glen=102, tlen=263, kl=0.00414, act_lr=4.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.17it/s, pg=-0.167, ret=0.000461, glen=100, tlen=261, kl=0.00436, act_lr=4.8e-7, ent=1.8]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.17it/s, pg=-0.167, ret=0.000461, glen=100, tlen=261, kl=0.00436, act_lr=4.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.17it/s, pg=0.0262, ret=-0.00115, glen=93, tlen=254, kl=0.00428, act_lr=4.8e-7, ent=1.79]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:05,  1.17it/s, pg=0.0262, ret=-0.00115, glen=93, tlen=254, kl=0.00428, act_lr=4.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:05,  1.17it/s, pg=0.0593, ret=-2.63e-5, glen=110, tlen=270, kl=0.00364, act_lr=4.8e-7, ent=1.9]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.17it/s, pg=0.0593, ret=-2.63e-5, glen=110, tlen=270, kl=0.00364, act_lr=4.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.17it/s, pg=0.065, ret=0.000422, glen=108, tlen=268, kl=0.00394, act_lr=4.8e-7, ent=1.92]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.17it/s, pg=0.065, ret=0.000422, glen=108, tlen=268, kl=0.00394, act_lr=4.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.17it/s, pg=-0.141, ret=0.00233, glen=103, tlen=264, kl=0.00368, act_lr=4.8e-7, ent=1.71]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.141, ret=0.00233, glen=103, tlen=264, kl=0.00368, act_lr=4.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=0.0123, ret=-0.000102, glen=104, tlen=264, kl=0.004, act_lr=4.8e-7, ent=1.84]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.0123, ret=-0.000102, glen=104, tlen=264, kl=0.004, act_lr=4.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:46<00:02,  1.17it/s, pg=0.00662, ret=-0.0018, glen=97.8, tlen=258, kl=0.00402, act_lr=4.8e-7, ent=1.89]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=0.00662, ret=-0.0018, glen=97.8, tlen=258, kl=0.00402, act_lr=4.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=0.0568, ret=0.000377, glen=173, tlen=333, kl=0.00358, act_lr=4.8e-7, ent=2.13] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.16it/s, pg=0.0568, ret=0.000377, glen=173, tlen=333, kl=0.00358, act_lr=4.8e-7, ent=2.13]
2025-07-23 13:23:53.347 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.87s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.16it/s, pg=-0.123, ret=0.0013, glen=90.3, tlen=251, kl=0.00455, act_lr=5e-7, ent=1.83]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=-0.123, ret=0.0013, glen=90.3, tlen=251, kl=0.00455, act_lr=5e-7, ent=1.83]
2025-07-23 13:23:54.035 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 13:23:56.275 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.24s
2025-07-23 13:23:56.579 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 51.22s
2025-07-23 13:23:56.595 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.014212833751331677, 'actor_lr': 4.803636195240622e-07, 'clip_ratio': 0.0, 'entropy': 1.9306642900813709, 'kl': 0.004006715254350142, 'response_length': 113.36430220170455, 'total_length': 273.96581115722654, 'return': 8.050466342617504e-05, 'policy_update_steps': 1.0}

Episode [2/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [47:19<03:55, 235.77s/it][A2025-07-23 13:23:56.600 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   1%|          | 1/172 [00:00<00:48,  3.50it/s, est. speed input: 629.90 toks/s, output: 21.00 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   1%|          | 2/172 [00:00<00:34,  4.96it/s, est. speed input: 853.81 toks/s, output: 41.99 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:02<00:02, 44.43it/s, est. speed input: 4819.24 toks/s, output: 1299.65 toks/s]Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:02<00:01, 58.07it/s, est. speed input: 5386.77 toks/s, output: 1531.50 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:04<00:00, 22.98it/s, est. speed input: 6084.62 toks/s, output: 2693.64 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884796)[0m Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/172 [00:04<00:01, 17.30it/s, est. speed input: 5596.99 toks/s, output: 2595.89 toks/s][32m [repeated 110x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:10<00:00,  2.22it/s, est. speed input: 2990.31 toks/s, output: 1649.89 toks/s][32m [repeated 26x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00,  1.71it/s, est. speed input: 2649.88 toks/s, output: 1516.57 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00, 14.63it/s, est. speed input: 2649.88 toks/s, output: 1516.57 toks/s]
2025-07-23 13:24:13.109 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 501.7860,strategyqa_test/accuracy: 0.3974,eval_accuracy: 0.3974
2025-07-23 13:24:13.383 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:26:03.192 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:26:03.374 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:26:03.375 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 109.99s
2025-07-23 13:26:04.583 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0093,avg_pass_at_n: 1.0000,avg_num_tokens: 102.5026,std_num_tokens: 165.5677,avg_correct_num_tokens: 89.5689,std_correct_num_tokens: 77.3709,avg_incorrect_num_tokens: 111.6732,std_incorrect_num_tokens: 205.9182
2025-07-23 13:26:04.918 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 1.54s
2025-07-23 13:26:05.594 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 0.67s
2025-07-23 13:26:20.779 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 114
2025-07-23 13:26:20.779 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 15.18s
2025-07-23 13:26:21.263 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.48s
2025-07-23 13:26:21.263 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0015662947936155099, avg_kl: 0.004558897854989035, avg_response_length: 107.36318126477693, avg_orm_score: 0.0, avg_custom_rewards: -0.0015662947936155099
2025-07-23 13:26:21.292 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter25_replay_buffer.jsonl
2025-07-23 13:26:22.107 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 0.82s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s]
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:15<00:00,  1.69s/it, est. speed input: 2030.88 toks/s, output: 1283.54 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:15<00:00, 11.18it/s, est. speed input: 2030.88 toks/s, output: 1283.54 toks/s][32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/29 [00:00<?, ?it/s, pg=-0.0582, ret=0.000922, glen=91.7, tlen=252, kl=0.00455, act_lr=5e-7, ent=1.81]Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:00<00:26,  1.07it/s, pg=-0.0582, ret=0.000922, glen=91.7, tlen=252, kl=0.00455, act_lr=5e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   3%|‚ñé         | 1/29 [00:01<00:26,  1.07it/s, pg=-0.104, ret=0.000594, glen=84.6, tlen=245, kl=0.00539, act_lr=5e-7, ent=1.76] Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:01<00:23,  1.13it/s, pg=-0.104, ret=0.000594, glen=84.6, tlen=245, kl=0.00539, act_lr=5e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/29 [00:02<00:23,  1.13it/s, pg=0.11, ret=7.95e-5, glen=114, tlen=274, kl=0.00457, act_lr=5e-7, ent=2.12]    Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:02<00:22,  1.14it/s, pg=0.11, ret=7.95e-5, glen=114, tlen=274, kl=0.00457, act_lr=5e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 3/29 [00:03<00:22,  1.14it/s, pg=-0.136, ret=0.000841, glen=102, tlen=262, kl=0.00447, act_lr=5e-7, ent=2.03]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:03<00:22,  1.13it/s, pg=-0.136, ret=0.000841, glen=102, tlen=262, kl=0.00447, act_lr=5e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 4/29 [00:04<00:22,  1.13it/s, pg=-0.0502, ret=0.00119, glen=116, tlen=277, kl=0.00421, act_lr=5e-7, ent=1.89]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:04<00:20,  1.15it/s, pg=-0.0502, ret=0.00119, glen=116, tlen=277, kl=0.00421, act_lr=5e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 5/29 [00:05<00:20,  1.15it/s, pg=-0.155, ret=0.000232, glen=96, tlen=256, kl=0.00452, act_lr=5e-7, ent=1.78] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:05<00:19,  1.16it/s, pg=-0.155, ret=0.000232, glen=96, tlen=256, kl=0.00452, act_lr=5e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 6/29 [00:06<00:19,  1.16it/s, pg=-0.0533, ret=-0.000963, glen=103, tlen=264, kl=0.00438, act_lr=5e-7, ent=1.87]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:06<00:19,  1.14it/s, pg=-0.0533, ret=-0.000963, glen=103, tlen=264, kl=0.00438, act_lr=5e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 7/29 [00:07<00:19,  1.14it/s, pg=-0.0504, ret=5.64e-5, glen=99.5, tlen=260, kl=0.00459, act_lr=5e-7, ent=1.77] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.12it/s, pg=-0.0504, ret=5.64e-5, glen=99.5, tlen=260, kl=0.00459, act_lr=5e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 8/29 [00:07<00:18,  1.12it/s, pg=-0.112, ret=0.000698, glen=96.7, tlen=257, kl=0.00461, act_lr=5e-7, ent=1.69]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:07<00:17,  1.14it/s, pg=-0.112, ret=0.000698, glen=96.7, tlen=257, kl=0.00461, act_lr=5e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 9/29 [00:08<00:17,  1.14it/s, pg=0.116, ret=-0.00137, glen=85.7, tlen=246, kl=0.00537, act_lr=5e-7, ent=1.71] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:08<00:16,  1.15it/s, pg=0.116, ret=-0.00137, glen=85.7, tlen=246, kl=0.00537, act_lr=5e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:09<00:16,  1.15it/s, pg=0.0879, ret=-4.49e-5, glen=97.6, tlen=259, kl=0.0046, act_lr=5e-7, ent=1.87]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:09<00:15,  1.14it/s, pg=0.0879, ret=-4.49e-5, glen=97.6, tlen=259, kl=0.0046, act_lr=5e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:10<00:15,  1.14it/s, pg=-0.0625, ret=-0.000234, glen=84.1, tlen=245, kl=0.00536, act_lr=5e-7, ent=1.76]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:10<00:14,  1.15it/s, pg=-0.0625, ret=-0.000234, glen=84.1, tlen=245, kl=0.00536, act_lr=5e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:11<00:14,  1.15it/s, pg=-0.0516, ret=-0.00313, glen=168, tlen=329, kl=0.00405, act_lr=5e-7, ent=2.19]  Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:11<00:13,  1.15it/s, pg=-0.0516, ret=-0.00313, glen=168, tlen=329, kl=0.00405, act_lr=5e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:12<00:13,  1.15it/s, pg=0.192, ret=-0.000934, glen=138, tlen=298, kl=0.00393, act_lr=5e-7, ent=2.14] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:12<00:13,  1.15it/s, pg=0.192, ret=-0.000934, glen=138, tlen=298, kl=0.00393, act_lr=5e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:13<00:13,  1.15it/s, pg=-0.0106, ret=0.000142, glen=92.7, tlen=253, kl=0.00484, act_lr=5e-7, ent=1.85]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.16it/s, pg=-0.0106, ret=0.000142, glen=92.7, tlen=253, kl=0.00484, act_lr=5e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:13<00:12,  1.16it/s, pg=-0.0575, ret=0.000694, glen=112, tlen=273, kl=0.00425, act_lr=5e-7, ent=1.72] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:13<00:11,  1.16it/s, pg=-0.0575, ret=0.000694, glen=112, tlen=273, kl=0.00425, act_lr=5e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:14<00:11,  1.16it/s, pg=0.144, ret=-0.000289, glen=121, tlen=281, kl=0.00426, act_lr=5e-7, ent=1.56] Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:14<00:10,  1.16it/s, pg=0.144, ret=-0.000289, glen=121, tlen=281, kl=0.00426, act_lr=5e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:15<00:10,  1.16it/s, pg=-0.0674, ret=-0.00019, glen=102, tlen=262, kl=0.00449, act_lr=5e-7, ent=1.84]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:15<00:09,  1.15it/s, pg=-0.0674, ret=-0.00019, glen=102, tlen=262, kl=0.00449, act_lr=5e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:16<00:09,  1.15it/s, pg=0.105, ret=-0.000948, glen=106, tlen=267, kl=0.0044, act_lr=5e-7, ent=1.98]  Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:16<00:08,  1.15it/s, pg=0.105, ret=-0.000948, glen=106, tlen=267, kl=0.0044, act_lr=5e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:17<00:08,  1.15it/s, pg=-0.188, ret=0.00223, glen=99.8, tlen=260, kl=0.00428, act_lr=5e-7, ent=1.94]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:17<00:07,  1.16it/s, pg=-0.188, ret=0.00223, glen=99.8, tlen=260, kl=0.00428, act_lr=5e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:18<00:07,  1.16it/s, pg=0.0132, ret=1.3e-5, glen=94.3, tlen=255, kl=0.00474, act_lr=5e-7, ent=1.68] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:18<00:06,  1.16it/s, pg=0.0132, ret=1.3e-5, glen=94.3, tlen=255, kl=0.00474, act_lr=5e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:19<00:06,  1.16it/s, pg=-0.126, ret=-0.000541, glen=106, tlen=266, kl=0.00465, act_lr=5e-7, ent=1.8]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=-0.126, ret=-0.000541, glen=106, tlen=266, kl=0.00465, act_lr=5e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:19<00:05,  1.17it/s, pg=0.0772, ret=-0.000668, glen=90.5, tlen=251, kl=0.00495, act_lr=5e-7, ent=1.78]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:19<00:05,  1.17it/s, pg=0.0772, ret=-0.000668, glen=90.5, tlen=251, kl=0.00495, act_lr=5e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:20<00:05,  1.17it/s, pg=0.15, ret=-0.00261, glen=195, tlen=355, kl=0.00393, act_lr=5e-7, ent=2.74]    Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:20<00:04,  1.15it/s, pg=0.15, ret=-0.00261, glen=195, tlen=355, kl=0.00393, act_lr=5e-7, ent=2.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:21<00:04,  1.15it/s, pg=-0.126, ret=3.15e-5, glen=101, tlen=262, kl=0.00505, act_lr=5e-7, ent=1.73]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:21<00:03,  1.15it/s, pg=-0.126, ret=3.15e-5, glen=101, tlen=262, kl=0.00505, act_lr=5e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:22<00:03,  1.15it/s, pg=0.0601, ret=-0.000124, glen=113, tlen=274, kl=0.00406, act_lr=5e-7, ent=1.93]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:22<00:02,  1.16it/s, pg=0.0601, ret=-0.000124, glen=113, tlen=274, kl=0.00406, act_lr=5e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:23<00:02,  1.16it/s, pg=0.094, ret=-0.00158, glen=109, tlen=269, kl=0.00426, act_lr=5e-7, ent=1.94]  Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:23<00:01,  1.16it/s, pg=0.094, ret=-0.00158, glen=109, tlen=269, kl=0.00426, act_lr=5e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:24<00:01,  1.16it/s, pg=-0.104, ret=0.00175, glen=103, tlen=263, kl=0.00437, act_lr=5e-7, ent=2.11]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:24<00:00,  1.07it/s, pg=-0.104, ret=0.00175, glen=103, tlen=263, kl=0.00437, act_lr=5e-7, ent=2.11]
2025-07-23 13:26:47.782 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 25.52s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.07it/s, pg=-0.0841, ret=-0.000157, glen=89, tlen=250, kl=0.00513, act_lr=5.2e-7, ent=1.72]Actor Train epoch [1/1]:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:25<00:00,  1.10it/s, pg=-0.0841, ret=-0.000157, glen=89, tlen=250, kl=0.00513, act_lr=5.2e-7, ent=1.72]
2025-07-23 13:26:48.469 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 13:26:50.650 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.18s
2025-07-23 13:26:50.954 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 28.81s
2025-07-23 13:26:50.958 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.01545642984324488, 'actor_lr': 5.006896544571269e-07, 'clip_ratio': 0.0, 'entropy': 1.885971472181123, 'kl': 0.004560536351697198, 'response_length': 107.27940736967942, 'total_length': 267.7860638848666, 'return': -0.00014864908095252508, 'policy_update_steps': 1.0}

Episode [2/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [50:13<00:00, 217.17s/it][A[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,322] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 2.51 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,322] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 106.01 GB, percent = 21.1%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=885532)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=885532)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=885532)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=885532)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=885532)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=885532)[0m     "profile": false
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "start_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "end_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=885532)[0m     "model_info": null, 
[36m(RefRayActorBase pid=885532)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=885532)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=885532)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=885532)[0m     "fast": true, 
[36m(RefRayActorBase pid=885532)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=885532)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=885532)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=885532)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x764c3e1dbef0>
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:37,323] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 02025-07-23 13:27:00.069 | INFO     | orz.ppo.trainer:train:193 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:40:41,528] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 12:36:36,962] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:43:43,287] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:47:50,779] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:51:58,645] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 12:56:06,640] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:00:15,415] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:08:44,490] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:15:48,560] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:19:48,565] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:26:47,776] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:57,859] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:58,065] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1017, num_elems = 5.33B
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,579] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,580] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,588] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,589] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,875] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,875] [INFO] [utils.py:782:see_memory_usage] MA 2.22 GB         Max_MA 7.18 GB         CA 3.24 GB         Max_CA 37 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:26:59,876] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 104.43 GB, percent = 20.7%
[36m(RefRayActorBase pid=885532)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [3/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [2/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [50:22<00:00, 232.52s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,063] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,064] [INFO] [utils.py:782:see_memory_usage] MA 2.22 GB         Max_MA 2.22 GB         CA 3.24 GB         Max_CA 3 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,064] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 104.41 GB, percent = 20.7%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=885532)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=885532)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=885532)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=885532)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=885532)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=885532)[0m     "profile": false
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "start_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "end_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=885532)[0m     "model_info": null, 
[36m(RefRayActorBase pid=885532)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=885532)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=885532)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=885532)[0m     "fast": true, 
[36m(RefRayActorBase pid=885532)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=885532)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=885532)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=885532)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x764c2eb3a3f0>
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,065] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=885532)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=885532)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=885532)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=885532)[0m     "detailed": true, 
[36m(RefRayActorBase pid=885532)[0m     "output_file": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=885532)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=885532)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=885532)[0m     "load_path": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 13:27:00,066] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=885532)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=885532)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=885532)[0m         "stage": 3, 
[36m(RefRayActorBase pid=885532)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=885532)[0m         "offload_param": {
[36m(RefRayActorBase pid=885532)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=885532)[0m             "pin_memory": true
[36m(RefRayActorBase pid=885532)[0m         }
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "bf16": {
[36m(RefRayActorBase pid=885532)[0m         "enabled": true
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=885532)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=885532)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=885532)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=885532)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-23 13:27:00.318 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:29:25.192 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:29:25.370 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:29:25.370 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 145.05s
2025-07-23 13:29:27.335 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0152,avg_reflection_pattern_score: 0.0110,avg_pass_at_n: 1.0000,avg_num_tokens: 101.5839,std_num_tokens: 154.5895,avg_correct_num_tokens: 89.9539,std_correct_num_tokens: 78.0099,avg_incorrect_num_tokens: 110.8104,std_incorrect_num_tokens: 194.5169
2025-07-23 13:29:27.766 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.40s
2025-07-23 13:29:29.311 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.54s
2025-07-23 13:29:57.822 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 218
2025-07-23 13:29:57.823 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 28.51s
2025-07-23 13:29:58.698 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.87s
2025-07-23 13:29:58.698 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0001383226750944015, avg_kl: 0.0, avg_response_length: 107.20418552083707, avg_orm_score: 0.0, avg_custom_rewards: 0.0001383226750944015
2025-07-23 13:29:58.752 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter26_replay_buffer.jsonl
2025-07-23 13:30:00.270 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.52s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/55 [00:00<?, ?it/s, pg=0.0317, ret=0.00044, glen=111, tlen=271, kl=0, act_lr=5.2e-7, ent=2.11]Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:00<00:52,  1.02it/s, pg=0.0317, ret=0.00044, glen=111, tlen=271, kl=0, act_lr=5.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/55 [00:01<00:52,  1.02it/s, pg=-6.1e-5, ret=-0.00128, glen=93.8, tlen=254, kl=0, act_lr=5.2e-7, ent=1.85]Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:01<00:48,  1.10it/s, pg=-6.1e-5, ret=-0.00128, glen=93.8, tlen=254, kl=0, act_lr=5.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/55 [00:02<00:48,  1.10it/s, pg=0.0501, ret=0.000267, glen=105, tlen=266, kl=0, act_lr=5.2e-7, ent=1.98]  Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:02<00:45,  1.13it/s, pg=0.0501, ret=0.000267, glen=105, tlen=266, kl=0, act_lr=5.2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   5%|‚ñå         | 3/55 [00:03<00:45,  1.13it/s, pg=0.0489, ret=-0.000534, glen=97.1, tlen=257, kl=0, act_lr=5.2e-7, ent=1.78]Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:03<00:45,  1.13it/s, pg=0.0489, ret=-0.000534, glen=97.1, tlen=257, kl=0, act_lr=5.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/55 [00:04<00:45,  1.13it/s, pg=0.135, ret=-0.0015, glen=94.4, tlen=255, kl=0, act_lr=5.2e-7, ent=1.75]   Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:04<00:43,  1.14it/s, pg=0.135, ret=-0.0015, glen=94.4, tlen=255, kl=0, act_lr=5.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/55 [00:05<00:43,  1.14it/s, pg=-0.0182, ret=4.17e-5, glen=94.4, tlen=255, kl=0, act_lr=5.2e-7, ent=1.9]Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:05<00:42,  1.15it/s, pg=-0.0182, ret=4.17e-5, glen=94.4, tlen=255, kl=0, act_lr=5.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/55 [00:06<00:42,  1.15it/s, pg=-0.0774, ret=0.000688, glen=104, tlen=265, kl=0, act_lr=5.2e-7, ent=1.91]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:06<00:41,  1.16it/s, pg=-0.0774, ret=0.000688, glen=104, tlen=265, kl=0, act_lr=5.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/55 [00:07<00:41,  1.16it/s, pg=-0.0707, ret=0.000592, glen=104, tlen=265, kl=0, act_lr=5.2e-7, ent=1.92]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=-0.0707, ret=0.000592, glen=104, tlen=265, kl=0, act_lr=5.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/55 [00:07<00:41,  1.14it/s, pg=-0.0531, ret=4.64e-5, glen=89.7, tlen=250, kl=0, act_lr=5.2e-7, ent=1.74]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:07<00:39,  1.15it/s, pg=-0.0531, ret=4.64e-5, glen=89.7, tlen=250, kl=0, act_lr=5.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 9/55 [00:08<00:39,  1.15it/s, pg=0.0488, ret=0.000748, glen=119, tlen=279, kl=0, act_lr=5.2e-7, ent=2.31] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:08<00:39,  1.15it/s, pg=0.0488, ret=0.000748, glen=119, tlen=279, kl=0, act_lr=5.2e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 10/55 [00:09<00:39,  1.15it/s, pg=0.103, ret=-0.00115, glen=103, tlen=263, kl=0, act_lr=5.2e-7, ent=1.97] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:09<00:38,  1.16it/s, pg=0.103, ret=-0.00115, glen=103, tlen=263, kl=0, act_lr=5.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/55 [00:10<00:38,  1.16it/s, pg=-0.0337, ret=-2.15e-5, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.92]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:10<00:37,  1.16it/s, pg=-0.0337, ret=-2.15e-5, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/55 [00:11<00:37,  1.16it/s, pg=0.0616, ret=-0.00031, glen=107, tlen=267, kl=0, act_lr=5.2e-7, ent=2.12] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:11<00:36,  1.16it/s, pg=0.0616, ret=-0.00031, glen=107, tlen=267, kl=0, act_lr=5.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 13/55 [00:12<00:36,  1.16it/s, pg=-0.131, ret=0.000688, glen=95.6, tlen=256, kl=0, act_lr=5.2e-7, ent=1.91]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:12<00:35,  1.17it/s, pg=-0.131, ret=0.000688, glen=95.6, tlen=256, kl=0, act_lr=5.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 14/55 [00:13<00:35,  1.17it/s, pg=-0.227, ret=0.00112, glen=90.5, tlen=251, kl=0, act_lr=5.2e-7, ent=1.77] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.227, ret=0.00112, glen=90.5, tlen=251, kl=0, act_lr=5.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 15/55 [00:13<00:34,  1.17it/s, pg=-0.178, ret=0.00147, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.9]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:13<00:33,  1.17it/s, pg=-0.178, ret=0.00147, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 16/55 [00:14<00:33,  1.17it/s, pg=0.00562, ret=0.000483, glen=116, tlen=276, kl=0, act_lr=5.2e-7, ent=2.06]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:14<00:33,  1.15it/s, pg=0.00562, ret=0.000483, glen=116, tlen=276, kl=0, act_lr=5.2e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 17/55 [00:15<00:33,  1.15it/s, pg=-0.216, ret=0.000942, glen=92.1, tlen=252, kl=0, act_lr=5.2e-7, ent=1.71]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:15<00:32,  1.15it/s, pg=-0.216, ret=0.000942, glen=92.1, tlen=252, kl=0, act_lr=5.2e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/55 [00:16<00:32,  1.15it/s, pg=0.151, ret=-0.00106, glen=92, tlen=252, kl=0, act_lr=5.2e-7, ent=1.82]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:16<00:31,  1.16it/s, pg=0.151, ret=-0.00106, glen=92, tlen=252, kl=0, act_lr=5.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 19/55 [00:17<00:31,  1.16it/s, pg=-0.152, ret=0.00124, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.96]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:17<00:30,  1.16it/s, pg=-0.152, ret=0.00124, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñã      | 20/55 [00:18<00:30,  1.16it/s, pg=-0.0601, ret=0.00101, glen=99.4, tlen=260, kl=0, act_lr=5.2e-7, ent=1.84]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:18<00:29,  1.17it/s, pg=-0.0601, ret=0.00101, glen=99.4, tlen=260, kl=0, act_lr=5.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 21/55 [00:19<00:29,  1.17it/s, pg=0.123, ret=-0.0011, glen=95.7, tlen=256, kl=0, act_lr=5.2e-7, ent=1.89]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=0.123, ret=-0.0011, glen=95.7, tlen=256, kl=0, act_lr=5.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 22/55 [00:19<00:28,  1.17it/s, pg=-0.158, ret=0.000681, glen=96.7, tlen=257, kl=0, act_lr=5.2e-7, ent=1.85]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:19<00:27,  1.17it/s, pg=-0.158, ret=0.000681, glen=96.7, tlen=257, kl=0, act_lr=5.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 23/55 [00:20<00:27,  1.17it/s, pg=-0.268, ret=0.00158, glen=87, tlen=248, kl=0, act_lr=5.2e-7, ent=1.82]   Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:20<00:26,  1.17it/s, pg=-0.268, ret=0.00158, glen=87, tlen=248, kl=0, act_lr=5.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24/55 [00:21<00:26,  1.17it/s, pg=-0.0678, ret=-0.000122, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=1.99]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:21<00:25,  1.17it/s, pg=-0.0678, ret=-0.000122, glen=110, tlen=270, kl=0, act_lr=5.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 25/55 [00:22<00:25,  1.17it/s, pg=-0.0265, ret=-0.00072, glen=106, tlen=267, kl=0, act_lr=5.2e-7, ent=2.06] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:22<00:24,  1.17it/s, pg=-0.0265, ret=-0.00072, glen=106, tlen=267, kl=0, act_lr=5.2e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 26/55 [00:23<00:24,  1.17it/s, pg=0.23, ret=-0.00124, glen=117, tlen=277, kl=0, act_lr=5.2e-7, ent=2.28]   Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:23<00:26,  1.07it/s, pg=0.23, ret=-0.00124, glen=117, tlen=277, kl=0, act_lr=5.2e-7, ent=2.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 27/55 [00:24<00:26,  1.07it/s, pg=-0.148, ret=0.000755, glen=111, tlen=272, kl=0, act_lr=5.2e-7, ent=2.47]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:24<00:24,  1.10it/s, pg=-0.148, ret=0.000755, glen=111, tlen=272, kl=0, act_lr=5.2e-7, ent=2.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 28/55 [00:25<00:24,  1.10it/s, pg=0.105, ret=-0.00148, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.89] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:25<00:23,  1.12it/s, pg=0.105, ret=-0.00148, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/55 [00:26<00:23,  1.12it/s, pg=-0.0537, ret=-0.000203, glen=96.8, tlen=257, kl=0, act_lr=5.2e-7, ent=1.92]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:26<00:22,  1.13it/s, pg=-0.0537, ret=-0.000203, glen=96.8, tlen=257, kl=0, act_lr=5.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 30/55 [00:27<00:22,  1.13it/s, pg=0.201, ret=-0.000242, glen=122, tlen=283, kl=0, act_lr=5.2e-7, ent=2.28]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.14it/s, pg=0.201, ret=-0.000242, glen=122, tlen=283, kl=0, act_lr=5.2e-7, ent=2.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/55 [00:27<00:21,  1.14it/s, pg=-0.0759, ret=0.000153, glen=91, tlen=251, kl=0, act_lr=5.2e-7, ent=1.94]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:27<00:20,  1.15it/s, pg=-0.0759, ret=0.000153, glen=91, tlen=251, kl=0, act_lr=5.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 32/55 [00:28<00:20,  1.15it/s, pg=-0.119, ret=-0.00108, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.92]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:28<00:19,  1.15it/s, pg=-0.119, ret=-0.00108, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/55 [00:29<00:19,  1.15it/s, pg=0.201, ret=-0.000736, glen=104, tlen=264, kl=0, act_lr=5.2e-7, ent=2.02]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:29<00:18,  1.16it/s, pg=0.201, ret=-0.000736, glen=104, tlen=264, kl=0, act_lr=5.2e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 34/55 [00:30<00:18,  1.16it/s, pg=-0.0428, ret=-0.000632, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.96]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:30<00:17,  1.16it/s, pg=-0.0428, ret=-0.000632, glen=112, tlen=272, kl=0, act_lr=5.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 35/55 [00:31<00:17,  1.16it/s, pg=-0.231, ret=0.00156, glen=94.7, tlen=255, kl=0, act_lr=5.2e-7, ent=1.79]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:31<00:16,  1.17it/s, pg=-0.231, ret=0.00156, glen=94.7, tlen=255, kl=0, act_lr=5.2e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 36/55 [00:32<00:16,  1.17it/s, pg=-0.056, ret=0.000503, glen=90.6, tlen=251, kl=0, act_lr=5.2e-7, ent=1.77]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:32<00:15,  1.17it/s, pg=-0.056, ret=0.000503, glen=90.6, tlen=251, kl=0, act_lr=5.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 37/55 [00:33<00:15,  1.17it/s, pg=0.131, ret=-0.00159, glen=96.1, tlen=256, kl=0, act_lr=5.2e-7, ent=1.81] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=0.131, ret=-0.00159, glen=96.1, tlen=256, kl=0, act_lr=5.2e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 38/55 [00:33<00:14,  1.17it/s, pg=-0.0916, ret=0.000614, glen=98.8, tlen=260, kl=0, act_lr=5.2e-7, ent=1.77]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:33<00:13,  1.17it/s, pg=-0.0916, ret=0.000614, glen=98.8, tlen=260, kl=0, act_lr=5.2e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 39/55 [00:34<00:13,  1.17it/s, pg=0.0529, ret=-0.000643, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.9]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:34<00:13,  1.15it/s, pg=0.0529, ret=-0.000643, glen=106, tlen=266, kl=0, act_lr=5.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 40/55 [00:35<00:13,  1.15it/s, pg=-0.00671, ret=-0.000948, glen=116, tlen=277, kl=0, act_lr=5.2e-7, ent=1.82]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:35<00:12,  1.16it/s, pg=-0.00671, ret=-0.000948, glen=116, tlen=277, kl=0, act_lr=5.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 41/55 [00:36<00:12,  1.16it/s, pg=-0.0361, ret=-0.000108, glen=85.3, tlen=246, kl=0, act_lr=5.2e-7, ent=1.64]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:36<00:11,  1.16it/s, pg=-0.0361, ret=-0.000108, glen=85.3, tlen=246, kl=0, act_lr=5.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 42/55 [00:37<00:11,  1.16it/s, pg=0.0206, ret=-0.00105, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.92]   Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:37<00:10,  1.16it/s, pg=0.0206, ret=-0.00105, glen=107, tlen=268, kl=0, act_lr=5.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 43/55 [00:38<00:10,  1.16it/s, pg=-0.156, ret=-0.000194, glen=95.7, tlen=256, kl=0, act_lr=5.2e-7, ent=1.64]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:38<00:09,  1.17it/s, pg=-0.156, ret=-0.000194, glen=95.7, tlen=256, kl=0, act_lr=5.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 44/55 [00:39<00:09,  1.17it/s, pg=-0.0205, ret=0.00433, glen=335, tlen=496, kl=0, act_lr=5.2e-7, ent=2.43]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.13it/s, pg=-0.0205, ret=0.00433, glen=335, tlen=496, kl=0, act_lr=5.2e-7, ent=2.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 45/55 [00:39<00:08,  1.13it/s, pg=0.0291, ret=-0.00135, glen=90.8, tlen=251, kl=0, act_lr=5.2e-7, ent=1.91]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:39<00:07,  1.14it/s, pg=0.0291, ret=-0.00135, glen=90.8, tlen=251, kl=0, act_lr=5.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 46/55 [00:40<00:07,  1.14it/s, pg=0.182, ret=-0.000553, glen=154, tlen=314, kl=0, act_lr=5.2e-7, ent=2.83] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:40<00:06,  1.15it/s, pg=0.182, ret=-0.000553, glen=154, tlen=314, kl=0, act_lr=5.2e-7, ent=2.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 47/55 [00:41<00:06,  1.15it/s, pg=-0.00537, ret=-0.000734, glen=91.9, tlen=252, kl=0, act_lr=5.2e-7, ent=1.82]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:41<00:06,  1.16it/s, pg=-0.00537, ret=-0.000734, glen=91.9, tlen=252, kl=0, act_lr=5.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 48/55 [00:42<00:06,  1.16it/s, pg=-0.00763, ret=-0.00014, glen=96.6, tlen=256, kl=0, act_lr=5.2e-7, ent=1.83] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:42<00:05,  1.16it/s, pg=-0.00763, ret=-0.00014, glen=96.6, tlen=256, kl=0, act_lr=5.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 49/55 [00:43<00:05,  1.16it/s, pg=0.122, ret=-0.000286, glen=105, tlen=266, kl=0, act_lr=5.2e-7, ent=2]      Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:43<00:04,  1.16it/s, pg=0.122, ret=-0.000286, glen=105, tlen=266, kl=0, act_lr=5.2e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 50/55 [00:44<00:04,  1.16it/s, pg=-0.105, ret=0.00112, glen=97.9, tlen=258, kl=0, act_lr=5.2e-7, ent=1.64]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:44<00:03,  1.17it/s, pg=-0.105, ret=0.00112, glen=97.9, tlen=258, kl=0, act_lr=5.2e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 51/55 [00:45<00:03,  1.17it/s, pg=-0.0239, ret=0.000126, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=2.01]Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=-0.0239, ret=0.000126, glen=102, tlen=262, kl=0, act_lr=5.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 52/55 [00:45<00:02,  1.17it/s, pg=0.0634, ret=-0.000261, glen=96, tlen=257, kl=0, act_lr=5.2e-7, ent=1.91] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:45<00:01,  1.17it/s, pg=0.0634, ret=-0.000261, glen=96, tlen=257, kl=0, act_lr=5.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 53/55 [00:46<00:01,  1.17it/s, pg=0.0252, ret=0.000456, glen=149, tlen=310, kl=0, act_lr=5.2e-7, ent=1.55]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:46<00:00,  1.16it/s, pg=0.0252, ret=0.000456, glen=149, tlen=310, kl=0, act_lr=5.2e-7, ent=1.55]
2025-07-23 13:30:48.267 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.84s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.16it/s, pg=-0.29, ret=0.00243, glen=96.7, tlen=257, kl=0, act_lr=5.4e-7, ent=1.87] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 54/55 [00:47<00:00,  1.13it/s, pg=-0.29, ret=0.00243, glen=96.7, tlen=257, kl=0, act_lr=5.4e-7, ent=1.87]
2025-07-23 13:30:49.119 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 13:30:51.768 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.65s
2025-07-23 13:30:52.101 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 51.79s
2025-07-23 13:30:52.107 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.019707905162464488, 'actor_lr': 5.203636502218697e-07, 'clip_ratio': 0.0, 'entropy': 1.9340386759151111, 'kl': 0.0, 'response_length': 107.11760975230824, 'total_length': 267.5164389870383, 'return': 5.0888576317282225e-05, 'policy_update_steps': 1.0}
Episode [3/20]:   8%|‚ñä         | 1/13 [03:52<46:24, 232.04s/it]2025-07-23 13:30:52.139 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:33:24.019 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:33:24.206 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 13:33:24.206 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 152.07s
2025-07-23 13:33:26.945 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0004,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0116,avg_pass_at_n: 1.0000,avg_num_tokens: 97.7792,std_num_tokens: 182.5880,avg_correct_num_tokens: 85.4493,std_correct_num_tokens: 72.1311,avg_incorrect_num_tokens: 108.3637,std_incorrect_num_tokens: 239.2665
2025-07-23 13:33:27.383 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 3.18s
2025-07-23 13:33:28.670 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.29s
2025-07-23 13:33:56.509 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 215
2025-07-23 13:33:56.509 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.84s
2025-07-23 13:33:57.361 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.85s
2025-07-23 13:33:57.362 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.00047245477407434306, avg_kl: 0.0009336294129837391, avg_response_length: 107.86311375817587, avg_orm_score: 0.0, avg_custom_rewards: 0.00047245477407434306
2025-07-23 13:33:57.394 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter27_replay_buffer.jsonl
2025-07-23 13:33:58.854 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.46s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:00<?, ?it/s, pg=0.117, ret=-0.000347, glen=96.4, tlen=256, kl=0.000927, act_lr=5.4e-7, ent=2.3]Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:00<00:52,  1.01it/s, pg=0.117, ret=-0.000347, glen=96.4, tlen=256, kl=0.000927, act_lr=5.4e-7, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:01<00:52,  1.01it/s, pg=0.148, ret=-0.00165, glen=85.5, tlen=246, kl=0.000938, act_lr=5.4e-7, ent=1.82]Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:01<00:47,  1.10it/s, pg=0.148, ret=-0.00165, glen=85.5, tlen=246, kl=0.000938, act_lr=5.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:02<00:47,  1.10it/s, pg=0.0136, ret=0.000162, glen=107, tlen=267, kl=0.000955, act_lr=5.4e-7, ent=2.1] Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:02<00:45,  1.13it/s, pg=0.0136, ret=0.000162, glen=107, tlen=267, kl=0.000955, act_lr=5.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:03<00:45,  1.13it/s, pg=-0.168, ret=0.000605, glen=80.7, tlen=241, kl=0.000936, act_lr=5.4e-7, ent=1.97]Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:03<00:44,  1.11it/s, pg=-0.168, ret=0.000605, glen=80.7, tlen=241, kl=0.000936, act_lr=5.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:04<00:44,  1.11it/s, pg=-0.102, ret=0.000633, glen=110, tlen=270, kl=0.000843, act_lr=5.4e-7, ent=2.42] Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:04<00:43,  1.13it/s, pg=-0.102, ret=0.000633, glen=110, tlen=270, kl=0.000843, act_lr=5.4e-7, ent=2.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:05<00:43,  1.13it/s, pg=-0.143, ret=6.66e-6, glen=89.3, tlen=250, kl=0.000919, act_lr=5.4e-7, ent=1.84]Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:05<00:42,  1.12it/s, pg=-0.143, ret=6.66e-6, glen=89.3, tlen=250, kl=0.000919, act_lr=5.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:06<00:42,  1.12it/s, pg=0.144, ret=-0.00105, glen=108, tlen=268, kl=0.000949, act_lr=5.4e-7, ent=2.11] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:06<00:41,  1.14it/s, pg=0.144, ret=-0.00105, glen=108, tlen=268, kl=0.000949, act_lr=5.4e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:07<00:41,  1.14it/s, pg=-0.234, ret=0.00159, glen=105, tlen=266, kl=0.000941, act_lr=5.4e-7, ent=2.31]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:07<00:40,  1.13it/s, pg=-0.234, ret=0.00159, glen=105, tlen=266, kl=0.000941, act_lr=5.4e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:07<00:40,  1.13it/s, pg=-0.0928, ret=0.000868, glen=96.3, tlen=257, kl=0.000937, act_lr=5.4e-7, ent=1.73]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:07<00:39,  1.14it/s, pg=-0.0928, ret=0.000868, glen=96.3, tlen=257, kl=0.000937, act_lr=5.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:08<00:39,  1.14it/s, pg=-0.122, ret=0.000558, glen=87.9, tlen=248, kl=0.000938, act_lr=5.4e-7, ent=1.82] Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:08<00:38,  1.15it/s, pg=-0.122, ret=0.000558, glen=87.9, tlen=248, kl=0.000938, act_lr=5.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:09<00:38,  1.15it/s, pg=-0.193, ret=0.000869, glen=103, tlen=264, kl=0.000893, act_lr=5.4e-7, ent=1.78] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:09<00:37,  1.16it/s, pg=-0.193, ret=0.000869, glen=103, tlen=264, kl=0.000893, act_lr=5.4e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:10<00:37,  1.16it/s, pg=-0.0225, ret=0.00069, glen=96.5, tlen=257, kl=0.000871, act_lr=5.4e-7, ent=2.02]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:10<00:36,  1.16it/s, pg=-0.0225, ret=0.00069, glen=96.5, tlen=257, kl=0.000871, act_lr=5.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:11<00:36,  1.16it/s, pg=-0.319, ret=0.000667, glen=96.2, tlen=256, kl=0.000923, act_lr=5.4e-7, ent=1.85]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:11<00:35,  1.17it/s, pg=-0.319, ret=0.000667, glen=96.2, tlen=256, kl=0.000923, act_lr=5.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:12<00:35,  1.17it/s, pg=0.19, ret=-0.001, glen=100, tlen=260, kl=0.000932, act_lr=5.4e-7, ent=1.92]     Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:12<00:34,  1.17it/s, pg=0.19, ret=-0.001, glen=100, tlen=260, kl=0.000932, act_lr=5.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:13<00:34,  1.17it/s, pg=0.258, ret=-0.00127, glen=94.5, tlen=255, kl=0.000976, act_lr=5.4e-7, ent=1.88]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:13<00:33,  1.17it/s, pg=0.258, ret=-0.00127, glen=94.5, tlen=255, kl=0.000976, act_lr=5.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:13<00:33,  1.17it/s, pg=-0.134, ret=0.00096, glen=97.9, tlen=258, kl=0.000919, act_lr=5.4e-7, ent=1.75]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:13<00:32,  1.17it/s, pg=-0.134, ret=0.00096, glen=97.9, tlen=258, kl=0.000919, act_lr=5.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:14<00:32,  1.17it/s, pg=0.0498, ret=-0.000515, glen=95.1, tlen=255, kl=0.000916, act_lr=5.4e-7, ent=1.83]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:14<00:31,  1.17it/s, pg=0.0498, ret=-0.000515, glen=95.1, tlen=255, kl=0.000916, act_lr=5.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:15<00:31,  1.17it/s, pg=0.0538, ret=0.00299, glen=109, tlen=270, kl=0.000979, act_lr=5.4e-7, ent=2.34]   Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:15<00:30,  1.17it/s, pg=0.0538, ret=0.00299, glen=109, tlen=270, kl=0.000979, act_lr=5.4e-7, ent=2.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:16<00:30,  1.17it/s, pg=-0.128, ret=0.000614, glen=87.1, tlen=248, kl=0.000994, act_lr=5.4e-7, ent=1.82]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:16<00:29,  1.17it/s, pg=-0.128, ret=0.000614, glen=87.1, tlen=248, kl=0.000994, act_lr=5.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:17<00:29,  1.17it/s, pg=-0.107, ret=-0.000454, glen=93.3, tlen=253, kl=0.000946, act_lr=5.4e-7, ent=1.96]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:17<00:28,  1.17it/s, pg=-0.107, ret=-0.000454, glen=93.3, tlen=253, kl=0.000946, act_lr=5.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:18<00:28,  1.17it/s, pg=0.00134, ret=0.000449, glen=112, tlen=272, kl=0.000966, act_lr=5.4e-7, ent=2.04] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:18<00:28,  1.17it/s, pg=0.00134, ret=0.000449, glen=112, tlen=272, kl=0.000966, act_lr=5.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:19<00:28,  1.17it/s, pg=-0.142, ret=0.000525, glen=89.2, tlen=250, kl=0.000919, act_lr=5.4e-7, ent=1.82]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:19<00:27,  1.17it/s, pg=-0.142, ret=0.000525, glen=89.2, tlen=250, kl=0.000919, act_lr=5.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:19<00:27,  1.17it/s, pg=-0.141, ret=0.000564, glen=97.8, tlen=259, kl=0.000894, act_lr=5.4e-7, ent=1.85]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:19<00:26,  1.17it/s, pg=-0.141, ret=0.000564, glen=97.8, tlen=259, kl=0.000894, act_lr=5.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:20<00:26,  1.17it/s, pg=-0.0119, ret=0.000994, glen=88.2, tlen=248, kl=0.000973, act_lr=5.4e-7, ent=1.92]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:20<00:25,  1.17it/s, pg=-0.0119, ret=0.000994, glen=88.2, tlen=248, kl=0.000973, act_lr=5.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:21<00:25,  1.17it/s, pg=0.234, ret=-0.000771, glen=159, tlen=319, kl=0.00091, act_lr=5.4e-7, ent=2.46]   Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:21<00:25,  1.16it/s, pg=0.234, ret=-0.000771, glen=159, tlen=319, kl=0.00091, act_lr=5.4e-7, ent=2.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:22<00:25,  1.16it/s, pg=-0.00995, ret=-0.000391, glen=106, tlen=267, kl=0.00096, act_lr=5.4e-7, ent=1.91]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:22<00:24,  1.16it/s, pg=-0.00995, ret=-0.000391, glen=106, tlen=267, kl=0.00096, act_lr=5.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:23<00:24,  1.16it/s, pg=-0.175, ret=0.000268, glen=85.1, tlen=246, kl=0.000965, act_lr=5.4e-7, ent=1.93] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:23<00:27,  1.03s/it, pg=-0.175, ret=0.000268, glen=85.1, tlen=246, kl=0.000965, act_lr=5.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:24<00:27,  1.03s/it, pg=-0.0886, ret=0.000602, glen=95, tlen=255, kl=0.000972, act_lr=5.4e-7, ent=1.87] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:24<00:25,  1.02it/s, pg=-0.0886, ret=0.000602, glen=95, tlen=255, kl=0.000972, act_lr=5.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:25<00:25,  1.02it/s, pg=0.122, ret=-0.000827, glen=103, tlen=264, kl=0.000987, act_lr=5.4e-7, ent=1.88]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:25<00:23,  1.06it/s, pg=0.122, ret=-0.000827, glen=103, tlen=264, kl=0.000987, act_lr=5.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:26<00:23,  1.06it/s, pg=0.0254, ret=-0.00126, glen=97.7, tlen=258, kl=0.000941, act_lr=5.4e-7, ent=1.85]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:26<00:21,  1.10it/s, pg=0.0254, ret=-0.00126, glen=97.7, tlen=258, kl=0.000941, act_lr=5.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:27<00:21,  1.10it/s, pg=0.103, ret=0.000356, glen=98.2, tlen=258, kl=0.001, act_lr=5.4e-7, ent=1.98]    Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:27<00:20,  1.12it/s, pg=0.103, ret=0.000356, glen=98.2, tlen=258, kl=0.001, act_lr=5.4e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:28<00:20,  1.12it/s, pg=0.209, ret=0.000618, glen=137, tlen=297, kl=0.000883, act_lr=5.4e-7, ent=2.46]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:28<00:19,  1.12it/s, pg=0.209, ret=0.000618, glen=137, tlen=297, kl=0.000883, act_lr=5.4e-7, ent=2.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:29<00:19,  1.12it/s, pg=-0.0661, ret=-0.00693, glen=373, tlen=534, kl=0.000719, act_lr=5.4e-7, ent=1.5]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:29<00:18,  1.11it/s, pg=-0.0661, ret=-0.00693, glen=373, tlen=534, kl=0.000719, act_lr=5.4e-7, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:30<00:18,  1.11it/s, pg=-0.0143, ret=0.00066, glen=86.3, tlen=247, kl=0.000972, act_lr=5.4e-7, ent=1.87]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:30<00:17,  1.13it/s, pg=-0.0143, ret=0.00066, glen=86.3, tlen=247, kl=0.000972, act_lr=5.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:30<00:17,  1.13it/s, pg=0.0251, ret=-0.00861, glen=368, tlen=529, kl=0.000742, act_lr=5.4e-7, ent=1.4]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:30<00:17,  1.11it/s, pg=0.0251, ret=-0.00861, glen=368, tlen=529, kl=0.000742, act_lr=5.4e-7, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:31<00:17,  1.11it/s, pg=0.149, ret=-0.0018, glen=97.7, tlen=258, kl=0.0009, act_lr=5.4e-7, ent=1.85]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:31<00:15,  1.13it/s, pg=0.149, ret=-0.0018, glen=97.7, tlen=258, kl=0.0009, act_lr=5.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:32<00:15,  1.13it/s, pg=-0.159, ret=0.00042, glen=95.8, tlen=256, kl=0.000916, act_lr=5.4e-7, ent=2.13]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:32<00:14,  1.14it/s, pg=-0.159, ret=0.00042, glen=95.8, tlen=256, kl=0.000916, act_lr=5.4e-7, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:33<00:14,  1.14it/s, pg=-0.117, ret=0.00199, glen=95.1, tlen=255, kl=0.000936, act_lr=5.4e-7, ent=2.05]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:33<00:13,  1.15it/s, pg=-0.117, ret=0.00199, glen=95.1, tlen=255, kl=0.000936, act_lr=5.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:34<00:13,  1.15it/s, pg=-0.252, ret=0.00101, glen=92.8, tlen=253, kl=0.000955, act_lr=5.4e-7, ent=1.95]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:34<00:12,  1.16it/s, pg=-0.252, ret=0.00101, glen=92.8, tlen=253, kl=0.000955, act_lr=5.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:35<00:12,  1.16it/s, pg=-0.0243, ret=0.000563, glen=87.9, tlen=248, kl=0.000938, act_lr=5.4e-7, ent=1.82]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:35<00:12,  1.16it/s, pg=-0.0243, ret=0.000563, glen=87.9, tlen=248, kl=0.000938, act_lr=5.4e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:36<00:12,  1.16it/s, pg=0.135, ret=-0.00119, glen=99.3, tlen=260, kl=0.000938, act_lr=5.4e-7, ent=2.09]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:36<00:11,  1.17it/s, pg=0.135, ret=-0.00119, glen=99.3, tlen=260, kl=0.000938, act_lr=5.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:36<00:11,  1.17it/s, pg=-0.0587, ret=0.000989, glen=97.7, tlen=258, kl=0.000963, act_lr=5.4e-7, ent=1.9]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:36<00:10,  1.17it/s, pg=-0.0587, ret=0.000989, glen=97.7, tlen=258, kl=0.000963, act_lr=5.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:37<00:10,  1.17it/s, pg=0.000488, ret=-0.000869, glen=107, tlen=267, kl=0.000908, act_lr=5.4e-7, ent=2.26]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:37<00:09,  1.17it/s, pg=0.000488, ret=-0.000869, glen=107, tlen=267, kl=0.000908, act_lr=5.4e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:38<00:09,  1.17it/s, pg=-0.0558, ret=-7.48e-5, glen=88.6, tlen=249, kl=0.000963, act_lr=5.4e-7, ent=1.86] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:38<00:08,  1.17it/s, pg=-0.0558, ret=-7.48e-5, glen=88.6, tlen=249, kl=0.000963, act_lr=5.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:39<00:08,  1.17it/s, pg=-0.00146, ret=-5.91e-5, glen=90.1, tlen=250, kl=0.000944, act_lr=5.4e-7, ent=1.81]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:39<00:07,  1.17it/s, pg=-0.00146, ret=-5.91e-5, glen=90.1, tlen=250, kl=0.000944, act_lr=5.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:40<00:07,  1.17it/s, pg=-0.00415, ret=-0.000198, glen=93.5, tlen=254, kl=0.000957, act_lr=5.4e-7, ent=1.91]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:40<00:06,  1.17it/s, pg=-0.00415, ret=-0.000198, glen=93.5, tlen=254, kl=0.000957, act_lr=5.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:41<00:06,  1.17it/s, pg=0.151, ret=-0.00191, glen=146, tlen=306, kl=0.000835, act_lr=5.4e-7, ent=1.75]     Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:41<00:06,  1.16it/s, pg=0.151, ret=-0.00191, glen=146, tlen=306, kl=0.000835, act_lr=5.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:42<00:06,  1.16it/s, pg=0.181, ret=0.0107, glen=344, tlen=503, kl=0.000832, act_lr=5.4e-7, ent=2.73]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:42<00:05,  1.13it/s, pg=0.181, ret=0.0107, glen=344, tlen=503, kl=0.000832, act_lr=5.4e-7, ent=2.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:42<00:05,  1.13it/s, pg=-0.0825, ret=0.000492, glen=85.4, tlen=246, kl=0.000958, act_lr=5.4e-7, ent=1.84]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:42<00:04,  1.14it/s, pg=-0.0825, ret=0.000492, glen=85.4, tlen=246, kl=0.000958, act_lr=5.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:43<00:04,  1.14it/s, pg=-0.00391, ret=-0.000141, glen=83, tlen=243, kl=0.000945, act_lr=5.4e-7, ent=1.91]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:43<00:03,  1.15it/s, pg=-0.00391, ret=-0.000141, glen=83, tlen=243, kl=0.000945, act_lr=5.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:44<00:03,  1.15it/s, pg=-0.0303, ret=-0.00084, glen=80.8, tlen=241, kl=0.000963, act_lr=5.4e-7, ent=1.83]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:44<00:02,  1.16it/s, pg=-0.0303, ret=-0.00084, glen=80.8, tlen=241, kl=0.000963, act_lr=5.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:45<00:02,  1.16it/s, pg=0.00586, ret=-0.000804, glen=92.4, tlen=253, kl=0.00101, act_lr=5.4e-7, ent=1.85]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:45<00:01,  1.16it/s, pg=0.00586, ret=-0.000804, glen=92.4, tlen=253, kl=0.00101, act_lr=5.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:46<00:01,  1.16it/s, pg=-0.126, ret=0.000118, glen=88.5, tlen=249, kl=0.000959, act_lr=5.4e-7, ent=1.89] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:46<00:00,  1.17it/s, pg=-0.126, ret=0.000118, glen=88.5, tlen=249, kl=0.000959, act_lr=5.4e-7, ent=1.89]
2025-07-23 13:34:46.366 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.35s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:47<00:00,  1.17it/s, pg=0.12, ret=-0.00231, glen=100, tlen=261, kl=0.000938, act_lr=5.6e-7, ent=1.88]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:47<00:00,  1.12it/s, pg=0.12, ret=-0.00231, glen=100, tlen=261, kl=0.000938, act_lr=5.6e-7, ent=1.88]
2025-07-23 13:34:47.045 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 13:34:49.083 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.04s
2025-07-23 13:34:49.394 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 50.49s
2025-07-23 13:34:49.400 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.016531626383463543, 'actor_lr': 5.403703442897056e-07, 'clip_ratio': 0.0, 'entropy': 1.9552189155861184, 'kl': 0.0009296117005524812, 'response_length': 112.9609690065737, 'total_length': 273.27751103153935, 'return': -5.096222917523442e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  15%|‚ñà‚ñå        | 2/13 [07:49<43:06, 235.13s/it]2025-07-23 13:34:49.436 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:37:25.896 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:37:26.071 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.17s
2025-07-23 13:37:26.072 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 156.64s
2025-07-23 13:37:28.184 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0153,avg_reflection_pattern_score: 0.0100,avg_pass_at_n: 1.0000,avg_num_tokens: 96.4977,std_num_tokens: 169.7512,avg_correct_num_tokens: 87.8771,std_correct_num_tokens: 89.4646,avg_incorrect_num_tokens: 103.8653,std_incorrect_num_tokens: 215.6023
2025-07-23 13:37:28.620 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.55s
2025-07-23 13:37:29.925 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.30s
2025-07-23 13:37:57.358 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 214
2025-07-23 13:37:57.358 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.43s
2025-07-23 13:37:58.199 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.84s
2025-07-23 13:37:58.199 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0006103338891309555, avg_kl: 0.0009614084368554231, avg_response_length: 106.01153860359548, avg_orm_score: 0.0, avg_custom_rewards: -0.0006103338891309555
2025-07-23 13:37:58.230 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter28_replay_buffer.jsonl
2025-07-23 13:37:59.684 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.46s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/54 [00:01<?, ?it/s, pg=-0.0786, ret=0.00093, glen=91.9, tlen=252, kl=0.000966, act_lr=5.6e-7, ent=1.92]Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:01<00:53,  1.01s/it, pg=-0.0786, ret=0.00093, glen=91.9, tlen=252, kl=0.000966, act_lr=5.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/54 [00:01<00:53,  1.01s/it, pg=-0.175, ret=0.00139, glen=90.2, tlen=250, kl=0.000984, act_lr=5.6e-7, ent=1.83] Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:01<00:47,  1.08it/s, pg=-0.175, ret=0.00139, glen=90.2, tlen=250, kl=0.000984, act_lr=5.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 2/54 [00:02<00:47,  1.08it/s, pg=-0.217, ret=0.0018, glen=88.1, tlen=248, kl=0.00102, act_lr=5.6e-7, ent=1.84]  Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:02<00:45,  1.12it/s, pg=-0.217, ret=0.0018, glen=88.1, tlen=248, kl=0.00102, act_lr=5.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/54 [00:03<00:45,  1.12it/s, pg=0.00851, ret=-0.000259, glen=86, tlen=246, kl=0.000957, act_lr=5.6e-7, ent=1.71]Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:03<00:43,  1.14it/s, pg=0.00851, ret=-0.000259, glen=86, tlen=246, kl=0.000957, act_lr=5.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 4/54 [00:04<00:43,  1.14it/s, pg=-0.161, ret=0.00147, glen=105, tlen=265, kl=0.000996, act_lr=5.6e-7, ent=1.98]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:04<00:43,  1.13it/s, pg=-0.161, ret=0.00147, glen=105, tlen=265, kl=0.000996, act_lr=5.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/54 [00:05<00:43,  1.13it/s, pg=0.103, ret=0.00259, glen=187, tlen=347, kl=0.000912, act_lr=5.6e-7, ent=2.64] Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:05<00:43,  1.09it/s, pg=0.103, ret=0.00259, glen=187, tlen=347, kl=0.000912, act_lr=5.6e-7, ent=2.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 6/54 [00:06<00:43,  1.09it/s, pg=0.177, ret=0.000406, glen=335, tlen=495, kl=0.000723, act_lr=5.6e-7, ent=1.44]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:06<00:44,  1.05it/s, pg=0.177, ret=0.000406, glen=335, tlen=495, kl=0.000723, act_lr=5.6e-7, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/54 [00:07<00:44,  1.05it/s, pg=-0.11, ret=0.00023, glen=93.6, tlen=254, kl=0.000963, act_lr=5.6e-7, ent=1.86]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:07<00:42,  1.09it/s, pg=-0.11, ret=0.00023, glen=93.6, tlen=254, kl=0.000963, act_lr=5.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 8/54 [00:08<00:42,  1.09it/s, pg=0.0178, ret=-0.000329, glen=88.3, tlen=249, kl=0.000937, act_lr=5.6e-7, ent=1.79]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:08<00:40,  1.11it/s, pg=0.0178, ret=-0.000329, glen=88.3, tlen=249, kl=0.000937, act_lr=5.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/54 [00:09<00:40,  1.11it/s, pg=0.00669, ret=-0.000625, glen=84.1, tlen=244, kl=0.00101, act_lr=5.6e-7, ent=1.76]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:09<00:39,  1.13it/s, pg=0.00669, ret=-0.000625, glen=84.1, tlen=244, kl=0.00101, act_lr=5.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 10/54 [00:09<00:39,  1.13it/s, pg=-0.161, ret=0.000185, glen=90.5, tlen=250, kl=0.000961, act_lr=5.6e-7, ent=1.71] Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:09<00:37,  1.14it/s, pg=-0.161, ret=0.000185, glen=90.5, tlen=250, kl=0.000961, act_lr=5.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 11/54 [00:10<00:37,  1.14it/s, pg=-0.0811, ret=0.000972, glen=101, tlen=260, kl=0.001, act_lr=5.6e-7, ent=1.87]   Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:10<00:36,  1.15it/s, pg=-0.0811, ret=0.000972, glen=101, tlen=260, kl=0.001, act_lr=5.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 12/54 [00:11<00:36,  1.15it/s, pg=0.156, ret=-0.00128, glen=87, tlen=247, kl=0.00101, act_lr=5.6e-7, ent=1.74] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:11<00:35,  1.16it/s, pg=0.156, ret=-0.00128, glen=87, tlen=247, kl=0.00101, act_lr=5.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 13/54 [00:12<00:35,  1.16it/s, pg=-0.0713, ret=0.00048, glen=90.6, tlen=251, kl=0.00102, act_lr=5.6e-7, ent=1.85]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:12<00:34,  1.16it/s, pg=-0.0713, ret=0.00048, glen=90.6, tlen=251, kl=0.00102, act_lr=5.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 14/54 [00:13<00:34,  1.16it/s, pg=-0.041, ret=0.000752, glen=96.7, tlen=257, kl=0.00101, act_lr=5.6e-7, ent=1.7] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:13<00:33,  1.17it/s, pg=-0.041, ret=0.000752, glen=96.7, tlen=257, kl=0.00101, act_lr=5.6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/54 [00:14<00:33,  1.17it/s, pg=-0.00867, ret=0.000315, glen=88.2, tlen=248, kl=0.000978, act_lr=5.6e-7, ent=1.92]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:14<00:32,  1.17it/s, pg=-0.00867, ret=0.000315, glen=88.2, tlen=248, kl=0.000978, act_lr=5.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 16/54 [00:15<00:32,  1.17it/s, pg=0.173, ret=-0.00111, glen=107, tlen=267, kl=0.000947, act_lr=5.6e-7, ent=2.11]    Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:15<00:31,  1.17it/s, pg=0.173, ret=-0.00111, glen=107, tlen=267, kl=0.000947, act_lr=5.6e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 17/54 [00:15<00:31,  1.17it/s, pg=0.105, ret=-0.000162, glen=101, tlen=261, kl=0.000986, act_lr=5.6e-7, ent=2.08]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:15<00:30,  1.17it/s, pg=0.105, ret=-0.000162, glen=101, tlen=261, kl=0.000986, act_lr=5.6e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 18/54 [00:16<00:30,  1.17it/s, pg=-0.0213, ret=-0.000363, glen=84.1, tlen=244, kl=0.000996, act_lr=5.6e-7, ent=1.76]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:16<00:29,  1.17it/s, pg=-0.0213, ret=-0.000363, glen=84.1, tlen=244, kl=0.000996, act_lr=5.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 19/54 [00:17<00:29,  1.17it/s, pg=-0.00775, ret=-0.000351, glen=95.2, tlen=255, kl=0.000995, act_lr=5.6e-7, ent=1.88]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:17<00:29,  1.17it/s, pg=-0.00775, ret=-0.000351, glen=95.2, tlen=255, kl=0.000995, act_lr=5.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 20/54 [00:18<00:29,  1.17it/s, pg=0.151, ret=-0.00167, glen=106, tlen=266, kl=0.000955, act_lr=5.6e-7, ent=2.08]     Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:18<00:28,  1.17it/s, pg=0.151, ret=-0.00167, glen=106, tlen=266, kl=0.000955, act_lr=5.6e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 21/54 [00:19<00:28,  1.17it/s, pg=-0.0737, ret=0.000604, glen=93.6, tlen=253, kl=0.000986, act_lr=5.6e-7, ent=1.71]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:19<00:27,  1.17it/s, pg=-0.0737, ret=0.000604, glen=93.6, tlen=253, kl=0.000986, act_lr=5.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 22/54 [00:20<00:27,  1.17it/s, pg=-0.204, ret=0.00229, glen=100, tlen=261, kl=0.000949, act_lr=5.6e-7, ent=1.93]   Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:20<00:26,  1.17it/s, pg=-0.204, ret=0.00229, glen=100, tlen=261, kl=0.000949, act_lr=5.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/54 [00:20<00:26,  1.17it/s, pg=0.00397, ret=-0.00095, glen=94.7, tlen=255, kl=0.000928, act_lr=5.6e-7, ent=1.79]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:20<00:25,  1.17it/s, pg=0.00397, ret=-0.00095, glen=94.7, tlen=255, kl=0.000928, act_lr=5.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 24/54 [00:21<00:25,  1.17it/s, pg=0.0483, ret=-0.000643, glen=98.4, tlen=258, kl=0.000945, act_lr=5.6e-7, ent=1.92]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:21<00:24,  1.17it/s, pg=0.0483, ret=-0.000643, glen=98.4, tlen=258, kl=0.000945, act_lr=5.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/54 [00:22<00:24,  1.17it/s, pg=0.0141, ret=0.000284, glen=86.3, tlen=247, kl=0.000955, act_lr=5.6e-7, ent=1.6]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:22<00:23,  1.17it/s, pg=0.0141, ret=0.000284, glen=86.3, tlen=247, kl=0.000955, act_lr=5.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 26/54 [00:23<00:23,  1.17it/s, pg=-0.0583, ret=0.000426, glen=118, tlen=278, kl=0.000919, act_lr=5.6e-7, ent=2.23]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:23<00:25,  1.06it/s, pg=-0.0583, ret=0.000426, glen=118, tlen=278, kl=0.000919, act_lr=5.6e-7, ent=2.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/54 [00:24<00:25,  1.06it/s, pg=0.0485, ret=-0.00107, glen=89.2, tlen=250, kl=0.000948, act_lr=5.6e-7, ent=1.84]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:24<00:23,  1.09it/s, pg=0.0485, ret=-0.00107, glen=89.2, tlen=250, kl=0.000948, act_lr=5.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 28/54 [00:25<00:23,  1.09it/s, pg=0.0108, ret=-0.000988, glen=94.8, tlen=255, kl=0.000932, act_lr=5.6e-7, ent=1.84]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:25<00:22,  1.11it/s, pg=0.0108, ret=-0.000988, glen=94.8, tlen=255, kl=0.000932, act_lr=5.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 29/54 [00:26<00:22,  1.11it/s, pg=-0.0249, ret=-0.000898, glen=95.6, tlen=255, kl=0.000978, act_lr=5.6e-7, ent=1.82]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:26<00:21,  1.13it/s, pg=-0.0249, ret=-0.000898, glen=95.6, tlen=255, kl=0.000978, act_lr=5.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 30/54 [00:27<00:21,  1.13it/s, pg=-0.17, ret=0.00159, glen=92, tlen=252, kl=0.00096, act_lr=5.6e-7, ent=1.73]       Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:27<00:20,  1.12it/s, pg=-0.17, ret=0.00159, glen=92, tlen=252, kl=0.00096, act_lr=5.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 31/54 [00:28<00:20,  1.12it/s, pg=0.0789, ret=-0.00235, glen=335, tlen=495, kl=0.000869, act_lr=5.6e-7, ent=2.53]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:28<00:19,  1.10it/s, pg=0.0789, ret=-0.00235, glen=335, tlen=495, kl=0.000869, act_lr=5.6e-7, ent=2.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 32/54 [00:29<00:19,  1.10it/s, pg=-0.119, ret=0.0005, glen=96.9, tlen=257, kl=0.000983, act_lr=5.6e-7, ent=1.87] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:29<00:18,  1.12it/s, pg=-0.119, ret=0.0005, glen=96.9, tlen=257, kl=0.000983, act_lr=5.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 33/54 [00:29<00:18,  1.12it/s, pg=0.217, ret=-0.00244, glen=102, tlen=262, kl=0.000947, act_lr=5.6e-7, ent=2.02]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:29<00:17,  1.14it/s, pg=0.217, ret=-0.00244, glen=102, tlen=262, kl=0.000947, act_lr=5.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 34/54 [00:30<00:17,  1.14it/s, pg=0.0893, ret=-0.000929, glen=133, tlen=293, kl=0.000854, act_lr=5.6e-7, ent=2.36]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:30<00:16,  1.14it/s, pg=0.0893, ret=-0.000929, glen=133, tlen=293, kl=0.000854, act_lr=5.6e-7, ent=2.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 35/54 [00:31<00:16,  1.14it/s, pg=-0.169, ret=0.000238, glen=88.1, tlen=248, kl=0.000992, act_lr=5.6e-7, ent=1.91]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:31<00:15,  1.15it/s, pg=-0.169, ret=0.000238, glen=88.1, tlen=248, kl=0.000992, act_lr=5.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 36/54 [00:32<00:15,  1.15it/s, pg=-0.156, ret=0.00147, glen=99.9, tlen=260, kl=0.000983, act_lr=5.6e-7, ent=1.88] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:32<00:14,  1.16it/s, pg=-0.156, ret=0.00147, glen=99.9, tlen=260, kl=0.000983, act_lr=5.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 37/54 [00:33<00:14,  1.16it/s, pg=-0.152, ret=-0.000411, glen=104, tlen=264, kl=0.000943, act_lr=5.6e-7, ent=1.98]Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:33<00:13,  1.16it/s, pg=-0.152, ret=-0.000411, glen=104, tlen=264, kl=0.000943, act_lr=5.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 38/54 [00:34<00:13,  1.16it/s, pg=-0.0962, ret=0.00059, glen=94, tlen=254, kl=0.000974, act_lr=5.6e-7, ent=1.9]   Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:34<00:12,  1.17it/s, pg=-0.0962, ret=0.00059, glen=94, tlen=254, kl=0.000974, act_lr=5.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 39/54 [00:35<00:12,  1.17it/s, pg=0.081, ret=0.000219, glen=108, tlen=268, kl=0.000945, act_lr=5.6e-7, ent=1.93]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:35<00:11,  1.17it/s, pg=0.081, ret=0.000219, glen=108, tlen=268, kl=0.000945, act_lr=5.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 40/54 [00:35<00:11,  1.17it/s, pg=0.12, ret=-0.00233, glen=96.8, tlen=257, kl=0.000988, act_lr=5.6e-7, ent=2.02]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:35<00:11,  1.17it/s, pg=0.12, ret=-0.00233, glen=96.8, tlen=257, kl=0.000988, act_lr=5.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 41/54 [00:36<00:11,  1.17it/s, pg=-0.0787, ret=0.000185, glen=100, tlen=260, kl=0.000979, act_lr=5.6e-7, ent=1.81]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:36<00:10,  1.17it/s, pg=-0.0787, ret=0.000185, glen=100, tlen=260, kl=0.000979, act_lr=5.6e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 42/54 [00:37<00:10,  1.17it/s, pg=0.164, ret=-0.000982, glen=97, tlen=257, kl=0.000959, act_lr=5.6e-7, ent=1.84]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:37<00:09,  1.17it/s, pg=0.164, ret=-0.000982, glen=97, tlen=257, kl=0.000959, act_lr=5.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 43/54 [00:38<00:09,  1.17it/s, pg=0.075, ret=-0.000366, glen=96.6, tlen=256, kl=0.000971, act_lr=5.6e-7, ent=1.89]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:38<00:08,  1.17it/s, pg=0.075, ret=-0.000366, glen=96.6, tlen=256, kl=0.000971, act_lr=5.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 44/54 [00:39<00:08,  1.17it/s, pg=-0.00806, ret=-0.00343, glen=82.8, tlen=243, kl=0.000974, act_lr=5.6e-7, ent=1.65]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:39<00:07,  1.17it/s, pg=-0.00806, ret=-0.00343, glen=82.8, tlen=243, kl=0.000974, act_lr=5.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 45/54 [00:40<00:07,  1.17it/s, pg=0.119, ret=-0.000778, glen=93.2, tlen=253, kl=0.000978, act_lr=5.6e-7, ent=2]     Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:40<00:06,  1.17it/s, pg=0.119, ret=-0.000778, glen=93.2, tlen=253, kl=0.000978, act_lr=5.6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 46/54 [00:41<00:06,  1.17it/s, pg=-0.0955, ret=0.000539, glen=85.4, tlen=246, kl=0.00102, act_lr=5.6e-7, ent=1.71]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:41<00:05,  1.17it/s, pg=-0.0955, ret=0.000539, glen=85.4, tlen=246, kl=0.00102, act_lr=5.6e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 47/54 [00:41<00:05,  1.17it/s, pg=-0.14, ret=0.000416, glen=108, tlen=268, kl=0.000949, act_lr=5.6e-7, ent=1.95]  Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:41<00:05,  1.17it/s, pg=-0.14, ret=0.000416, glen=108, tlen=268, kl=0.000949, act_lr=5.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 48/54 [00:42<00:05,  1.17it/s, pg=0.0381, ret=-0.000814, glen=94.3, tlen=254, kl=0.000952, act_lr=5.6e-7, ent=1.79]Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:42<00:04,  1.17it/s, pg=0.0381, ret=-0.000814, glen=94.3, tlen=254, kl=0.000952, act_lr=5.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 49/54 [00:43<00:04,  1.17it/s, pg=-0.0644, ret=0.000423, glen=86.5, tlen=247, kl=0.000962, act_lr=5.6e-7, ent=1.72]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:43<00:03,  1.17it/s, pg=-0.0644, ret=0.000423, glen=86.5, tlen=247, kl=0.000962, act_lr=5.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 50/54 [00:44<00:03,  1.17it/s, pg=-0.151, ret=0.00111, glen=90.9, tlen=251, kl=0.000979, act_lr=5.6e-7, ent=1.79]  Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:44<00:02,  1.17it/s, pg=-0.151, ret=0.00111, glen=90.9, tlen=251, kl=0.000979, act_lr=5.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 51/54 [00:45<00:02,  1.17it/s, pg=-0.131, ret=-8.53e-5, glen=87.1, tlen=247, kl=0.000965, act_lr=5.6e-7, ent=1.74]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:45<00:01,  1.17it/s, pg=-0.131, ret=-8.53e-5, glen=87.1, tlen=247, kl=0.000965, act_lr=5.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 52/54 [00:46<00:01,  1.17it/s, pg=0.0755, ret=-0.00079, glen=91.2, tlen=251, kl=0.000971, act_lr=5.6e-7, ent=1.79]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:46<00:00,  1.17it/s, pg=0.0755, ret=-0.00079, glen=91.2, tlen=251, kl=0.000971, act_lr=5.6e-7, ent=1.79]
2025-07-23 13:38:47.160 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 47.11s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:47<00:00,  1.17it/s, pg=-0.0335, ret=0.000469, glen=86.4, tlen=246, kl=0.00095, act_lr=5.8e-7, ent=1.72]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 53/54 [00:47<00:00,  1.13it/s, pg=-0.0335, ret=0.000469, glen=86.4, tlen=246, kl=0.00095, act_lr=5.8e-7, ent=1.72]
2025-07-23 13:38:47.835 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 13:38:50.086 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.25s
2025-07-23 13:38:50.395 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 50.46s
2025-07-23 13:38:50.401 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.018116491812246817, 'actor_lr': 5.603703601553678e-07, 'clip_ratio': 0.0, 'entropy': 1.8826057425251714, 'kl': 0.0009610012725547508, 'response_length': 105.88387326841001, 'total_length': 265.90230645073785, 'return': -6.508726343141731e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:50<39:38, 237.81s/it]2025-07-23 13:38:50.433 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:40:31.900 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:40:32.089 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 13:40:32.090 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 101.66s
2025-07-23 13:40:33.973 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0100,avg_pass_at_n: 1.0000,avg_num_tokens: 90.4451,std_num_tokens: 120.9484,avg_correct_num_tokens: 84.7876,std_correct_num_tokens: 103.3810,avg_incorrect_num_tokens: 97.0908,std_incorrect_num_tokens: 138.4775
2025-07-23 13:40:34.368 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.28s
2025-07-23 13:40:35.815 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.44s
2025-07-23 13:41:02.476 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 208
2025-07-23 13:41:02.476 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.66s
2025-07-23 13:41:03.171 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.69s
2025-07-23 13:41:03.171 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.00011131128233570892, avg_kl: 0.004041341634897085, avg_response_length: 92.4759259223938, avg_orm_score: 0.0, avg_custom_rewards: 0.00011131128233570892
2025-07-23 13:41:03.197 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter29_replay_buffer.jsonl
2025-07-23 13:41:04.569 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.37s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s, pg=-0.0532, ret=-0.000305, glen=86.1, tlen=247, kl=0.00496, act_lr=5.8e-7, ent=1.71]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:00<00:48,  1.05it/s, pg=-0.0532, ret=-0.000305, glen=86.1, tlen=247, kl=0.00496, act_lr=5.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:48,  1.05it/s, pg=0.0848, ret=0.000766, glen=110, tlen=270, kl=0.00406, act_lr=5.8e-7, ent=1.92]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:45,  1.11it/s, pg=0.0848, ret=0.000766, glen=110, tlen=270, kl=0.00406, act_lr=5.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:45,  1.11it/s, pg=-0.0634, ret=-0.000297, glen=101, tlen=262, kl=0.00405, act_lr=5.8e-7, ent=1.86]Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:43,  1.13it/s, pg=-0.0634, ret=-0.000297, glen=101, tlen=262, kl=0.00405, act_lr=5.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:43,  1.13it/s, pg=-0.0773, ret=0.00015, glen=98.3, tlen=259, kl=0.00378, act_lr=5.8e-7, ent=1.96] Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:41,  1.15it/s, pg=-0.0773, ret=0.00015, glen=98.3, tlen=259, kl=0.00378, act_lr=5.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:41,  1.15it/s, pg=-0.0466, ret=0.00103, glen=90.2, tlen=251, kl=0.00439, act_lr=5.8e-7, ent=1.83]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:40,  1.16it/s, pg=-0.0466, ret=0.00103, glen=90.2, tlen=251, kl=0.00439, act_lr=5.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:40,  1.16it/s, pg=0.0522, ret=-0.000332, glen=86.7, tlen=247, kl=0.00394, act_lr=5.8e-7, ent=1.84]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:39,  1.16it/s, pg=0.0522, ret=-0.000332, glen=86.7, tlen=247, kl=0.00394, act_lr=5.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:39,  1.16it/s, pg=-0.0398, ret=0.000563, glen=84.6, tlen=245, kl=0.0036, act_lr=5.8e-7, ent=1.91] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:38,  1.17it/s, pg=-0.0398, ret=0.000563, glen=84.6, tlen=245, kl=0.0036, act_lr=5.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:38,  1.17it/s, pg=0.24, ret=-0.00141, glen=105, tlen=265, kl=0.0034, act_lr=5.8e-7, ent=2.18]    Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:06<00:37,  1.17it/s, pg=0.24, ret=-0.00141, glen=105, tlen=265, kl=0.0034, act_lr=5.8e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:37,  1.17it/s, pg=-0.21, ret=0.00138, glen=81.3, tlen=242, kl=0.00402, act_lr=5.8e-7, ent=1.79]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:07<00:36,  1.17it/s, pg=-0.21, ret=0.00138, glen=81.3, tlen=242, kl=0.00402, act_lr=5.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:36,  1.17it/s, pg=-0.0751, ret=0.000527, glen=90.3, tlen=251, kl=0.00405, act_lr=5.8e-7, ent=1.85]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:35,  1.17it/s, pg=-0.0751, ret=0.000527, glen=90.3, tlen=251, kl=0.00405, act_lr=5.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:35,  1.17it/s, pg=-0.0338, ret=0.000328, glen=118, tlen=279, kl=0.00343, act_lr=5.8e-7, ent=2.15] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:35,  1.15it/s, pg=-0.0338, ret=0.000328, glen=118, tlen=279, kl=0.00343, act_lr=5.8e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:35,  1.15it/s, pg=-0.0967, ret=0.000532, glen=95.8, tlen=256, kl=0.00353, act_lr=5.8e-7, ent=2.01]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.15it/s, pg=-0.0967, ret=0.000532, glen=95.8, tlen=256, kl=0.00353, act_lr=5.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.15it/s, pg=-0.134, ret=0.000859, glen=82.3, tlen=243, kl=0.00427, act_lr=5.8e-7, ent=1.91] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.16it/s, pg=-0.134, ret=0.000859, glen=82.3, tlen=243, kl=0.00427, act_lr=5.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.16it/s, pg=-0.123, ret=0.000225, glen=93.6, tlen=254, kl=0.0042, act_lr=5.8e-7, ent=2.01] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.16it/s, pg=-0.123, ret=0.000225, glen=93.6, tlen=254, kl=0.0042, act_lr=5.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.16it/s, pg=-0.0117, ret=-0.00073, glen=88.3, tlen=249, kl=0.00399, act_lr=5.8e-7, ent=1.79]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:12<00:31,  1.16it/s, pg=-0.0117, ret=-0.00073, glen=88.3, tlen=249, kl=0.00399, act_lr=5.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.16it/s, pg=0.049, ret=-0.000741, glen=91.8, tlen=252, kl=0.00419, act_lr=5.8e-7, ent=1.92] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:13<00:30,  1.16it/s, pg=0.049, ret=-0.000741, glen=91.8, tlen=252, kl=0.00419, act_lr=5.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.16it/s, pg=-0.00983, ret=-0.000966, glen=81.5, tlen=242, kl=0.00455, act_lr=5.8e-7, ent=1.78]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:29,  1.17it/s, pg=-0.00983, ret=-0.000966, glen=81.5, tlen=242, kl=0.00455, act_lr=5.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:29,  1.17it/s, pg=-0.0624, ret=-0.000386, glen=83.6, tlen=244, kl=0.00428, act_lr=5.8e-7, ent=1.77] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:29,  1.17it/s, pg=-0.0624, ret=-0.000386, glen=83.6, tlen=244, kl=0.00428, act_lr=5.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:29,  1.17it/s, pg=-0.0104, ret=-0.000137, glen=86, tlen=246, kl=0.00417, act_lr=5.8e-7, ent=1.83]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.17it/s, pg=-0.0104, ret=-0.000137, glen=86, tlen=246, kl=0.00417, act_lr=5.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.17it/s, pg=-0.00461, ret=0.00026, glen=84.4, tlen=245, kl=0.00405, act_lr=5.8e-7, ent=1.77]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.17it/s, pg=-0.00461, ret=0.00026, glen=84.4, tlen=245, kl=0.00405, act_lr=5.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.17it/s, pg=-0.0846, ret=0.00075, glen=83.8, tlen=245, kl=0.00438, act_lr=5.8e-7, ent=1.69] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:26,  1.17it/s, pg=-0.0846, ret=0.00075, glen=83.8, tlen=245, kl=0.00438, act_lr=5.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:26,  1.17it/s, pg=-0.0146, ret=0.00043, glen=84.9, tlen=246, kl=0.00397, act_lr=5.8e-7, ent=1.92]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:18<00:25,  1.18it/s, pg=-0.0146, ret=0.00043, glen=84.9, tlen=246, kl=0.00397, act_lr=5.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.18it/s, pg=0.0731, ret=-0.000531, glen=86.4, tlen=247, kl=0.00382, act_lr=5.8e-7, ent=2.05]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:19<00:24,  1.18it/s, pg=0.0731, ret=-0.000531, glen=86.4, tlen=247, kl=0.00382, act_lr=5.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:24,  1.18it/s, pg=-0.00146, ret=0.000812, glen=85.3, tlen=246, kl=0.00414, act_lr=5.8e-7, ent=1.88]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:20<00:23,  1.18it/s, pg=-0.00146, ret=0.000812, glen=85.3, tlen=246, kl=0.00414, act_lr=5.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:23,  1.18it/s, pg=0.154, ret=0.000737, glen=93.3, tlen=253, kl=0.0038, act_lr=5.8e-7, ent=2.15]    Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:22,  1.18it/s, pg=0.154, ret=0.000737, glen=93.3, tlen=253, kl=0.0038, act_lr=5.8e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:22,  1.18it/s, pg=-0.0906, ret=0.00098, glen=92.8, tlen=253, kl=0.00436, act_lr=5.8e-7, ent=2.05]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.18it/s, pg=-0.0906, ret=0.00098, glen=92.8, tlen=253, kl=0.00436, act_lr=5.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:23<00:22,  1.18it/s, pg=-0.11, ret=-0.000292, glen=168, tlen=329, kl=0.00403, act_lr=5.8e-7, ent=2.71] Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:23<00:23,  1.06it/s, pg=-0.11, ret=-0.000292, glen=168, tlen=329, kl=0.00403, act_lr=5.8e-7, ent=2.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:23,  1.06it/s, pg=-0.0299, ret=7.55e-6, glen=95.3, tlen=256, kl=0.00367, act_lr=5.8e-7, ent=1.93]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:21,  1.09it/s, pg=-0.0299, ret=7.55e-6, glen=95.3, tlen=256, kl=0.00367, act_lr=5.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:21,  1.09it/s, pg=-0.129, ret=0.000337, glen=83.9, tlen=245, kl=0.00427, act_lr=5.8e-7, ent=1.82]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:20,  1.12it/s, pg=-0.129, ret=0.000337, glen=83.9, tlen=245, kl=0.00427, act_lr=5.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:20,  1.12it/s, pg=-0.194, ret=0.00195, glen=87.6, tlen=248, kl=0.00403, act_lr=5.8e-7, ent=1.88] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:19,  1.13it/s, pg=-0.194, ret=0.00195, glen=87.6, tlen=248, kl=0.00403, act_lr=5.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:19,  1.13it/s, pg=-0.074, ret=-0.000581, glen=83.4, tlen=244, kl=0.00425, act_lr=5.8e-7, ent=1.76]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:26<00:18,  1.15it/s, pg=-0.074, ret=-0.000581, glen=83.4, tlen=244, kl=0.00425, act_lr=5.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:18,  1.15it/s, pg=0.00208, ret=-0.00208, glen=87.7, tlen=249, kl=0.00409, act_lr=5.8e-7, ent=1.96]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:27<00:17,  1.14it/s, pg=0.00208, ret=-0.00208, glen=87.7, tlen=249, kl=0.00409, act_lr=5.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.14it/s, pg=0.205, ret=-0.000725, glen=110, tlen=271, kl=0.00361, act_lr=5.8e-7, ent=1.6]   Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:28<00:16,  1.14it/s, pg=0.205, ret=-0.000725, glen=110, tlen=271, kl=0.00361, act_lr=5.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.14it/s, pg=0.143, ret=-0.000362, glen=108, tlen=268, kl=0.00378, act_lr=5.8e-7, ent=2.19]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:29<00:15,  1.14it/s, pg=0.143, ret=-0.000362, glen=108, tlen=268, kl=0.00378, act_lr=5.8e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.14it/s, pg=-0.0117, ret=0.000587, glen=88, tlen=248, kl=0.00425, act_lr=5.8e-7, ent=1.95]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.15it/s, pg=-0.0117, ret=0.000587, glen=88, tlen=248, kl=0.00425, act_lr=5.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.15it/s, pg=-0.00134, ret=0.000209, glen=84.3, tlen=245, kl=0.00393, act_lr=5.8e-7, ent=1.75]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.16it/s, pg=-0.00134, ret=0.000209, glen=84.3, tlen=245, kl=0.00393, act_lr=5.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.16it/s, pg=0.0435, ret=-0.00156, glen=84, tlen=245, kl=0.00443, act_lr=5.8e-7, ent=1.82]    Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.16it/s, pg=0.0435, ret=-0.00156, glen=84, tlen=245, kl=0.00443, act_lr=5.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.16it/s, pg=-0.0618, ret=0.000221, glen=87.6, tlen=248, kl=0.00415, act_lr=5.8e-7, ent=1.84]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:32<00:11,  1.17it/s, pg=-0.0618, ret=0.000221, glen=87.6, tlen=248, kl=0.00415, act_lr=5.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:11,  1.17it/s, pg=-0.00232, ret=0.000526, glen=90.7, tlen=251, kl=0.00399, act_lr=5.8e-7, ent=2.01]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:33<00:11,  1.17it/s, pg=-0.00232, ret=0.000526, glen=90.7, tlen=251, kl=0.00399, act_lr=5.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.17it/s, pg=0.133, ret=-0.00161, glen=84.8, tlen=246, kl=0.00492, act_lr=5.8e-7, ent=1.95]   Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:34<00:10,  1.17it/s, pg=0.133, ret=-0.00161, glen=84.8, tlen=246, kl=0.00492, act_lr=5.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.17it/s, pg=0.103, ret=-0.00132, glen=95, tlen=255, kl=0.00448, act_lr=5.8e-7, ent=1.93]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:35<00:09,  1.17it/s, pg=0.103, ret=-0.00132, glen=95, tlen=255, kl=0.00448, act_lr=5.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.17it/s, pg=0.0278, ret=-0.000336, glen=94.9, tlen=256, kl=0.00391, act_lr=5.8e-7, ent=1.81]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:36<00:08,  1.17it/s, pg=0.0278, ret=-0.000336, glen=94.9, tlen=256, kl=0.00391, act_lr=5.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.17it/s, pg=0.0208, ret=-0.000644, glen=80.1, tlen=240, kl=0.00433, act_lr=5.8e-7, ent=1.84]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.17it/s, pg=0.0208, ret=-0.000644, glen=80.1, tlen=240, kl=0.00433, act_lr=5.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.17it/s, pg=-0.175, ret=0.000581, glen=81.8, tlen=242, kl=0.00387, act_lr=5.8e-7, ent=1.97] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.17it/s, pg=-0.175, ret=0.000581, glen=81.8, tlen=242, kl=0.00387, act_lr=5.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.17it/s, pg=0.0807, ret=-0.00112, glen=92.3, tlen=253, kl=0.00376, act_lr=5.8e-7, ent=1.88]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:38<00:05,  1.17it/s, pg=0.0807, ret=-0.00112, glen=92.3, tlen=253, kl=0.00376, act_lr=5.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.17it/s, pg=0.233, ret=0.00142, glen=120, tlen=280, kl=0.0037, act_lr=5.8e-7, ent=2.38]    Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:39<00:05,  1.16it/s, pg=0.233, ret=0.00142, glen=120, tlen=280, kl=0.0037, act_lr=5.8e-7, ent=2.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.16it/s, pg=-0.0217, ret=0.000856, glen=84.9, tlen=245, kl=0.00414, act_lr=5.8e-7, ent=1.77]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:40<00:04,  1.17it/s, pg=-0.0217, ret=0.000856, glen=84.9, tlen=245, kl=0.00414, act_lr=5.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.17it/s, pg=-0.0547, ret=-0.000554, glen=76.7, tlen=237, kl=0.00372, act_lr=5.8e-7, ent=1.71]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:41<00:03,  1.17it/s, pg=-0.0547, ret=-0.000554, glen=76.7, tlen=237, kl=0.00372, act_lr=5.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=0.123, ret=-0.000896, glen=123, tlen=284, kl=0.00313, act_lr=5.8e-7, ent=2.57]   Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.16it/s, pg=0.123, ret=-0.000896, glen=123, tlen=284, kl=0.00313, act_lr=5.8e-7, ent=2.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.16it/s, pg=-0.0193, ret=0.000563, glen=89.5, tlen=250, kl=0.00377, act_lr=5.8e-7, ent=1.99]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.17it/s, pg=-0.0193, ret=0.000563, glen=89.5, tlen=250, kl=0.00377, act_lr=5.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.17it/s, pg=0.0247, ret=1.8e-5, glen=80, tlen=240, kl=0.00478, act_lr=5.8e-7, ent=1.76]     Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=0.0247, ret=1.8e-5, glen=80, tlen=240, kl=0.00478, act_lr=5.8e-7, ent=1.76]
2025-07-23 13:41:49.701 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.98s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=-0.116, ret=0.00101, glen=82.3, tlen=243, kl=0.0038, act_lr=6e-7, ent=1.9] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.13it/s, pg=-0.116, ret=0.00101, glen=82.3, tlen=243, kl=0.0038, act_lr=6e-7, ent=1.9]
2025-07-23 13:41:50.550 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 13:41:53.202 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.65s
2025-07-23 13:41:53.507 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.90s
2025-07-23 13:41:53.512 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.00865481450007512, 'actor_lr': 5.803846210465753e-07, 'clip_ratio': 0.0, 'entropy': 1.9269588635518, 'kl': 0.004041341634897085, 'response_length': 92.47592603243314, 'total_length': 253.02589856661282, 'return': 1.3431810237726984e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:53<32:25, 216.22s/it]2025-07-23 13:41:53.517 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:   1%|          | 1/171 [00:00<01:06,  2.56it/s, est. speed input: 468.32 toks/s, output: 28.15 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/172 [00:03<00:00, 39.63it/s, est. speed input: 7520.02 toks/s, output: 3130.97 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 163/172 [00:04<00:00, 32.57it/s, est. speed input: 7313.16 toks/s, output: 3160.97 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:04<00:00, 21.65it/s, est. speed input: 6506.62 toks/s, output: 2667.46 toks/s][32m [repeated 99x across cluster][0m
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:07<00:00,  4.64it/s, est. speed input: 4043.47 toks/s, output: 2081.24 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:07<00:00, 22.25it/s, est. speed input: 4043.47 toks/s, output: 2081.24 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:08<00:00,  2.17it/s, est. speed input: 3456.09 toks/s, output: 1561.69 toks/s][32m [repeated 19x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:18<00:00,  1.45s/it, est. speed input: 1638.08 toks/s, output: 872.68 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:18<00:00,  9.04it/s, est. speed input: 1638.08 toks/s, output: 872.68 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:10<00:00,  1.58it/s, est. speed input: 2940.21 toks/s, output: 1383.22 toks/s]
2025-07-23 13:42:13.429 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 422.0801,strategyqa_test/accuracy: 0.4338,eval_accuracy: 0.4338
2025-07-23 13:42:13.715 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:44:38.388 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:44:38.573 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:44:38.574 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 144.86s
2025-07-23 13:44:40.451 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0090,avg_pass_at_n: 1.0000,avg_num_tokens: 91.9716,std_num_tokens: 151.8554,avg_correct_num_tokens: 86.6424,std_correct_num_tokens: 77.5159,avg_incorrect_num_tokens: 97.7736,std_incorrect_num_tokens: 203.8622
2025-07-23 13:44:40.753 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.18s
2025-07-23 13:44:42.243 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.49s
2025-07-23 13:45:09.313 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 211
2025-07-23 13:45:09.313 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 27.07s
2025-07-23 13:45:10.068 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.75s
2025-07-23 13:45:10.068 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0012413521968457247, avg_kl: 0.009389063758307723, avg_response_length: 98.90310739453935, avg_orm_score: 0.0, avg_custom_rewards: -0.0012413521968457247
2025-07-23 13:45:10.099 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter30_replay_buffer.jsonl
2025-07-23 13:45:11.505 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.41s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/53 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/53 [00:00<?, ?it/s, pg=0.156, ret=-0.00126, glen=99.2, tlen=260, kl=0.00925, act_lr=6e-7, ent=2.09]Actor Train epoch [1/1]:   2%|‚ñè         | 1/53 [00:00<00:50,  1.03it/s, pg=0.156, ret=-0.00126, glen=99.2, tlen=260, kl=0.00925, act_lr=6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/53 [00:01<00:50,  1.03it/s, pg=0.00665, ret=-0.000578, glen=89.5, tlen=251, kl=0.00854, act_lr=6e-7, ent=1.94]Actor Train epoch [1/1]:   4%|‚ñç         | 2/53 [00:01<00:46,  1.10it/s, pg=0.00665, ret=-0.000578, glen=89.5, tlen=251, kl=0.00854, act_lr=6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/53 [00:02<00:46,  1.10it/s, pg=0.056, ret=-0.000451, glen=91.7, tlen=253, kl=0.00806, act_lr=6e-7, ent=1.92]  Actor Train epoch [1/1]:   6%|‚ñå         | 3/53 [00:02<00:44,  1.13it/s, pg=0.056, ret=-0.000451, glen=91.7, tlen=253, kl=0.00806, act_lr=6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/53 [00:03<00:44,  1.13it/s, pg=-0.0349, ret=-0.00072, glen=106, tlen=267, kl=0.00705, act_lr=6e-7, ent=2.15]Actor Train epoch [1/1]:   8%|‚ñä         | 4/53 [00:03<00:42,  1.15it/s, pg=-0.0349, ret=-0.00072, glen=106, tlen=267, kl=0.00705, act_lr=6e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/53 [00:04<00:42,  1.15it/s, pg=0.129, ret=-0.00425, glen=283, tlen=444, kl=0.00643, act_lr=6e-7, ent=1.76]  Actor Train epoch [1/1]:   9%|‚ñâ         | 5/53 [00:04<00:43,  1.11it/s, pg=0.129, ret=-0.00425, glen=283, tlen=444, kl=0.00643, act_lr=6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   9%|‚ñâ         | 5/53 [00:05<00:43,  1.11it/s, pg=-0.0624, ret=0.000576, glen=83.1, tlen=244, kl=0.00826, act_lr=6e-7, ent=1.88]Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 6/53 [00:05<00:41,  1.13it/s, pg=-0.0624, ret=0.000576, glen=83.1, tlen=244, kl=0.00826, act_lr=6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà‚ñè        | 6/53 [00:06<00:41,  1.13it/s, pg=0.00519, ret=-0.000604, glen=79.4, tlen=240, kl=0.0106, act_lr=6e-7, ent=1.84]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/53 [00:06<00:40,  1.14it/s, pg=0.00519, ret=-0.000604, glen=79.4, tlen=240, kl=0.0106, act_lr=6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/53 [00:07<00:40,  1.14it/s, pg=-0.0571, ret=-0.000844, glen=90, tlen=251, kl=0.00813, act_lr=6e-7, ent=1.91] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/53 [00:07<00:39,  1.15it/s, pg=-0.0571, ret=-0.000844, glen=90, tlen=251, kl=0.00813, act_lr=6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/53 [00:07<00:39,  1.15it/s, pg=0.0795, ret=-0.000765, glen=93.4, tlen=255, kl=0.00887, act_lr=6e-7, ent=2.14]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/53 [00:07<00:38,  1.16it/s, pg=0.0795, ret=-0.000765, glen=93.4, tlen=255, kl=0.00887, act_lr=6e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/53 [00:08<00:38,  1.16it/s, pg=-0.195, ret=0.00153, glen=94.5, tlen=256, kl=0.00892, act_lr=6e-7, ent=1.85]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/53 [00:08<00:37,  1.16it/s, pg=-0.195, ret=0.00153, glen=94.5, tlen=256, kl=0.00892, act_lr=6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/53 [00:09<00:37,  1.16it/s, pg=-0.0377, ret=-0.000599, glen=96.6, tlen=257, kl=0.009, act_lr=6e-7, ent=1.91]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/53 [00:09<00:36,  1.16it/s, pg=-0.0377, ret=-0.000599, glen=96.6, tlen=257, kl=0.009, act_lr=6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/53 [00:10<00:36,  1.16it/s, pg=0.0268, ret=-0.00157, glen=85.5, tlen=246, kl=0.0107, act_lr=6e-7, ent=1.87] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/53 [00:10<00:35,  1.16it/s, pg=0.0268, ret=-0.00157, glen=85.5, tlen=246, kl=0.0107, act_lr=6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/53 [00:11<00:35,  1.16it/s, pg=-0.0481, ret=0.00086, glen=89.9, tlen=251, kl=0.0105, act_lr=6e-7, ent=2.04]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 13/53 [00:11<00:34,  1.16it/s, pg=-0.0481, ret=0.00086, glen=89.9, tlen=251, kl=0.0105, act_lr=6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñç       | 13/53 [00:12<00:34,  1.16it/s, pg=-0.0221, ret=-0.000248, glen=85.2, tlen=246, kl=0.00937, act_lr=6e-7, ent=2.01]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 14/53 [00:12<00:33,  1.17it/s, pg=-0.0221, ret=-0.000248, glen=85.2, tlen=246, kl=0.00937, act_lr=6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñã       | 14/53 [00:13<00:33,  1.17it/s, pg=0.0771, ret=-0.000273, glen=93.4, tlen=255, kl=0.00769, act_lr=6e-7, ent=2.18] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/53 [00:13<00:32,  1.17it/s, pg=0.0771, ret=-0.000273, glen=93.4, tlen=255, kl=0.00769, act_lr=6e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 15/53 [00:13<00:32,  1.17it/s, pg=0.0477, ret=-0.00131, glen=94.5, tlen=255, kl=0.0108, act_lr=6e-7, ent=1.94]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 16/53 [00:13<00:31,  1.17it/s, pg=0.0477, ret=-0.00131, glen=94.5, tlen=255, kl=0.0108, act_lr=6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 16/53 [00:14<00:31,  1.17it/s, pg=-0.03, ret=0.000587, glen=89, tlen=250, kl=0.00987, act_lr=6e-7, ent=1.94]  Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 17/53 [00:14<00:30,  1.17it/s, pg=-0.03, ret=0.000587, glen=89, tlen=250, kl=0.00987, act_lr=6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 17/53 [00:15<00:30,  1.17it/s, pg=-0.13, ret=0.000186, glen=79.6, tlen=241, kl=0.0131, act_lr=6e-7, ent=1.75]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 18/53 [00:15<00:30,  1.16it/s, pg=-0.13, ret=0.000186, glen=79.6, tlen=241, kl=0.0131, act_lr=6e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 18/53 [00:16<00:30,  1.16it/s, pg=0.0022, ret=4.61e-5, glen=80.3, tlen=241, kl=0.00966, act_lr=6e-7, ent=1.8]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 19/53 [00:16<00:29,  1.16it/s, pg=0.0022, ret=4.61e-5, glen=80.3, tlen=241, kl=0.00966, act_lr=6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 19/53 [00:17<00:29,  1.16it/s, pg=0.128, ret=-0.00171, glen=90.1, tlen=251, kl=0.00893, act_lr=6e-7, ent=1.97]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/53 [00:17<00:28,  1.16it/s, pg=0.128, ret=-0.00171, glen=90.1, tlen=251, kl=0.00893, act_lr=6e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/53 [00:18<00:28,  1.16it/s, pg=0.103, ret=-0.00117, glen=84.6, tlen=245, kl=0.00845, act_lr=6e-7, ent=1.85]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 21/53 [00:18<00:27,  1.17it/s, pg=0.103, ret=-0.00117, glen=84.6, tlen=245, kl=0.00845, act_lr=6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 21/53 [00:19<00:27,  1.17it/s, pg=0.0774, ret=-0.000897, glen=95.2, tlen=256, kl=0.00973, act_lr=6e-7, ent=2.14]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/53 [00:19<00:26,  1.17it/s, pg=0.0774, ret=-0.000897, glen=95.2, tlen=256, kl=0.00973, act_lr=6e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/53 [00:19<00:26,  1.17it/s, pg=-0.0546, ret=-0.000166, glen=86.3, tlen=247, kl=0.0102, act_lr=6e-7, ent=2.1] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/53 [00:19<00:25,  1.17it/s, pg=-0.0546, ret=-0.000166, glen=86.3, tlen=247, kl=0.0102, act_lr=6e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 23/53 [00:20<00:25,  1.17it/s, pg=-0.0825, ret=0.000931, glen=90.8, tlen=252, kl=0.0075, act_lr=6e-7, ent=1.94]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/53 [00:20<00:24,  1.17it/s, pg=-0.0825, ret=0.000931, glen=90.8, tlen=252, kl=0.0075, act_lr=6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/53 [00:21<00:24,  1.17it/s, pg=-0.255, ret=0.0019, glen=93.2, tlen=254, kl=0.00925, act_lr=6e-7, ent=2.18]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/53 [00:21<00:23,  1.17it/s, pg=-0.255, ret=0.0019, glen=93.2, tlen=254, kl=0.00925, act_lr=6e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 25/53 [00:22<00:23,  1.17it/s, pg=-0.01, ret=2.56e-5, glen=83.9, tlen=245, kl=0.0117, act_lr=6e-7, ent=1.79] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26/53 [00:22<00:23,  1.17it/s, pg=-0.01, ret=2.56e-5, glen=83.9, tlen=245, kl=0.0117, act_lr=6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26/53 [00:23<00:23,  1.17it/s, pg=0.205, ret=-0.00304, glen=331, tlen=492, kl=0.00745, act_lr=6e-7, ent=1.7]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/53 [00:23<00:24,  1.04it/s, pg=0.205, ret=-0.00304, glen=331, tlen=492, kl=0.00745, act_lr=6e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 27/53 [00:24<00:24,  1.04it/s, pg=0.0662, ret=-0.00206, glen=84.4, tlen=245, kl=0.00941, act_lr=6e-7, ent=1.91]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28/53 [00:24<00:23,  1.08it/s, pg=0.0662, ret=-0.00206, glen=84.4, tlen=245, kl=0.00941, act_lr=6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28/53 [00:25<00:23,  1.08it/s, pg=-0.0394, ret=0.000363, glen=104, tlen=265, kl=0.00778, act_lr=6e-7, ent=2.02]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 29/53 [00:25<00:21,  1.10it/s, pg=-0.0394, ret=0.000363, glen=104, tlen=265, kl=0.00778, act_lr=6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 29/53 [00:26<00:21,  1.10it/s, pg=-0.0961, ret=0.000931, glen=94.7, tlen=256, kl=0.00935, act_lr=6e-7, ent=2.05]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30/53 [00:26<00:20,  1.12it/s, pg=-0.0961, ret=0.000931, glen=94.7, tlen=256, kl=0.00935, act_lr=6e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30/53 [00:27<00:20,  1.12it/s, pg=-0.0607, ret=-0.000234, glen=89.4, tlen=250, kl=0.0126, act_lr=6e-7, ent=2]   Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 31/53 [00:27<00:19,  1.14it/s, pg=-0.0607, ret=-0.000234, glen=89.4, tlen=250, kl=0.0126, act_lr=6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 31/53 [00:27<00:19,  1.14it/s, pg=-0.0479, ret=0.00132, glen=80.4, tlen=242, kl=0.0122, act_lr=6e-7, ent=1.87]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 32/53 [00:27<00:18,  1.15it/s, pg=-0.0479, ret=0.00132, glen=80.4, tlen=242, kl=0.0122, act_lr=6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 32/53 [00:28<00:18,  1.15it/s, pg=-0.104, ret=0.000648, glen=82.7, tlen=244, kl=0.011, act_lr=6e-7, ent=1.84] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 33/53 [00:28<00:17,  1.15it/s, pg=-0.104, ret=0.000648, glen=82.7, tlen=244, kl=0.011, act_lr=6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 33/53 [00:29<00:17,  1.15it/s, pg=0.0466, ret=-0.00058, glen=95.3, tlen=256, kl=0.00957, act_lr=6e-7, ent=1.93]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34/53 [00:29<00:16,  1.16it/s, pg=0.0466, ret=-0.00058, glen=95.3, tlen=256, kl=0.00957, act_lr=6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34/53 [00:30<00:16,  1.16it/s, pg=-0.114, ret=0.000472, glen=88.5, tlen=250, kl=0.00856, act_lr=6e-7, ent=2.12]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 35/53 [00:30<00:15,  1.16it/s, pg=-0.114, ret=0.000472, glen=88.5, tlen=250, kl=0.00856, act_lr=6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 35/53 [00:31<00:15,  1.16it/s, pg=-0.0982, ret=-0.00061, glen=95.2, tlen=256, kl=0.00896, act_lr=6e-7, ent=1.99]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 36/53 [00:31<00:14,  1.17it/s, pg=-0.0982, ret=-0.00061, glen=95.2, tlen=256, kl=0.00896, act_lr=6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 36/53 [00:32<00:14,  1.17it/s, pg=0.0677, ret=-4.34e-6, glen=90.2, tlen=251, kl=0.01, act_lr=6e-7, ent=1.95]    Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 37/53 [00:32<00:13,  1.17it/s, pg=0.0677, ret=-4.34e-6, glen=90.2, tlen=251, kl=0.01, act_lr=6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 37/53 [00:33<00:13,  1.17it/s, pg=-0.0898, ret=-0.000266, glen=85.6, tlen=246, kl=0.00945, act_lr=6e-7, ent=1.82]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38/53 [00:33<00:12,  1.17it/s, pg=-0.0898, ret=-0.000266, glen=85.6, tlen=246, kl=0.00945, act_lr=6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38/53 [00:33<00:12,  1.17it/s, pg=0.123, ret=0.00383, glen=89, tlen=250, kl=0.00752, act_lr=6e-7, ent=2.07]      Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 39/53 [00:33<00:11,  1.17it/s, pg=0.123, ret=0.00383, glen=89, tlen=250, kl=0.00752, act_lr=6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 39/53 [00:34<00:11,  1.17it/s, pg=-0.0582, ret=0.000548, glen=92.3, tlen=253, kl=0.00914, act_lr=6e-7, ent=1.99]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 40/53 [00:34<00:11,  1.17it/s, pg=-0.0582, ret=0.000548, glen=92.3, tlen=253, kl=0.00914, act_lr=6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 40/53 [00:35<00:11,  1.17it/s, pg=-0.0721, ret=0.00029, glen=101, tlen=262, kl=0.00774, act_lr=6e-7, ent=2.07]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 41/53 [00:35<00:10,  1.17it/s, pg=-0.0721, ret=0.00029, glen=101, tlen=262, kl=0.00774, act_lr=6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 41/53 [00:36<00:10,  1.17it/s, pg=-0.0949, ret=0.000679, glen=81.1, tlen=242, kl=0.0104, act_lr=6e-7, ent=1.8]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 42/53 [00:36<00:09,  1.17it/s, pg=-0.0949, ret=0.000679, glen=81.1, tlen=242, kl=0.0104, act_lr=6e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 42/53 [00:37<00:09,  1.17it/s, pg=-0.094, ret=0.000956, glen=95.8, tlen=257, kl=0.00854, act_lr=6e-7, ent=2.06]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 43/53 [00:37<00:08,  1.17it/s, pg=-0.094, ret=0.000956, glen=95.8, tlen=257, kl=0.00854, act_lr=6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 43/53 [00:38<00:08,  1.17it/s, pg=0.123, ret=0.000136, glen=105, tlen=266, kl=0.00779, act_lr=6e-7, ent=1.86]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 44/53 [00:38<00:07,  1.17it/s, pg=0.123, ret=0.000136, glen=105, tlen=266, kl=0.00779, act_lr=6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 44/53 [00:39<00:07,  1.17it/s, pg=-0.0763, ret=0.00131, glen=96.9, tlen=258, kl=0.00887, act_lr=6e-7, ent=2] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 45/53 [00:39<00:06,  1.17it/s, pg=-0.0763, ret=0.00131, glen=96.9, tlen=258, kl=0.00887, act_lr=6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 45/53 [00:39<00:06,  1.17it/s, pg=-0.0565, ret=0.000215, glen=87.5, tlen=248, kl=0.009, act_lr=6e-7, ent=1.87]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 46/53 [00:39<00:05,  1.17it/s, pg=-0.0565, ret=0.000215, glen=87.5, tlen=248, kl=0.009, act_lr=6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 46/53 [00:40<00:05,  1.17it/s, pg=0.0465, ret=-0.00043, glen=90, tlen=251, kl=0.00963, act_lr=6e-7, ent=1.83] Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 47/53 [00:40<00:05,  1.17it/s, pg=0.0465, ret=-0.00043, glen=90, tlen=251, kl=0.00963, act_lr=6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 47/53 [00:41<00:05,  1.17it/s, pg=-0.188, ret=0.000965, glen=91, tlen=252, kl=0.0158, act_lr=6e-7, ent=1.92] Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 48/53 [00:41<00:04,  1.17it/s, pg=-0.188, ret=0.000965, glen=91, tlen=252, kl=0.0158, act_lr=6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 48/53 [00:42<00:04,  1.17it/s, pg=-0.00555, ret=-7.88e-5, glen=92.6, tlen=253, kl=0.00856, act_lr=6e-7, ent=2.02]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 49/53 [00:42<00:03,  1.17it/s, pg=-0.00555, ret=-7.88e-5, glen=92.6, tlen=253, kl=0.00856, act_lr=6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 49/53 [00:43<00:03,  1.17it/s, pg=-0.0579, ret=0.000259, glen=83.5, tlen=245, kl=0.00872, act_lr=6e-7, ent=2.15] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 50/53 [00:43<00:02,  1.17it/s, pg=-0.0579, ret=0.000259, glen=83.5, tlen=245, kl=0.00872, act_lr=6e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 50/53 [00:44<00:02,  1.17it/s, pg=0.0855, ret=-0.00086, glen=86.1, tlen=248, kl=0.00903, act_lr=6e-7, ent=1.9]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 51/53 [00:44<00:01,  1.17it/s, pg=0.0855, ret=-0.00086, glen=86.1, tlen=248, kl=0.00903, act_lr=6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 51/53 [00:45<00:01,  1.17it/s, pg=0.0461, ret=0.000626, glen=91.1, tlen=252, kl=0.0116, act_lr=6e-7, ent=2.05]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:45<00:00,  1.17it/s, pg=0.0461, ret=0.000626, glen=91.1, tlen=252, kl=0.0116, act_lr=6e-7, ent=2.05]
2025-07-23 13:45:57.659 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 45.96s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:45<00:00,  1.17it/s, pg=-0.17, ret=0.00072, glen=104, tlen=265, kl=0.00838, act_lr=6.2e-7, ent=2.12]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 52/53 [00:45<00:00,  1.13it/s, pg=-0.17, ret=0.00072, glen=104, tlen=265, kl=0.00838, act_lr=6.2e-7, ent=2.12]
2025-07-23 13:45:58.482 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.76s
2025-07-23 13:46:01.052 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-23 13:46:01.364 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 49.78s
2025-07-23 13:46:01.369 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.015799748447706114, 'actor_lr': 6.00377378939914e-07, 'clip_ratio': 0.0, 'entropy': 1.9580889063061409, 'kl': 0.009389976285538584, 'response_length': 98.79866761981316, 'total_length': 259.7996469173791, 'return': -8.810766056105319e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [19:01<30:21, 227.63s/it]2025-07-23 13:46:01.382 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:47:40.892 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:47:41.075 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:47:41.076 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 99.69s
2025-07-23 13:47:42.849 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0093,avg_pass_at_n: 1.0000,avg_num_tokens: 85.7938,std_num_tokens: 117.7492,avg_correct_num_tokens: 81.8672,std_correct_num_tokens: 84.2732,avg_incorrect_num_tokens: 90.3100,std_incorrect_num_tokens: 146.9862
2025-07-23 13:47:43.277 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.20s
2025-07-23 13:47:44.679 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.40s
2025-07-23 13:48:10.781 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 204
2025-07-23 13:48:10.782 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.10s
2025-07-23 13:48:11.579 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.79s
2025-07-23 13:48:11.579 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0007366556572949276, avg_kl: 0.01165307736864277, avg_response_length: 88.21717331456203, avg_orm_score: 0.0, avg_custom_rewards: -0.0007366556572949276
2025-07-23 13:48:11.605 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter31_replay_buffer.jsonl
2025-07-23 13:48:12.929 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.33s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=-0.0273, ret=0.000143, glen=94.3, tlen=254, kl=0.00946, act_lr=6.2e-7, ent=2.23]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:48,  1.02it/s, pg=-0.0273, ret=0.000143, glen=94.3, tlen=254, kl=0.00946, act_lr=6.2e-7, ent=2.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:48,  1.02it/s, pg=-0.106, ret=-0.000288, glen=84.1, tlen=245, kl=0.0112, act_lr=6.2e-7, ent=1.96] Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.10it/s, pg=-0.106, ret=-0.000288, glen=84.1, tlen=245, kl=0.0112, act_lr=6.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.10it/s, pg=0.118, ret=-0.00114, glen=75.4, tlen=236, kl=0.0143, act_lr=6.2e-7, ent=1.93]  Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:43,  1.10it/s, pg=0.118, ret=-0.00114, glen=75.4, tlen=236, kl=0.0143, act_lr=6.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:43,  1.10it/s, pg=0.0757, ret=-0.00076, glen=81.2, tlen=241, kl=0.0134, act_lr=6.2e-7, ent=1.92]Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:41,  1.13it/s, pg=0.0757, ret=-0.00076, glen=81.2, tlen=241, kl=0.0134, act_lr=6.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:41,  1.13it/s, pg=-0.0712, ret=0.000318, glen=89.2, tlen=250, kl=0.0115, act_lr=6.2e-7, ent=1.92]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.14it/s, pg=-0.0712, ret=0.000318, glen=89.2, tlen=250, kl=0.0115, act_lr=6.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.14it/s, pg=0.00232, ret=-3.65e-5, glen=74.1, tlen=234, kl=0.011, act_lr=6.2e-7, ent=1.89] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.15it/s, pg=0.00232, ret=-3.65e-5, glen=74.1, tlen=234, kl=0.011, act_lr=6.2e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.15it/s, pg=0.0339, ret=0.000703, glen=105, tlen=265, kl=0.0101, act_lr=6.2e-7, ent=2.42] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.14it/s, pg=0.0339, ret=0.000703, glen=105, tlen=265, kl=0.0101, act_lr=6.2e-7, ent=2.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.14it/s, pg=-0.0131, ret=0.00134, glen=90.7, tlen=251, kl=0.0104, act_lr=6.2e-7, ent=2.17]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=-0.0131, ret=0.00134, glen=90.7, tlen=251, kl=0.0104, act_lr=6.2e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=-0.0365, ret=-0.000553, glen=79.7, tlen=240, kl=0.0126, act_lr=6.2e-7, ent=2.11]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.16it/s, pg=-0.0365, ret=-0.000553, glen=79.7, tlen=240, kl=0.0126, act_lr=6.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.16it/s, pg=0.0332, ret=-0.000296, glen=82.8, tlen=243, kl=0.0113, act_lr=6.2e-7, ent=1.88] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.16it/s, pg=0.0332, ret=-0.000296, glen=82.8, tlen=243, kl=0.0113, act_lr=6.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.16it/s, pg=0.0737, ret=-0.000393, glen=77.5, tlen=238, kl=0.0107, act_lr=6.2e-7, ent=1.91]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.14it/s, pg=0.0737, ret=-0.000393, glen=77.5, tlen=238, kl=0.0107, act_lr=6.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.14it/s, pg=-0.13, ret=0.00164, glen=85.1, tlen=245, kl=0.00977, act_lr=6.2e-7, ent=1.95]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.15it/s, pg=-0.13, ret=0.00164, glen=85.1, tlen=245, kl=0.00977, act_lr=6.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.15it/s, pg=-0.0166, ret=0.000123, glen=80.8, tlen=240, kl=0.0116, act_lr=6.2e-7, ent=1.94]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.16it/s, pg=-0.0166, ret=0.000123, glen=80.8, tlen=240, kl=0.0116, act_lr=6.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.16it/s, pg=-0.0573, ret=-0.000286, glen=79.9, tlen=240, kl=0.0158, act_lr=6.2e-7, ent=2.03]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:31,  1.16it/s, pg=-0.0573, ret=-0.000286, glen=79.9, tlen=240, kl=0.0158, act_lr=6.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:31,  1.16it/s, pg=-0.00775, ret=-0.000576, glen=74.3, tlen=235, kl=0.0112, act_lr=6.2e-7, ent=1.91]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.14it/s, pg=-0.00775, ret=-0.000576, glen=74.3, tlen=235, kl=0.0112, act_lr=6.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.14it/s, pg=0.114, ret=-0.000927, glen=74.5, tlen=235, kl=0.014, act_lr=6.2e-7, ent=1.88]    Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:30,  1.15it/s, pg=0.114, ret=-0.000927, glen=74.5, tlen=235, kl=0.014, act_lr=6.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.15it/s, pg=0.303, ret=-0.00209, glen=178, tlen=338, kl=0.00978, act_lr=6.2e-7, ent=2.85]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.14it/s, pg=0.303, ret=-0.00209, glen=178, tlen=338, kl=0.00978, act_lr=6.2e-7, ent=2.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.14it/s, pg=-0.131, ret=0.000624, glen=87.4, tlen=248, kl=0.0116, act_lr=6.2e-7, ent=2.03]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.15it/s, pg=-0.131, ret=0.000624, glen=87.4, tlen=248, kl=0.0116, act_lr=6.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.15it/s, pg=-0.138, ret=0.000501, glen=91.8, tlen=252, kl=0.0123, act_lr=6.2e-7, ent=2.06]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=-0.138, ret=0.000501, glen=91.8, tlen=252, kl=0.0123, act_lr=6.2e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=0.0671, ret=-0.000296, glen=83.7, tlen=244, kl=0.0115, act_lr=6.2e-7, ent=2.2]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.16it/s, pg=0.0671, ret=-0.000296, glen=83.7, tlen=244, kl=0.0115, act_lr=6.2e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.16it/s, pg=0.0486, ret=-0.00067, glen=77.5, tlen=238, kl=0.012, act_lr=6.2e-7, ent=1.9]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.0486, ret=-0.00067, glen=77.5, tlen=238, kl=0.012, act_lr=6.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=0.19, ret=-0.000413, glen=88.7, tlen=249, kl=0.0128, act_lr=6.2e-7, ent=2.18]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=0.19, ret=-0.000413, glen=88.7, tlen=249, kl=0.0128, act_lr=6.2e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:24,  1.17it/s, pg=-0.0083, ret=0.000234, glen=87.2, tlen=247, kl=0.0108, act_lr=6.2e-7, ent=2.03]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.0083, ret=0.000234, glen=87.2, tlen=247, kl=0.0108, act_lr=6.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.0961, ret=0.00147, glen=76.8, tlen=238, kl=0.0133, act_lr=6.2e-7, ent=2.05] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.17it/s, pg=-0.0961, ret=0.00147, glen=76.8, tlen=238, kl=0.0133, act_lr=6.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=-0.0428, ret=0.00112, glen=79.9, tlen=240, kl=0.0133, act_lr=6.2e-7, ent=2.05]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.17it/s, pg=-0.0428, ret=0.00112, glen=79.9, tlen=240, kl=0.0133, act_lr=6.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.17it/s, pg=0.117, ret=-0.00144, glen=127, tlen=287, kl=0.0104, act_lr=6.2e-7, ent=1.67]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.14it/s, pg=0.117, ret=-0.00144, glen=127, tlen=287, kl=0.0104, act_lr=6.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:24<00:21,  1.14it/s, pg=-0.0984, ret=0.00109, glen=95.2, tlen=256, kl=0.0107, act_lr=6.2e-7, ent=2.3]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:25,  1.05s/it, pg=-0.0984, ret=0.00109, glen=95.2, tlen=256, kl=0.0107, act_lr=6.2e-7, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:25,  1.05s/it, pg=0.0215, ret=-7.52e-5, glen=79.3, tlen=240, kl=0.0127, act_lr=6.2e-7, ent=1.93]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:22,  1.01it/s, pg=0.0215, ret=-7.52e-5, glen=79.3, tlen=240, kl=0.0127, act_lr=6.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:22,  1.01it/s, pg=0.305, ret=-0.000497, glen=101, tlen=262, kl=0.0114, act_lr=6.2e-7, ent=2.21] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.05it/s, pg=0.305, ret=-0.000497, glen=101, tlen=262, kl=0.0114, act_lr=6.2e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.05it/s, pg=-0.0479, ret=0.000429, glen=91.8, tlen=252, kl=0.0094, act_lr=6.2e-7, ent=2.11]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:19,  1.08it/s, pg=-0.0479, ret=0.000429, glen=91.8, tlen=252, kl=0.0094, act_lr=6.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:19,  1.08it/s, pg=-0.0126, ret=-1.73e-5, glen=80.8, tlen=241, kl=0.0124, act_lr=6.2e-7, ent=1.92]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:18,  1.11it/s, pg=-0.0126, ret=-1.73e-5, glen=80.8, tlen=241, kl=0.0124, act_lr=6.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:18,  1.11it/s, pg=-0.107, ret=0.000719, glen=84.8, tlen=245, kl=0.0113, act_lr=6.2e-7, ent=2.08] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.13it/s, pg=-0.107, ret=0.000719, glen=84.8, tlen=245, kl=0.0113, act_lr=6.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:29<00:16,  1.13it/s, pg=-0.0215, ret=0.000345, glen=79.9, tlen=240, kl=0.0128, act_lr=6.2e-7, ent=1.94]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.14it/s, pg=-0.0215, ret=0.000345, glen=79.9, tlen=240, kl=0.0128, act_lr=6.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:30<00:15,  1.14it/s, pg=-0.227, ret=0.00082, glen=81.8, tlen=242, kl=0.0116, act_lr=6.2e-7, ent=1.97]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.15it/s, pg=-0.227, ret=0.00082, glen=81.8, tlen=242, kl=0.0116, act_lr=6.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.15it/s, pg=0.142, ret=-0.000668, glen=81.8, tlen=242, kl=0.0119, act_lr=6.2e-7, ent=1.87]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.16it/s, pg=0.142, ret=-0.000668, glen=81.8, tlen=242, kl=0.0119, act_lr=6.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=-0.213, ret=-0.000705, glen=93.3, tlen=253, kl=0.011, act_lr=6.2e-7, ent=2.17]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.16it/s, pg=-0.213, ret=-0.000705, glen=93.3, tlen=253, kl=0.011, act_lr=6.2e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.16it/s, pg=-0.0853, ret=-0.00234, glen=73.7, tlen=234, kl=0.0133, act_lr=6.2e-7, ent=2.01]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:12,  1.17it/s, pg=-0.0853, ret=-0.00234, glen=73.7, tlen=234, kl=0.0133, act_lr=6.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:12,  1.17it/s, pg=0.00647, ret=2.42e-5, glen=79.4, tlen=239, kl=0.0125, act_lr=6.2e-7, ent=2.11] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.00647, ret=2.42e-5, glen=79.4, tlen=239, kl=0.0125, act_lr=6.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.17it/s, pg=-0.129, ret=6.37e-5, glen=74.7, tlen=235, kl=0.0126, act_lr=6.2e-7, ent=1.78] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-0.129, ret=6.37e-5, glen=74.7, tlen=235, kl=0.0126, act_lr=6.2e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:35<00:10,  1.17it/s, pg=-0.132, ret=0.000624, glen=97.7, tlen=258, kl=0.00972, act_lr=6.2e-7, ent=2.23]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.132, ret=0.000624, glen=97.7, tlen=258, kl=0.00972, act_lr=6.2e-7, ent=2.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:36<00:09,  1.17it/s, pg=0.0503, ret=5.78e-5, glen=77.6, tlen=237, kl=0.0126, act_lr=6.2e-7, ent=1.9]   Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=0.0503, ret=5.78e-5, glen=77.6, tlen=237, kl=0.0126, act_lr=6.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=-0.135, ret=0.000863, glen=88.6, tlen=249, kl=0.012, act_lr=6.2e-7, ent=1.98]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.17it/s, pg=-0.135, ret=0.000863, glen=88.6, tlen=249, kl=0.012, act_lr=6.2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.17it/s, pg=-0.102, ret=-0.000496, glen=88.1, tlen=248, kl=0.0108, act_lr=6.2e-7, ent=1.87]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.17it/s, pg=-0.102, ret=-0.000496, glen=88.1, tlen=248, kl=0.0108, act_lr=6.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.17it/s, pg=-0.122, ret=-0.000283, glen=88.4, tlen=249, kl=0.0126, act_lr=6.2e-7, ent=2.08]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=-0.122, ret=-0.000283, glen=88.4, tlen=249, kl=0.0126, act_lr=6.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.17it/s, pg=0.174, ret=-0.000132, glen=108, tlen=269, kl=0.00855, act_lr=6.2e-7, ent=1.94] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.17it/s, pg=0.174, ret=-0.000132, glen=108, tlen=269, kl=0.00855, act_lr=6.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.17it/s, pg=-0.0636, ret=-0.000545, glen=86.4, tlen=247, kl=0.0115, act_lr=6.2e-7, ent=2.07]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=-0.0636, ret=-0.000545, glen=86.4, tlen=247, kl=0.0115, act_lr=6.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:41<00:04,  1.17it/s, pg=0.0581, ret=-0.00061, glen=109, tlen=269, kl=0.0107, act_lr=6.2e-7, ent=2.4]    Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.17it/s, pg=0.0581, ret=-0.00061, glen=109, tlen=269, kl=0.0107, act_lr=6.2e-7, ent=2.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:42<00:03,  1.17it/s, pg=-0.0974, ret=-0.000139, glen=92, tlen=252, kl=0.0122, act_lr=6.2e-7, ent=2.14]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=-0.0974, ret=-0.000139, glen=92, tlen=252, kl=0.0122, act_lr=6.2e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=0.0214, ret=-0.000117, glen=71.8, tlen=232, kl=0.0128, act_lr=6.2e-7, ent=1.83]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=0.0214, ret=-0.000117, glen=71.8, tlen=232, kl=0.0128, act_lr=6.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=0.0202, ret=-0.000586, glen=96.7, tlen=257, kl=0.00977, act_lr=6.2e-7, ent=1.94]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=0.0202, ret=-0.000586, glen=96.7, tlen=257, kl=0.00977, act_lr=6.2e-7, ent=1.94]
2025-07-23 13:48:57.737 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.65s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=-0.0549, ret=0.000103, glen=91, tlen=251, kl=0.0114, act_lr=6.4e-7, ent=2.29]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.12it/s, pg=-0.0549, ret=0.000103, glen=91, tlen=251, kl=0.0114, act_lr=6.4e-7, ent=2.29]
2025-07-23 13:48:58.565 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-23 13:49:01.162 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-23 13:49:01.486 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.52s
2025-07-23 13:49:01.492 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.010891353382783778, 'actor_lr': 6.20392137418593e-07, 'clip_ratio': 0.0, 'entropy': 2.04196186392915, 'kl': 0.01165307736864277, 'response_length': 88.21717355765549, 'total_length': 248.42809460209864, 'return': -7.920479915543076e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [22:01<24:40, 211.47s/it]2025-07-23 13:49:01.524 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:50:52.791 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:50:52.970 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:50:52.971 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 111.45s
2025-07-23 13:50:54.956 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0104,avg_pass_at_n: 1.0000,avg_num_tokens: 86.6428,std_num_tokens: 113.5144,avg_correct_num_tokens: 83.0392,std_correct_num_tokens: 75.2328,avg_incorrect_num_tokens: 91.0765,std_incorrect_num_tokens: 147.4437
2025-07-23 13:50:55.407 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.44s
2025-07-23 13:50:56.853 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.45s
2025-07-23 13:51:23.370 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 205
2025-07-23 13:51:23.370 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.51s
2025-07-23 13:51:24.229 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.85s
2025-07-23 13:51:24.230 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00027704650263597324, avg_kl: 0.012585895817454269, avg_response_length: 88.67714078949719, avg_orm_score: 0.0, avg_custom_rewards: -0.00027704650263597324
2025-07-23 13:51:24.269 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter32_replay_buffer.jsonl
2025-07-23 13:51:25.621 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.35s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:01<?, ?it/s, pg=0.179, ret=-0.00139, glen=87, tlen=247, kl=0.013, act_lr=6.4e-7, ent=2.08]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:51,  1.00s/it, pg=0.179, ret=-0.00139, glen=87, tlen=247, kl=0.013, act_lr=6.4e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:51,  1.00s/it, pg=-0.025, ret=0.000437, glen=87.6, tlen=248, kl=0.0112, act_lr=6.4e-7, ent=1.99]Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:45,  1.09it/s, pg=-0.025, ret=0.000437, glen=87.6, tlen=248, kl=0.0112, act_lr=6.4e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:45,  1.09it/s, pg=-0.00378, ret=0.000528, glen=89.8, tlen=250, kl=0.0116, act_lr=6.4e-7, ent=2.26]Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:45,  1.09it/s, pg=-0.00378, ret=0.000528, glen=89.8, tlen=250, kl=0.0116, act_lr=6.4e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:45,  1.09it/s, pg=0.0509, ret=8.14e-6, glen=94.4, tlen=255, kl=0.0115, act_lr=6.4e-7, ent=1.96]   Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:42,  1.12it/s, pg=0.0509, ret=8.14e-6, glen=94.4, tlen=255, kl=0.0115, act_lr=6.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:42,  1.12it/s, pg=0.0703, ret=-0.000684, glen=156, tlen=317, kl=0.00803, act_lr=6.4e-7, ent=1.68]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:43,  1.09it/s, pg=0.0703, ret=-0.000684, glen=156, tlen=317, kl=0.00803, act_lr=6.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:43,  1.09it/s, pg=-0.18, ret=0.000615, glen=82, tlen=243, kl=0.0116, act_lr=6.4e-7, ent=1.93]    Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:41,  1.11it/s, pg=-0.18, ret=0.000615, glen=82, tlen=243, kl=0.0116, act_lr=6.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:41,  1.11it/s, pg=0.037, ret=-0.0004, glen=80.3, tlen=240, kl=0.0108, act_lr=6.4e-7, ent=2]  Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:39,  1.13it/s, pg=0.037, ret=-0.0004, glen=80.3, tlen=240, kl=0.0108, act_lr=6.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:07<00:39,  1.13it/s, pg=-0.149, ret=0.000643, glen=86.8, tlen=247, kl=0.0118, act_lr=6.4e-7, ent=2.05]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:38,  1.15it/s, pg=-0.149, ret=0.000643, glen=86.8, tlen=247, kl=0.0118, act_lr=6.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:08<00:38,  1.15it/s, pg=0.00961, ret=-0.000296, glen=72.6, tlen=232, kl=0.0169, act_lr=6.4e-7, ent=1.89]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.13it/s, pg=0.00961, ret=-0.000296, glen=72.6, tlen=232, kl=0.0169, act_lr=6.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.13it/s, pg=-0.0404, ret=-0.000507, glen=90.9, tlen=251, kl=0.0106, act_lr=6.4e-7, ent=2.03]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:36,  1.14it/s, pg=-0.0404, ret=-0.000507, glen=90.9, tlen=251, kl=0.0106, act_lr=6.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:36,  1.14it/s, pg=0.156, ret=-0.00134, glen=83.4, tlen=243, kl=0.0147, act_lr=6.4e-7, ent=1.95]   Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:36,  1.14it/s, pg=0.156, ret=-0.00134, glen=83.4, tlen=243, kl=0.0147, act_lr=6.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:36,  1.14it/s, pg=0.278, ret=-0.00129, glen=99.7, tlen=260, kl=0.0117, act_lr=6.4e-7, ent=2.26]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.15it/s, pg=0.278, ret=-0.00129, glen=99.7, tlen=260, kl=0.0117, act_lr=6.4e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.15it/s, pg=-0.215, ret=-8.36e-5, glen=87.8, tlen=248, kl=0.0122, act_lr=6.4e-7, ent=2.3]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.15it/s, pg=-0.215, ret=-8.36e-5, glen=87.8, tlen=248, kl=0.0122, act_lr=6.4e-7, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.15it/s, pg=-0.0449, ret=0.000954, glen=94.1, tlen=255, kl=0.0103, act_lr=6.4e-7, ent=2.3]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.16it/s, pg=-0.0449, ret=0.000954, glen=94.1, tlen=255, kl=0.0103, act_lr=6.4e-7, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:13<00:32,  1.16it/s, pg=-0.0918, ret=0.000716, glen=77.8, tlen=238, kl=0.0122, act_lr=6.4e-7, ent=1.95]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.16it/s, pg=-0.0918, ret=0.000716, glen=77.8, tlen=238, kl=0.0122, act_lr=6.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:14<00:31,  1.16it/s, pg=0.148, ret=-0.00839, glen=83.7, tlen=244, kl=0.0148, act_lr=6.4e-7, ent=1.84]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=0.148, ret=-0.00839, glen=83.7, tlen=244, kl=0.0148, act_lr=6.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=-0.0386, ret=0.000714, glen=82.3, tlen=243, kl=0.0104, act_lr=6.4e-7, ent=1.97]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:29,  1.17it/s, pg=-0.0386, ret=0.000714, glen=82.3, tlen=243, kl=0.0104, act_lr=6.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:29,  1.17it/s, pg=-0.117, ret=0.00159, glen=87.5, tlen=248, kl=0.0119, act_lr=6.4e-7, ent=2.04]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:29,  1.17it/s, pg=-0.117, ret=0.00159, glen=87.5, tlen=248, kl=0.0119, act_lr=6.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:29,  1.17it/s, pg=-0.242, ret=0.00191, glen=84, tlen=245, kl=0.0119, act_lr=6.4e-7, ent=2.04]  Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.17it/s, pg=-0.242, ret=0.00191, glen=84, tlen=245, kl=0.0119, act_lr=6.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.17it/s, pg=0.146, ret=-0.00111, glen=83.9, tlen=244, kl=0.0173, act_lr=6.4e-7, ent=1.9]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.17it/s, pg=0.146, ret=-0.00111, glen=83.9, tlen=244, kl=0.0173, act_lr=6.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.17it/s, pg=-0.131, ret=0.000995, glen=79.3, tlen=239, kl=0.0129, act_lr=6.4e-7, ent=1.79]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:27,  1.15it/s, pg=-0.131, ret=0.000995, glen=79.3, tlen=239, kl=0.0129, act_lr=6.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:19<00:27,  1.15it/s, pg=-0.132, ret=0.00013, glen=91.8, tlen=252, kl=0.0126, act_lr=6.4e-7, ent=1.95] Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:26,  1.15it/s, pg=-0.132, ret=0.00013, glen=91.8, tlen=252, kl=0.0126, act_lr=6.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:20<00:26,  1.15it/s, pg=0.119, ret=-0.00128, glen=75.2, tlen=235, kl=0.0138, act_lr=6.4e-7, ent=2.1] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:25,  1.16it/s, pg=0.119, ret=-0.00128, glen=75.2, tlen=235, kl=0.0138, act_lr=6.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:21<00:25,  1.16it/s, pg=-0.137, ret=0.00125, glen=85.1, tlen=245, kl=0.0128, act_lr=6.4e-7, ent=2.2]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:24,  1.15it/s, pg=-0.137, ret=0.00125, glen=85.1, tlen=245, kl=0.0128, act_lr=6.4e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:24,  1.15it/s, pg=-0.0878, ret=6.86e-5, glen=79.2, tlen=240, kl=0.0127, act_lr=6.4e-7, ent=1.83]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:23,  1.13it/s, pg=-0.0878, ret=6.86e-5, glen=79.2, tlen=240, kl=0.0127, act_lr=6.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:23,  1.13it/s, pg=-0.0961, ret=-0.000215, glen=78.4, tlen=239, kl=0.0131, act_lr=6.4e-7, ent=1.77]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.14it/s, pg=-0.0961, ret=-0.000215, glen=78.4, tlen=239, kl=0.0131, act_lr=6.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:23<00:22,  1.14it/s, pg=-0.048, ret=-3.82e-5, glen=90.1, tlen=251, kl=0.0117, act_lr=6.4e-7, ent=2.1]   Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:23<00:23,  1.05it/s, pg=-0.048, ret=-3.82e-5, glen=90.1, tlen=251, kl=0.0117, act_lr=6.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:23,  1.05it/s, pg=0.403, ret=-0.000721, glen=134, tlen=294, kl=0.00771, act_lr=6.4e-7, ent=2.66]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:22,  1.07it/s, pg=0.403, ret=-0.000721, glen=134, tlen=294, kl=0.00771, act_lr=6.4e-7, ent=2.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:22,  1.07it/s, pg=-0.0719, ret=-0.000791, glen=91.8, tlen=252, kl=0.0138, act_lr=6.4e-7, ent=2.21]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:20,  1.10it/s, pg=-0.0719, ret=-0.000791, glen=91.8, tlen=252, kl=0.0138, act_lr=6.4e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:20,  1.10it/s, pg=-0.0266, ret=0.00171, glen=133, tlen=294, kl=0.0113, act_lr=6.4e-7, ent=2.43]   Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:19,  1.11it/s, pg=-0.0266, ret=0.00171, glen=133, tlen=294, kl=0.0113, act_lr=6.4e-7, ent=2.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:27<00:19,  1.11it/s, pg=0.15, ret=-0.00145, glen=74.1, tlen=235, kl=0.0166, act_lr=6.4e-7, ent=1.89] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:18,  1.13it/s, pg=0.15, ret=-0.00145, glen=74.1, tlen=235, kl=0.0166, act_lr=6.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:28<00:18,  1.13it/s, pg=-0.261, ret=0.00164, glen=85, tlen=246, kl=0.0154, act_lr=6.4e-7, ent=2]    Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.14it/s, pg=-0.261, ret=0.00164, glen=85, tlen=246, kl=0.0154, act_lr=6.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:29<00:17,  1.14it/s, pg=-0.0706, ret=0.000519, glen=78.9, tlen=239, kl=0.0131, act_lr=6.4e-7, ent=1.91]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.15it/s, pg=-0.0706, ret=0.000519, glen=78.9, tlen=239, kl=0.0131, act_lr=6.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.15it/s, pg=0.129, ret=-0.000276, glen=78.5, tlen=239, kl=0.0112, act_lr=6.4e-7, ent=1.81] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:29<00:15,  1.16it/s, pg=0.129, ret=-0.000276, glen=78.5, tlen=239, kl=0.0112, act_lr=6.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.16it/s, pg=-0.0206, ret=-0.000438, glen=81.7, tlen=242, kl=0.014, act_lr=6.4e-7, ent=2]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.17it/s, pg=-0.0206, ret=-0.000438, glen=81.7, tlen=242, kl=0.014, act_lr=6.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.17it/s, pg=0.0234, ret=-0.000167, glen=103, tlen=264, kl=0.0106, act_lr=6.4e-7, ent=2.39]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.16it/s, pg=0.0234, ret=-0.000167, glen=103, tlen=264, kl=0.0106, act_lr=6.4e-7, ent=2.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.16it/s, pg=0.0887, ret=-0.00111, glen=80.2, tlen=241, kl=0.0124, act_lr=6.4e-7, ent=2.05]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.17it/s, pg=0.0887, ret=-0.00111, glen=80.2, tlen=241, kl=0.0124, act_lr=6.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:33<00:12,  1.17it/s, pg=0.183, ret=0.000229, glen=92.4, tlen=253, kl=0.00963, act_lr=6.4e-7, ent=2.2] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:11,  1.17it/s, pg=0.183, ret=0.000229, glen=92.4, tlen=253, kl=0.00963, act_lr=6.4e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:34<00:11,  1.17it/s, pg=-0.098, ret=-0.00058, glen=91, tlen=251, kl=0.0141, act_lr=6.4e-7, ent=2.42] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.17it/s, pg=-0.098, ret=-0.00058, glen=91, tlen=251, kl=0.0141, act_lr=6.4e-7, ent=2.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:35<00:11,  1.17it/s, pg=-0.0603, ret=0.00106, glen=78.1, tlen=238, kl=0.0135, act_lr=6.4e-7, ent=1.81]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.17it/s, pg=-0.0603, ret=0.00106, glen=78.1, tlen=238, kl=0.0135, act_lr=6.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.17it/s, pg=-0.00757, ret=0.000847, glen=88.5, tlen=249, kl=0.0123, act_lr=6.4e-7, ent=2.03]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:35<00:09,  1.17it/s, pg=-0.00757, ret=0.000847, glen=88.5, tlen=249, kl=0.0123, act_lr=6.4e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.17it/s, pg=0.0988, ret=-0.000607, glen=79.6, tlen=240, kl=0.0156, act_lr=6.4e-7, ent=1.89] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:36<00:08,  1.17it/s, pg=0.0988, ret=-0.000607, glen=79.6, tlen=240, kl=0.0156, act_lr=6.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.17it/s, pg=-0.0473, ret=0.000589, glen=85.4, tlen=246, kl=0.0181, act_lr=6.4e-7, ent=2]   Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.17it/s, pg=-0.0473, ret=0.000589, glen=85.4, tlen=246, kl=0.0181, act_lr=6.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.17it/s, pg=0.127, ret=-0.00109, glen=93.8, tlen=254, kl=0.0103, act_lr=6.4e-7, ent=2.29]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.18it/s, pg=0.127, ret=-0.00109, glen=93.8, tlen=254, kl=0.0103, act_lr=6.4e-7, ent=2.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:39<00:06,  1.18it/s, pg=-0.105, ret=0.00105, glen=84.9, tlen=245, kl=0.0116, act_lr=6.4e-7, ent=1.92]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.18it/s, pg=-0.105, ret=0.00105, glen=84.9, tlen=245, kl=0.0116, act_lr=6.4e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:40<00:05,  1.18it/s, pg=0.0516, ret=-0.00062, glen=84.9, tlen=245, kl=0.0136, act_lr=6.4e-7, ent=2.11]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.15it/s, pg=0.0516, ret=-0.00062, glen=84.9, tlen=245, kl=0.0136, act_lr=6.4e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:41<00:05,  1.15it/s, pg=0.169, ret=-0.000611, glen=81.2, tlen=241, kl=0.0122, act_lr=6.4e-7, ent=2.09]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.15it/s, pg=0.169, ret=-0.000611, glen=81.2, tlen=241, kl=0.0122, act_lr=6.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.15it/s, pg=0.0962, ret=-0.000279, glen=81.9, tlen=242, kl=0.0124, act_lr=6.4e-7, ent=2.1]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:41<00:03,  1.16it/s, pg=0.0962, ret=-0.000279, glen=81.9, tlen=242, kl=0.0124, act_lr=6.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.16it/s, pg=-0.262, ret=0.00127, glen=97.6, tlen=257, kl=0.0114, act_lr=6.4e-7, ent=2.49] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.16it/s, pg=-0.262, ret=0.00127, glen=97.6, tlen=257, kl=0.0114, act_lr=6.4e-7, ent=2.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.16it/s, pg=-0.0203, ret=-0.000242, glen=81.1, tlen=241, kl=0.0129, act_lr=6.4e-7, ent=1.77]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.17it/s, pg=-0.0203, ret=-0.000242, glen=81.1, tlen=241, kl=0.0129, act_lr=6.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.17it/s, pg=-0.0437, ret=0.000641, glen=96.1, tlen=256, kl=0.0111, act_lr=6.4e-7, ent=2.1]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=-0.0437, ret=0.000641, glen=96.1, tlen=256, kl=0.0111, act_lr=6.4e-7, ent=2.1]
2025-07-23 13:52:11.195 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 45.41s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.17it/s, pg=-0.16, ret=-6.52e-5, glen=78.6, tlen=239, kl=0.0151, act_lr=6.6e-7, ent=2.08] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.12it/s, pg=-0.16, ret=-6.52e-5, glen=78.6, tlen=239, kl=0.0151, act_lr=6.6e-7, ent=2.08]
2025-07-23 13:52:12.065 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-23 13:52:14.626 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-23 13:52:14.932 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 49.27s
2025-07-23 13:52:14.938 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0061746744009164665, 'actor_lr': 6.403846118001428e-07, 'clip_ratio': 0.0, 'entropy': 2.0545765046889963, 'kl': 0.012569280771108774, 'response_length': 88.59063515296349, 'total_length': 248.88959855299728, 'return': -0.00011466743469030077, 'policy_update_steps': 1.0}
Episode [3/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [25:14<20:33, 205.58s/it]2025-07-23 13:52:14.969 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:53:40.982 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:53:41.167 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:53:41.168 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 86.20s
2025-07-23 13:53:43.226 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 84.0886,std_num_tokens: 94.4145,avg_correct_num_tokens: 82.5119,std_correct_num_tokens: 86.5352,avg_incorrect_num_tokens: 85.8290,std_incorrect_num_tokens: 102.3816
2025-07-23 13:53:43.633 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.47s
2025-07-23 13:53:44.861 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.22s
2025-07-23 13:54:10.870 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 202
2025-07-23 13:54:10.870 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.01s
2025-07-23 13:54:11.665 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.79s
2025-07-23 13:54:11.666 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 2.5837179642207553e-05, avg_kl: 0.013322886854115099, avg_response_length: 85.12436929079566, avg_orm_score: 0.0, avg_custom_rewards: 2.5837179642207553e-05
2025-07-23 13:54:11.697 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter33_replay_buffer.jsonl
2025-07-23 13:54:13.168 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.47s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=0.0903, ret=-0.000885, glen=96.5, tlen=257, kl=0.0135, act_lr=6.6e-7, ent=2.1]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:49,  1.01it/s, pg=0.0903, ret=-0.000885, glen=96.5, tlen=257, kl=0.0135, act_lr=6.6e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:49,  1.01it/s, pg=-0.05, ret=-0.000181, glen=80.7, tlen=241, kl=0.0145, act_lr=6.6e-7, ent=2.03]Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.10it/s, pg=-0.05, ret=-0.000181, glen=80.7, tlen=241, kl=0.0145, act_lr=6.6e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.10it/s, pg=-0.0875, ret=0.00126, glen=92.5, tlen=252, kl=0.0125, act_lr=6.6e-7, ent=2.12]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:43,  1.10it/s, pg=-0.0875, ret=0.00126, glen=92.5, tlen=252, kl=0.0125, act_lr=6.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:43,  1.10it/s, pg=0.269, ret=-0.00205, glen=84, tlen=244, kl=0.0166, act_lr=6.6e-7, ent=2.06]   Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:41,  1.13it/s, pg=0.269, ret=-0.00205, glen=84, tlen=244, kl=0.0166, act_lr=6.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:41,  1.13it/s, pg=0.31, ret=-0.00274, glen=109, tlen=269, kl=0.0116, act_lr=6.6e-7, ent=2.37]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.13it/s, pg=0.31, ret=-0.00274, glen=109, tlen=269, kl=0.0116, act_lr=6.6e-7, ent=2.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.13it/s, pg=-0.164, ret=0.000931, glen=82.5, tlen=243, kl=0.0122, act_lr=6.6e-7, ent=2.07]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.13it/s, pg=-0.164, ret=0.000931, glen=82.5, tlen=243, kl=0.0122, act_lr=6.6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.13it/s, pg=-0.189, ret=0.00188, glen=82.3, tlen=242, kl=0.0136, act_lr=6.6e-7, ent=2.02] Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.14it/s, pg=-0.189, ret=0.00188, glen=82.3, tlen=242, kl=0.0136, act_lr=6.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.14it/s, pg=-0.0217, ret=-0.000216, glen=78, tlen=238, kl=0.0131, act_lr=6.6e-7, ent=2.01]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:38,  1.12it/s, pg=-0.0217, ret=-0.000216, glen=78, tlen=238, kl=0.0131, act_lr=6.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:08<00:38,  1.12it/s, pg=0.0737, ret=-0.000221, glen=82.7, tlen=243, kl=0.013, act_lr=6.6e-7, ent=1.96]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:37,  1.13it/s, pg=0.0737, ret=-0.000221, glen=82.7, tlen=243, kl=0.013, act_lr=6.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:37,  1.13it/s, pg=0.169, ret=-0.00147, glen=95.2, tlen=256, kl=0.0117, act_lr=6.6e-7, ent=2.16] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.14it/s, pg=0.169, ret=-0.00147, glen=95.2, tlen=256, kl=0.0117, act_lr=6.6e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.14it/s, pg=-0.158, ret=0.000824, glen=83.4, tlen=244, kl=0.0137, act_lr=6.6e-7, ent=2.02]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.15it/s, pg=-0.158, ret=0.000824, glen=83.4, tlen=244, kl=0.0137, act_lr=6.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.15it/s, pg=-0.155, ret=0.000592, glen=86.6, tlen=247, kl=0.0121, act_lr=6.6e-7, ent=2.1] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.16it/s, pg=-0.155, ret=0.000592, glen=86.6, tlen=247, kl=0.0121, act_lr=6.6e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.16it/s, pg=0.0494, ret=-7.84e-5, glen=82.1, tlen=242, kl=0.0137, act_lr=6.6e-7, ent=2.12]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.16it/s, pg=0.0494, ret=-7.84e-5, glen=82.1, tlen=242, kl=0.0137, act_lr=6.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.16it/s, pg=-0.17, ret=0.00112, glen=84.6, tlen=245, kl=0.0125, act_lr=6.6e-7, ent=1.9]   Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:31,  1.17it/s, pg=-0.17, ret=0.00112, glen=84.6, tlen=245, kl=0.0125, act_lr=6.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:31,  1.17it/s, pg=-0.0822, ret=0.000167, glen=72.5, tlen=233, kl=0.0133, act_lr=6.6e-7, ent=2.02]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:30,  1.17it/s, pg=-0.0822, ret=0.000167, glen=72.5, tlen=233, kl=0.0133, act_lr=6.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:30,  1.17it/s, pg=0.00653, ret=0.000217, glen=71.7, tlen=232, kl=0.0154, act_lr=6.6e-7, ent=1.92]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:29,  1.17it/s, pg=0.00653, ret=0.000217, glen=71.7, tlen=232, kl=0.0154, act_lr=6.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:29,  1.17it/s, pg=-0.0964, ret=0.00035, glen=81.2, tlen=242, kl=0.0123, act_lr=6.6e-7, ent=1.99] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.17it/s, pg=-0.0964, ret=0.00035, glen=81.2, tlen=242, kl=0.0123, act_lr=6.6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.17it/s, pg=0.0716, ret=-1.72e-5, glen=108, tlen=268, kl=0.011, act_lr=6.6e-7, ent=2.38]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.17it/s, pg=0.0716, ret=-1.72e-5, glen=108, tlen=268, kl=0.011, act_lr=6.6e-7, ent=2.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.17it/s, pg=0.0649, ret=-0.000477, glen=80.6, tlen=242, kl=0.0136, act_lr=6.6e-7, ent=1.9]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.17it/s, pg=0.0649, ret=-0.000477, glen=80.6, tlen=242, kl=0.0136, act_lr=6.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.17it/s, pg=-0.0192, ret=-6e-5, glen=76.5, tlen=237, kl=0.0135, act_lr=6.6e-7, ent=2.05]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=-0.0192, ret=-6e-5, glen=76.5, tlen=237, kl=0.0135, act_lr=6.6e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=0.00647, ret=5.15e-6, glen=76, tlen=237, kl=0.0171, act_lr=6.6e-7, ent=1.9] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.00647, ret=5.15e-6, glen=76, tlen=237, kl=0.0171, act_lr=6.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=-0.0501, ret=3.57e-5, glen=80.8, tlen=241, kl=0.0137, act_lr=6.6e-7, ent=1.93]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:25,  1.15it/s, pg=-0.0501, ret=3.57e-5, glen=80.8, tlen=241, kl=0.0137, act_lr=6.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:25,  1.15it/s, pg=-0.0284, ret=0.00103, glen=104, tlen=264, kl=0.0106, act_lr=6.6e-7, ent=2.26] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:24,  1.15it/s, pg=-0.0284, ret=0.00103, glen=104, tlen=264, kl=0.0106, act_lr=6.6e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:24,  1.15it/s, pg=-0.12, ret=0.000416, glen=82.2, tlen=243, kl=0.0119, act_lr=6.6e-7, ent=2]   Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.16it/s, pg=-0.12, ret=0.000416, glen=82.2, tlen=243, kl=0.0119, act_lr=6.6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.16it/s, pg=-0.0242, ret=-0.000339, glen=75.1, tlen=235, kl=0.0144, act_lr=6.6e-7, ent=2]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.16it/s, pg=-0.0242, ret=-0.000339, glen=75.1, tlen=235, kl=0.0144, act_lr=6.6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.16it/s, pg=0.175, ret=-0.00188, glen=79.7, tlen=240, kl=0.0131, act_lr=6.6e-7, ent=1.89]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.17it/s, pg=0.175, ret=-0.00188, glen=79.7, tlen=240, kl=0.0131, act_lr=6.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.17it/s, pg=0.0277, ret=-0.000747, glen=84.1, tlen=245, kl=0.0132, act_lr=6.6e-7, ent=2.11]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:23,  1.02it/s, pg=0.0277, ret=-0.000747, glen=84.1, tlen=245, kl=0.0132, act_lr=6.6e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:23,  1.02it/s, pg=-0.0274, ret=-4.29e-5, glen=84.2, tlen=244, kl=0.0117, act_lr=6.6e-7, ent=1.96]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:21,  1.06it/s, pg=-0.0274, ret=-4.29e-5, glen=84.2, tlen=244, kl=0.0117, act_lr=6.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:21,  1.06it/s, pg=0.0203, ret=-0.000415, glen=78.2, tlen=239, kl=0.0122, act_lr=6.6e-7, ent=1.92]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.09it/s, pg=0.0203, ret=-0.000415, glen=78.2, tlen=239, kl=0.0122, act_lr=6.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.09it/s, pg=0.174, ret=-0.00153, glen=82.9, tlen=244, kl=0.0136, act_lr=6.6e-7, ent=2.11]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.11it/s, pg=0.174, ret=-0.00153, glen=82.9, tlen=244, kl=0.0136, act_lr=6.6e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:18,  1.11it/s, pg=0.117, ret=-0.00104, glen=86.8, tlen=247, kl=0.0134, act_lr=6.6e-7, ent=2.06]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.13it/s, pg=0.117, ret=-0.00104, glen=86.8, tlen=247, kl=0.0134, act_lr=6.6e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.13it/s, pg=0.274, ret=-0.000845, glen=99.4, tlen=260, kl=0.0129, act_lr=6.6e-7, ent=2.32]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=0.274, ret=-0.000845, glen=99.4, tlen=260, kl=0.0129, act_lr=6.6e-7, ent=2.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=0.274, ret=-0.00179, glen=83.4, tlen=244, kl=0.013, act_lr=6.6e-7, ent=1.87]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.15it/s, pg=0.274, ret=-0.00179, glen=83.4, tlen=244, kl=0.013, act_lr=6.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=-0.0641, ret=-0.000179, glen=82.4, tlen=243, kl=0.0138, act_lr=6.6e-7, ent=1.98]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.16it/s, pg=-0.0641, ret=-0.000179, glen=82.4, tlen=243, kl=0.0138, act_lr=6.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=-0.0312, ret=0.00134, glen=84.9, tlen=245, kl=0.0138, act_lr=6.6e-7, ent=2.16]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.16it/s, pg=-0.0312, ret=0.00134, glen=84.9, tlen=245, kl=0.0138, act_lr=6.6e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=0.0283, ret=0.000184, glen=77.4, tlen=237, kl=0.0177, act_lr=6.6e-7, ent=1.86]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=0.0283, ret=0.000184, glen=77.4, tlen=237, kl=0.0177, act_lr=6.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=-0.128, ret=-5.09e-5, glen=84.4, tlen=244, kl=0.0154, act_lr=6.6e-7, ent=2.13]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=-0.128, ret=-5.09e-5, glen=84.4, tlen=244, kl=0.0154, act_lr=6.6e-7, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:11,  1.17it/s, pg=0.011, ret=-0.000667, glen=75.8, tlen=236, kl=0.0116, act_lr=6.6e-7, ent=1.89]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.011, ret=-0.000667, glen=75.8, tlen=236, kl=0.0116, act_lr=6.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.17it/s, pg=-0.0187, ret=0.000251, glen=79.9, tlen=240, kl=0.0134, act_lr=6.6e-7, ent=1.94]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-0.0187, ret=0.000251, glen=79.9, tlen=240, kl=0.0134, act_lr=6.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=0.18, ret=-0.000765, glen=91.9, tlen=252, kl=0.0123, act_lr=6.6e-7, ent=2.12]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.17it/s, pg=0.18, ret=-0.000765, glen=91.9, tlen=252, kl=0.0123, act_lr=6.6e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.0543, ret=0.000697, glen=90, tlen=250, kl=0.0129, act_lr=6.6e-7, ent=1.98]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.17it/s, pg=-0.0543, ret=0.000697, glen=90, tlen=250, kl=0.0129, act_lr=6.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=0.032, ret=0.000491, glen=90, tlen=251, kl=0.0132, act_lr=6.6e-7, ent=2.14]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.17it/s, pg=0.032, ret=0.000491, glen=90, tlen=251, kl=0.0132, act_lr=6.6e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.17it/s, pg=0.151, ret=-8.11e-5, glen=89.2, tlen=249, kl=0.013, act_lr=6.6e-7, ent=2.09]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.17it/s, pg=0.151, ret=-8.11e-5, glen=89.2, tlen=249, kl=0.013, act_lr=6.6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.17it/s, pg=-0.0895, ret=0.000308, glen=80.8, tlen=241, kl=0.0137, act_lr=6.6e-7, ent=1.82]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=-0.0895, ret=0.000308, glen=80.8, tlen=241, kl=0.0137, act_lr=6.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.17it/s, pg=-0.0192, ret=-0.000204, glen=84.2, tlen=244, kl=0.0131, act_lr=6.6e-7, ent=2.02]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.17it/s, pg=-0.0192, ret=-0.000204, glen=84.2, tlen=244, kl=0.0131, act_lr=6.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.17it/s, pg=-0.0184, ret=0.000444, glen=75.5, tlen=236, kl=0.0169, act_lr=6.6e-7, ent=1.72] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=-0.0184, ret=0.000444, glen=75.5, tlen=236, kl=0.0169, act_lr=6.6e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=-0.122, ret=0.000458, glen=110, tlen=270, kl=0.00992, act_lr=6.6e-7, ent=2.45] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.17it/s, pg=-0.122, ret=0.000458, glen=110, tlen=270, kl=0.00992, act_lr=6.6e-7, ent=2.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.17it/s, pg=-0.123, ret=0.00139, glen=84, tlen=244, kl=0.0133, act_lr=6.6e-7, ent=1.9]    Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=-0.123, ret=0.00139, glen=84, tlen=244, kl=0.0133, act_lr=6.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=-0.128, ret=0.000894, glen=81.1, tlen=241, kl=0.0134, act_lr=6.6e-7, ent=2]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=-0.128, ret=0.000894, glen=81.1, tlen=241, kl=0.0134, act_lr=6.6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.244, ret=0.00132, glen=81.7, tlen=242, kl=0.013, act_lr=6.6e-7, ent=2.11]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=-0.244, ret=0.00132, glen=81.7, tlen=242, kl=0.013, act_lr=6.6e-7, ent=2.11]
2025-07-23 13:54:57.768 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.43s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=-0.202, ret=0.00161, glen=92.6, tlen=253, kl=0.0145, act_lr=6.8e-7, ent=2.23]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=-0.202, ret=0.00161, glen=92.6, tlen=253, kl=0.0145, act_lr=6.8e-7, ent=2.23]
2025-07-23 13:54:58.631 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 13:55:01.159 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.53s
2025-07-23 13:55:01.465 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.24s
2025-07-23 13:55:01.471 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0021309572107651655, 'actor_lr': 6.603921691499172e-07, 'clip_ratio': 0.0, 'entropy': 2.042788451793147, 'kl': 0.013317631740196078, 'response_length': 85.14484106325635, 'total_length': 245.45882101619944, 'return': -1.4413146452759119e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [28:01<16:05, 193.15s/it]2025-07-23 13:55:01.502 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 13:57:28.884 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 13:57:29.070 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 13:57:29.071 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 147.57s
2025-07-23 13:57:31.020 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0085,avg_pass_at_n: 1.0000,avg_num_tokens: 86.4542,std_num_tokens: 135.9716,avg_correct_num_tokens: 81.5871,std_correct_num_tokens: 81.9085,avg_incorrect_num_tokens: 92.0906,std_incorrect_num_tokens: 179.0816
2025-07-23 13:57:31.466 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.39s
2025-07-23 13:57:33.012 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.55s
2025-07-23 13:57:59.580 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 205
2025-07-23 13:57:59.581 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.57s
2025-07-23 13:58:00.368 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.78s
2025-07-23 13:58:00.369 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.00013966554017714794, avg_kl: 0.014559638790967987, avg_response_length: 91.70477951794136, avg_orm_score: 0.0, avg_custom_rewards: 0.00013966554017714794
2025-07-23 13:58:00.399 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter34_replay_buffer.jsonl
2025-07-23 13:58:01.730 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.33s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s, pg=0.0795, ret=-1.32e-5, glen=97.2, tlen=257, kl=0.0113, act_lr=6.8e-7, ent=2.06]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:00<00:50,  1.00it/s, pg=0.0795, ret=-1.32e-5, glen=97.2, tlen=257, kl=0.0113, act_lr=6.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:50,  1.00it/s, pg=0.0299, ret=-0.000729, glen=79.5, tlen=240, kl=0.0143, act_lr=6.8e-7, ent=2.05]Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:45,  1.09it/s, pg=0.0299, ret=-0.000729, glen=79.5, tlen=240, kl=0.0143, act_lr=6.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:45,  1.09it/s, pg=-0.0308, ret=-0.000398, glen=81.3, tlen=241, kl=0.0137, act_lr=6.8e-7, ent=2.04]Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:43,  1.13it/s, pg=-0.0308, ret=-0.000398, glen=81.3, tlen=241, kl=0.0137, act_lr=6.8e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:43,  1.13it/s, pg=0.452, ret=7.26e-6, glen=140, tlen=301, kl=0.0133, act_lr=6.8e-7, ent=2.8]      Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:43,  1.10it/s, pg=0.452, ret=7.26e-6, glen=140, tlen=301, kl=0.0133, act_lr=6.8e-7, ent=2.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:43,  1.10it/s, pg=0.0678, ret=-0.000336, glen=83.8, tlen=244, kl=0.0191, act_lr=6.8e-7, ent=1.91]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:42,  1.10it/s, pg=0.0678, ret=-0.000336, glen=83.8, tlen=244, kl=0.0191, act_lr=6.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:42,  1.10it/s, pg=-0.0242, ret=-0.00101, glen=90.1, tlen=250, kl=0.0161, act_lr=6.8e-7, ent=2.15]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:41,  1.12it/s, pg=-0.0242, ret=-0.00101, glen=90.1, tlen=250, kl=0.0161, act_lr=6.8e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:41,  1.12it/s, pg=0.0162, ret=0.000235, glen=85.3, tlen=245, kl=0.0158, act_lr=6.8e-7, ent=2.05] Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:39,  1.13it/s, pg=0.0162, ret=0.000235, glen=85.3, tlen=245, kl=0.0158, act_lr=6.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:07<00:39,  1.13it/s, pg=0.0692, ret=-0.00105, glen=80.5, tlen=240, kl=0.0141, act_lr=6.8e-7, ent=1.87]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:38,  1.15it/s, pg=0.0692, ret=-0.00105, glen=80.5, tlen=240, kl=0.0141, act_lr=6.8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:38,  1.15it/s, pg=0.0154, ret=-0.000184, glen=76.1, tlen=236, kl=0.0201, act_lr=6.8e-7, ent=1.98]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:07<00:37,  1.15it/s, pg=0.0154, ret=-0.000184, glen=76.1, tlen=236, kl=0.0201, act_lr=6.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.15it/s, pg=-0.0751, ret=0.000344, glen=89.2, tlen=249, kl=0.0116, act_lr=6.8e-7, ent=2.18]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:36,  1.16it/s, pg=-0.0751, ret=0.000344, glen=89.2, tlen=249, kl=0.0116, act_lr=6.8e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:36,  1.16it/s, pg=0.0199, ret=-0.000407, glen=86.4, tlen=247, kl=0.0149, act_lr=6.8e-7, ent=1.93]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:35,  1.16it/s, pg=0.0199, ret=-0.000407, glen=86.4, tlen=247, kl=0.0149, act_lr=6.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:35,  1.16it/s, pg=-0.112, ret=0.000159, glen=86, tlen=246, kl=0.0127, act_lr=6.8e-7, ent=2.02]   Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.16it/s, pg=-0.112, ret=0.000159, glen=86, tlen=246, kl=0.0127, act_lr=6.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.16it/s, pg=-0.14, ret=0.00166, glen=80.6, tlen=241, kl=0.0179, act_lr=6.8e-7, ent=1.93]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.17it/s, pg=-0.14, ret=0.00166, glen=80.6, tlen=241, kl=0.0179, act_lr=6.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.17it/s, pg=-0.24, ret=0.00104, glen=77.5, tlen=238, kl=0.0149, act_lr=6.8e-7, ent=1.9] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.17it/s, pg=-0.24, ret=0.00104, glen=77.5, tlen=238, kl=0.0149, act_lr=6.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:13<00:32,  1.17it/s, pg=-0.119, ret=0.000183, glen=89.6, tlen=249, kl=0.0138, act_lr=6.8e-7, ent=2.19]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=-0.119, ret=0.000183, glen=89.6, tlen=249, kl=0.0138, act_lr=6.8e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=0.0404, ret=0.000157, glen=101, tlen=260, kl=0.0122, act_lr=6.8e-7, ent=2.31] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:13<00:30,  1.17it/s, pg=0.0404, ret=0.000157, glen=101, tlen=260, kl=0.0122, act_lr=6.8e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=0.228, ret=-0.00209, glen=92.6, tlen=252, kl=0.0104, act_lr=6.8e-7, ent=2.26]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:29,  1.17it/s, pg=0.228, ret=-0.00209, glen=92.6, tlen=252, kl=0.0104, act_lr=6.8e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:29,  1.17it/s, pg=0.0155, ret=-0.000273, glen=80.6, tlen=241, kl=0.0157, act_lr=6.8e-7, ent=2.02]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:29,  1.17it/s, pg=0.0155, ret=-0.000273, glen=80.6, tlen=241, kl=0.0157, act_lr=6.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:29,  1.17it/s, pg=-0.161, ret=0.000525, glen=90.2, tlen=250, kl=0.0126, act_lr=6.8e-7, ent=2.41] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.17it/s, pg=-0.161, ret=0.000525, glen=90.2, tlen=250, kl=0.0126, act_lr=6.8e-7, ent=2.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.17it/s, pg=0.0679, ret=-1.97e-5, glen=89.7, tlen=250, kl=0.0132, act_lr=6.8e-7, ent=2.06]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.17it/s, pg=0.0679, ret=-1.97e-5, glen=89.7, tlen=250, kl=0.0132, act_lr=6.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.17it/s, pg=-0.064, ret=0.00056, glen=85.1, tlen=245, kl=0.013, act_lr=6.8e-7, ent=2.25]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:26,  1.17it/s, pg=-0.064, ret=0.00056, glen=85.1, tlen=245, kl=0.013, act_lr=6.8e-7, ent=2.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:19<00:26,  1.17it/s, pg=-0.151, ret=0.000617, glen=74.4, tlen=234, kl=0.016, act_lr=6.8e-7, ent=1.91]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.17it/s, pg=-0.151, ret=0.000617, glen=74.4, tlen=234, kl=0.016, act_lr=6.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.17it/s, pg=0.0393, ret=-0.00034, glen=75.8, tlen=235, kl=0.0166, act_lr=6.8e-7, ent=2.09]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:19<00:24,  1.17it/s, pg=0.0393, ret=-0.00034, glen=75.8, tlen=235, kl=0.0166, act_lr=6.8e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:24,  1.17it/s, pg=0.024, ret=0.000682, glen=80.4, tlen=241, kl=0.0216, act_lr=6.8e-7, ent=1.89] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:20<00:23,  1.18it/s, pg=0.024, ret=0.000682, glen=80.4, tlen=241, kl=0.0216, act_lr=6.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:23,  1.18it/s, pg=-0.105, ret=0.0011, glen=74.6, tlen=234, kl=0.017, act_lr=6.8e-7, ent=2]     Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:23,  1.17it/s, pg=-0.105, ret=0.0011, glen=74.6, tlen=234, kl=0.017, act_lr=6.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:23,  1.17it/s, pg=0.204, ret=-0.00566, glen=78, tlen=238, kl=0.0152, act_lr=6.8e-7, ent=1.73]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.18it/s, pg=0.204, ret=-0.00566, glen=78, tlen=238, kl=0.0152, act_lr=6.8e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:23<00:22,  1.18it/s, pg=0.138, ret=-0.0014, glen=78.9, tlen=239, kl=0.0165, act_lr=6.8e-7, ent=1.96]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:23<00:24,  1.01it/s, pg=0.138, ret=-0.0014, glen=78.9, tlen=239, kl=0.0165, act_lr=6.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:24,  1.01it/s, pg=-0.2, ret=0.000381, glen=83.9, tlen=245, kl=0.0123, act_lr=6.8e-7, ent=1.95]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:22,  1.05it/s, pg=-0.2, ret=0.000381, glen=83.9, tlen=245, kl=0.0123, act_lr=6.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:22,  1.05it/s, pg=-0.0535, ret=0.000539, glen=82.2, tlen=242, kl=0.0166, act_lr=6.8e-7, ent=2.04]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:21,  1.09it/s, pg=-0.0535, ret=0.000539, glen=82.2, tlen=242, kl=0.0166, act_lr=6.8e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:21,  1.09it/s, pg=0.193, ret=-0.00108, glen=82.8, tlen=243, kl=0.0126, act_lr=6.8e-7, ent=2]     Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:19,  1.11it/s, pg=0.193, ret=-0.00108, glen=82.8, tlen=243, kl=0.0126, act_lr=6.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:27<00:19,  1.11it/s, pg=0.239, ret=-0.00195, glen=95.8, tlen=256, kl=0.0118, act_lr=6.8e-7, ent=2.12]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:18,  1.13it/s, pg=0.239, ret=-0.00195, glen=95.8, tlen=256, kl=0.0118, act_lr=6.8e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:28<00:18,  1.13it/s, pg=-0.143, ret=0.000673, glen=85.6, tlen=246, kl=0.0191, act_lr=6.8e-7, ent=2.09]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.14it/s, pg=-0.143, ret=0.000673, glen=85.6, tlen=246, kl=0.0191, act_lr=6.8e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.14it/s, pg=-0.12, ret=0.000139, glen=90, tlen=250, kl=0.0147, act_lr=6.8e-7, ent=2.24]   Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:28<00:16,  1.15it/s, pg=-0.12, ret=0.000139, glen=90, tlen=250, kl=0.0147, act_lr=6.8e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.15it/s, pg=-0.00291, ret=0.000592, glen=78.8, tlen=239, kl=0.0151, act_lr=6.8e-7, ent=2.08]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:29<00:15,  1.16it/s, pg=-0.00291, ret=0.000592, glen=78.8, tlen=239, kl=0.0151, act_lr=6.8e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.16it/s, pg=0.0367, ret=-0.000244, glen=115, tlen=275, kl=0.0106, act_lr=6.8e-7, ent=2.49]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.16it/s, pg=0.0367, ret=-0.000244, glen=115, tlen=275, kl=0.0106, act_lr=6.8e-7, ent=2.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.16it/s, pg=-0.0748, ret=0.000101, glen=93.8, tlen=255, kl=0.0129, act_lr=6.8e-7, ent=2.03]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.16it/s, pg=-0.0748, ret=0.000101, glen=93.8, tlen=255, kl=0.0129, act_lr=6.8e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.16it/s, pg=0.0991, ret=-0.000495, glen=78.7, tlen=239, kl=0.0153, act_lr=6.8e-7, ent=1.88]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.17it/s, pg=0.0991, ret=-0.000495, glen=78.7, tlen=239, kl=0.0153, act_lr=6.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:33<00:12,  1.17it/s, pg=-0.24, ret=0.0013, glen=90.3, tlen=251, kl=0.0132, act_lr=6.8e-7, ent=1.92]    Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:11,  1.17it/s, pg=-0.24, ret=0.0013, glen=90.3, tlen=251, kl=0.0132, act_lr=6.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:34<00:11,  1.17it/s, pg=0.0908, ret=-0.000156, glen=80.5, tlen=241, kl=0.0199, act_lr=6.8e-7, ent=1.95]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.17it/s, pg=0.0908, ret=-0.000156, glen=80.5, tlen=241, kl=0.0199, act_lr=6.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.17it/s, pg=-0.0209, ret=4.46e-5, glen=80.5, tlen=240, kl=0.0135, act_lr=6.8e-7, ent=2.02] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:34<00:10,  1.17it/s, pg=-0.0209, ret=4.46e-5, glen=80.5, tlen=240, kl=0.0135, act_lr=6.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.17it/s, pg=-0.217, ret=0.0011, glen=93.7, tlen=254, kl=0.0153, act_lr=6.8e-7, ent=2.28]  Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:35<00:09,  1.17it/s, pg=-0.217, ret=0.0011, glen=93.7, tlen=254, kl=0.0153, act_lr=6.8e-7, ent=2.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.17it/s, pg=-0.0559, ret=-0.000384, glen=99.7, tlen=260, kl=0.0115, act_lr=6.8e-7, ent=2.11]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:36<00:08,  1.15it/s, pg=-0.0559, ret=-0.000384, glen=99.7, tlen=260, kl=0.0115, act_lr=6.8e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.15it/s, pg=0.0819, ret=-0.000291, glen=90, tlen=250, kl=0.0124, act_lr=6.8e-7, ent=2.34]   Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.16it/s, pg=0.0819, ret=-0.000291, glen=90, tlen=250, kl=0.0124, act_lr=6.8e-7, ent=2.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.16it/s, pg=-0.234, ret=0.000995, glen=86.4, tlen=247, kl=0.0133, act_lr=6.8e-7, ent=2.11]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.16it/s, pg=-0.234, ret=0.000995, glen=86.4, tlen=247, kl=0.0133, act_lr=6.8e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:39<00:06,  1.16it/s, pg=-0.14, ret=0.000129, glen=82, tlen=242, kl=0.016, act_lr=6.8e-7, ent=1.97]    Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.17it/s, pg=-0.14, ret=0.000129, glen=82, tlen=242, kl=0.016, act_lr=6.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:40<00:05,  1.17it/s, pg=-0.0428, ret=0.000442, glen=81.9, tlen=242, kl=0.015, act_lr=6.8e-7, ent=1.97]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=-0.0428, ret=0.000442, glen=81.9, tlen=242, kl=0.015, act_lr=6.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=-0.119, ret=0.000974, glen=83.9, tlen=244, kl=0.0147, act_lr=6.8e-7, ent=2]   Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:40<00:04,  1.17it/s, pg=-0.119, ret=0.000974, glen=83.9, tlen=244, kl=0.0147, act_lr=6.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.17it/s, pg=0.0538, ret=0.000484, glen=84.7, tlen=244, kl=0.0154, act_lr=6.8e-7, ent=2.18]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:41<00:03,  1.17it/s, pg=0.0538, ret=0.000484, glen=84.7, tlen=244, kl=0.0154, act_lr=6.8e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=0.207, ret=-0.00127, glen=98, tlen=258, kl=0.0125, act_lr=6.8e-7, ent=2.04]   Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.17it/s, pg=0.207, ret=-0.00127, glen=98, tlen=258, kl=0.0125, act_lr=6.8e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.17it/s, pg=0.0386, ret=-0.000299, glen=82.7, tlen=243, kl=0.0138, act_lr=6.8e-7, ent=2.17]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.17it/s, pg=0.0386, ret=-0.000299, glen=82.7, tlen=243, kl=0.0138, act_lr=6.8e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.17it/s, pg=-0.0273, ret=0.000108, glen=80.3, tlen=241, kl=0.0123, act_lr=6.8e-7, ent=1.95]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=-0.0273, ret=0.000108, glen=80.3, tlen=241, kl=0.0123, act_lr=6.8e-7, ent=1.95]
2025-07-23 13:58:47.578 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 45.32s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.17it/s, pg=-0.0546, ret=0.00684, glen=329, tlen=488, kl=0.0132, act_lr=7e-7, ent=1.46]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.13it/s, pg=-0.0546, ret=0.00684, glen=329, tlen=488, kl=0.0132, act_lr=7e-7, ent=1.46]
2025-07-23 13:58:48.334 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.69s
2025-07-23 13:58:50.903 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-23 13:58:51.229 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 49.09s
2025-07-23 13:58:51.234 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.008128698055560771, 'actor_lr': 6.803846424383245e-07, 'clip_ratio': 0.0, 'entropy': 2.064189619742907, 'kl': 0.014549768888033353, 'response_length': 91.4287762275109, 'total_length': 251.51054646418646, 'return': 3.8968556439683125e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [31:51<13:38, 204.60s/it]2025-07-23 13:58:51.271 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:00:24.657 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:00:24.847 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:00:24.847 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 93.58s
2025-07-23 14:00:26.765 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0104,avg_pass_at_n: 1.0000,avg_num_tokens: 83.9957,std_num_tokens: 94.2627,avg_correct_num_tokens: 82.7364,std_correct_num_tokens: 88.3454,avg_incorrect_num_tokens: 85.3912,std_incorrect_num_tokens: 100.3947
2025-07-23 14:00:27.204 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.36s
2025-07-23 14:00:28.632 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.43s
2025-07-23 14:00:54.567 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 202
2025-07-23 14:00:54.567 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.93s
2025-07-23 14:00:55.359 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.79s
2025-07-23 14:00:55.360 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0003066863837285373, avg_kl: 0.01498836101871906, avg_response_length: 85.09933696406902, avg_orm_score: 0.0, avg_custom_rewards: -0.0003066863837285373
2025-07-23 14:00:55.390 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter35_replay_buffer.jsonl
2025-07-23 14:00:56.680 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.29s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=-0.253, ret=0.00218, glen=89.7, tlen=250, kl=0.0134, act_lr=7e-7, ent=2.09]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:48,  1.04it/s, pg=-0.253, ret=0.00218, glen=89.7, tlen=250, kl=0.0134, act_lr=7e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:48,  1.04it/s, pg=-0.0261, ret=-0.000412, glen=82.8, tlen=243, kl=0.0152, act_lr=7e-7, ent=2.16]Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.11it/s, pg=-0.0261, ret=-0.000412, glen=82.8, tlen=243, kl=0.0152, act_lr=7e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.11it/s, pg=-0.137, ret=0.00214, glen=77.8, tlen=238, kl=0.0199, act_lr=7e-7, ent=1.87]   Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:42,  1.14it/s, pg=-0.137, ret=0.00214, glen=77.8, tlen=238, kl=0.0199, act_lr=7e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:42,  1.14it/s, pg=-0.0172, ret=-0.000298, glen=84, tlen=244, kl=0.0146, act_lr=7e-7, ent=2.01]Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:40,  1.15it/s, pg=-0.0172, ret=-0.000298, glen=84, tlen=244, kl=0.0146, act_lr=7e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:40,  1.15it/s, pg=-0.0478, ret=0.000738, glen=87.5, tlen=247, kl=0.0126, act_lr=7e-7, ent=2]  Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:39,  1.16it/s, pg=-0.0478, ret=0.000738, glen=87.5, tlen=247, kl=0.0126, act_lr=7e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:39,  1.16it/s, pg=0.209, ret=0.000283, glen=109, tlen=269, kl=0.011, act_lr=7e-7, ent=2.75] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.14it/s, pg=0.209, ret=0.000283, glen=109, tlen=269, kl=0.011, act_lr=7e-7, ent=2.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.14it/s, pg=-0.0745, ret=0.000386, glen=84.3, tlen=245, kl=0.0181, act_lr=7e-7, ent=2.23]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:39,  1.12it/s, pg=-0.0745, ret=0.000386, glen=84.3, tlen=245, kl=0.0181, act_lr=7e-7, ent=2.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:39,  1.12it/s, pg=0.0269, ret=0.000219, glen=75.9, tlen=236, kl=0.0147, act_lr=7e-7, ent=1.9]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:38,  1.13it/s, pg=0.0269, ret=0.000219, glen=75.9, tlen=236, kl=0.0147, act_lr=7e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:38,  1.13it/s, pg=0.0433, ret=7.13e-5, glen=77.8, tlen=238, kl=0.0121, act_lr=7e-7, ent=1.98]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.14it/s, pg=0.0433, ret=7.13e-5, glen=77.8, tlen=238, kl=0.0121, act_lr=7e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.14it/s, pg=0.0533, ret=0.000491, glen=85.9, tlen=246, kl=0.0155, act_lr=7e-7, ent=2.01]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.15it/s, pg=0.0533, ret=0.000491, glen=85.9, tlen=246, kl=0.0155, act_lr=7e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.15it/s, pg=0.00488, ret=-0.000401, glen=79.1, tlen=240, kl=0.0143, act_lr=7e-7, ent=1.9]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.16it/s, pg=0.00488, ret=-0.000401, glen=79.1, tlen=240, kl=0.0143, act_lr=7e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.16it/s, pg=0.209, ret=-0.00168, glen=90.4, tlen=251, kl=0.0208, act_lr=7e-7, ent=2.32]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.16it/s, pg=0.209, ret=-0.00168, glen=90.4, tlen=251, kl=0.0208, act_lr=7e-7, ent=2.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.16it/s, pg=0.213, ret=-0.000593, glen=89.6, tlen=250, kl=0.0132, act_lr=7e-7, ent=2.22]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:33,  1.15it/s, pg=0.213, ret=-0.000593, glen=89.6, tlen=250, kl=0.0132, act_lr=7e-7, ent=2.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:33,  1.15it/s, pg=-0.108, ret=0.00142, glen=88.1, tlen=249, kl=0.0158, act_lr=7e-7, ent=1.95] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:32,  1.15it/s, pg=-0.108, ret=0.00142, glen=88.1, tlen=249, kl=0.0158, act_lr=7e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:32,  1.15it/s, pg=0.231, ret=-0.00168, glen=109, tlen=270, kl=0.0127, act_lr=7e-7, ent=2.22] Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.16it/s, pg=0.231, ret=-0.00168, glen=109, tlen=270, kl=0.0127, act_lr=7e-7, ent=2.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.16it/s, pg=-0.0417, ret=0.000589, glen=72.4, tlen=232, kl=0.0157, act_lr=7e-7, ent=1.93]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:30,  1.16it/s, pg=-0.0417, ret=0.000589, glen=72.4, tlen=232, kl=0.0157, act_lr=7e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.16it/s, pg=-0.12, ret=0.000848, glen=84.1, tlen=244, kl=0.0156, act_lr=7e-7, ent=1.89]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.17it/s, pg=-0.12, ret=0.000848, glen=84.1, tlen=244, kl=0.0156, act_lr=7e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.17it/s, pg=0.0608, ret=0.000164, glen=75.3, tlen=236, kl=0.0146, act_lr=7e-7, ent=1.91]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.16it/s, pg=0.0608, ret=0.000164, glen=75.3, tlen=236, kl=0.0146, act_lr=7e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.16it/s, pg=-0.244, ret=-7.33e-5, glen=84.3, tlen=244, kl=0.0134, act_lr=7e-7, ent=1.91]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=-0.244, ret=-7.33e-5, glen=84.3, tlen=244, kl=0.0134, act_lr=7e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=-0.08, ret=-0.00115, glen=87.2, tlen=247, kl=0.014, act_lr=7e-7, ent=1.96]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=-0.08, ret=-0.00115, glen=87.2, tlen=247, kl=0.014, act_lr=7e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=0.123, ret=-0.000348, glen=89.5, tlen=250, kl=0.0153, act_lr=7e-7, ent=2.27]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.123, ret=-0.000348, glen=89.5, tlen=250, kl=0.0153, act_lr=7e-7, ent=2.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=-0.197, ret=0.000288, glen=77.5, tlen=238, kl=0.0155, act_lr=7e-7, ent=1.95]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.197, ret=0.000288, glen=77.5, tlen=238, kl=0.0155, act_lr=7e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.101, ret=0.000738, glen=94.3, tlen=255, kl=0.0156, act_lr=7e-7, ent=2.06]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:19<00:23,  1.17it/s, pg=-0.101, ret=0.000738, glen=94.3, tlen=255, kl=0.0156, act_lr=7e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.0261, ret=-0.000145, glen=77.1, tlen=237, kl=0.0151, act_lr=7e-7, ent=1.9]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:22,  1.17it/s, pg=-0.0261, ret=-0.000145, glen=77.1, tlen=237, kl=0.0151, act_lr=7e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:22,  1.17it/s, pg=0.00732, ret=-0.000402, glen=80.8, tlen=241, kl=0.0142, act_lr=7e-7, ent=1.81]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.17it/s, pg=0.00732, ret=-0.000402, glen=80.8, tlen=241, kl=0.0142, act_lr=7e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.17it/s, pg=0.0448, ret=-0.000679, glen=78.8, tlen=239, kl=0.0196, act_lr=7e-7, ent=1.77] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.18it/s, pg=0.0448, ret=-0.000679, glen=78.8, tlen=239, kl=0.0196, act_lr=7e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.18it/s, pg=0.0283, ret=0.000709, glen=77.6, tlen=238, kl=0.0142, act_lr=7e-7, ent=2.08] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:24,  1.02s/it, pg=0.0283, ret=0.000709, glen=77.6, tlen=238, kl=0.0142, act_lr=7e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:24,  1.02s/it, pg=-0.0374, ret=0.0008, glen=78.3, tlen=239, kl=0.014, act_lr=7e-7, ent=2]     Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:22,  1.03it/s, pg=-0.0374, ret=0.0008, glen=78.3, tlen=239, kl=0.014, act_lr=7e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:22,  1.03it/s, pg=-0.257, ret=-0.000203, glen=108, tlen=268, kl=0.0125, act_lr=7e-7, ent=2.4]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.06it/s, pg=-0.257, ret=-0.000203, glen=108, tlen=268, kl=0.0125, act_lr=7e-7, ent=2.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.06it/s, pg=-0.172, ret=0.000939, glen=87.7, tlen=248, kl=0.0149, act_lr=7e-7, ent=2.15]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:19,  1.09it/s, pg=-0.172, ret=0.000939, glen=87.7, tlen=248, kl=0.0149, act_lr=7e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:19,  1.09it/s, pg=0.112, ret=-0.000837, glen=89.6, tlen=250, kl=0.0142, act_lr=7e-7, ent=2.04]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.12it/s, pg=0.112, ret=-0.000837, glen=89.6, tlen=250, kl=0.0142, act_lr=7e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.12it/s, pg=-0.0234, ret=-0.000565, glen=87.1, tlen=247, kl=0.0131, act_lr=7e-7, ent=2.08]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.13it/s, pg=-0.0234, ret=-0.000565, glen=87.1, tlen=247, kl=0.0131, act_lr=7e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:29<00:16,  1.13it/s, pg=0.0691, ret=-0.000787, glen=82.6, tlen=243, kl=0.0127, act_lr=7e-7, ent=2.12] Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=0.0691, ret=-0.000787, glen=82.6, tlen=243, kl=0.0127, act_lr=7e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=-0.077, ret=0.000507, glen=93.4, tlen=254, kl=0.0123, act_lr=7e-7, ent=2.01] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.16it/s, pg=-0.077, ret=0.000507, glen=93.4, tlen=254, kl=0.0123, act_lr=7e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=0.057, ret=-0.000628, glen=75, tlen=235, kl=0.0189, act_lr=7e-7, ent=1.77]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.16it/s, pg=0.057, ret=-0.000628, glen=75, tlen=235, kl=0.0189, act_lr=7e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=0.0273, ret=0.000114, glen=81.8, tlen=242, kl=0.0163, act_lr=7e-7, ent=1.92]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=0.0273, ret=0.000114, glen=81.8, tlen=242, kl=0.0163, act_lr=7e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=0.0355, ret=-0.00149, glen=88.2, tlen=249, kl=0.0149, act_lr=7e-7, ent=1.95]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=0.0355, ret=-0.00149, glen=88.2, tlen=249, kl=0.0149, act_lr=7e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:11,  1.17it/s, pg=0.134, ret=-0.000754, glen=87.7, tlen=248, kl=0.0168, act_lr=7e-7, ent=2.01]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.134, ret=-0.000754, glen=87.7, tlen=248, kl=0.0168, act_lr=7e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.17it/s, pg=0.0397, ret=-0.000878, glen=78.4, tlen=239, kl=0.0156, act_lr=7e-7, ent=1.97]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=0.0397, ret=-0.000878, glen=78.4, tlen=239, kl=0.0156, act_lr=7e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:35<00:10,  1.17it/s, pg=-0.0326, ret=-9.62e-5, glen=78.1, tlen=238, kl=0.0177, act_lr=7e-7, ent=1.91]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.16it/s, pg=-0.0326, ret=-9.62e-5, glen=78.1, tlen=238, kl=0.0177, act_lr=7e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.16it/s, pg=-0.142, ret=0.000838, glen=87.8, tlen=248, kl=0.0142, act_lr=7e-7, ent=2.08] Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.16it/s, pg=-0.142, ret=0.000838, glen=87.8, tlen=248, kl=0.0142, act_lr=7e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.16it/s, pg=0.104, ret=-0.000465, glen=79.8, tlen=240, kl=0.014, act_lr=7e-7, ent=2.07] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.17it/s, pg=0.104, ret=-0.000465, glen=79.8, tlen=240, kl=0.014, act_lr=7e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.17it/s, pg=-0.0706, ret=0.000627, glen=74.2, tlen=235, kl=0.0144, act_lr=7e-7, ent=1.87]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.17it/s, pg=-0.0706, ret=0.000627, glen=74.2, tlen=235, kl=0.0144, act_lr=7e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.17it/s, pg=-0.0773, ret=0.000763, glen=86.5, tlen=247, kl=0.0129, act_lr=7e-7, ent=2.03]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=-0.0773, ret=0.000763, glen=86.5, tlen=247, kl=0.0129, act_lr=7e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.17it/s, pg=0.488, ret=-0.00316, glen=110, tlen=270, kl=0.0125, act_lr=7e-7, ent=2.36]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.17it/s, pg=0.488, ret=-0.00316, glen=110, tlen=270, kl=0.0125, act_lr=7e-7, ent=2.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.17it/s, pg=0.186, ret=-0.000837, glen=89.7, tlen=250, kl=0.0172, act_lr=7e-7, ent=2.2]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=0.186, ret=-0.000837, glen=89.7, tlen=250, kl=0.0172, act_lr=7e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=-0.151, ret=0.00153, glen=90.8, tlen=251, kl=0.0149, act_lr=7e-7, ent=2.09]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.17it/s, pg=-0.151, ret=0.00153, glen=90.8, tlen=251, kl=0.0149, act_lr=7e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.17it/s, pg=0.101, ret=-0.000988, glen=80.9, tlen=241, kl=0.0148, act_lr=7e-7, ent=2.08]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=0.101, ret=-0.000988, glen=80.9, tlen=241, kl=0.0148, act_lr=7e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=-0.106, ret=-2.5e-5, glen=80, tlen=241, kl=0.0138, act_lr=7e-7, ent=1.89]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=-0.106, ret=-2.5e-5, glen=80, tlen=241, kl=0.0138, act_lr=7e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.179, ret=0.00124, glen=76, tlen=236, kl=0.0159, act_lr=7e-7, ent=1.85]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.18it/s, pg=-0.179, ret=0.00124, glen=76, tlen=236, kl=0.0159, act_lr=7e-7, ent=1.85]
2025-07-23 14:01:41.317 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.48s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.18it/s, pg=-0.109, ret=0.000474, glen=80.5, tlen=241, kl=0.0175, act_lr=7.2e-7, ent=2.01]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=-0.109, ret=0.000474, glen=80.5, tlen=241, kl=0.0175, act_lr=7.2e-7, ent=2.01]
2025-07-23 14:01:42.164 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-23 14:01:44.671 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-23 14:01:44.978 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.26s
2025-07-23 14:01:44.982 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.005902907427619486, 'actor_lr': 7.003921440378227e-07, 'clip_ratio': 0.0, 'entropy': 2.037031241491729, 'kl': 0.014959597120098039, 'response_length': 85.11558383118873, 'total_length': 245.44373366411995, 'return': -9.661416412449862e-06, 'policy_update_steps': 1.0}
Episode [3/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [34:44<09:45, 195.07s/it]2025-07-23 14:01:45.013 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:03:41.146 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:03:41.335 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:03:41.335 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 116.32s
2025-07-23 14:03:43.344 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0158,avg_reflection_pattern_score: 0.0098,avg_pass_at_n: 1.0000,avg_num_tokens: 86.8962,std_num_tokens: 135.0741,avg_correct_num_tokens: 80.9452,std_correct_num_tokens: 76.8654,avg_incorrect_num_tokens: 93.3217,std_incorrect_num_tokens: 177.4419
2025-07-23 14:03:43.735 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.40s
2025-07-23 14:03:44.947 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.21s
2025-07-23 14:04:11.417 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 205
2025-07-23 14:04:11.417 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.47s
2025-07-23 14:04:12.349 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.93s
2025-07-23 14:04:12.350 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 9.66515139358618e-05, avg_kl: 0.015444425257240853, avg_response_length: 89.64096891821885, avg_orm_score: 0.0, avg_custom_rewards: 9.66515139358618e-05
2025-07-23 14:04:12.380 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter36_replay_buffer.jsonl
2025-07-23 14:04:13.714 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.34s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s, pg=0.0137, ret=-0.000957, glen=84.9, tlen=245, kl=0.0182, act_lr=7.2e-7, ent=1.91]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:00<00:50,  1.01it/s, pg=0.0137, ret=-0.000957, glen=84.9, tlen=245, kl=0.0182, act_lr=7.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:50,  1.01it/s, pg=0.109, ret=-0.00174, glen=90.9, tlen=251, kl=0.0146, act_lr=7.2e-7, ent=2.05]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:45,  1.09it/s, pg=0.109, ret=-0.00174, glen=90.9, tlen=251, kl=0.0146, act_lr=7.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:45,  1.09it/s, pg=-0.115, ret=0.000492, glen=76, tlen=236, kl=0.0174, act_lr=7.2e-7, ent=1.75] Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:44,  1.10it/s, pg=-0.115, ret=0.000492, glen=76, tlen=236, kl=0.0174, act_lr=7.2e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:44,  1.10it/s, pg=-0.166, ret=0.000295, glen=76.5, tlen=237, kl=0.0169, act_lr=7.2e-7, ent=1.97]Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:42,  1.13it/s, pg=-0.166, ret=0.000295, glen=76.5, tlen=237, kl=0.0169, act_lr=7.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:42,  1.13it/s, pg=0.203, ret=-0.000887, glen=103, tlen=264, kl=0.0193, act_lr=7.2e-7, ent=2.44] Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:42,  1.12it/s, pg=0.203, ret=-0.000887, glen=103, tlen=264, kl=0.0193, act_lr=7.2e-7, ent=2.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:42,  1.12it/s, pg=0.0195, ret=0.000161, glen=92, tlen=253, kl=0.0167, act_lr=7.2e-7, ent=2.07] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:40,  1.13it/s, pg=0.0195, ret=0.000161, glen=92, tlen=253, kl=0.0167, act_lr=7.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:40,  1.13it/s, pg=0.0407, ret=-0.000465, glen=89.7, tlen=250, kl=0.015, act_lr=7.2e-7, ent=2.09]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:39,  1.14it/s, pg=0.0407, ret=-0.000465, glen=89.7, tlen=250, kl=0.015, act_lr=7.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:07<00:39,  1.14it/s, pg=0.0243, ret=-0.000941, glen=93.3, tlen=254, kl=0.0132, act_lr=7.2e-7, ent=2.01]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:38,  1.15it/s, pg=0.0243, ret=-0.000941, glen=93.3, tlen=254, kl=0.0132, act_lr=7.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:38,  1.15it/s, pg=0.163, ret=-0.0023, glen=94.5, tlen=255, kl=0.0147, act_lr=7.2e-7, ent=2.17]   Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:07<00:37,  1.14it/s, pg=0.163, ret=-0.0023, glen=94.5, tlen=255, kl=0.0147, act_lr=7.2e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.14it/s, pg=-0.0256, ret=0.000262, glen=91.5, tlen=252, kl=0.0119, act_lr=7.2e-7, ent=2.14]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:36,  1.15it/s, pg=-0.0256, ret=0.000262, glen=91.5, tlen=252, kl=0.0119, act_lr=7.2e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:36,  1.15it/s, pg=-0.0939, ret=0.000425, glen=82.5, tlen=243, kl=0.015, act_lr=7.2e-7, ent=1.86] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:35,  1.16it/s, pg=-0.0939, ret=0.000425, glen=82.5, tlen=243, kl=0.015, act_lr=7.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:35,  1.16it/s, pg=0.0452, ret=0.000186, glen=82, tlen=242, kl=0.014, act_lr=7.2e-7, ent=2.01]   Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.16it/s, pg=0.0452, ret=0.000186, glen=82, tlen=242, kl=0.014, act_lr=7.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.16it/s, pg=0.0358, ret=-0.000814, glen=74.5, tlen=235, kl=0.0163, act_lr=7.2e-7, ent=1.86]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.17it/s, pg=0.0358, ret=-0.000814, glen=74.5, tlen=235, kl=0.0163, act_lr=7.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.17it/s, pg=-0.137, ret=0.00193, glen=74.1, tlen=235, kl=0.0195, act_lr=7.2e-7, ent=1.82]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.17it/s, pg=-0.137, ret=0.00193, glen=74.1, tlen=235, kl=0.0195, act_lr=7.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:13<00:32,  1.17it/s, pg=-0.028, ret=0.000431, glen=83.1, tlen=244, kl=0.0134, act_lr=7.2e-7, ent=2.07]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=-0.028, ret=0.000431, glen=83.1, tlen=244, kl=0.0134, act_lr=7.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=-0.0206, ret=1.86e-5, glen=89.9, tlen=250, kl=0.0152, act_lr=7.2e-7, ent=2.11]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:13<00:30,  1.17it/s, pg=-0.0206, ret=1.86e-5, glen=89.9, tlen=250, kl=0.0152, act_lr=7.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=-0.0851, ret=-0.000314, glen=85.1, tlen=245, kl=0.0169, act_lr=7.2e-7, ent=2.01]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:29,  1.17it/s, pg=-0.0851, ret=-0.000314, glen=85.1, tlen=245, kl=0.0169, act_lr=7.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:29,  1.17it/s, pg=-0.183, ret=0.0014, glen=81.5, tlen=242, kl=0.0166, act_lr=7.2e-7, ent=1.9]     Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:28,  1.17it/s, pg=-0.183, ret=0.0014, glen=81.5, tlen=242, kl=0.0166, act_lr=7.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:28,  1.17it/s, pg=-0.0217, ret=1.64e-5, glen=83.3, tlen=244, kl=0.0159, act_lr=7.2e-7, ent=1.95]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.18it/s, pg=-0.0217, ret=1.64e-5, glen=83.3, tlen=244, kl=0.0159, act_lr=7.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.18it/s, pg=-0.0541, ret=-0.000519, glen=70.1, tlen=231, kl=0.0155, act_lr=7.2e-7, ent=1.86]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.18it/s, pg=-0.0541, ret=-0.000519, glen=70.1, tlen=231, kl=0.0155, act_lr=7.2e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.18it/s, pg=-0.0556, ret=0.000198, glen=88.5, tlen=249, kl=0.014, act_lr=7.2e-7, ent=1.91]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:26,  1.18it/s, pg=-0.0556, ret=0.000198, glen=88.5, tlen=249, kl=0.014, act_lr=7.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:19<00:26,  1.18it/s, pg=0.1, ret=-0.000419, glen=77.5, tlen=238, kl=0.0163, act_lr=7.2e-7, ent=2.24]  Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.18it/s, pg=0.1, ret=-0.000419, glen=77.5, tlen=238, kl=0.0163, act_lr=7.2e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.18it/s, pg=-0.015, ret=0.000107, glen=93.7, tlen=254, kl=0.0162, act_lr=7.2e-7, ent=2.28]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:19<00:25,  1.15it/s, pg=-0.015, ret=0.000107, glen=93.7, tlen=254, kl=0.0162, act_lr=7.2e-7, ent=2.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:25,  1.15it/s, pg=0.0553, ret=0.000407, glen=90.7, tlen=251, kl=0.0179, act_lr=7.2e-7, ent=1.94]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:20<00:24,  1.16it/s, pg=0.0553, ret=0.000407, glen=90.7, tlen=251, kl=0.0179, act_lr=7.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:24,  1.16it/s, pg=-0.0896, ret=0.000454, glen=87.1, tlen=248, kl=0.0153, act_lr=7.2e-7, ent=1.94]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:23,  1.16it/s, pg=-0.0896, ret=0.000454, glen=87.1, tlen=248, kl=0.0153, act_lr=7.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:23,  1.16it/s, pg=0.0874, ret=-8.6e-5, glen=82.1, tlen=242, kl=0.016, act_lr=7.2e-7, ent=1.92]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.17it/s, pg=0.0874, ret=-8.6e-5, glen=82.1, tlen=242, kl=0.016, act_lr=7.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:23<00:22,  1.17it/s, pg=0.456, ret=0.00106, glen=199, tlen=359, kl=0.00858, act_lr=7.2e-7, ent=3.34]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:23<00:24,  1.00it/s, pg=0.456, ret=0.00106, glen=199, tlen=359, kl=0.00858, act_lr=7.2e-7, ent=3.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:24,  1.00it/s, pg=-0.0464, ret=-0.000507, glen=69.7, tlen=230, kl=0.0165, act_lr=7.2e-7, ent=1.83]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:22,  1.05it/s, pg=-0.0464, ret=-0.000507, glen=69.7, tlen=230, kl=0.0165, act_lr=7.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:22,  1.05it/s, pg=-0.153, ret=0.000867, glen=74.8, tlen=235, kl=0.0186, act_lr=7.2e-7, ent=1.87]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:21,  1.08it/s, pg=-0.153, ret=0.000867, glen=74.8, tlen=235, kl=0.0186, act_lr=7.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:21,  1.08it/s, pg=-0.164, ret=0.00108, glen=83.6, tlen=244, kl=0.0123, act_lr=7.2e-7, ent=1.98] Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:19,  1.11it/s, pg=-0.164, ret=0.00108, glen=83.6, tlen=244, kl=0.0123, act_lr=7.2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:27<00:19,  1.11it/s, pg=-0.149, ret=0.00136, glen=85.5, tlen=246, kl=0.0156, act_lr=7.2e-7, ent=1.92]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:18,  1.13it/s, pg=-0.149, ret=0.00136, glen=85.5, tlen=246, kl=0.0156, act_lr=7.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:28<00:18,  1.13it/s, pg=-0.108, ret=-7.83e-5, glen=72.3, tlen=232, kl=0.0178, act_lr=7.2e-7, ent=1.87]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.14it/s, pg=-0.108, ret=-7.83e-5, glen=72.3, tlen=232, kl=0.0178, act_lr=7.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.14it/s, pg=0.138, ret=0.000699, glen=112, tlen=272, kl=0.012, act_lr=7.2e-7, ent=2.27]   Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:28<00:16,  1.13it/s, pg=0.138, ret=0.000699, glen=112, tlen=272, kl=0.012, act_lr=7.2e-7, ent=2.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.13it/s, pg=-0.0526, ret=-0.000158, glen=79.6, tlen=240, kl=0.0191, act_lr=7.2e-7, ent=1.91]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:29<00:15,  1.14it/s, pg=-0.0526, ret=-0.000158, glen=79.6, tlen=240, kl=0.0191, act_lr=7.2e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.14it/s, pg=0.0378, ret=-0.00127, glen=84.2, tlen=245, kl=0.0156, act_lr=7.2e-7, ent=2.07]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.15it/s, pg=0.0378, ret=-0.00127, glen=84.2, tlen=245, kl=0.0156, act_lr=7.2e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.15it/s, pg=-0.0743, ret=0.000411, glen=87.9, tlen=248, kl=0.0165, act_lr=7.2e-7, ent=2.12]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.16it/s, pg=-0.0743, ret=0.000411, glen=87.9, tlen=248, kl=0.0165, act_lr=7.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.16it/s, pg=-0.0525, ret=-0.000338, glen=76.1, tlen=236, kl=0.0161, act_lr=7.2e-7, ent=1.96]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.17it/s, pg=-0.0525, ret=-0.000338, glen=76.1, tlen=236, kl=0.0161, act_lr=7.2e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:33<00:12,  1.17it/s, pg=0.235, ret=0.000836, glen=147, tlen=308, kl=0.0123, act_lr=7.2e-7, ent=2.02]    Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:12,  1.16it/s, pg=0.235, ret=0.000836, glen=147, tlen=308, kl=0.0123, act_lr=7.2e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:34<00:12,  1.16it/s, pg=-0.0904, ret=0.000737, glen=78.6, tlen=239, kl=0.0143, act_lr=7.2e-7, ent=2.03]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.16it/s, pg=-0.0904, ret=0.000737, glen=78.6, tlen=239, kl=0.0143, act_lr=7.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.16it/s, pg=0.129, ret=-0.00129, glen=92.7, tlen=253, kl=0.0138, act_lr=7.2e-7, ent=2.11]  Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:34<00:10,  1.16it/s, pg=0.129, ret=-0.00129, glen=92.7, tlen=253, kl=0.0138, act_lr=7.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.16it/s, pg=-0.0317, ret=0.000227, glen=92.1, tlen=253, kl=0.0147, act_lr=7.2e-7, ent=2.08]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:35<00:09,  1.17it/s, pg=-0.0317, ret=0.000227, glen=92.1, tlen=253, kl=0.0147, act_lr=7.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.17it/s, pg=-0.298, ret=0.000671, glen=107, tlen=268, kl=0.0112, act_lr=7.2e-7, ent=2.24]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:36<00:08,  1.17it/s, pg=-0.298, ret=0.000671, glen=107, tlen=268, kl=0.0112, act_lr=7.2e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.17it/s, pg=0.183, ret=-0.00106, glen=108, tlen=268, kl=0.0132, act_lr=7.2e-7, ent=2.53] Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.17it/s, pg=0.183, ret=-0.00106, glen=108, tlen=268, kl=0.0132, act_lr=7.2e-7, ent=2.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.17it/s, pg=-0.0765, ret=0.000373, glen=78.9, tlen=239, kl=0.02, act_lr=7.2e-7, ent=1.97]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.17it/s, pg=-0.0765, ret=0.000373, glen=78.9, tlen=239, kl=0.02, act_lr=7.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:39<00:06,  1.17it/s, pg=-0.381, ret=0.00339, glen=87.6, tlen=248, kl=0.0155, act_lr=7.2e-7, ent=1.95]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.17it/s, pg=-0.381, ret=0.00339, glen=87.6, tlen=248, kl=0.0155, act_lr=7.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:40<00:05,  1.17it/s, pg=-0.158, ret=0.000296, glen=84.7, tlen=245, kl=0.0145, act_lr=7.2e-7, ent=1.83]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=-0.158, ret=0.000296, glen=84.7, tlen=245, kl=0.0145, act_lr=7.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=0.0906, ret=-0.00148, glen=75.2, tlen=235, kl=0.0214, act_lr=7.2e-7, ent=1.95]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:40<00:04,  1.18it/s, pg=0.0906, ret=-0.00148, glen=75.2, tlen=235, kl=0.0214, act_lr=7.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.18it/s, pg=0.115, ret=-0.00106, glen=81.3, tlen=242, kl=0.016, act_lr=7.2e-7, ent=1.98]  Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:41<00:03,  1.18it/s, pg=0.115, ret=-0.00106, glen=81.3, tlen=242, kl=0.016, act_lr=7.2e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.18it/s, pg=0.00879, ret=-0.000308, glen=85.6, tlen=246, kl=0.0149, act_lr=7.2e-7, ent=1.93]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.17it/s, pg=0.00879, ret=-0.000308, glen=85.6, tlen=246, kl=0.0149, act_lr=7.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.17it/s, pg=0.098, ret=5.91e-5, glen=100, tlen=261, kl=0.015, act_lr=7.2e-7, ent=2.09]      Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.17it/s, pg=0.098, ret=5.91e-5, glen=100, tlen=261, kl=0.015, act_lr=7.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.17it/s, pg=-0.354, ret=0.00102, glen=99.7, tlen=260, kl=0.0137, act_lr=7.2e-7, ent=2.27]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=-0.354, ret=0.00102, glen=99.7, tlen=260, kl=0.0137, act_lr=7.2e-7, ent=2.27]
2025-07-23 14:04:59.210 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 45.29s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.17it/s, pg=0.0383, ret=-0.00116, glen=94.8, tlen=256, kl=0.0125, act_lr=7.4e-7, ent=2.02]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.13it/s, pg=0.0383, ret=-0.00116, glen=94.8, tlen=256, kl=0.0125, act_lr=7.4e-7, ent=2.02]
2025-07-23 14:04:59.869 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 14:05:01.982 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.11s
2025-07-23 14:05:02.305 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.51s
2025-07-23 14:05:02.310 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.01640620598426232, 'actor_lr': 7.203846184193725e-07, 'clip_ratio': 0.0, 'entropy': 2.046571910381317, 'kl': 0.015450844397911659, 'response_length': 89.55942065899188, 'total_length': 249.9918063237117, 'return': 3.316591559828689e-05, 'policy_update_steps': 1.0}
Episode [3/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [38:02<06:31, 195.76s/it]2025-07-23 14:05:02.345 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:07:25.773 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:07:25.958 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:07:25.958 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 143.61s
2025-07-23 14:07:27.736 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0115,avg_pass_at_n: 1.0000,avg_num_tokens: 88.9886,std_num_tokens: 142.9611,avg_correct_num_tokens: 85.2846,std_correct_num_tokens: 89.0005,avg_incorrect_num_tokens: 92.6495,std_incorrect_num_tokens: 181.0577
2025-07-23 14:07:28.175 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.22s
2025-07-23 14:07:29.621 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.44s
2025-07-23 14:07:56.304 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 207
2025-07-23 14:07:56.304 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.68s
2025-07-23 14:07:57.125 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.82s
2025-07-23 14:07:57.125 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0013381140001493862, avg_kl: 0.01549408401268116, avg_response_length: 96.68454237491036, avg_orm_score: 0.0, avg_custom_rewards: -0.0013381140001493862
2025-07-23 14:07:57.159 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter37_replay_buffer.jsonl
2025-07-23 14:07:58.525 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.37s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:01<?, ?it/s, pg=-0.228, ret=0.000266, glen=89.9, tlen=251, kl=0.0135, act_lr=7.4e-7, ent=2.15]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:51,  1.00s/it, pg=-0.228, ret=0.000266, glen=89.9, tlen=251, kl=0.0135, act_lr=7.4e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:51,  1.00s/it, pg=0.16, ret=0.00166, glen=138, tlen=299, kl=0.0137, act_lr=7.4e-7, ent=2.58]    Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:46,  1.07it/s, pg=0.16, ret=0.00166, glen=138, tlen=299, kl=0.0137, act_lr=7.4e-7, ent=2.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:46,  1.07it/s, pg=-0.116, ret=2.46e-5, glen=78.6, tlen=239, kl=0.019, act_lr=7.4e-7, ent=2] Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:45,  1.09it/s, pg=-0.116, ret=2.46e-5, glen=78.6, tlen=239, kl=0.019, act_lr=7.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:45,  1.09it/s, pg=-0.174, ret=0.00115, glen=83.3, tlen=244, kl=0.0181, act_lr=7.4e-7, ent=2.14]Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:43,  1.12it/s, pg=-0.174, ret=0.00115, glen=83.3, tlen=244, kl=0.0181, act_lr=7.4e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:43,  1.12it/s, pg=-0.302, ret=0.000828, glen=94.1, tlen=255, kl=0.0146, act_lr=7.4e-7, ent=2.27]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:41,  1.13it/s, pg=-0.302, ret=0.000828, glen=94.1, tlen=255, kl=0.0146, act_lr=7.4e-7, ent=2.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:41,  1.13it/s, pg=-0.144, ret=0.000867, glen=87, tlen=248, kl=0.0149, act_lr=7.4e-7, ent=2.09]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:40,  1.15it/s, pg=-0.144, ret=0.000867, glen=87, tlen=248, kl=0.0149, act_lr=7.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:40,  1.15it/s, pg=-0.0303, ret=0.000246, glen=92.8, tlen=254, kl=0.0136, act_lr=7.4e-7, ent=2.21]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:39,  1.13it/s, pg=-0.0303, ret=0.000246, glen=92.8, tlen=254, kl=0.0136, act_lr=7.4e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:07<00:39,  1.13it/s, pg=-0.0364, ret=-0.000518, glen=83.2, tlen=245, kl=0.0138, act_lr=7.4e-7, ent=1.97]Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:39,  1.11it/s, pg=-0.0364, ret=-0.000518, glen=83.2, tlen=245, kl=0.0138, act_lr=7.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:08<00:39,  1.11it/s, pg=-0.0464, ret=-0.000288, glen=88.2, tlen=249, kl=0.0203, act_lr=7.4e-7, ent=1.93]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:38,  1.13it/s, pg=-0.0464, ret=-0.000288, glen=88.2, tlen=249, kl=0.0203, act_lr=7.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:38,  1.13it/s, pg=-0.0296, ret=-2.02e-5, glen=115, tlen=276, kl=0.0108, act_lr=7.4e-7, ent=2.44]  Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:36,  1.14it/s, pg=-0.0296, ret=-2.02e-5, glen=115, tlen=276, kl=0.0108, act_lr=7.4e-7, ent=2.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:36,  1.14it/s, pg=-0.042, ret=0.000841, glen=104, tlen=265, kl=0.0141, act_lr=7.4e-7, ent=2.34] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:35,  1.15it/s, pg=-0.042, ret=0.000841, glen=104, tlen=265, kl=0.0141, act_lr=7.4e-7, ent=2.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:35,  1.15it/s, pg=-0.1, ret=0.000639, glen=93.8, tlen=254, kl=0.017, act_lr=7.4e-7, ent=2.2]   Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.16it/s, pg=-0.1, ret=0.000639, glen=93.8, tlen=254, kl=0.017, act_lr=7.4e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.16it/s, pg=-0.091, ret=4.53e-5, glen=76, tlen=237, kl=0.0188, act_lr=7.4e-7, ent=1.87]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.16it/s, pg=-0.091, ret=4.53e-5, glen=76, tlen=237, kl=0.0188, act_lr=7.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.16it/s, pg=0.145, ret=-0.000464, glen=84.7, tlen=246, kl=0.0167, act_lr=7.4e-7, ent=2.1]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.16it/s, pg=0.145, ret=-0.000464, glen=84.7, tlen=246, kl=0.0167, act_lr=7.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:13<00:32,  1.16it/s, pg=-0.0847, ret=-0.000249, glen=87.7, tlen=249, kl=0.0152, act_lr=7.4e-7, ent=1.84]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=-0.0847, ret=-0.000249, glen=87.7, tlen=249, kl=0.0152, act_lr=7.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:14<00:31,  1.17it/s, pg=-0.184, ret=0.00167, glen=84.8, tlen=246, kl=0.0146, act_lr=7.4e-7, ent=1.93]   Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=-0.184, ret=0.00167, glen=84.8, tlen=246, kl=0.0146, act_lr=7.4e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=-0.0899, ret=0.000718, glen=88.7, tlen=250, kl=0.0154, act_lr=7.4e-7, ent=2.1]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:30,  1.15it/s, pg=-0.0899, ret=0.000718, glen=88.7, tlen=250, kl=0.0154, act_lr=7.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:30,  1.15it/s, pg=0.421, ret=-0.0158, glen=456, tlen=617, kl=0.0103, act_lr=7.4e-7, ent=3.1]    Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:30,  1.13it/s, pg=0.421, ret=-0.0158, glen=456, tlen=617, kl=0.0103, act_lr=7.4e-7, ent=3.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:30,  1.13it/s, pg=0.098, ret=9e-5, glen=89.1, tlen=250, kl=0.0209, act_lr=7.4e-7, ent=2.12] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.14it/s, pg=0.098, ret=9e-5, glen=89.1, tlen=250, kl=0.0209, act_lr=7.4e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.14it/s, pg=0.0437, ret=-0.000496, glen=77.8, tlen=239, kl=0.0164, act_lr=7.4e-7, ent=1.94]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.15it/s, pg=0.0437, ret=-0.000496, glen=77.8, tlen=239, kl=0.0164, act_lr=7.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.15it/s, pg=-0.333, ret=0.00198, glen=86.3, tlen=247, kl=0.0166, act_lr=7.4e-7, ent=2.08]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:26,  1.16it/s, pg=-0.333, ret=0.00198, glen=86.3, tlen=247, kl=0.0166, act_lr=7.4e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:19<00:26,  1.16it/s, pg=-0.0806, ret=0.000321, glen=83.8, tlen=245, kl=0.0165, act_lr=7.4e-7, ent=1.9]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:25,  1.16it/s, pg=-0.0806, ret=0.000321, glen=83.8, tlen=245, kl=0.0165, act_lr=7.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:20<00:25,  1.16it/s, pg=-0.121, ret=0.000165, glen=93, tlen=254, kl=0.0151, act_lr=7.4e-7, ent=2.04]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:24,  1.16it/s, pg=-0.121, ret=0.000165, glen=93, tlen=254, kl=0.0151, act_lr=7.4e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:24,  1.16it/s, pg=0.0579, ret=-0.00139, glen=79.2, tlen=240, kl=0.0162, act_lr=7.4e-7, ent=2.05]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:20<00:23,  1.17it/s, pg=0.0579, ret=-0.00139, glen=79.2, tlen=240, kl=0.0162, act_lr=7.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:23,  1.17it/s, pg=-0.0279, ret=-0.000466, glen=91.6, tlen=253, kl=0.0126, act_lr=7.4e-7, ent=2.21]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:23,  1.17it/s, pg=-0.0279, ret=-0.000466, glen=91.6, tlen=253, kl=0.0126, act_lr=7.4e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:23,  1.17it/s, pg=0.065, ret=0.000264, glen=89.7, tlen=250, kl=0.0178, act_lr=7.4e-7, ent=2.12]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.17it/s, pg=0.065, ret=0.000264, glen=89.7, tlen=250, kl=0.0178, act_lr=7.4e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:24<00:22,  1.17it/s, pg=0.181, ret=-0.00153, glen=77.8, tlen=238, kl=0.0163, act_lr=7.4e-7, ent=1.98]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:25,  1.03s/it, pg=0.181, ret=-0.00153, glen=77.8, tlen=238, kl=0.0163, act_lr=7.4e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:25,  1.03s/it, pg=0.092, ret=-0.00143, glen=91.2, tlen=252, kl=0.0138, act_lr=7.4e-7, ent=2.33]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:23,  1.02it/s, pg=0.092, ret=-0.00143, glen=91.2, tlen=252, kl=0.0138, act_lr=7.4e-7, ent=2.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:23,  1.02it/s, pg=0.0619, ret=-0.000339, glen=78.1, tlen=239, kl=0.0208, act_lr=7.4e-7, ent=1.91]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:21,  1.06it/s, pg=0.0619, ret=-0.000339, glen=78.1, tlen=239, kl=0.0208, act_lr=7.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:21,  1.06it/s, pg=0.191, ret=-0.00131, glen=83, tlen=243, kl=0.0176, act_lr=7.4e-7, ent=2.13]    Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:20,  1.09it/s, pg=0.191, ret=-0.00131, glen=83, tlen=243, kl=0.0176, act_lr=7.4e-7, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:27<00:20,  1.09it/s, pg=0.119, ret=-4.56e-5, glen=78.8, tlen=240, kl=0.0194, act_lr=7.4e-7, ent=1.95]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:18,  1.12it/s, pg=0.119, ret=-4.56e-5, glen=78.8, tlen=240, kl=0.0194, act_lr=7.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:28<00:18,  1.12it/s, pg=-0.0664, ret=0.000197, glen=87.6, tlen=249, kl=0.0135, act_lr=7.4e-7, ent=2.02]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.13it/s, pg=-0.0664, ret=0.000197, glen=87.6, tlen=249, kl=0.0135, act_lr=7.4e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:29<00:17,  1.13it/s, pg=0.00342, ret=-0.000421, glen=79.1, tlen=240, kl=0.0168, act_lr=7.4e-7, ent=1.96]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.15it/s, pg=0.00342, ret=-0.000421, glen=79.1, tlen=240, kl=0.0168, act_lr=7.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:30<00:16,  1.15it/s, pg=0.104, ret=-0.00134, glen=80.7, tlen=241, kl=0.0206, act_lr=7.4e-7, ent=1.89]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.15it/s, pg=0.104, ret=-0.00134, glen=80.7, tlen=241, kl=0.0206, act_lr=7.4e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.15it/s, pg=-0.0337, ret=0.000317, glen=87.2, tlen=248, kl=0.014, act_lr=7.4e-7, ent=1.95]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.16it/s, pg=-0.0337, ret=0.000317, glen=87.2, tlen=248, kl=0.014, act_lr=7.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.16it/s, pg=-0.0814, ret=0.000208, glen=95.6, tlen=257, kl=0.0148, act_lr=7.4e-7, ent=2.26]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.16it/s, pg=-0.0814, ret=0.000208, glen=95.6, tlen=257, kl=0.0148, act_lr=7.4e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.16it/s, pg=-0.309, ret=0.00222, glen=80.8, tlen=242, kl=0.0151, act_lr=7.4e-7, ent=1.95]  Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.17it/s, pg=-0.309, ret=0.00222, glen=80.8, tlen=242, kl=0.0151, act_lr=7.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:33<00:12,  1.17it/s, pg=0.13, ret=-0.00144, glen=146, tlen=307, kl=0.0113, act_lr=7.4e-7, ent=2.74]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:12,  1.16it/s, pg=0.13, ret=-0.00144, glen=146, tlen=307, kl=0.0113, act_lr=7.4e-7, ent=2.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:34<00:12,  1.16it/s, pg=-0.131, ret=0.000818, glen=82, tlen=242, kl=0.0134, act_lr=7.4e-7, ent=2.07]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.16it/s, pg=-0.131, ret=0.000818, glen=82, tlen=242, kl=0.0134, act_lr=7.4e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:35<00:11,  1.16it/s, pg=0.0899, ret=-0.000817, glen=108, tlen=268, kl=0.0132, act_lr=7.4e-7, ent=2.55]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.16it/s, pg=0.0899, ret=-0.000817, glen=108, tlen=268, kl=0.0132, act_lr=7.4e-7, ent=2.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:36<00:10,  1.16it/s, pg=-0.104, ret=-0.000298, glen=92.4, tlen=253, kl=0.0139, act_lr=7.4e-7, ent=2.05]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.15it/s, pg=-0.104, ret=-0.000298, glen=92.4, tlen=253, kl=0.0139, act_lr=7.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:37<00:09,  1.15it/s, pg=0.201, ret=-0.00115, glen=81.8, tlen=243, kl=0.0168, act_lr=7.4e-7, ent=2.1]   Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.15it/s, pg=0.201, ret=-0.00115, glen=81.8, tlen=243, kl=0.0168, act_lr=7.4e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.15it/s, pg=-0.123, ret=0.000634, glen=78, tlen=239, kl=0.0205, act_lr=7.4e-7, ent=1.73]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.16it/s, pg=-0.123, ret=0.000634, glen=78, tlen=239, kl=0.0205, act_lr=7.4e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.16it/s, pg=0.206, ret=-0.00256, glen=80, tlen=241, kl=0.0141, act_lr=7.4e-7, ent=2.19] Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.16it/s, pg=0.206, ret=-0.00256, glen=80, tlen=241, kl=0.0141, act_lr=7.4e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:39<00:06,  1.16it/s, pg=-0.0421, ret=-0.000236, glen=81.4, tlen=242, kl=0.0157, act_lr=7.4e-7, ent=2.07]Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.17it/s, pg=-0.0421, ret=-0.000236, glen=81.4, tlen=242, kl=0.0157, act_lr=7.4e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:40<00:05,  1.17it/s, pg=0.201, ret=-0.00201, glen=97.3, tlen=258, kl=0.0132, act_lr=7.4e-7, ent=2.19]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.16it/s, pg=0.201, ret=-0.00201, glen=97.3, tlen=258, kl=0.0132, act_lr=7.4e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:41<00:05,  1.16it/s, pg=-0.185, ret=0.000847, glen=84.4, tlen=245, kl=0.0141, act_lr=7.4e-7, ent=2.15]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.17it/s, pg=-0.185, ret=0.000847, glen=84.4, tlen=245, kl=0.0141, act_lr=7.4e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:42<00:04,  1.17it/s, pg=-0.0325, ret=0.00037, glen=103, tlen=264, kl=0.0141, act_lr=7.4e-7, ent=2.24] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=-0.0325, ret=0.00037, glen=103, tlen=264, kl=0.0141, act_lr=7.4e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=-0.0345, ret=0.000997, glen=89.5, tlen=250, kl=0.0152, act_lr=7.4e-7, ent=2.17]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.17it/s, pg=-0.0345, ret=0.000997, glen=89.5, tlen=250, kl=0.0152, act_lr=7.4e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.17it/s, pg=0.0619, ret=-2.9e-5, glen=95.9, tlen=257, kl=0.0133, act_lr=7.4e-7, ent=2.31]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.17it/s, pg=0.0619, ret=-2.9e-5, glen=95.9, tlen=257, kl=0.0133, act_lr=7.4e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.17it/s, pg=-0.0534, ret=-0.000276, glen=87, tlen=248, kl=0.013, act_lr=7.4e-7, ent=2.15]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=-0.0534, ret=-0.000276, glen=87, tlen=248, kl=0.013, act_lr=7.4e-7, ent=2.15]
2025-07-23 14:08:44.328 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 45.64s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.17it/s, pg=-0.14, ret=0.000869, glen=84.5, tlen=245, kl=0.014, act_lr=7.6e-7, ent=1.93] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.12it/s, pg=-0.14, ret=0.000869, glen=84.5, tlen=245, kl=0.014, act_lr=7.6e-7, ent=1.93]
2025-07-23 14:08:44.986 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-23 14:08:47.302 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.32s
2025-07-23 14:08:47.604 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 49.03s
2025-07-23 14:08:47.610 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.018558768125680778, 'actor_lr': 7.40384633191892e-07, 'clip_ratio': 0.0, 'entropy': 2.128777971634498, 'kl': 0.015477987436147837, 'response_length': 96.66608913128192, 'total_length': 257.48454959575946, 'return': -0.0003004712990122453, 'policy_update_steps': 1.0}
Episode [3/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [41:47<03:24, 204.75s/it]2025-07-23 14:08:47.615 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   1%|          | 1/172 [00:00<00:33,  5.17it/s, est. speed input: 936.42 toks/s, output: 10.35 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:  16%|‚ñà‚ñã        | 28/172 [00:01<00:04, 32.50it/s, est. speed input: 4037.49 toks/s, output: 619.18 toks/s]Processed prompts:  22%|‚ñà‚ñà‚ñè       | 37/172 [00:01<00:02, 47.09it/s, est. speed input: 4935.76 toks/s, output: 826.68 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:03<00:00, 27.11it/s, est. speed input: 7593.06 toks/s, output: 2748.44 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884796)[0m Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/172 [00:03<00:00, 28.15it/s, est. speed input: 7266.32 toks/s, output: 2907.55 toks/s][32m [repeated 87x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:09<00:00,  2.26it/s, est. speed input: 3397.81 toks/s, output: 1575.80 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:09<00:00, 18.72it/s, est. speed input: 3397.81 toks/s, output: 1575.80 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:08<00:00, 21.09it/s, est. speed input: 3823.67 toks/s, output: 1721.87 toks/s][32m [repeated 18x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 171/172 [00:22<00:01,  1.30s/it, est. speed input: 1361.88 toks/s, output: 720.53 toks/s] [32m [repeated 2x across cluster][0m
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:39<00:00,  2.89s/it, est. speed input: 788.12 toks/s, output: 485.28 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:39<00:00,  4.35it/s, est. speed input: 788.12 toks/s, output: 485.28 toks/s]
2025-07-23 14:09:28.062 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 408.6288,strategyqa_test/accuracy: 0.4978,eval_accuracy: 0.4978
2025-07-23 14:09:28.321 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:10:42.975 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:10:43.180 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.20s
2025-07-23 14:10:43.180 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 74.86s
2025-07-23 14:10:44.392 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0163,avg_reflection_pattern_score: 0.0091,avg_pass_at_n: 1.0000,avg_num_tokens: 82.4487,std_num_tokens: 111.7844,avg_correct_num_tokens: 79.0570,std_correct_num_tokens: 75.7836,avg_incorrect_num_tokens: 87.0000,std_incorrect_num_tokens: 146.6999
2025-07-23 14:10:44.689 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 1.51s
2025-07-23 14:10:45.330 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 0.64s
2025-07-23 14:10:59.467 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 105
2025-07-23 14:10:59.468 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 14.14s
2025-07-23 14:11:00.034 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.56s
2025-07-23 14:11:00.035 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0005670206022581884, avg_kl: 0.018662806919642855, avg_response_length: 84.06330773489816, avg_orm_score: 0.0, avg_custom_rewards: 0.0005670206022581884
2025-07-23 14:11:00.064 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter38_replay_buffer.jsonl
2025-07-23 14:11:00.740 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 0.68s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/27 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/27 [00:00<?, ?it/s, pg=0.0915, ret=0.000791, glen=81.9, tlen=242, kl=0.0141, act_lr=7.6e-7, ent=2.09]Actor Train epoch [1/1]:   4%|‚ñé         | 1/27 [00:00<00:24,  1.05it/s, pg=0.0915, ret=0.000791, glen=81.9, tlen=242, kl=0.0141, act_lr=7.6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñé         | 1/27 [00:01<00:24,  1.05it/s, pg=-0.285, ret=0.00148, glen=84.1, tlen=244, kl=0.0191, act_lr=7.6e-7, ent=2.11] Actor Train epoch [1/1]:   7%|‚ñã         | 2/27 [00:01<00:23,  1.07it/s, pg=-0.285, ret=0.00148, glen=84.1, tlen=244, kl=0.0191, act_lr=7.6e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   7%|‚ñã         | 2/27 [00:02<00:23,  1.07it/s, pg=0.105, ret=-7.87e-5, glen=75.5, tlen=236, kl=0.0405, act_lr=7.6e-7, ent=2.01]Actor Train epoch [1/1]:  11%|‚ñà         | 3/27 [00:02<00:21,  1.11it/s, pg=0.105, ret=-7.87e-5, glen=75.5, tlen=236, kl=0.0405, act_lr=7.6e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  11%|‚ñà         | 3/27 [00:03<00:21,  1.11it/s, pg=0.0148, ret=-5.39e-5, glen=72.9, tlen=234, kl=0.0197, act_lr=7.6e-7, ent=1.86]Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 4/27 [00:03<00:20,  1.14it/s, pg=0.0148, ret=-5.39e-5, glen=72.9, tlen=234, kl=0.0197, act_lr=7.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 4/27 [00:04<00:20,  1.14it/s, pg=0.194, ret=-0.000766, glen=87.2, tlen=247, kl=0.0192, act_lr=7.6e-7, ent=2.19]Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 5/27 [00:04<00:19,  1.15it/s, pg=0.194, ret=-0.000766, glen=87.2, tlen=247, kl=0.0192, act_lr=7.6e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñä        | 5/27 [00:05<00:19,  1.15it/s, pg=-0.149, ret=9.2e-5, glen=74.4, tlen=235, kl=0.0197, act_lr=7.6e-7, ent=1.96]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 6/27 [00:05<00:18,  1.15it/s, pg=-0.149, ret=9.2e-5, glen=74.4, tlen=235, kl=0.0197, act_lr=7.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 6/27 [00:06<00:18,  1.15it/s, pg=0.122, ret=-0.00102, glen=80.6, tlen=241, kl=0.0195, act_lr=7.6e-7, ent=2.08]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 7/27 [00:06<00:17,  1.16it/s, pg=0.122, ret=-0.00102, glen=80.6, tlen=241, kl=0.0195, act_lr=7.6e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 7/27 [00:07<00:17,  1.16it/s, pg=0.171, ret=-0.000362, glen=88.3, tlen=248, kl=0.0167, act_lr=7.6e-7, ent=1.96]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 8/27 [00:07<00:16,  1.14it/s, pg=0.171, ret=-0.000362, glen=88.3, tlen=248, kl=0.0167, act_lr=7.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñâ       | 8/27 [00:07<00:16,  1.14it/s, pg=0.344, ret=-0.000115, glen=126, tlen=286, kl=0.0116, act_lr=7.6e-7, ent=2.85] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 9/27 [00:07<00:15,  1.15it/s, pg=0.344, ret=-0.000115, glen=126, tlen=286, kl=0.0116, act_lr=7.6e-7, ent=2.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 9/27 [00:08<00:15,  1.15it/s, pg=-0.0234, ret=0.000414, glen=81.8, tlen=243, kl=0.018, act_lr=7.6e-7, ent=1.92]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 10/27 [00:08<00:14,  1.16it/s, pg=-0.0234, ret=0.000414, glen=81.8, tlen=243, kl=0.018, act_lr=7.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 10/27 [00:09<00:14,  1.16it/s, pg=0.121, ret=-0.00156, glen=76.5, tlen=237, kl=0.0193, act_lr=7.6e-7, ent=1.84] Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 11/27 [00:09<00:14,  1.14it/s, pg=0.121, ret=-0.00156, glen=76.5, tlen=237, kl=0.0193, act_lr=7.6e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 11/27 [00:10<00:14,  1.14it/s, pg=0.0424, ret=-0.000724, glen=85.7, tlen=246, kl=0.0184, act_lr=7.6e-7, ent=2.07]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 12/27 [00:10<00:13,  1.14it/s, pg=0.0424, ret=-0.000724, glen=85.7, tlen=246, kl=0.0184, act_lr=7.6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 12/27 [00:11<00:13,  1.14it/s, pg=-0.0682, ret=-9.68e-5, glen=73.2, tlen=233, kl=0.02, act_lr=7.6e-7, ent=2.04]  Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 13/27 [00:11<00:12,  1.13it/s, pg=-0.0682, ret=-9.68e-5, glen=73.2, tlen=233, kl=0.02, act_lr=7.6e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 13/27 [00:12<00:12,  1.13it/s, pg=-0.241, ret=0.00123, glen=82.7, tlen=243, kl=0.0174, act_lr=7.6e-7, ent=2.16]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 14/27 [00:12<00:11,  1.14it/s, pg=-0.241, ret=0.00123, glen=82.7, tlen=243, kl=0.0174, act_lr=7.6e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 14/27 [00:13<00:11,  1.14it/s, pg=0.0487, ret=-0.000984, glen=86.3, tlen=247, kl=0.0168, act_lr=7.6e-7, ent=1.99]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 15/27 [00:13<00:10,  1.15it/s, pg=0.0487, ret=-0.000984, glen=86.3, tlen=247, kl=0.0168, act_lr=7.6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 15/27 [00:14<00:10,  1.15it/s, pg=0.123, ret=-8.19e-5, glen=122, tlen=282, kl=0.019, act_lr=7.6e-7, ent=2.57]    Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 16/27 [00:14<00:09,  1.15it/s, pg=0.123, ret=-8.19e-5, glen=122, tlen=282, kl=0.019, act_lr=7.6e-7, ent=2.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 16/27 [00:14<00:09,  1.15it/s, pg=0.0125, ret=-0.00029, glen=65.5, tlen=226, kl=0.0229, act_lr=7.6e-7, ent=1.85]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 17/27 [00:14<00:08,  1.15it/s, pg=0.0125, ret=-0.00029, glen=65.5, tlen=226, kl=0.0229, act_lr=7.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 17/27 [00:15<00:08,  1.15it/s, pg=0.12, ret=-0.00125, glen=85.1, tlen=246, kl=0.0219, act_lr=7.6e-7, ent=2.25]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 18/27 [00:15<00:07,  1.16it/s, pg=0.12, ret=-0.00125, glen=85.1, tlen=246, kl=0.0219, act_lr=7.6e-7, ent=2.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 18/27 [00:16<00:07,  1.16it/s, pg=0.175, ret=6.85e-5, glen=90, tlen=250, kl=0.0154, act_lr=7.6e-7, ent=2.19]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 19/27 [00:16<00:06,  1.16it/s, pg=0.175, ret=6.85e-5, glen=90, tlen=250, kl=0.0154, act_lr=7.6e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 19/27 [00:17<00:06,  1.16it/s, pg=0.0489, ret=0.000103, glen=86.4, tlen=247, kl=0.016, act_lr=7.6e-7, ent=2.02]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 20/27 [00:17<00:06,  1.16it/s, pg=0.0489, ret=0.000103, glen=86.4, tlen=247, kl=0.016, act_lr=7.6e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 20/27 [00:18<00:06,  1.16it/s, pg=-0.271, ret=0.000968, glen=76.9, tlen=237, kl=0.0189, act_lr=7.6e-7, ent=1.95]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 21/27 [00:18<00:05,  1.17it/s, pg=-0.271, ret=0.000968, glen=76.9, tlen=237, kl=0.0189, act_lr=7.6e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 21/27 [00:19<00:05,  1.17it/s, pg=-0.189, ret=0.00165, glen=80.9, tlen=241, kl=0.015, act_lr=7.6e-7, ent=1.98]  Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 22/27 [00:19<00:04,  1.17it/s, pg=-0.189, ret=0.00165, glen=80.9, tlen=241, kl=0.015, act_lr=7.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 22/27 [00:20<00:04,  1.17it/s, pg=-0.161, ret=0.000178, glen=84.1, tlen=245, kl=0.0163, act_lr=7.6e-7, ent=2.08]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 23/27 [00:20<00:03,  1.17it/s, pg=-0.161, ret=0.000178, glen=84.1, tlen=245, kl=0.0163, act_lr=7.6e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 23/27 [00:20<00:03,  1.17it/s, pg=-0.111, ret=0.000404, glen=80.4, tlen=241, kl=0.0172, act_lr=7.6e-7, ent=2.03]Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 24/27 [00:20<00:02,  1.17it/s, pg=-0.111, ret=0.000404, glen=80.4, tlen=241, kl=0.0172, act_lr=7.6e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 24/27 [00:21<00:02,  1.17it/s, pg=0.038, ret=-0.000773, glen=76.3, tlen=237, kl=0.0177, act_lr=7.6e-7, ent=1.91]Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 25/27 [00:21<00:01,  1.17it/s, pg=0.038, ret=-0.000773, glen=76.3, tlen=237, kl=0.0177, act_lr=7.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 25/27 [00:22<00:01,  1.17it/s, pg=-0.0206, ret=0.000461, glen=79.6, tlen=240, kl=0.0179, act_lr=7.6e-7, ent=1.9]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 26/27 [00:22<00:00,  1.17it/s, pg=-0.0206, ret=0.000461, glen=79.6, tlen=240, kl=0.0179, act_lr=7.6e-7, ent=1.9]
2025-07-23 14:11:24.407 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 23.49s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 26/27 [00:23<00:00,  1.17it/s, pg=-0.326, ret=0.000699, glen=92.4, tlen=253, kl=0.0151, act_lr=7.8e-7, ent=2.36]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 26/27 [00:23<00:00,  1.11it/s, pg=-0.326, ret=0.000699, glen=92.4, tlen=253, kl=0.0151, act_lr=7.8e-7, ent=2.36]
2025-07-23 14:11:25.084 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 14:11:27.352 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-23 14:11:27.658 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 26.86s
2025-07-23 14:11:27.661 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0026583141750759548, 'actor_lr': 7.607407189459115e-07, 'clip_ratio': 0.0, 'entropy': 2.081821388668484, 'kl': 0.018647370515046297, 'response_length': 84.31563313802083, 'total_length': 244.66721541793257, 'return': 1.5007260016217413e-05, 'policy_update_steps': 1.0}
Episode [3/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [44:27<00:00, 191.21s/it]
2025-07-23 14:11:36.664 | INFO     | orz.ppo.trainer:train:193 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:30:48,261] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(RefRayActorBase pid=886178)[0m [2025-07-23 13:26:59,527] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:34:46,359] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:38:47,153] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:48:57,730] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:52:11,189] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:54:57,761] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 13:58:47,570] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:01:41,310] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:04:59,204] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:08:44,321] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:11:24,401] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:34,485] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:34,662] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1356, num_elems = 7.11B
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,171] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,172] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,179] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,180] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,483] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,483] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 7.91 GB         CA 3.97 GB         Max_CA 45 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,483] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 105.95 GB, percent = 21.0%
[36m(RefRayActorBase pid=885532)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params

Episode [4/20]:   0%|          | 0/13 [00:00<?, ?it/s][AEpisode [3/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [44:36<00:00, 205.89s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,658] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,659] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 2.95 GB         CA 3.97 GB         Max_CA 4 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,659] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 105.95 GB, percent = 21.0%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=885532)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=885532)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=885532)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=885532)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=885532)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=885532)[0m     "profile": false
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "start_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "end_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=885532)[0m     "model_info": null, 
[36m(RefRayActorBase pid=885532)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=885532)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=885532)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=885532)[0m     "fast": true, 
[36m(RefRayActorBase pid=885532)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=885532)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=885532)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=885532)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x764c2c18c3b0>
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=885532)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=885532)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=885532)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=885532)[0m     "detailed": true, 
[36m(RefRayActorBase pid=885532)[0m     "output_file": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=885532)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=885532)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=885532)[0m     "load_path": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=885532)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=885532)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=885532)[0m         "stage": 3, 
[36m(RefRayActorBase pid=885532)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=885532)[0m         "offload_param": {huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=885532)[0m             "pin_memory": true
[36m(RefRayActorBase pid=885532)[0m         }
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "bf16": {
[36m(RefRayActorBase pid=885532)[0m         "enabled": true
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=885532)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=885532)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=885532)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=885532)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-23 14:11:36.938 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:13:57.744 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:13:57.929 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:13:57.930 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 140.99s
2025-07-23 14:13:59.970 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0162,avg_reflection_pattern_score: 0.0082,avg_pass_at_n: 1.0000,avg_num_tokens: 84.1527,std_num_tokens: 148.1700,avg_correct_num_tokens: 78.3167,std_correct_num_tokens: 107.0242,avg_incorrect_num_tokens: 91.4545,std_incorrect_num_tokens: 187.0719
2025-07-23 14:14:00.417 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.49s
2025-07-23 14:14:01.865 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.44s
2025-07-23 14:14:28.671 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 203
2025-07-23 14:14:28.671 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.80s
2025-07-23 14:14:29.518 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.84s
2025-07-23 14:14:29.519 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0006424891481432935, avg_kl: 0.0, avg_response_length: 90.1308398693066, avg_orm_score: 0.0, avg_custom_rewards: -0.0006424891481432935
2025-07-23 14:14:29.568 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter39_replay_buffer.jsonl
2025-07-23 14:14:30.892 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.32s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=-0.154, ret=0.000133, glen=80.9, tlen=241, kl=0, act_lr=7.8e-7, ent=2.11]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:49,  1.01it/s, pg=-0.154, ret=0.000133, glen=80.9, tlen=241, kl=0, act_lr=7.8e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:49,  1.01it/s, pg=-0.0934, ret=0.000663, glen=76.3, tlen=237, kl=0, act_lr=7.8e-7, ent=1.98]Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.09it/s, pg=-0.0934, ret=0.000663, glen=76.3, tlen=237, kl=0, act_lr=7.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.09it/s, pg=-0.078, ret=-0.000357, glen=82.6, tlen=243, kl=0, act_lr=7.8e-7, ent=1.89]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:42,  1.13it/s, pg=-0.078, ret=-0.000357, glen=82.6, tlen=243, kl=0, act_lr=7.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:42,  1.13it/s, pg=-0.0647, ret=0.000937, glen=83.1, tlen=243, kl=0, act_lr=7.8e-7, ent=2.02]Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:41,  1.14it/s, pg=-0.0647, ret=0.000937, glen=83.1, tlen=243, kl=0, act_lr=7.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:41,  1.14it/s, pg=0.0273, ret=-0.000471, glen=76.3, tlen=236, kl=0, act_lr=7.8e-7, ent=1.93]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:39,  1.15it/s, pg=0.0273, ret=-0.000471, glen=76.3, tlen=236, kl=0, act_lr=7.8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:39,  1.15it/s, pg=-0.0518, ret=-0.00016, glen=86.5, tlen=247, kl=0, act_lr=7.8e-7, ent=2.16]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:38,  1.16it/s, pg=-0.0518, ret=-0.00016, glen=86.5, tlen=247, kl=0, act_lr=7.8e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:38,  1.16it/s, pg=0.00354, ret=-0.000742, glen=73.3, tlen=234, kl=0, act_lr=7.8e-7, ent=2.03]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:37,  1.16it/s, pg=0.00354, ret=-0.000742, glen=73.3, tlen=234, kl=0, act_lr=7.8e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:37,  1.16it/s, pg=0.162, ret=-0.00132, glen=110, tlen=270, kl=0, act_lr=7.8e-7, ent=2.17]    Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:06<00:37,  1.16it/s, pg=0.162, ret=-0.00132, glen=110, tlen=270, kl=0, act_lr=7.8e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.16it/s, pg=0.0891, ret=-0.000347, glen=73, tlen=233, kl=0, act_lr=7.8e-7, ent=1.9]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.17it/s, pg=0.0891, ret=-0.000347, glen=73, tlen=233, kl=0, act_lr=7.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.17it/s, pg=-0.063, ret=8.12e-5, glen=75.7, tlen=237, kl=0, act_lr=7.8e-7, ent=1.86]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.17it/s, pg=-0.063, ret=8.12e-5, glen=75.7, tlen=237, kl=0, act_lr=7.8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.17it/s, pg=0.195, ret=-0.00171, glen=93.9, tlen=255, kl=0, act_lr=7.8e-7, ent=2.14]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.17it/s, pg=0.195, ret=-0.00171, glen=93.9, tlen=255, kl=0, act_lr=7.8e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.17it/s, pg=-0.00165, ret=0.000243, glen=71.3, tlen=232, kl=0, act_lr=7.8e-7, ent=1.89]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.17it/s, pg=-0.00165, ret=0.000243, glen=71.3, tlen=232, kl=0, act_lr=7.8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.17it/s, pg=0.129, ret=0.000762, glen=97.1, tlen=257, kl=0, act_lr=7.8e-7, ent=2.36]   Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.17it/s, pg=0.129, ret=0.000762, glen=97.1, tlen=257, kl=0, act_lr=7.8e-7, ent=2.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.17it/s, pg=0.174, ret=-0.000606, glen=76.4, tlen=237, kl=0, act_lr=7.8e-7, ent=2.01]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:31,  1.17it/s, pg=0.174, ret=-0.000606, glen=76.4, tlen=237, kl=0, act_lr=7.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:31,  1.17it/s, pg=-0.147, ret=0.00189, glen=95, tlen=256, kl=0, act_lr=7.8e-7, ent=2.08]   Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:12<00:30,  1.17it/s, pg=-0.147, ret=0.00189, glen=95, tlen=256, kl=0, act_lr=7.8e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:30,  1.17it/s, pg=-0.00998, ret=-0.000232, glen=80, tlen=240, kl=0, act_lr=7.8e-7, ent=1.98]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:29,  1.17it/s, pg=-0.00998, ret=-0.000232, glen=80, tlen=240, kl=0, act_lr=7.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:29,  1.17it/s, pg=0.0902, ret=-0.00121, glen=80.2, tlen=240, kl=0, act_lr=7.8e-7, ent=2.05] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:28,  1.17it/s, pg=0.0902, ret=-0.00121, glen=80.2, tlen=240, kl=0, act_lr=7.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:28,  1.17it/s, pg=-0.109, ret=0.000408, glen=73, tlen=234, kl=0, act_lr=7.8e-7, ent=1.8]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.17it/s, pg=-0.109, ret=0.000408, glen=73, tlen=234, kl=0, act_lr=7.8e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.17it/s, pg=-0.0385, ret=-0.000215, glen=82.6, tlen=243, kl=0, act_lr=7.8e-7, ent=2.01]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.17it/s, pg=-0.0385, ret=-0.000215, glen=82.6, tlen=243, kl=0, act_lr=7.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.17it/s, pg=-0.112, ret=-0.000211, glen=74.3, tlen=235, kl=0, act_lr=7.8e-7, ent=2]    Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=-0.112, ret=-0.000211, glen=74.3, tlen=235, kl=0, act_lr=7.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=0.0791, ret=-0.000596, glen=80.2, tlen=241, kl=0, act_lr=7.8e-7, ent=1.85]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.0791, ret=-0.000596, glen=80.2, tlen=241, kl=0, act_lr=7.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.17, ret=-0.000916, glen=82.5, tlen=243, kl=0, act_lr=7.8e-7, ent=2.09]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:18<00:24,  1.17it/s, pg=0.17, ret=-0.000916, glen=82.5, tlen=243, kl=0, act_lr=7.8e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.0106, ret=-0.000287, glen=85.4, tlen=246, kl=0, act_lr=7.8e-7, ent=1.95]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:19<00:23,  1.17it/s, pg=-0.0106, ret=-0.000287, glen=85.4, tlen=246, kl=0, act_lr=7.8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.0562, ret=0.000586, glen=83.4, tlen=244, kl=0, act_lr=7.8e-7, ent=2.01] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.17it/s, pg=-0.0562, ret=0.000586, glen=83.4, tlen=244, kl=0, act_lr=7.8e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=0.0333, ret=-0.00755, glen=333, tlen=493, kl=0, act_lr=7.8e-7, ent=2.78]  Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.14it/s, pg=0.0333, ret=-0.00755, glen=333, tlen=493, kl=0, act_lr=7.8e-7, ent=2.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.14it/s, pg=0.0115, ret=-0.000245, glen=75.6, tlen=236, kl=0, act_lr=7.8e-7, ent=1.96]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.15it/s, pg=0.0115, ret=-0.000245, glen=75.6, tlen=236, kl=0, act_lr=7.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.15it/s, pg=-0.172, ret=0.00144, glen=94.6, tlen=255, kl=0, act_lr=7.8e-7, ent=2.31]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:24,  1.01s/it, pg=-0.172, ret=0.00144, glen=94.6, tlen=255, kl=0, act_lr=7.8e-7, ent=2.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:24,  1.01s/it, pg=0.311, ret=6.1e-5, glen=119, tlen=279, kl=0, act_lr=7.8e-7, ent=1.75]   Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:22,  1.03it/s, pg=0.311, ret=6.1e-5, glen=119, tlen=279, kl=0, act_lr=7.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:22,  1.03it/s, pg=0.12, ret=-0.000251, glen=84, tlen=244, kl=0, act_lr=7.8e-7, ent=1.99]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.07it/s, pg=0.12, ret=-0.000251, glen=84, tlen=244, kl=0, act_lr=7.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.07it/s, pg=-0.0383, ret=-0.000494, glen=88.6, tlen=249, kl=0, act_lr=7.8e-7, ent=2.07]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:19,  1.10it/s, pg=-0.0383, ret=-0.000494, glen=88.6, tlen=249, kl=0, act_lr=7.8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:19,  1.10it/s, pg=0.0472, ret=-0.000675, glen=81.2, tlen=242, kl=0, act_lr=7.8e-7, ent=1.91] Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.12it/s, pg=0.0472, ret=-0.000675, glen=81.2, tlen=242, kl=0, act_lr=7.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.12it/s, pg=-0.0551, ret=-0.000104, glen=76, tlen=236, kl=0, act_lr=7.8e-7, ent=2.05] Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=-0.0551, ret=-0.000104, glen=76, tlen=236, kl=0, act_lr=7.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=-0.0321, ret=0.000447, glen=84.9, tlen=245, kl=0, act_lr=7.8e-7, ent=1.99]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.14it/s, pg=-0.0321, ret=0.000447, glen=84.9, tlen=245, kl=0, act_lr=7.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.14it/s, pg=-0.151, ret=0.000763, glen=80.9, tlen=241, kl=0, act_lr=7.8e-7, ent=2.16] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.15it/s, pg=-0.151, ret=0.000763, glen=80.9, tlen=241, kl=0, act_lr=7.8e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.15it/s, pg=-0.00503, ret=-0.000609, glen=91, tlen=251, kl=0, act_lr=7.8e-7, ent=2.19]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.16it/s, pg=-0.00503, ret=-0.000609, glen=91, tlen=251, kl=0, act_lr=7.8e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=-0.0143, ret=0.000516, glen=76.5, tlen=237, kl=0, act_lr=7.8e-7, ent=1.92]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=-0.0143, ret=0.000516, glen=76.5, tlen=237, kl=0, act_lr=7.8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=0.0657, ret=-0.00014, glen=79.9, tlen=240, kl=0, act_lr=7.8e-7, ent=2.05] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=0.0657, ret=-0.00014, glen=79.9, tlen=240, kl=0, act_lr=7.8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:11,  1.17it/s, pg=-0.0923, ret=0.000232, glen=76.8, tlen=237, kl=0, act_lr=7.8e-7, ent=2]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=-0.0923, ret=0.000232, glen=76.8, tlen=237, kl=0, act_lr=7.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.17it/s, pg=-6.1e-5, ret=-0.000338, glen=77.6, tlen=238, kl=0, act_lr=7.8e-7, ent=1.96]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-6.1e-5, ret=-0.000338, glen=77.6, tlen=238, kl=0, act_lr=7.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=0.271, ret=0.000488, glen=113, tlen=273, kl=0, act_lr=7.8e-7, ent=2.32]    Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.16it/s, pg=0.271, ret=0.000488, glen=113, tlen=273, kl=0, act_lr=7.8e-7, ent=2.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.16it/s, pg=-0.053, ret=-0.000302, glen=76, tlen=236, kl=0, act_lr=7.8e-7, ent=1.9]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.17it/s, pg=-0.053, ret=-0.000302, glen=76, tlen=236, kl=0, act_lr=7.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=-0.00545, ret=0.000177, glen=84, tlen=245, kl=0, act_lr=7.8e-7, ent=2.16]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.17it/s, pg=-0.00545, ret=0.000177, glen=84, tlen=245, kl=0, act_lr=7.8e-7, ent=2.16]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.17it/s, pg=0.064, ret=-0.000655, glen=85.1, tlen=246, kl=0, act_lr=7.8e-7, ent=2]   Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.17it/s, pg=0.064, ret=-0.000655, glen=85.1, tlen=246, kl=0, act_lr=7.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.17it/s, pg=-0.0381, ret=-0.000118, glen=80.7, tlen=241, kl=0, act_lr=7.8e-7, ent=2.04]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=-0.0381, ret=-0.000118, glen=80.7, tlen=241, kl=0, act_lr=7.8e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.17it/s, pg=-0.0328, ret=0.000755, glen=83.4, tlen=244, kl=0, act_lr=7.8e-7, ent=2.13] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=-0.0328, ret=0.000755, glen=83.4, tlen=244, kl=0, act_lr=7.8e-7, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.18it/s, pg=-0.639, ret=0.00386, glen=150, tlen=311, kl=0, act_lr=7.8e-7, ent=1.71]   Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.16it/s, pg=-0.639, ret=0.00386, glen=150, tlen=311, kl=0, act_lr=7.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.16it/s, pg=0.0123, ret=-0.00029, glen=81.7, tlen=242, kl=0, act_lr=7.8e-7, ent=2.11]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.16it/s, pg=0.0123, ret=-0.00029, glen=81.7, tlen=242, kl=0, act_lr=7.8e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.16it/s, pg=0.0733, ret=0.00053, glen=92.6, tlen=253, kl=0, act_lr=7.8e-7, ent=2.62] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.16it/s, pg=0.0733, ret=0.00053, glen=92.6, tlen=253, kl=0, act_lr=7.8e-7, ent=2.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.16it/s, pg=-0.117, ret=-2.1e-5, glen=85.7, tlen=246, kl=0, act_lr=7.8e-7, ent=2.12]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=-0.117, ret=-2.1e-5, glen=85.7, tlen=246, kl=0, act_lr=7.8e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.0714, ret=-0.000415, glen=89.3, tlen=250, kl=0, act_lr=7.8e-7, ent=2.02]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=-0.0714, ret=-0.000415, glen=89.3, tlen=250, kl=0, act_lr=7.8e-7, ent=2.02]
2025-07-23 14:15:15.490 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.39s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=-0.11, ret=0.00021, glen=82.3, tlen=242, kl=0, act_lr=8e-7, ent=2.14]      Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=-0.11, ret=0.00021, glen=82.3, tlen=242, kl=0, act_lr=8e-7, ent=2.14]
2025-07-23 14:15:16.180 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 14:15:18.468 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-23 14:15:18.858 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.88s
2025-07-23 14:15:18.864 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.009572459202186735, 'actor_lr': 7.803921506570523e-07, 'clip_ratio': 0.0, 'entropy': 2.051248407831379, 'kl': 0.0, 'response_length': 90.11389923095703, 'total_length': 250.50172454235602, 'return': -0.00012561298567829107, 'policy_update_steps': 1.0}

Episode [4/20]:   8%|‚ñä         | 1/13 [03:42<44:26, 222.20s/it][A2025-07-23 14:15:18.938 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 1/172 [00:00<01:10,  2.42it/s, est. speed input: 443.44 toks/s, output: 29.08 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 66/172 [00:01<00:01, 53.33it/s, est. speed input: 6093.13 toks/s, output: 1341.65 toks/s]Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 77/172 [00:02<00:01, 68.43it/s, est. speed input: 6763.40 toks/s, output: 1613.88 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 161/172 [00:03<00:00, 31.34it/s, est. speed input: 7948.29 toks/s, output: 2961.63 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:04<00:00, 17.77it/s, est. speed input: 6683.02 toks/s, output: 2503.76 toks/s][32m [repeated 90x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/172 [00:09<00:00,  2.64it/s, est. speed input: 3280.41 toks/s, output: 1546.76 toks/s][32m [repeated 20x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:10<00:00,  2.47it/s, est. speed input: 2995.96 toks/s, output: 1514.00 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:10<00:00, 16.51it/s, est. speed input: 2995.96 toks/s, output: 1514.00 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:09<00:00, 17.92it/s, est. speed input: 3256.66 toks/s, output: 1407.01 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:21<00:00,  1.59s/it, est. speed input: 1430.75 toks/s, output: 714.52 toks/s] Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:21<00:00,  7.90it/s, est. speed input: 1430.75 toks/s, output: 714.52 toks/s]
2025-07-23 14:15:41.633 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 391.9098,strategyqa_test/accuracy: 0.5051,eval_accuracy: 0.5051
2025-07-23 14:15:41.898 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:18:13.207 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:18:13.387 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:18:13.388 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 151.49s
2025-07-23 14:18:15.598 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0002,avg_repeat_score: 0.0160,avg_reflection_pattern_score: 0.0079,avg_pass_at_n: 1.0000,avg_num_tokens: 85.3691,std_num_tokens: 170.2335,avg_correct_num_tokens: 80.0834,std_correct_num_tokens: 79.3287,avg_incorrect_num_tokens: 91.9923,std_incorrect_num_tokens: 239.4311
2025-07-23 14:18:16.033 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.64s
2025-07-23 14:18:17.237 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.20s
2025-07-23 14:18:43.643 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 204
2025-07-23 14:18:43.644 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.40s
2025-07-23 14:18:44.503 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.86s
2025-07-23 14:18:44.504 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0008331412954801493, avg_kl: 0.0009810994653140798, avg_response_length: 97.03463242100734, avg_orm_score: 0.0, avg_custom_rewards: -0.0008331412954801493
2025-07-23 14:18:44.536 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter40_replay_buffer.jsonl
2025-07-23 14:18:45.853 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.32s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=0.0208, ret=-0.000309, glen=86.3, tlen=247, kl=0.000981, act_lr=8e-7, ent=2.14]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:48,  1.03it/s, pg=0.0208, ret=-0.000309, glen=86.3, tlen=247, kl=0.000981, act_lr=8e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:48,  1.03it/s, pg=0.111, ret=-0.000232, glen=82.7, tlen=242, kl=0.000992, act_lr=8e-7, ent=2.26] Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.10it/s, pg=0.111, ret=-0.000232, glen=82.7, tlen=242, kl=0.000992, act_lr=8e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.10it/s, pg=-0.0486, ret=-0.000271, glen=76, tlen=236, kl=0.00096, act_lr=8e-7, ent=1.9]  Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:43,  1.10it/s, pg=-0.0486, ret=-0.000271, glen=76, tlen=236, kl=0.00096, act_lr=8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:43,  1.10it/s, pg=0.0474, ret=-0.000199, glen=82.4, tlen=242, kl=0.000986, act_lr=8e-7, ent=2]Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:41,  1.13it/s, pg=0.0474, ret=-0.000199, glen=82.4, tlen=242, kl=0.000986, act_lr=8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:41,  1.13it/s, pg=-0.0753, ret=0.000578, glen=77.8, tlen=238, kl=0.000993, act_lr=8e-7, ent=1.92]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.14it/s, pg=-0.0753, ret=0.000578, glen=77.8, tlen=238, kl=0.000993, act_lr=8e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.14it/s, pg=-0.124, ret=0.000699, glen=80.8, tlen=242, kl=0.000965, act_lr=8e-7, ent=1.9]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.15it/s, pg=-0.124, ret=0.000699, glen=80.8, tlen=242, kl=0.000965, act_lr=8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.15it/s, pg=-0.123, ret=0.000144, glen=84.1, tlen=245, kl=0.000998, act_lr=8e-7, ent=1.86]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:37,  1.16it/s, pg=-0.123, ret=0.000144, glen=84.1, tlen=245, kl=0.000998, act_lr=8e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:37,  1.16it/s, pg=-0.0346, ret=-0.00085, glen=88.8, tlen=249, kl=0.00097, act_lr=8e-7, ent=2.17]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.14it/s, pg=-0.0346, ret=-0.00085, glen=88.8, tlen=249, kl=0.00097, act_lr=8e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.14it/s, pg=-0.00781, ret=-0.000755, glen=88.1, tlen=248, kl=0.00101, act_lr=8e-7, ent=2.24]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.15it/s, pg=-0.00781, ret=-0.000755, glen=88.1, tlen=248, kl=0.00101, act_lr=8e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.15it/s, pg=-0.258, ret=0.000858, glen=72.3, tlen=232, kl=0.00105, act_lr=8e-7, ent=1.95]   Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.16it/s, pg=-0.258, ret=0.000858, glen=72.3, tlen=232, kl=0.00105, act_lr=8e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.16it/s, pg=-0.236, ret=0.000526, glen=84.5, tlen=245, kl=0.000962, act_lr=8e-7, ent=1.94]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.16it/s, pg=-0.236, ret=0.000526, glen=84.5, tlen=245, kl=0.000962, act_lr=8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.16it/s, pg=0.0191, ret=-0.000563, glen=76.1, tlen=236, kl=0.00102, act_lr=8e-7, ent=1.87]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.17it/s, pg=0.0191, ret=-0.000563, glen=76.1, tlen=236, kl=0.00102, act_lr=8e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.17it/s, pg=-0.104, ret=0.000558, glen=77.5, tlen=237, kl=0.00102, act_lr=8e-7, ent=2.05] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.17it/s, pg=-0.104, ret=0.000558, glen=77.5, tlen=237, kl=0.00102, act_lr=8e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.17it/s, pg=-0.024, ret=-0.00112, glen=86.6, tlen=247, kl=0.000982, act_lr=8e-7, ent=2.12]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:32,  1.13it/s, pg=-0.024, ret=-0.00112, glen=86.6, tlen=247, kl=0.000982, act_lr=8e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:32,  1.13it/s, pg=0.392, ret=-0.00108, glen=248, tlen=408, kl=0.000817, act_lr=8e-7, ent=3.39]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:32,  1.12it/s, pg=0.392, ret=-0.00108, glen=248, tlen=408, kl=0.000817, act_lr=8e-7, ent=3.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:14<00:32,  1.12it/s, pg=-0.0569, ret=0.000648, glen=79.2, tlen=239, kl=0.000967, act_lr=8e-7, ent=2.1]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.13it/s, pg=-0.0569, ret=0.000648, glen=79.2, tlen=239, kl=0.000967, act_lr=8e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.13it/s, pg=-0.0293, ret=0.000247, glen=82.5, tlen=242, kl=0.000992, act_lr=8e-7, ent=2.02]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.14it/s, pg=-0.0293, ret=0.000247, glen=82.5, tlen=242, kl=0.000992, act_lr=8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.14it/s, pg=-0.208, ret=-1.45e-5, glen=91.7, tlen=252, kl=0.001, act_lr=8e-7, ent=2.02]    Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.15it/s, pg=-0.208, ret=-1.45e-5, glen=91.7, tlen=252, kl=0.001, act_lr=8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.15it/s, pg=-0.0542, ret=0.000535, glen=79.7, tlen=239, kl=0.000998, act_lr=8e-7, ent=2.21]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=-0.0542, ret=0.000535, glen=79.7, tlen=239, kl=0.000998, act_lr=8e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=-0.137, ret=0.000948, glen=82.2, tlen=242, kl=0.00104, act_lr=8e-7, ent=2.18]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.16it/s, pg=-0.137, ret=0.000948, glen=82.2, tlen=242, kl=0.00104, act_lr=8e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.16it/s, pg=-0.174, ret=0.00148, glen=80.1, tlen=241, kl=0.001, act_lr=8e-7, ent=2.03]   Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=-0.174, ret=0.00148, glen=80.1, tlen=241, kl=0.001, act_lr=8e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=-0.136, ret=0.000968, glen=89, tlen=250, kl=0.000994, act_lr=8e-7, ent=2.09]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.136, ret=0.000968, glen=89, tlen=250, kl=0.000994, act_lr=8e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:24,  1.17it/s, pg=0.121, ret=-0.00142, glen=77.5, tlen=238, kl=0.00101, act_lr=8e-7, ent=1.88]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=0.121, ret=-0.00142, glen=77.5, tlen=238, kl=0.00101, act_lr=8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.269, ret=0.000347, glen=95.6, tlen=255, kl=0.000951, act_lr=8e-7, ent=2.48]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.15it/s, pg=-0.269, ret=0.000347, glen=95.6, tlen=255, kl=0.000951, act_lr=8e-7, ent=2.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.15it/s, pg=0.293, ret=-0.000762, glen=384, tlen=544, kl=0.000787, act_lr=8e-7, ent=1.74] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:23,  1.12it/s, pg=0.293, ret=-0.000762, glen=384, tlen=544, kl=0.000787, act_lr=8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:23,  1.12it/s, pg=0.111, ret=-0.00147, glen=77.5, tlen=237, kl=0.000996, act_lr=8e-7, ent=2.07]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.14it/s, pg=0.111, ret=-0.00147, glen=77.5, tlen=237, kl=0.000996, act_lr=8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:24<00:21,  1.14it/s, pg=0.152, ret=-0.0062, glen=330, tlen=491, kl=0.000769, act_lr=8e-7, ent=1.51]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:24,  1.02s/it, pg=0.152, ret=-0.0062, glen=330, tlen=491, kl=0.000769, act_lr=8e-7, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:24,  1.02s/it, pg=0.203, ret=-0.00149, glen=77.2, tlen=237, kl=0.000949, act_lr=8e-7, ent=2.06]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:22,  1.03it/s, pg=0.203, ret=-0.00149, glen=77.2, tlen=237, kl=0.000949, act_lr=8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:22,  1.03it/s, pg=-0.0898, ret=0.000127, glen=83.8, tlen=244, kl=0.000982, act_lr=8e-7, ent=2] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.07it/s, pg=-0.0898, ret=0.000127, glen=83.8, tlen=244, kl=0.000982, act_lr=8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.07it/s, pg=-0.0842, ret=-0.000129, glen=90.8, tlen=251, kl=0.000954, act_lr=8e-7, ent=2.26]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:19,  1.10it/s, pg=-0.0842, ret=-0.000129, glen=90.8, tlen=251, kl=0.000954, act_lr=8e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:19,  1.10it/s, pg=0.107, ret=-0.00115, glen=75.7, tlen=236, kl=0.00106, act_lr=8e-7, ent=1.97]    Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.12it/s, pg=0.107, ret=-0.00115, glen=75.7, tlen=236, kl=0.00106, act_lr=8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.12it/s, pg=-0.237, ret=0.00131, glen=89.7, tlen=250, kl=0.000967, act_lr=8e-7, ent=2]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=-0.237, ret=0.00131, glen=89.7, tlen=250, kl=0.000967, act_lr=8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:29<00:16,  1.14it/s, pg=-0.172, ret=0.000721, glen=81.7, tlen=242, kl=0.000969, act_lr=8e-7, ent=2.03]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=-0.172, ret=0.000721, glen=81.7, tlen=242, kl=0.000969, act_lr=8e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:30<00:15,  1.15it/s, pg=0.142, ret=-0.000169, glen=83.3, tlen=244, kl=0.00099, act_lr=8e-7, ent=2.07] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=0.142, ret=-0.000169, glen=83.3, tlen=244, kl=0.00099, act_lr=8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=-0.0353, ret=-0.00144, glen=81, tlen=241, kl=0.000991, act_lr=8e-7, ent=1.93]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.16it/s, pg=-0.0353, ret=-0.00144, glen=81, tlen=241, kl=0.000991, act_lr=8e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=-0.0354, ret=-7.48e-5, glen=83.7, tlen=244, kl=0.00101, act_lr=8e-7, ent=2.14]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=-0.0354, ret=-7.48e-5, glen=83.7, tlen=244, kl=0.00101, act_lr=8e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=-0.172, ret=0.000428, glen=80.6, tlen=240, kl=0.000988, act_lr=8e-7, ent=2.02]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=-0.172, ret=0.000428, glen=80.6, tlen=240, kl=0.000988, act_lr=8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:11,  1.17it/s, pg=-0.131, ret=-0.000479, glen=81.6, tlen=242, kl=0.000968, act_lr=8e-7, ent=1.98]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=-0.131, ret=-0.000479, glen=81.6, tlen=242, kl=0.000968, act_lr=8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.17it/s, pg=-0.0277, ret=6.04e-5, glen=79.1, tlen=239, kl=0.00105, act_lr=8e-7, ent=1.96]  Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-0.0277, ret=6.04e-5, glen=79.1, tlen=239, kl=0.00105, act_lr=8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:35<00:10,  1.17it/s, pg=0.042, ret=-0.00117, glen=88.7, tlen=249, kl=0.000957, act_lr=8e-7, ent=2.07]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=0.042, ret=-0.00117, glen=88.7, tlen=249, kl=0.000957, act_lr=8e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.00879, ret=0.000743, glen=75.9, tlen=236, kl=0.000935, act_lr=8e-7, ent=1.89]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.18it/s, pg=-0.00879, ret=0.000743, glen=75.9, tlen=236, kl=0.000935, act_lr=8e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.18it/s, pg=-0.201, ret=0.00097, glen=77.3, tlen=238, kl=0.00101, act_lr=8e-7, ent=1.94]    Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.18it/s, pg=-0.201, ret=0.00097, glen=77.3, tlen=238, kl=0.00101, act_lr=8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.18it/s, pg=0.112, ret=-0.000633, glen=93.1, tlen=254, kl=0.00104, act_lr=8e-7, ent=2.11]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.18it/s, pg=0.112, ret=-0.000633, glen=93.1, tlen=254, kl=0.00104, act_lr=8e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.18it/s, pg=-0.006, ret=0.000221, glen=88.5, tlen=249, kl=0.000977, act_lr=8e-7, ent=1.94]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.18it/s, pg=-0.006, ret=0.000221, glen=88.5, tlen=249, kl=0.000977, act_lr=8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.18it/s, pg=-0.24, ret=0.00167, glen=97.1, tlen=258, kl=0.00099, act_lr=8e-7, ent=2.18]   Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=-0.24, ret=0.00167, glen=97.1, tlen=258, kl=0.00099, act_lr=8e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.18it/s, pg=-0.209, ret=0.000974, glen=76.2, tlen=236, kl=0.00102, act_lr=8e-7, ent=1.94]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.18it/s, pg=-0.209, ret=0.000974, glen=76.2, tlen=236, kl=0.00102, act_lr=8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:41<00:04,  1.18it/s, pg=-0.0419, ret=-0.000272, glen=78.9, tlen=239, kl=0.00105, act_lr=8e-7, ent=1.91]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.18it/s, pg=-0.0419, ret=-0.000272, glen=78.9, tlen=239, kl=0.00105, act_lr=8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.18it/s, pg=0.294, ret=-0.000795, glen=96.1, tlen=256, kl=0.000928, act_lr=8e-7, ent=2.45] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=0.294, ret=-0.000795, glen=96.1, tlen=256, kl=0.000928, act_lr=8e-7, ent=2.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=0.133, ret=-0.000139, glen=97.4, tlen=258, kl=0.000978, act_lr=8e-7, ent=2.14]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=0.133, ret=-0.000139, glen=97.4, tlen=258, kl=0.000978, act_lr=8e-7, ent=2.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.26, ret=0.00172, glen=75.7, tlen=236, kl=0.00107, act_lr=8e-7, ent=1.98]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=-0.26, ret=0.00172, glen=75.7, tlen=236, kl=0.00107, act_lr=8e-7, ent=1.98]
2025-07-23 14:19:30.581 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.57s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=0.097, ret=-0.00083, glen=74.3, tlen=234, kl=0.00099, act_lr=8.2e-7, ent=1.97]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.12it/s, pg=0.097, ret=-0.00083, glen=74.3, tlen=234, kl=0.00099, act_lr=8.2e-7, ent=1.97]
2025-07-23 14:19:31.416 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 14:19:34.020 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.60s
2025-07-23 14:19:34.325 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.43s
2025-07-23 14:19:34.331 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0323636298086129, 'actor_lr': 8.003921665227145e-07, 'clip_ratio': 0.0, 'entropy': 2.058533210380405, 'kl': 0.0009810994653140798, 'response_length': 97.03463311288871, 'total_length': 257.1827928131702, 'return': -0.00012853623119036796, 'policy_update_steps': 1.0}

Episode [4/20]:  15%|‚ñà‚ñå        | 2/13 [07:57<44:19, 241.77s/it][A2025-07-23 14:19:34.363 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:22:06.252 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:22:06.439 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:22:06.440 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 152.08s
2025-07-23 14:22:08.276 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0161,avg_reflection_pattern_score: 0.0083,avg_pass_at_n: 1.0000,avg_num_tokens: 86.5212,std_num_tokens: 149.6388,avg_correct_num_tokens: 84.6518,std_correct_num_tokens: 119.3003,avg_incorrect_num_tokens: 88.9332,std_incorrect_num_tokens: 181.4066
2025-07-23 14:22:08.710 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.27s
2025-07-23 14:22:10.145 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.43s
2025-07-23 14:22:36.761 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 205
2025-07-23 14:22:36.761 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.61s
2025-07-23 14:22:37.508 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.75s
2025-07-23 14:22:37.509 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0006922680027101461, avg_kl: 0.0010680873219559833, avg_response_length: 92.95391060433737, avg_orm_score: 0.0, avg_custom_rewards: -0.0006922680027101461
2025-07-23 14:22:37.535 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter41_replay_buffer.jsonl
2025-07-23 14:22:38.869 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.34s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/52 [00:00<?, ?it/s, pg=0.0732, ret=-0.000609, glen=76.2, tlen=237, kl=0.0011, act_lr=8.2e-7, ent=1.95]Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:00<00:49,  1.03it/s, pg=0.0732, ret=-0.000609, glen=76.2, tlen=237, kl=0.0011, act_lr=8.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/52 [00:01<00:49,  1.03it/s, pg=-0.0728, ret=-0.00249, glen=83.1, tlen=244, kl=0.00103, act_lr=8.2e-7, ent=2.19]Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:01<00:45,  1.10it/s, pg=-0.0728, ret=-0.00249, glen=83.1, tlen=244, kl=0.00103, act_lr=8.2e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/52 [00:02<00:45,  1.10it/s, pg=-0.102, ret=0.000682, glen=88.9, tlen=249, kl=0.00103, act_lr=8.2e-7, ent=2.22] Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:02<00:44,  1.10it/s, pg=-0.102, ret=0.000682, glen=88.9, tlen=249, kl=0.00103, act_lr=8.2e-7, ent=2.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/52 [00:03<00:44,  1.10it/s, pg=-0.0327, ret=-0.000188, glen=73.3, tlen=234, kl=0.00112, act_lr=8.2e-7, ent=1.85]Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:03<00:43,  1.10it/s, pg=-0.0327, ret=-0.000188, glen=73.3, tlen=234, kl=0.00112, act_lr=8.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/52 [00:04<00:43,  1.10it/s, pg=-0.0265, ret=-0.000407, glen=80.4, tlen=241, kl=0.00112, act_lr=8.2e-7, ent=2.05]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:04<00:41,  1.12it/s, pg=-0.0265, ret=-0.000407, glen=80.4, tlen=241, kl=0.00112, act_lr=8.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/52 [00:05<00:41,  1.12it/s, pg=-0.145, ret=-0.000132, glen=78.7, tlen=239, kl=0.0011, act_lr=8.2e-7, ent=1.94]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:05<00:40,  1.14it/s, pg=-0.145, ret=-0.000132, glen=78.7, tlen=239, kl=0.0011, act_lr=8.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/52 [00:06<00:40,  1.14it/s, pg=-0.0661, ret=-0.000921, glen=77.8, tlen=238, kl=0.0011, act_lr=8.2e-7, ent=1.95]Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:06<00:39,  1.13it/s, pg=-0.0661, ret=-0.000921, glen=77.8, tlen=238, kl=0.0011, act_lr=8.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  13%|‚ñà‚ñé        | 7/52 [00:07<00:39,  1.13it/s, pg=0.141, ret=-0.000497, glen=100, tlen=260, kl=0.000977, act_lr=8.2e-7, ent=2.44] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:07<00:39,  1.12it/s, pg=0.141, ret=-0.000497, glen=100, tlen=260, kl=0.000977, act_lr=8.2e-7, ent=2.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 8/52 [00:08<00:39,  1.12it/s, pg=-0.142, ret=0.00019, glen=81.2, tlen=241, kl=0.00107, act_lr=8.2e-7, ent=2.03] Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.14it/s, pg=-0.142, ret=0.00019, glen=81.2, tlen=241, kl=0.00107, act_lr=8.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 9/52 [00:08<00:37,  1.14it/s, pg=-0.11, ret=0.00142, glen=80.6, tlen=242, kl=0.00103, act_lr=8.2e-7, ent=1.87] Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:08<00:36,  1.15it/s, pg=-0.11, ret=0.00142, glen=80.6, tlen=242, kl=0.00103, act_lr=8.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 10/52 [00:09<00:36,  1.15it/s, pg=0.113, ret=0.000442, glen=84.2, tlen=245, kl=0.00106, act_lr=8.2e-7, ent=2.12]Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:09<00:35,  1.16it/s, pg=0.113, ret=0.000442, glen=84.2, tlen=245, kl=0.00106, act_lr=8.2e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 11/52 [00:10<00:35,  1.16it/s, pg=-0.0653, ret=-7.21e-5, glen=77, tlen=237, kl=0.0011, act_lr=8.2e-7, ent=1.84] Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:10<00:34,  1.16it/s, pg=-0.0653, ret=-7.21e-5, glen=77, tlen=237, kl=0.0011, act_lr=8.2e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 12/52 [00:11<00:34,  1.16it/s, pg=0.109, ret=-0.000991, glen=86.8, tlen=248, kl=0.00109, act_lr=8.2e-7, ent=1.9]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:11<00:33,  1.17it/s, pg=0.109, ret=-0.000991, glen=86.8, tlen=248, kl=0.00109, act_lr=8.2e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/52 [00:12<00:33,  1.17it/s, pg=0.108, ret=0.000213, glen=101, tlen=262, kl=0.00106, act_lr=8.2e-7, ent=2.32] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:12<00:32,  1.17it/s, pg=0.108, ret=0.000213, glen=101, tlen=262, kl=0.00106, act_lr=8.2e-7, ent=2.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/52 [00:13<00:32,  1.17it/s, pg=0.0513, ret=-0.000651, glen=80.1, tlen=241, kl=0.00108, act_lr=8.2e-7, ent=2.04]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=0.0513, ret=-0.000651, glen=80.1, tlen=241, kl=0.00108, act_lr=8.2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/52 [00:13<00:31,  1.17it/s, pg=-0.0774, ret=-0.000339, glen=78.8, tlen=239, kl=0.00108, act_lr=8.2e-7, ent=1.97]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:13<00:30,  1.17it/s, pg=-0.0774, ret=-0.000339, glen=78.8, tlen=239, kl=0.00108, act_lr=8.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 16/52 [00:14<00:30,  1.17it/s, pg=0.158, ret=-0.00112, glen=92.1, tlen=253, kl=0.00109, act_lr=8.2e-7, ent=2.2]    Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:14<00:29,  1.17it/s, pg=0.158, ret=-0.00112, glen=92.1, tlen=253, kl=0.00109, act_lr=8.2e-7, ent=2.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/52 [00:15<00:29,  1.17it/s, pg=-0.145, ret=-0.000325, glen=75.3, tlen=236, kl=0.00106, act_lr=8.2e-7, ent=1.94]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:15<00:28,  1.17it/s, pg=-0.145, ret=-0.000325, glen=75.3, tlen=236, kl=0.00106, act_lr=8.2e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:16<00:28,  1.17it/s, pg=0.167, ret=-0.000971, glen=97.6, tlen=257, kl=0.00107, act_lr=8.2e-7, ent=2.32] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:16<00:28,  1.17it/s, pg=0.167, ret=-0.000971, glen=97.6, tlen=257, kl=0.00107, act_lr=8.2e-7, ent=2.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/52 [00:17<00:28,  1.17it/s, pg=0.248, ret=0.0013, glen=105, tlen=265, kl=0.000928, act_lr=8.2e-7, ent=1.65]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:17<00:27,  1.16it/s, pg=0.248, ret=0.0013, glen=105, tlen=265, kl=0.000928, act_lr=8.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:18<00:27,  1.16it/s, pg=0.201, ret=-0.0105, glen=345, tlen=506, kl=0.000864, act_lr=8.2e-7, ent=3.54]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:18<00:27,  1.13it/s, pg=0.201, ret=-0.0105, glen=345, tlen=506, kl=0.000864, act_lr=8.2e-7, ent=3.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 21/52 [00:19<00:27,  1.13it/s, pg=-0.0955, ret=0.000179, glen=82.2, tlen=243, kl=0.00114, act_lr=8.2e-7, ent=1.92]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:19<00:26,  1.15it/s, pg=-0.0955, ret=0.000179, glen=82.2, tlen=243, kl=0.00114, act_lr=8.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22/52 [00:20<00:26,  1.15it/s, pg=0.0734, ret=-0.000808, glen=82.7, tlen=243, kl=0.00111, act_lr=8.2e-7, ent=2.08]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:25,  1.15it/s, pg=0.0734, ret=-0.000808, glen=82.7, tlen=243, kl=0.00111, act_lr=8.2e-7, ent=2.08]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:20<00:25,  1.15it/s, pg=0.14, ret=-0.0012, glen=85.5, tlen=246, kl=0.00106, act_lr=8.2e-7, ent=2.03]    Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:20<00:24,  1.16it/s, pg=0.14, ret=-0.0012, glen=85.5, tlen=246, kl=0.00106, act_lr=8.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 24/52 [00:21<00:24,  1.16it/s, pg=0.0125, ret=-0.00069, glen=88.2, tlen=249, kl=0.00109, act_lr=8.2e-7, ent=2.28]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:21<00:23,  1.16it/s, pg=0.0125, ret=-0.00069, glen=88.2, tlen=249, kl=0.00109, act_lr=8.2e-7, ent=2.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:22<00:23,  1.16it/s, pg=0.0252, ret=-0.0006, glen=78, tlen=239, kl=0.00115, act_lr=8.2e-7, ent=2.02]   Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:22<00:22,  1.17it/s, pg=0.0252, ret=-0.0006, glen=78, tlen=239, kl=0.00115, act_lr=8.2e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/52 [00:24<00:22,  1.17it/s, pg=0.108, ret=-0.000154, glen=86.2, tlen=247, kl=0.00105, act_lr=8.2e-7, ent=2.13]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:25,  1.03s/it, pg=0.108, ret=-0.000154, glen=86.2, tlen=247, kl=0.00105, act_lr=8.2e-7, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27/52 [00:24<00:25,  1.03s/it, pg=-0.0701, ret=0.000904, glen=81.5, tlen=242, kl=0.00102, act_lr=8.2e-7, ent=2.04]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:24<00:23,  1.02it/s, pg=-0.0701, ret=0.000904, glen=81.5, tlen=242, kl=0.00102, act_lr=8.2e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:25<00:23,  1.02it/s, pg=-0.0566, ret=0.000675, glen=114, tlen=275, kl=0.00109, act_lr=8.2e-7, ent=2.34] Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:25<00:21,  1.06it/s, pg=-0.0566, ret=0.000675, glen=114, tlen=275, kl=0.00109, act_lr=8.2e-7, ent=2.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29/52 [00:26<00:21,  1.06it/s, pg=-0.18, ret=0.00151, glen=81.3, tlen=242, kl=0.00112, act_lr=8.2e-7, ent=2.09]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:26<00:20,  1.09it/s, pg=-0.18, ret=0.00151, glen=81.3, tlen=242, kl=0.00112, act_lr=8.2e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30/52 [00:27<00:20,  1.09it/s, pg=0.0974, ret=-0.00113, glen=77.9, tlen=239, kl=0.0011, act_lr=8.2e-7, ent=2.11]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:27<00:18,  1.11it/s, pg=0.0974, ret=-0.00113, glen=77.9, tlen=239, kl=0.0011, act_lr=8.2e-7, ent=2.11]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:28<00:18,  1.11it/s, pg=-0.0825, ret=0.00076, glen=83, tlen=243, kl=0.00107, act_lr=8.2e-7, ent=2.02] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:28<00:17,  1.13it/s, pg=-0.0825, ret=0.00076, glen=83, tlen=243, kl=0.00107, act_lr=8.2e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32/52 [00:29<00:17,  1.13it/s, pg=0.0356, ret=-0.000908, glen=80.7, tlen=241, kl=0.00111, act_lr=8.2e-7, ent=1.95]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:29<00:16,  1.15it/s, pg=0.0356, ret=-0.000908, glen=80.7, tlen=241, kl=0.00111, act_lr=8.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:30<00:16,  1.15it/s, pg=0.0831, ret=0.00112, glen=93.7, tlen=254, kl=0.00107, act_lr=8.2e-7, ent=2.26]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.15it/s, pg=0.0831, ret=0.00112, glen=93.7, tlen=254, kl=0.00107, act_lr=8.2e-7, ent=2.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34/52 [00:30<00:15,  1.15it/s, pg=-0.079, ret=0.000403, glen=78.6, tlen=239, kl=0.00112, act_lr=8.2e-7, ent=1.87]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:30<00:14,  1.16it/s, pg=-0.079, ret=0.000403, glen=78.6, tlen=239, kl=0.00112, act_lr=8.2e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35/52 [00:31<00:14,  1.16it/s, pg=-0.13, ret=0.00416, glen=208, tlen=368, kl=0.00093, act_lr=8.2e-7, ent=2.24]   Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:31<00:13,  1.15it/s, pg=-0.13, ret=0.00416, glen=208, tlen=368, kl=0.00093, act_lr=8.2e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:32<00:13,  1.15it/s, pg=-0.103, ret=7.55e-5, glen=78.2, tlen=239, kl=0.00105, act_lr=8.2e-7, ent=2.03]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:32<00:12,  1.16it/s, pg=-0.103, ret=7.55e-5, glen=78.2, tlen=239, kl=0.00105, act_lr=8.2e-7, ent=2.03]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37/52 [00:33<00:12,  1.16it/s, pg=-0.16, ret=0.00108, glen=88.5, tlen=249, kl=0.00106, act_lr=8.2e-7, ent=1.95] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:33<00:12,  1.16it/s, pg=-0.16, ret=0.00108, glen=88.5, tlen=249, kl=0.00106, act_lr=8.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38/52 [00:34<00:12,  1.16it/s, pg=-0.0787, ret=7.19e-5, glen=75.8, tlen=236, kl=0.00114, act_lr=8.2e-7, ent=1.88]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:34<00:11,  1.13it/s, pg=-0.0787, ret=7.19e-5, glen=75.8, tlen=236, kl=0.00114, act_lr=8.2e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39/52 [00:35<00:11,  1.13it/s, pg=-0.18, ret=0.000708, glen=81.9, tlen=242, kl=0.00108, act_lr=8.2e-7, ent=2.06] Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:35<00:10,  1.15it/s, pg=-0.18, ret=0.000708, glen=81.9, tlen=242, kl=0.00108, act_lr=8.2e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:36<00:10,  1.15it/s, pg=0.0281, ret=0.000539, glen=97.2, tlen=258, kl=0.00107, act_lr=8.2e-7, ent=2.05]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.15it/s, pg=0.0281, ret=0.000539, glen=97.2, tlen=258, kl=0.00107, act_lr=8.2e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41/52 [00:36<00:09,  1.15it/s, pg=-0.161, ret=0.00122, glen=84.7, tlen=246, kl=0.00107, act_lr=8.2e-7, ent=2.02] Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:36<00:08,  1.16it/s, pg=-0.161, ret=0.00122, glen=84.7, tlen=246, kl=0.00107, act_lr=8.2e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:37<00:08,  1.16it/s, pg=-0.172, ret=0.00035, glen=101, tlen=262, kl=0.000955, act_lr=8.2e-7, ent=1.82]Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:37<00:07,  1.16it/s, pg=-0.172, ret=0.00035, glen=101, tlen=262, kl=0.000955, act_lr=8.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43/52 [00:38<00:07,  1.16it/s, pg=-0.0942, ret=0.000369, glen=82.4, tlen=243, kl=0.00103, act_lr=8.2e-7, ent=2.01]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:38<00:06,  1.17it/s, pg=-0.0942, ret=0.000369, glen=82.4, tlen=243, kl=0.00103, act_lr=8.2e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44/52 [00:39<00:06,  1.17it/s, pg=0.0695, ret=-0.00094, glen=93.1, tlen=254, kl=0.00109, act_lr=8.2e-7, ent=2.17] Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:39<00:05,  1.17it/s, pg=0.0695, ret=-0.00094, glen=93.1, tlen=254, kl=0.00109, act_lr=8.2e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:40<00:05,  1.17it/s, pg=0.144, ret=0.000314, glen=98.9, tlen=259, kl=0.00105, act_lr=8.2e-7, ent=2.3]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:40<00:05,  1.17it/s, pg=0.144, ret=0.000314, glen=98.9, tlen=259, kl=0.00105, act_lr=8.2e-7, ent=2.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46/52 [00:41<00:05,  1.17it/s, pg=-0.103, ret=0.000419, glen=88.5, tlen=249, kl=0.00106, act_lr=8.2e-7, ent=1.93]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:41<00:04,  1.17it/s, pg=-0.103, ret=0.000419, glen=88.5, tlen=249, kl=0.00106, act_lr=8.2e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:42<00:04,  1.17it/s, pg=-0.0305, ret=-0.000823, glen=84.4, tlen=245, kl=0.00116, act_lr=8.2e-7, ent=1.97]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=-0.0305, ret=-0.000823, glen=84.4, tlen=245, kl=0.00116, act_lr=8.2e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48/52 [00:42<00:03,  1.17it/s, pg=0.00476, ret=-0.00287, glen=85.7, tlen=246, kl=0.00107, act_lr=8.2e-7, ent=1.99] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:42<00:02,  1.17it/s, pg=0.00476, ret=-0.00287, glen=85.7, tlen=246, kl=0.00107, act_lr=8.2e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49/52 [00:43<00:02,  1.17it/s, pg=-0.193, ret=0.00116, glen=78.9, tlen=239, kl=0.00114, act_lr=8.2e-7, ent=1.95]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:43<00:01,  1.17it/s, pg=-0.193, ret=0.00116, glen=78.9, tlen=239, kl=0.00114, act_lr=8.2e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50/52 [00:44<00:01,  1.17it/s, pg=-0.0441, ret=-0.00175, glen=75.9, tlen=236, kl=0.00111, act_lr=8.2e-7, ent=1.95]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:44<00:00,  1.17it/s, pg=-0.0441, ret=-0.00175, glen=75.9, tlen=236, kl=0.00111, act_lr=8.2e-7, ent=1.95]
2025-07-23 14:23:24.590 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 45.56s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.17it/s, pg=-0.183, ret=0.00113, glen=84, tlen=244, kl=0.00104, act_lr=8.4e-7, ent=1.91]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51/52 [00:45<00:00,  1.12it/s, pg=-0.183, ret=0.00113, glen=84, tlen=244, kl=0.00104, act_lr=8.4e-7, ent=1.91]
2025-07-23 14:23:25.449 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 14:23:28.017 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.57s
2025-07-23 14:23:28.349 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 49.44s
2025-07-23 14:23:28.354 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.019004271580622747, 'actor_lr': 8.203846398111217e-07, 'clip_ratio': 0.0, 'entropy': 2.0710484110392056, 'kl': 0.0010681977638831506, 'response_length': 92.76606838519757, 'total_length': 253.27148818969727, 'return': -0.00020517969418366332, 'policy_update_steps': 1.0}

Episode [4/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [11:51<39:42, 238.23s/it][A2025-07-23 14:23:28.386 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:24:28.358 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:24:28.541 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:24:28.542 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 60.16s
2025-07-23 14:24:30.323 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0165,avg_reflection_pattern_score: 0.0090,avg_pass_at_n: 1.0000,avg_num_tokens: 80.5646,std_num_tokens: 79.7096,avg_correct_num_tokens: 78.1450,std_correct_num_tokens: 73.5235,avg_incorrect_num_tokens: 83.0500,std_incorrect_num_tokens: 85.5286
2025-07-23 14:24:30.764 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.22s
2025-07-23 14:24:32.173 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.41s
2025-07-23 14:24:57.708 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 200
2025-07-23 14:24:57.708 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.53s
2025-07-23 14:24:58.458 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.75s
2025-07-23 14:24:58.458 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0001167377657839097, avg_kl: 0.0012756729125976563, avg_response_length: 81.23221733093261, avg_orm_score: 0.0, avg_custom_rewards: 0.0001167377657839097
2025-07-23 14:24:58.489 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter42_replay_buffer.jsonl
2025-07-23 14:24:59.757 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.27s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s, pg=-0.148, ret=0.00128, glen=87, tlen=248, kl=0.00121, act_lr=8.4e-7, ent=2.12]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:00<00:47,  1.03it/s, pg=-0.148, ret=0.00128, glen=87, tlen=248, kl=0.00121, act_lr=8.4e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:47,  1.03it/s, pg=-0.202, ret=0.00159, glen=71.4, tlen=232, kl=0.00135, act_lr=8.4e-7, ent=1.75]Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:43,  1.11it/s, pg=-0.202, ret=0.00159, glen=71.4, tlen=232, kl=0.00135, act_lr=8.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:43,  1.11it/s, pg=-0.12, ret=0.000691, glen=76.5, tlen=237, kl=0.00135, act_lr=8.4e-7, ent=1.9] Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:41,  1.14it/s, pg=-0.12, ret=0.000691, glen=76.5, tlen=237, kl=0.00135, act_lr=8.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:41,  1.14it/s, pg=0.0245, ret=4.6e-5, glen=77.7, tlen=239, kl=0.00126, act_lr=8.4e-7, ent=1.85]Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:41,  1.12it/s, pg=0.0245, ret=4.6e-5, glen=77.7, tlen=239, kl=0.00126, act_lr=8.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:41,  1.12it/s, pg=-0.227, ret=0.000527, glen=80.9, tlen=242, kl=0.00122, act_lr=8.4e-7, ent=1.91]Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:39,  1.13it/s, pg=-0.227, ret=0.000527, glen=80.9, tlen=242, kl=0.00122, act_lr=8.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:39,  1.13it/s, pg=0.264, ret=-0.00114, glen=82.8, tlen=243, kl=0.00126, act_lr=8.4e-7, ent=2.12] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:38,  1.15it/s, pg=0.264, ret=-0.00114, glen=82.8, tlen=243, kl=0.00126, act_lr=8.4e-7, ent=2.12]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:38,  1.15it/s, pg=0.0545, ret=-9.2e-5, glen=80.4, tlen=242, kl=0.00125, act_lr=8.4e-7, ent=1.81]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:37,  1.16it/s, pg=0.0545, ret=-9.2e-5, glen=80.4, tlen=242, kl=0.00125, act_lr=8.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:37,  1.16it/s, pg=0.0173, ret=-0.000633, glen=75.9, tlen=236, kl=0.00132, act_lr=8.4e-7, ent=1.75]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.14it/s, pg=0.0173, ret=-0.000633, glen=75.9, tlen=236, kl=0.00132, act_lr=8.4e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.14it/s, pg=-0.061, ret=-0.000145, glen=76, tlen=237, kl=0.00128, act_lr=8.4e-7, ent=1.88]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:07<00:35,  1.15it/s, pg=-0.061, ret=-0.000145, glen=76, tlen=237, kl=0.00128, act_lr=8.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=0.14, ret=-0.000163, glen=81.6, tlen=242, kl=0.00133, act_lr=8.4e-7, ent=2.06]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.15it/s, pg=0.14, ret=-0.000163, glen=81.6, tlen=242, kl=0.00133, act_lr=8.4e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.15it/s, pg=0.121, ret=-0.00135, glen=68.9, tlen=229, kl=0.00133, act_lr=8.4e-7, ent=1.68]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.16it/s, pg=0.121, ret=-0.00135, glen=68.9, tlen=229, kl=0.00133, act_lr=8.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.16it/s, pg=-0.0223, ret=-0.000388, glen=86.8, tlen=247, kl=0.00127, act_lr=8.4e-7, ent=2.01]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:32,  1.16it/s, pg=-0.0223, ret=-0.000388, glen=86.8, tlen=247, kl=0.00127, act_lr=8.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:32,  1.16it/s, pg=0.0791, ret=-0.00028, glen=76.9, tlen=238, kl=0.00137, act_lr=8.4e-7, ent=1.84]  Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:31,  1.17it/s, pg=0.0791, ret=-0.00028, glen=76.9, tlen=238, kl=0.00137, act_lr=8.4e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:31,  1.17it/s, pg=0.0316, ret=-0.000511, glen=80, tlen=241, kl=0.00121, act_lr=8.4e-7, ent=1.74] Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:30,  1.17it/s, pg=0.0316, ret=-0.000511, glen=80, tlen=241, kl=0.00121, act_lr=8.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:30,  1.17it/s, pg=0.0879, ret=0.000193, glen=75.5, tlen=236, kl=0.00137, act_lr=8.4e-7, ent=1.81]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:29,  1.17it/s, pg=0.0879, ret=0.000193, glen=75.5, tlen=236, kl=0.00137, act_lr=8.4e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:29,  1.17it/s, pg=0.0781, ret=8.78e-5, glen=78.1, tlen=239, kl=0.00136, act_lr=8.4e-7, ent=1.97] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:13<00:29,  1.17it/s, pg=0.0781, ret=8.78e-5, glen=78.1, tlen=239, kl=0.00136, act_lr=8.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.17it/s, pg=0.185, ret=8.29e-5, glen=88.8, tlen=250, kl=0.00119, act_lr=8.4e-7, ent=2.05] Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:14<00:28,  1.17it/s, pg=0.185, ret=8.29e-5, glen=88.8, tlen=250, kl=0.00119, act_lr=8.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.17it/s, pg=-0.0824, ret=0.000469, glen=91.9, tlen=252, kl=0.00135, act_lr=8.4e-7, ent=2.15]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.17it/s, pg=-0.0824, ret=0.000469, glen=91.9, tlen=252, kl=0.00135, act_lr=8.4e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.17it/s, pg=0.0632, ret=-0.000951, glen=79.8, tlen=241, kl=0.00131, act_lr=8.4e-7, ent=1.94]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:27,  1.13it/s, pg=0.0632, ret=-0.000951, glen=79.8, tlen=241, kl=0.00131, act_lr=8.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:27,  1.13it/s, pg=-0.116, ret=0.000638, glen=75.7, tlen=236, kl=0.00126, act_lr=8.4e-7, ent=1.77] Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:26,  1.14it/s, pg=-0.116, ret=0.000638, glen=75.7, tlen=236, kl=0.00126, act_lr=8.4e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:26,  1.14it/s, pg=-0.178, ret=0.000986, glen=82.8, tlen=244, kl=0.00125, act_lr=8.4e-7, ent=1.97]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:25,  1.15it/s, pg=-0.178, ret=0.000986, glen=82.8, tlen=244, kl=0.00125, act_lr=8.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:25,  1.15it/s, pg=-0.112, ret=0.000779, glen=85.4, tlen=246, kl=0.00124, act_lr=8.4e-7, ent=2.01]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:24,  1.16it/s, pg=-0.112, ret=0.000779, glen=85.4, tlen=246, kl=0.00124, act_lr=8.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:20<00:24,  1.16it/s, pg=0.0961, ret=-0.000187, glen=73.4, tlen=234, kl=0.00129, act_lr=8.4e-7, ent=1.8]Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.14it/s, pg=0.0961, ret=-0.000187, glen=73.4, tlen=234, kl=0.00129, act_lr=8.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.14it/s, pg=-0.0713, ret=0.000593, glen=75.8, tlen=236, kl=0.0013, act_lr=8.4e-7, ent=1.91]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.15it/s, pg=-0.0713, ret=0.000593, glen=75.8, tlen=236, kl=0.0013, act_lr=8.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.15it/s, pg=0.117, ret=-0.00125, glen=86.9, tlen=247, kl=0.00128, act_lr=8.4e-7, ent=1.91] Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.16it/s, pg=0.117, ret=-0.00125, glen=86.9, tlen=247, kl=0.00128, act_lr=8.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.16it/s, pg=0.0094, ret=0.000294, glen=79.6, tlen=240, kl=0.00131, act_lr=8.4e-7, ent=2.21]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.16it/s, pg=0.0094, ret=0.000294, glen=79.6, tlen=240, kl=0.00131, act_lr=8.4e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:24<00:20,  1.16it/s, pg=-0.103, ret=0.000377, glen=83.4, tlen=244, kl=0.00124, act_lr=8.4e-7, ent=1.96]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:23,  1.04s/it, pg=-0.103, ret=0.000377, glen=83.4, tlen=244, kl=0.00124, act_lr=8.4e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:23,  1.04s/it, pg=0.175, ret=-0.000692, glen=85.1, tlen=246, kl=0.00132, act_lr=8.4e-7, ent=2]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:21,  1.02it/s, pg=0.175, ret=-0.000692, glen=85.1, tlen=246, kl=0.00132, act_lr=8.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:21,  1.02it/s, pg=0.0976, ret=-0.000952, glen=85.2, tlen=246, kl=0.00125, act_lr=8.4e-7, ent=1.91]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:19,  1.06it/s, pg=0.0976, ret=-0.000952, glen=85.2, tlen=246, kl=0.00125, act_lr=8.4e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:19,  1.06it/s, pg=0.0753, ret=-0.0004, glen=76.9, tlen=237, kl=0.00127, act_lr=8.4e-7, ent=1.83]  Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:18,  1.09it/s, pg=0.0753, ret=-0.0004, glen=76.9, tlen=237, kl=0.00127, act_lr=8.4e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:18,  1.09it/s, pg=0.0334, ret=-0.000159, glen=92, tlen=253, kl=0.00128, act_lr=8.4e-7, ent=2.09]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:17,  1.11it/s, pg=0.0334, ret=-0.000159, glen=92, tlen=253, kl=0.00128, act_lr=8.4e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:28<00:17,  1.11it/s, pg=0.126, ret=0.000106, glen=86.8, tlen=247, kl=0.00126, act_lr=8.4e-7, ent=2.05]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.13it/s, pg=0.126, ret=0.000106, glen=86.8, tlen=247, kl=0.00126, act_lr=8.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:29<00:15,  1.13it/s, pg=0.0731, ret=0.00081, glen=109, tlen=270, kl=0.00116, act_lr=8.4e-7, ent=2.38] Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.14it/s, pg=0.0731, ret=0.00081, glen=109, tlen=270, kl=0.00116, act_lr=8.4e-7, ent=2.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:30<00:14,  1.14it/s, pg=-0.0323, ret=-0.000127, glen=80.4, tlen=242, kl=0.00121, act_lr=8.4e-7, ent=1.76]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:13,  1.15it/s, pg=-0.0323, ret=-0.000127, glen=80.4, tlen=242, kl=0.00121, act_lr=8.4e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:13,  1.15it/s, pg=-0.173, ret=0.00118, glen=77.6, tlen=239, kl=0.00126, act_lr=8.4e-7, ent=1.85]   Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:12,  1.16it/s, pg=-0.173, ret=0.00118, glen=77.6, tlen=239, kl=0.00126, act_lr=8.4e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:12,  1.16it/s, pg=-0.098, ret=-0.000521, glen=85.7, tlen=246, kl=0.00131, act_lr=8.4e-7, ent=1.97]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.16it/s, pg=-0.098, ret=-0.000521, glen=85.7, tlen=246, kl=0.00131, act_lr=8.4e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.16it/s, pg=-0.102, ret=0.000944, glen=80.7, tlen=242, kl=0.00132, act_lr=8.4e-7, ent=1.94] Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.17it/s, pg=-0.102, ret=0.000944, glen=80.7, tlen=242, kl=0.00132, act_lr=8.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.17it/s, pg=-0.0025, ret=-0.000446, glen=87.1, tlen=248, kl=0.00125, act_lr=8.4e-7, ent=2.06]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=-0.0025, ret=-0.000446, glen=87.1, tlen=248, kl=0.00125, act_lr=8.4e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:34<00:10,  1.17it/s, pg=-0.141, ret=0.0019, glen=78.4, tlen=239, kl=0.00128, act_lr=8.4e-7, ent=1.87]    Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=-0.141, ret=0.0019, glen=78.4, tlen=239, kl=0.00128, act_lr=8.4e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:35<00:09,  1.17it/s, pg=-0.168, ret=0.000692, glen=81.7, tlen=242, kl=0.00135, act_lr=8.4e-7, ent=1.95]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.17it/s, pg=-0.168, ret=0.000692, glen=81.7, tlen=242, kl=0.00135, act_lr=8.4e-7, ent=1.95]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.17it/s, pg=0.126, ret=-0.00165, glen=78.5, tlen=239, kl=0.00126, act_lr=8.4e-7, ent=1.86] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:35<00:07,  1.17it/s, pg=0.126, ret=-0.00165, glen=78.5, tlen=239, kl=0.00126, act_lr=8.4e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.17it/s, pg=-0.117, ret=0.000631, glen=88.8, tlen=249, kl=0.00117, act_lr=8.4e-7, ent=1.94]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.17it/s, pg=-0.117, ret=0.000631, glen=88.8, tlen=249, kl=0.00117, act_lr=8.4e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.17it/s, pg=0.0308, ret=-0.000728, glen=73.2, tlen=234, kl=0.00128, act_lr=8.4e-7, ent=1.8]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:05,  1.17it/s, pg=0.0308, ret=-0.000728, glen=73.2, tlen=234, kl=0.00128, act_lr=8.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:05,  1.17it/s, pg=0.0724, ret=-3.04e-5, glen=79.9, tlen=241, kl=0.00128, act_lr=8.4e-7, ent=1.9] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.17it/s, pg=0.0724, ret=-3.04e-5, glen=79.9, tlen=241, kl=0.00128, act_lr=8.4e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.17it/s, pg=0.0446, ret=-0.000746, glen=78.4, tlen=239, kl=0.00115, act_lr=8.4e-7, ent=1.88]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=0.0446, ret=-0.000746, glen=78.4, tlen=239, kl=0.00115, act_lr=8.4e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:40<00:04,  1.17it/s, pg=-0.195, ret=0.00114, glen=76.6, tlen=237, kl=0.00124, act_lr=8.4e-7, ent=2]     Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=-0.195, ret=0.00114, glen=76.6, tlen=237, kl=0.00124, act_lr=8.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:41<00:03,  1.17it/s, pg=-0.166, ret=0.000776, glen=78.6, tlen=240, kl=0.00131, act_lr=8.4e-7, ent=2.05]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.166, ret=0.000776, glen=78.6, tlen=240, kl=0.00131, act_lr=8.4e-7, ent=2.05]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.121, ret=0.00052, glen=76.8, tlen=238, kl=0.00137, act_lr=8.4e-7, ent=2]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:41<00:01,  1.17it/s, pg=-0.121, ret=0.00052, glen=76.8, tlen=238, kl=0.00137, act_lr=8.4e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.17it/s, pg=0.125, ret=-0.00115, glen=70.2, tlen=231, kl=0.00126, act_lr=8.4e-7, ent=1.91]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.17it/s, pg=0.125, ret=-0.00115, glen=70.2, tlen=231, kl=0.00126, act_lr=8.4e-7, ent=1.91]
2025-07-23 14:25:43.632 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 43.72s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.17it/s, pg=0.212, ret=-0.0019, glen=93.7, tlen=254, kl=0.00119, act_lr=8.6e-7, ent=2.32] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.12it/s, pg=0.212, ret=-0.0019, glen=93.7, tlen=254, kl=0.00119, act_lr=8.6e-7, ent=2.32]
2025-07-23 14:25:44.448 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.75s
2025-07-23 14:25:47.038 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.59s
2025-07-23 14:25:47.361 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.56s
2025-07-23 14:25:47.367 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.003985671997070312, 'actor_lr': 8.403999845540966e-07, 'clip_ratio': 0.0, 'entropy': 1.9448373341560363, 'kl': 0.0012756729125976563, 'response_length': 81.23221786499023, 'total_length': 241.93251708984374, 'return': 1.4972883873269893e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [14:10<29:51, 199.06s/it][A2025-07-23 14:25:47.398 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:28:21.904 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:28:22.090 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:28:22.090 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 154.69s
2025-07-23 14:28:24.230 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0004,avg_repeat_score: 0.0161,avg_reflection_pattern_score: 0.0084,avg_pass_at_n: 1.0000,avg_num_tokens: 83.3939,std_num_tokens: 178.5851,avg_correct_num_tokens: 79.1512,std_correct_num_tokens: 69.2115,avg_incorrect_num_tokens: 88.2522,std_incorrect_num_tokens: 250.7636
2025-07-23 14:28:24.670 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.58s
2025-07-23 14:28:26.162 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.49s
2025-07-23 14:28:52.252 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 202
2025-07-23 14:28:52.252 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.09s
2025-07-23 14:28:53.006 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.75s
2025-07-23 14:28:53.006 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -2.27608850241943e-05, avg_kl: 0.0019365442861424813, avg_response_length: 94.99683329138425, avg_orm_score: 0.0, avg_custom_rewards: -2.27608850241943e-05
2025-07-23 14:28:53.035 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter43_replay_buffer.jsonl
2025-07-23 14:28:54.351 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.32s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:01<?, ?it/s, pg=-0.235, ret=0.00062, glen=84.5, tlen=245, kl=0.00208, act_lr=8.6e-7, ent=1.89]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:50,  1.01s/it, pg=-0.235, ret=0.00062, glen=84.5, tlen=245, kl=0.00208, act_lr=8.6e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:50,  1.01s/it, pg=-0.193, ret=-0.000719, glen=82.2, tlen=242, kl=0.00185, act_lr=8.6e-7, ent=1.93]Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:45,  1.08it/s, pg=-0.193, ret=-0.000719, glen=82.2, tlen=242, kl=0.00185, act_lr=8.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:45,  1.08it/s, pg=-0.0302, ret=-0.000268, glen=78.8, tlen=239, kl=0.00202, act_lr=8.6e-7, ent=1.85]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:42,  1.12it/s, pg=-0.0302, ret=-0.000268, glen=78.8, tlen=239, kl=0.00202, act_lr=8.6e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:42,  1.12it/s, pg=0.338, ret=0.00222, glen=300, tlen=461, kl=0.00147, act_lr=8.6e-7, ent=1.73]     Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:42,  1.10it/s, pg=0.338, ret=0.00222, glen=300, tlen=461, kl=0.00147, act_lr=8.6e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:42,  1.10it/s, pg=-0.0157, ret=-0.00141, glen=84.2, tlen=245, kl=0.00173, act_lr=8.6e-7, ent=1.87]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:42,  1.09it/s, pg=-0.0157, ret=-0.00141, glen=84.2, tlen=245, kl=0.00173, act_lr=8.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:42,  1.09it/s, pg=0.0464, ret=-0.00103, glen=94.2, tlen=255, kl=0.0018, act_lr=8.6e-7, ent=2.23]  Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:40,  1.12it/s, pg=0.0464, ret=-0.00103, glen=94.2, tlen=255, kl=0.0018, act_lr=8.6e-7, ent=2.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:40,  1.12it/s, pg=-0.0684, ret=0.000339, glen=102, tlen=263, kl=0.00179, act_lr=8.6e-7, ent=1.62]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:39,  1.10it/s, pg=-0.0684, ret=0.000339, glen=102, tlen=263, kl=0.00179, act_lr=8.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:39,  1.10it/s, pg=-0.116, ret=0.000612, glen=85.5, tlen=246, kl=0.00197, act_lr=8.6e-7, ent=1.93]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:38,  1.12it/s, pg=-0.116, ret=0.000612, glen=85.5, tlen=246, kl=0.00197, act_lr=8.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:08<00:38,  1.12it/s, pg=-0.294, ret=0.00125, glen=79.3, tlen=239, kl=0.0018, act_lr=8.6e-7, ent=1.93]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.14it/s, pg=-0.294, ret=0.00125, glen=79.3, tlen=239, kl=0.0018, act_lr=8.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.14it/s, pg=-0.0667, ret=0.000131, glen=64.1, tlen=225, kl=0.00208, act_lr=8.6e-7, ent=1.76]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.15it/s, pg=-0.0667, ret=0.000131, glen=64.1, tlen=225, kl=0.00208, act_lr=8.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.15it/s, pg=-0.104, ret=3.16e-5, glen=82.5, tlen=243, kl=0.00204, act_lr=8.6e-7, ent=2.07]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.16it/s, pg=-0.104, ret=3.16e-5, glen=82.5, tlen=243, kl=0.00204, act_lr=8.6e-7, ent=2.07]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.16it/s, pg=-0.079, ret=0.000303, glen=74.1, tlen=234, kl=0.00203, act_lr=8.6e-7, ent=1.82]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.16it/s, pg=-0.079, ret=0.000303, glen=74.1, tlen=234, kl=0.00203, act_lr=8.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.16it/s, pg=0.118, ret=-1.87e-6, glen=86.5, tlen=247, kl=0.00185, act_lr=8.6e-7, ent=2.34] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.16it/s, pg=0.118, ret=-1.87e-6, glen=86.5, tlen=247, kl=0.00185, act_lr=8.6e-7, ent=2.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.16it/s, pg=0.414, ret=-0.00299, glen=304, tlen=464, kl=0.00154, act_lr=8.6e-7, ent=3.17] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:32,  1.13it/s, pg=0.414, ret=-0.00299, glen=304, tlen=464, kl=0.00154, act_lr=8.6e-7, ent=3.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:32,  1.13it/s, pg=-0.0513, ret=-0.000515, glen=75.7, tlen=236, kl=0.00221, act_lr=8.6e-7, ent=1.83]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.14it/s, pg=-0.0513, ret=-0.000515, glen=75.7, tlen=236, kl=0.00221, act_lr=8.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:14<00:31,  1.14it/s, pg=-0.0401, ret=-0.000649, glen=81.5, tlen=241, kl=0.00195, act_lr=8.6e-7, ent=1.96]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.15it/s, pg=-0.0401, ret=-0.000649, glen=81.5, tlen=241, kl=0.00195, act_lr=8.6e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.15it/s, pg=-0.112, ret=0.000442, glen=84.9, tlen=245, kl=0.00182, act_lr=8.6e-7, ent=1.82]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.16it/s, pg=-0.112, ret=0.000442, glen=84.9, tlen=245, kl=0.00182, act_lr=8.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.16it/s, pg=-0.119, ret=-1.05e-5, glen=73, tlen=233, kl=0.00201, act_lr=8.6e-7, ent=1.67]  Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.15it/s, pg=-0.119, ret=-1.05e-5, glen=73, tlen=233, kl=0.00201, act_lr=8.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.15it/s, pg=-0.161, ret=0.000195, glen=80.8, tlen=241, kl=0.00201, act_lr=8.6e-7, ent=1.91]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=-0.161, ret=0.000195, glen=80.8, tlen=241, kl=0.00201, act_lr=8.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=-0.206, ret=0.00055, glen=77.6, tlen=238, kl=0.00188, act_lr=8.6e-7, ent=1.79] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:27,  1.14it/s, pg=-0.206, ret=0.00055, glen=77.6, tlen=238, kl=0.00188, act_lr=8.6e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:27,  1.14it/s, pg=-0.09, ret=0.000532, glen=73.2, tlen=234, kl=0.00195, act_lr=8.6e-7, ent=1.77]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:26,  1.15it/s, pg=-0.09, ret=0.000532, glen=73.2, tlen=234, kl=0.00195, act_lr=8.6e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:26,  1.15it/s, pg=-0.0304, ret=0.000651, glen=78, tlen=238, kl=0.00184, act_lr=8.6e-7, ent=1.98]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:25,  1.16it/s, pg=-0.0304, ret=0.000651, glen=78, tlen=238, kl=0.00184, act_lr=8.6e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:25,  1.16it/s, pg=-0.155, ret=0.000741, glen=70.6, tlen=231, kl=0.00208, act_lr=8.6e-7, ent=1.83]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:24,  1.16it/s, pg=-0.155, ret=0.000741, glen=70.6, tlen=231, kl=0.00208, act_lr=8.6e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:21<00:24,  1.16it/s, pg=0.00439, ret=-0.00103, glen=72.4, tlen=233, kl=0.00217, act_lr=8.6e-7, ent=1.78]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=0.00439, ret=-0.00103, glen=72.4, tlen=233, kl=0.00217, act_lr=8.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=0.0811, ret=-0.000985, glen=71.5, tlen=232, kl=0.00218, act_lr=8.6e-7, ent=1.74]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.17it/s, pg=0.0811, ret=-0.000985, glen=71.5, tlen=232, kl=0.00218, act_lr=8.6e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.17it/s, pg=0.119, ret=-0.00104, glen=93.9, tlen=254, kl=0.00177, act_lr=8.6e-7, ent=1.94]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.17it/s, pg=0.119, ret=-0.00104, glen=93.9, tlen=254, kl=0.00177, act_lr=8.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:24<00:21,  1.17it/s, pg=0.0158, ret=-0.000842, glen=87.8, tlen=248, kl=0.00181, act_lr=8.6e-7, ent=2] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:24,  1.03s/it, pg=0.0158, ret=-0.000842, glen=87.8, tlen=248, kl=0.00181, act_lr=8.6e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:25<00:24,  1.03s/it, pg=-0.0541, ret=-0.00117, glen=76.6, tlen=237, kl=0.00187, act_lr=8.6e-7, ent=1.86]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:22,  1.02it/s, pg=-0.0541, ret=-0.00117, glen=76.6, tlen=237, kl=0.00187, act_lr=8.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:22,  1.02it/s, pg=-0.129, ret=0.00034, glen=78.5, tlen=239, kl=0.00211, act_lr=8.6e-7, ent=1.91]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.06it/s, pg=-0.129, ret=0.00034, glen=78.5, tlen=239, kl=0.00211, act_lr=8.6e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.06it/s, pg=-0.0792, ret=-0.000682, glen=75.7, tlen=236, kl=0.00222, act_lr=8.6e-7, ent=1.78]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:19,  1.10it/s, pg=-0.0792, ret=-0.000682, glen=75.7, tlen=236, kl=0.00222, act_lr=8.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:19,  1.10it/s, pg=-0.169, ret=0.000277, glen=74.6, tlen=235, kl=0.00182, act_lr=8.6e-7, ent=1.78]  Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.12it/s, pg=-0.169, ret=0.000277, glen=74.6, tlen=235, kl=0.00182, act_lr=8.6e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.12it/s, pg=-0.0352, ret=-0.000314, glen=80.8, tlen=241, kl=0.00205, act_lr=8.6e-7, ent=1.9]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=-0.0352, ret=-0.000314, glen=80.8, tlen=241, kl=0.00205, act_lr=8.6e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:29<00:16,  1.14it/s, pg=0.0126, ret=-0.000503, glen=93.1, tlen=254, kl=0.00194, act_lr=8.6e-7, ent=2.19]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.14it/s, pg=0.0126, ret=-0.000503, glen=93.1, tlen=254, kl=0.00194, act_lr=8.6e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:30<00:15,  1.14it/s, pg=-0.15, ret=0.00113, glen=86, tlen=246, kl=0.00193, act_lr=8.6e-7, ent=1.94]     Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.15it/s, pg=-0.15, ret=0.00113, glen=86, tlen=246, kl=0.00193, act_lr=8.6e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:31<00:14,  1.15it/s, pg=-0.234, ret=-5.24e-5, glen=76.4, tlen=237, kl=0.00189, act_lr=8.6e-7, ent=1.76]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=-0.234, ret=-5.24e-5, glen=76.4, tlen=237, kl=0.00189, act_lr=8.6e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.16it/s, pg=-0.0481, ret=0.00012, glen=77.3, tlen=237, kl=0.00204, act_lr=8.6e-7, ent=1.87]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.16it/s, pg=-0.0481, ret=0.00012, glen=77.3, tlen=237, kl=0.00204, act_lr=8.6e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.16it/s, pg=-0.202, ret=-0.000326, glen=78.7, tlen=239, kl=0.00191, act_lr=8.6e-7, ent=1.93]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:12,  1.17it/s, pg=-0.202, ret=-0.000326, glen=78.7, tlen=239, kl=0.00191, act_lr=8.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:12,  1.17it/s, pg=-0.0472, ret=0.0013, glen=98.3, tlen=259, kl=0.00176, act_lr=8.6e-7, ent=2.09]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.16it/s, pg=-0.0472, ret=0.0013, glen=98.3, tlen=259, kl=0.00176, act_lr=8.6e-7, ent=2.09]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.16it/s, pg=-0.186, ret=0.000278, glen=83.8, tlen=244, kl=0.00177, act_lr=8.6e-7, ent=1.92]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-0.186, ret=0.000278, glen=83.8, tlen=244, kl=0.00177, act_lr=8.6e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:35<00:10,  1.17it/s, pg=-0.0417, ret=-0.000436, glen=84.9, tlen=245, kl=0.00208, act_lr=8.6e-7, ent=2.21]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.0417, ret=-0.000436, glen=84.9, tlen=245, kl=0.00208, act_lr=8.6e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:36<00:09,  1.17it/s, pg=0.316, ret=0.00162, glen=332, tlen=492, kl=0.00187, act_lr=8.6e-7, ent=2.5]      Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.14it/s, pg=0.316, ret=0.00162, glen=332, tlen=492, kl=0.00187, act_lr=8.6e-7, ent=2.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:37<00:08,  1.14it/s, pg=-0.321, ret=0.00181, glen=87.7, tlen=248, kl=0.00192, act_lr=8.6e-7, ent=1.88]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.15it/s, pg=-0.321, ret=0.00181, glen=87.7, tlen=248, kl=0.00192, act_lr=8.6e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.15it/s, pg=-0.0831, ret=0.000878, glen=86.5, tlen=247, kl=0.00201, act_lr=8.6e-7, ent=2.22]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.15it/s, pg=-0.0831, ret=0.000878, glen=86.5, tlen=247, kl=0.00201, act_lr=8.6e-7, ent=2.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.15it/s, pg=-0.0182, ret=-0.00122, glen=80.4, tlen=241, kl=0.00202, act_lr=8.6e-7, ent=1.86]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:06,  1.16it/s, pg=-0.0182, ret=-0.00122, glen=80.4, tlen=241, kl=0.00202, act_lr=8.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:06,  1.16it/s, pg=0.0947, ret=0.000783, glen=93.5, tlen=254, kl=0.00194, act_lr=8.6e-7, ent=2.19] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.16it/s, pg=0.0947, ret=0.000783, glen=93.5, tlen=254, kl=0.00194, act_lr=8.6e-7, ent=2.19]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.16it/s, pg=-0.00958, ret=-7.09e-5, glen=74.3, tlen=235, kl=0.00198, act_lr=8.6e-7, ent=1.86]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.17it/s, pg=-0.00958, ret=-7.09e-5, glen=74.3, tlen=235, kl=0.00198, act_lr=8.6e-7, ent=1.86]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:41<00:04,  1.17it/s, pg=0.147, ret=-0.00144, glen=76.3, tlen=236, kl=0.00206, act_lr=8.6e-7, ent=1.59]   Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.17it/s, pg=0.147, ret=-0.00144, glen=76.3, tlen=236, kl=0.00206, act_lr=8.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:42<00:03,  1.17it/s, pg=-0.00964, ret=4.31e-5, glen=86.1, tlen=246, kl=0.00186, act_lr=8.6e-7, ent=1.93]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=-0.00964, ret=4.31e-5, glen=86.1, tlen=246, kl=0.00186, act_lr=8.6e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:43<00:02,  1.17it/s, pg=-0.173, ret=0.00105, glen=74.8, tlen=235, kl=0.00197, act_lr=8.6e-7, ent=1.82]  Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.173, ret=0.00105, glen=74.8, tlen=235, kl=0.00197, act_lr=8.6e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.0191, ret=-0.000542, glen=68.2, tlen=228, kl=0.00208, act_lr=8.6e-7, ent=1.82]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=-0.0191, ret=-0.000542, glen=68.2, tlen=228, kl=0.00208, act_lr=8.6e-7, ent=1.82]
2025-07-23 14:29:39.346 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.83s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=-0.0798, ret=0.000252, glen=85.8, tlen=246, kl=0.00187, act_lr=8.8e-7, ent=1.93] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.12it/s, pg=-0.0798, ret=0.000252, glen=85.8, tlen=246, kl=0.00187, act_lr=8.8e-7, ent=1.93]
2025-07-23 14:29:40.038 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 14:29:42.487 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.45s
2025-07-23 14:29:42.799 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 48.40s
2025-07-23 14:29:42.805 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.049933489631204045, 'actor_lr': 8.60392157276282e-07, 'clip_ratio': 0.0, 'entropy': 1.9397575925378239, 'kl': 0.0019356222713694854, 'response_length': 94.79484139236749, 'total_length': 255.06157729204963, 'return': 4.829796179204577e-06, 'policy_update_steps': 1.0}

Episode [4/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [18:06<28:17, 212.18s/it][A2025-07-23 14:29:42.837 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:31:06.100 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:31:06.283 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:31:06.283 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 83.45s
2025-07-23 14:31:08.256 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0166,avg_reflection_pattern_score: 0.0076,avg_pass_at_n: 1.0000,avg_num_tokens: 77.7434,std_num_tokens: 106.6362,avg_correct_num_tokens: 77.5875,std_correct_num_tokens: 99.3379,avg_incorrect_num_tokens: 77.9267,std_incorrect_num_tokens: 114.6245
2025-07-23 14:31:08.571 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.29s
2025-07-23 14:31:09.989 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.42s
2025-07-23 14:31:35.384 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 198
2025-07-23 14:31:35.385 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.39s
2025-07-23 14:31:36.136 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.75s
2025-07-23 14:31:36.136 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0007023435001346198, avg_kl: 0.003980232007575758, avg_response_length: 79.00234983424947, avg_orm_score: 0.0, avg_custom_rewards: 0.0007023435001346198
2025-07-23 14:31:36.165 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter44_replay_buffer.jsonl
2025-07-23 14:31:37.403 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.24s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s, pg=-0.0916, ret=-0.00183, glen=95.6, tlen=256, kl=0.00339, act_lr=8.8e-7, ent=2.02]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:00<00:47,  1.02it/s, pg=-0.0916, ret=-0.00183, glen=95.6, tlen=256, kl=0.00339, act_lr=8.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:47,  1.02it/s, pg=0.178, ret=9.78e-5, glen=84.3, tlen=245, kl=0.00446, act_lr=8.8e-7, ent=2.02]   Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:43,  1.10it/s, pg=0.178, ret=9.78e-5, glen=84.3, tlen=245, kl=0.00446, act_lr=8.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:43,  1.10it/s, pg=0.0676, ret=0.000379, glen=70.7, tlen=231, kl=0.00414, act_lr=8.8e-7, ent=1.85]Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:42,  1.10it/s, pg=0.0676, ret=0.000379, glen=70.7, tlen=231, kl=0.00414, act_lr=8.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:42,  1.10it/s, pg=-0.0199, ret=-5.6e-5, glen=73.7, tlen=234, kl=0.00418, act_lr=8.8e-7, ent=1.82]Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:40,  1.12it/s, pg=-0.0199, ret=-5.6e-5, glen=73.7, tlen=234, kl=0.00418, act_lr=8.8e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:40,  1.12it/s, pg=0.206, ret=-0.00227, glen=85.1, tlen=246, kl=0.00372, act_lr=8.8e-7, ent=2.15] Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:40,  1.11it/s, pg=0.206, ret=-0.00227, glen=85.1, tlen=246, kl=0.00372, act_lr=8.8e-7, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:40,  1.11it/s, pg=-0.072, ret=-0.000224, glen=77.7, tlen=238, kl=0.004, act_lr=8.8e-7, ent=1.98]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:39,  1.13it/s, pg=-0.072, ret=-0.000224, glen=77.7, tlen=238, kl=0.004, act_lr=8.8e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:39,  1.13it/s, pg=0.0169, ret=0.000215, glen=74.9, tlen=236, kl=0.00377, act_lr=8.8e-7, ent=1.81]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:37,  1.14it/s, pg=0.0169, ret=0.000215, glen=74.9, tlen=236, kl=0.00377, act_lr=8.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:37,  1.14it/s, pg=0.16, ret=-0.0011, glen=71.1, tlen=232, kl=0.00409, act_lr=8.8e-7, ent=1.74]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.15it/s, pg=0.16, ret=-0.0011, glen=71.1, tlen=232, kl=0.00409, act_lr=8.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:08<00:36,  1.15it/s, pg=0.000793, ret=-0.000207, glen=75.7, tlen=237, kl=0.00368, act_lr=8.8e-7, ent=1.74]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:36,  1.13it/s, pg=0.000793, ret=-0.000207, glen=75.7, tlen=237, kl=0.00368, act_lr=8.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:36,  1.13it/s, pg=-0.0682, ret=0.000741, glen=68.3, tlen=229, kl=0.00436, act_lr=8.8e-7, ent=1.77]  Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.14it/s, pg=-0.0682, ret=0.000741, glen=68.3, tlen=229, kl=0.00436, act_lr=8.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.14it/s, pg=-0.0891, ret=0.00104, glen=94.2, tlen=255, kl=0.00378, act_lr=8.8e-7, ent=2.02] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.15it/s, pg=-0.0891, ret=0.00104, glen=94.2, tlen=255, kl=0.00378, act_lr=8.8e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.15it/s, pg=0.0338, ret=-0.000469, glen=74.8, tlen=235, kl=0.00386, act_lr=8.8e-7, ent=1.84]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:32,  1.16it/s, pg=0.0338, ret=-0.000469, glen=74.8, tlen=235, kl=0.00386, act_lr=8.8e-7, ent=1.84]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:32,  1.16it/s, pg=-0.0214, ret=-0.000969, glen=94.1, tlen=255, kl=0.00375, act_lr=8.8e-7, ent=1.53]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:32,  1.15it/s, pg=-0.0214, ret=-0.000969, glen=94.1, tlen=255, kl=0.00375, act_lr=8.8e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:32,  1.15it/s, pg=0.16, ret=0.000929, glen=82.1, tlen=242, kl=0.004, act_lr=8.8e-7, ent=2.17]      Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:31,  1.16it/s, pg=0.16, ret=0.000929, glen=82.1, tlen=242, kl=0.004, act_lr=8.8e-7, ent=2.17]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:31,  1.16it/s, pg=-0.0895, ret=0.00038, glen=71.7, tlen=232, kl=0.00412, act_lr=8.8e-7, ent=1.67]Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:30,  1.17it/s, pg=-0.0895, ret=0.00038, glen=71.7, tlen=232, kl=0.00412, act_lr=8.8e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:14<00:30,  1.17it/s, pg=-0.191, ret=0.0002, glen=102, tlen=262, kl=0.00345, act_lr=8.8e-7, ent=2.24]   Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.16it/s, pg=-0.191, ret=0.0002, glen=102, tlen=262, kl=0.00345, act_lr=8.8e-7, ent=2.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.16it/s, pg=0.167, ret=0.000115, glen=77.8, tlen=238, kl=0.00366, act_lr=8.8e-7, ent=1.77]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:14<00:28,  1.17it/s, pg=0.167, ret=0.000115, glen=77.8, tlen=238, kl=0.00366, act_lr=8.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.17it/s, pg=-0.171, ret=0.00121, glen=86.1, tlen=247, kl=0.00347, act_lr=8.8e-7, ent=1.99]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.17it/s, pg=-0.171, ret=0.00121, glen=86.1, tlen=247, kl=0.00347, act_lr=8.8e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.17it/s, pg=-0.0181, ret=0.000131, glen=70.9, tlen=232, kl=0.00448, act_lr=8.8e-7, ent=1.74]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:26,  1.17it/s, pg=-0.0181, ret=0.000131, glen=70.9, tlen=232, kl=0.00448, act_lr=8.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:26,  1.17it/s, pg=-0.0231, ret=0.000496, glen=68.7, tlen=229, kl=0.00415, act_lr=8.8e-7, ent=1.69]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:25,  1.17it/s, pg=-0.0231, ret=0.000496, glen=68.7, tlen=229, kl=0.00415, act_lr=8.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:25,  1.17it/s, pg=-0.0307, ret=0.000245, glen=74.4, tlen=235, kl=0.00393, act_lr=8.8e-7, ent=1.81]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:24,  1.17it/s, pg=-0.0307, ret=0.000245, glen=74.4, tlen=235, kl=0.00393, act_lr=8.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:24,  1.17it/s, pg=0.0397, ret=-0.000333, glen=69.4, tlen=230, kl=0.00459, act_lr=8.8e-7, ent=1.81]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:23,  1.18it/s, pg=0.0397, ret=-0.000333, glen=69.4, tlen=230, kl=0.00459, act_lr=8.8e-7, ent=1.81]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:23,  1.18it/s, pg=-0.103, ret=0.0015, glen=69, tlen=230, kl=0.00431, act_lr=8.8e-7, ent=1.69]     Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:19<00:22,  1.17it/s, pg=-0.103, ret=0.0015, glen=69, tlen=230, kl=0.00431, act_lr=8.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:22,  1.17it/s, pg=0.0642, ret=-0.000514, glen=76.8, tlen=237, kl=0.00409, act_lr=8.8e-7, ent=1.77]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.17it/s, pg=0.0642, ret=-0.000514, glen=76.8, tlen=237, kl=0.00409, act_lr=8.8e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.17it/s, pg=-0.152, ret=0.00381, glen=81.9, tlen=243, kl=0.00402, act_lr=8.8e-7, ent=2.18]  Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.15it/s, pg=-0.152, ret=0.00381, glen=81.9, tlen=243, kl=0.00402, act_lr=8.8e-7, ent=2.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.15it/s, pg=-0.0467, ret=0.000831, glen=74.1, tlen=234, kl=0.00379, act_lr=8.8e-7, ent=1.74]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.16it/s, pg=-0.0467, ret=0.000831, glen=74.1, tlen=234, kl=0.00379, act_lr=8.8e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:24<00:20,  1.16it/s, pg=0.0324, ret=0.000576, glen=75.1, tlen=236, kl=0.00412, act_lr=8.8e-7, ent=1.76] Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:23,  1.04s/it, pg=0.0324, ret=0.000576, glen=75.1, tlen=236, kl=0.00412, act_lr=8.8e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:23,  1.04s/it, pg=-0.0494, ret=0.000506, glen=75.1, tlen=235, kl=0.00407, act_lr=8.8e-7, ent=1.88]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:21,  1.01it/s, pg=-0.0494, ret=0.000506, glen=75.1, tlen=235, kl=0.00407, act_lr=8.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:21,  1.01it/s, pg=0.0862, ret=-0.000909, glen=69, tlen=230, kl=0.00395, act_lr=8.8e-7, ent=1.69]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:19,  1.06it/s, pg=0.0862, ret=-0.000909, glen=69, tlen=230, kl=0.00395, act_lr=8.8e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:19,  1.06it/s, pg=0.022, ret=-0.000251, glen=73, tlen=233, kl=0.00454, act_lr=8.8e-7, ent=1.85] Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:18,  1.09it/s, pg=0.022, ret=-0.000251, glen=73, tlen=233, kl=0.00454, act_lr=8.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:18,  1.09it/s, pg=0.0968, ret=-0.000194, glen=105, tlen=265, kl=0.00333, act_lr=8.8e-7, ent=2.63]Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:17,  1.11it/s, pg=0.0968, ret=-0.000194, glen=105, tlen=265, kl=0.00333, act_lr=8.8e-7, ent=2.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:28<00:17,  1.11it/s, pg=-0.0309, ret=0.001, glen=82.1, tlen=243, kl=0.00386, act_lr=8.8e-7, ent=1.96]  Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.13it/s, pg=-0.0309, ret=0.001, glen=82.1, tlen=243, kl=0.00386, act_lr=8.8e-7, ent=1.96]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:29<00:15,  1.13it/s, pg=-0.0014, ret=0.000325, glen=88.7, tlen=249, kl=0.00344, act_lr=8.8e-7, ent=1.91]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:15,  1.11it/s, pg=-0.0014, ret=0.000325, glen=88.7, tlen=249, kl=0.00344, act_lr=8.8e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:30<00:15,  1.11it/s, pg=0.0153, ret=-0.000502, glen=76.9, tlen=238, kl=0.00398, act_lr=8.8e-7, ent=1.85]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:14,  1.13it/s, pg=0.0153, ret=-0.000502, glen=76.9, tlen=238, kl=0.00398, act_lr=8.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:14,  1.13it/s, pg=-0.0274, ret=-0.000101, glen=72.7, tlen=233, kl=0.0045, act_lr=8.8e-7, ent=1.7] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:13,  1.14it/s, pg=-0.0274, ret=-0.000101, glen=72.7, tlen=233, kl=0.0045, act_lr=8.8e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:13,  1.14it/s, pg=0.0612, ret=-0.00147, glen=76.1, tlen=237, kl=0.00399, act_lr=8.8e-7, ent=1.9] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.15it/s, pg=0.0612, ret=-0.00147, glen=76.1, tlen=237, kl=0.00399, act_lr=8.8e-7, ent=1.9]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.15it/s, pg=-0.0423, ret=-3.96e-5, glen=82.1, tlen=243, kl=0.0034, act_lr=8.8e-7, ent=1.88]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.16it/s, pg=-0.0423, ret=-3.96e-5, glen=82.1, tlen=243, kl=0.0034, act_lr=8.8e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.16it/s, pg=-0.0138, ret=-0.000462, glen=70.7, tlen=231, kl=0.00393, act_lr=8.8e-7, ent=1.75]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=-0.0138, ret=-0.000462, glen=70.7, tlen=231, kl=0.00393, act_lr=8.8e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:34<00:10,  1.17it/s, pg=-0.108, ret=0.000462, glen=73.4, tlen=234, kl=0.00405, act_lr=8.8e-7, ent=1.78]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=-0.108, ret=0.000462, glen=73.4, tlen=234, kl=0.00405, act_lr=8.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:35<00:09,  1.17it/s, pg=-0.136, ret=0.000985, glen=77.2, tlen=238, kl=0.00422, act_lr=8.8e-7, ent=2.04]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.15it/s, pg=-0.136, ret=0.000985, glen=77.2, tlen=238, kl=0.00422, act_lr=8.8e-7, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:36<00:08,  1.15it/s, pg=-0.0259, ret=-0.000199, glen=98.3, tlen=259, kl=0.00386, act_lr=8.8e-7, ent=1.97]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.16it/s, pg=-0.0259, ret=-0.000199, glen=98.3, tlen=259, kl=0.00386, act_lr=8.8e-7, ent=1.97]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.16it/s, pg=0.108, ret=-0.000491, glen=79.3, tlen=240, kl=0.00369, act_lr=8.8e-7, ent=1.78]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.16it/s, pg=0.108, ret=-0.000491, glen=79.3, tlen=240, kl=0.00369, act_lr=8.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.16it/s, pg=-0.105, ret=0.000317, glen=74, tlen=234, kl=0.00367, act_lr=8.8e-7, ent=1.79]  Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:06,  1.17it/s, pg=-0.105, ret=0.000317, glen=74, tlen=234, kl=0.00367, act_lr=8.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:06,  1.17it/s, pg=-0.0177, ret=-0.000119, glen=71.1, tlen=232, kl=0.00396, act_lr=8.8e-7, ent=1.79]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.17it/s, pg=-0.0177, ret=-0.000119, glen=71.1, tlen=232, kl=0.00396, act_lr=8.8e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.17it/s, pg=-0.0341, ret=-0.000118, glen=71.5, tlen=232, kl=0.00417, act_lr=8.8e-7, ent=1.78]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=-0.0341, ret=-0.000118, glen=71.5, tlen=232, kl=0.00417, act_lr=8.8e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:40<00:04,  1.17it/s, pg=-0.0862, ret=0.000329, glen=73.7, tlen=234, kl=0.00506, act_lr=8.8e-7, ent=1.85] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=-0.0862, ret=0.000329, glen=73.7, tlen=234, kl=0.00506, act_lr=8.8e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:41<00:03,  1.17it/s, pg=0.153, ret=-0.000673, glen=78.2, tlen=239, kl=0.00413, act_lr=8.8e-7, ent=1.94] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=0.153, ret=-0.000673, glen=78.2, tlen=239, kl=0.00413, act_lr=8.8e-7, ent=1.94]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:42<00:02,  1.17it/s, pg=0.199, ret=-0.000711, glen=79.9, tlen=241, kl=0.00417, act_lr=8.8e-7, ent=2]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.18it/s, pg=0.199, ret=-0.000711, glen=79.9, tlen=241, kl=0.00417, act_lr=8.8e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.18it/s, pg=-0.108, ret=0.000893, glen=72.2, tlen=233, kl=0.00444, act_lr=8.8e-7, ent=1.76]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.18it/s, pg=-0.108, ret=0.000893, glen=72.2, tlen=233, kl=0.00444, act_lr=8.8e-7, ent=1.76]
2025-07-23 14:32:21.433 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 43.84s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.18it/s, pg=0.104, ret=0.000157, glen=103, tlen=264, kl=0.00298, act_lr=9e-7, ent=1.88]    Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.12it/s, pg=0.104, ret=0.000157, glen=103, tlen=264, kl=0.00298, act_lr=9e-7, ent=1.88]
2025-07-23 14:32:22.094 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-23 14:32:24.311 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.22s
2025-07-23 14:32:24.620 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.14s
2025-07-23 14:32:24.625 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -5.37872314453125e-06, 'actor_lr': 8.804000151485524e-07, 'clip_ratio': 0.0, 'entropy': 1.8729417276382447, 'kl': 0.003974456787109375, 'response_length': 78.86641708374023, 'total_length': 239.40603607177735, 'return': 7.321392331505194e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [20:47<22:45, 195.06s/it][A2025-07-23 14:32:24.661 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:34:45.955 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:34:46.141 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:34:46.142 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 141.48s
2025-07-23 14:34:47.978 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0165,avg_reflection_pattern_score: 0.0081,avg_pass_at_n: 1.0000,avg_num_tokens: 77.5935,std_num_tokens: 140.5970,avg_correct_num_tokens: 72.1188,std_correct_num_tokens: 80.9527,avg_incorrect_num_tokens: 84.5046,std_incorrect_num_tokens: 190.6908
2025-07-23 14:34:48.297 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.16s
2025-07-23 14:34:49.815 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.51s
2025-07-23 14:35:15.552 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 198
2025-07-23 14:35:15.552 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.73s
2025-07-23 14:35:16.281 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.72s
2025-07-23 14:35:16.281 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00010475285752262507, avg_kl: 0.0077919815525864106, avg_response_length: 82.49825876409358, avg_orm_score: 0.0, avg_custom_rewards: -0.00010475285752262507
2025-07-23 14:35:16.312 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter45_replay_buffer.jsonl
2025-07-23 14:35:17.545 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.24s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s, pg=-0.00867, ret=-0.000238, glen=76.6, tlen=237, kl=0.00719, act_lr=9e-7, ent=1.77]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:00<00:47,  1.03it/s, pg=-0.00867, ret=-0.000238, glen=76.6, tlen=237, kl=0.00719, act_lr=9e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:47,  1.03it/s, pg=-0.0234, ret=-0.00263, glen=76.7, tlen=237, kl=0.00723, act_lr=9e-7, ent=1.91]  Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:43,  1.11it/s, pg=-0.0234, ret=-0.00263, glen=76.7, tlen=237, kl=0.00723, act_lr=9e-7, ent=1.91]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:43,  1.11it/s, pg=-0.00677, ret=-0.0002, glen=72.1, tlen=233, kl=0.00858, act_lr=9e-7, ent=1.71]Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:42,  1.11it/s, pg=-0.00677, ret=-0.0002, glen=72.1, tlen=233, kl=0.00858, act_lr=9e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:42,  1.11it/s, pg=-0.14, ret=0.000749, glen=75.1, tlen=236, kl=0.00778, act_lr=9e-7, ent=1.72]  Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:40,  1.13it/s, pg=-0.14, ret=0.000749, glen=75.1, tlen=236, kl=0.00778, act_lr=9e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:40,  1.13it/s, pg=0.1, ret=-0.000952, glen=90.8, tlen=251, kl=0.00739, act_lr=9e-7, ent=2.02] Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:39,  1.13it/s, pg=0.1, ret=-0.000952, glen=90.8, tlen=251, kl=0.00739, act_lr=9e-7, ent=2.02]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:39,  1.13it/s, pg=-0.0512, ret=0.00102, glen=72.9, tlen=234, kl=0.00761, act_lr=9e-7, ent=1.69]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:38,  1.14it/s, pg=-0.0512, ret=0.00102, glen=72.9, tlen=234, kl=0.00761, act_lr=9e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:38,  1.14it/s, pg=0.0641, ret=-0.000645, glen=86.7, tlen=248, kl=0.00662, act_lr=9e-7, ent=1.78]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:37,  1.15it/s, pg=0.0641, ret=-0.000645, glen=86.7, tlen=248, kl=0.00662, act_lr=9e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:37,  1.15it/s, pg=0.49, ret=0.000339, glen=129, tlen=290, kl=0.00507, act_lr=9e-7, ent=1.58]    Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.14it/s, pg=0.49, ret=0.000339, glen=129, tlen=290, kl=0.00507, act_lr=9e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:36,  1.14it/s, pg=0.00208, ret=-0.000225, glen=75, tlen=236, kl=0.00748, act_lr=9e-7, ent=1.89]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:07<00:35,  1.15it/s, pg=0.00208, ret=-0.000225, glen=75, tlen=236, kl=0.00748, act_lr=9e-7, ent=1.89]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:35,  1.15it/s, pg=-0.0635, ret=-0.000867, glen=79.5, tlen=240, kl=0.00808, act_lr=9e-7, ent=1.93]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.16it/s, pg=-0.0635, ret=-0.000867, glen=79.5, tlen=240, kl=0.00808, act_lr=9e-7, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.16it/s, pg=0.0977, ret=-0.0012, glen=70.1, tlen=231, kl=0.00868, act_lr=9e-7, ent=1.72]   Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.16it/s, pg=0.0977, ret=-0.0012, glen=70.1, tlen=231, kl=0.00868, act_lr=9e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.16it/s, pg=0.111, ret=-0.00108, glen=65.2, tlen=225, kl=0.00909, act_lr=9e-7, ent=1.69]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:32,  1.17it/s, pg=0.111, ret=-0.00108, glen=65.2, tlen=225, kl=0.00909, act_lr=9e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:32,  1.17it/s, pg=-0.133, ret=-0.000161, glen=70.1, tlen=231, kl=0.00726, act_lr=9e-7, ent=1.71]Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:31,  1.17it/s, pg=-0.133, ret=-0.000161, glen=70.1, tlen=231, kl=0.00726, act_lr=9e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:31,  1.17it/s, pg=-0.0498, ret=-0.000112, glen=71.8, tlen=233, kl=0.00932, act_lr=9e-7, ent=1.73]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:30,  1.17it/s, pg=-0.0498, ret=-0.000112, glen=71.8, tlen=233, kl=0.00932, act_lr=9e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:30,  1.17it/s, pg=-0.0861, ret=0.00115, glen=68.7, tlen=229, kl=0.00899, act_lr=9e-7, ent=1.59]  Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:29,  1.17it/s, pg=-0.0861, ret=0.00115, glen=68.7, tlen=229, kl=0.00899, act_lr=9e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:29,  1.17it/s, pg=0.0236, ret=-0.0007, glen=70.1, tlen=231, kl=0.00816, act_lr=9e-7, ent=1.92] Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:13<00:28,  1.17it/s, pg=0.0236, ret=-0.0007, glen=70.1, tlen=231, kl=0.00816, act_lr=9e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:28,  1.17it/s, pg=0.0957, ret=-0.000726, glen=96.3, tlen=257, kl=0.00654, act_lr=9e-7, ent=2.1]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:14<00:28,  1.17it/s, pg=0.0957, ret=-0.000726, glen=96.3, tlen=257, kl=0.00654, act_lr=9e-7, ent=2.1]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.17it/s, pg=-0.141, ret=-0.000341, glen=71.7, tlen=232, kl=0.00831, act_lr=9e-7, ent=1.87]Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.17it/s, pg=-0.141, ret=-0.000341, glen=71.7, tlen=232, kl=0.00831, act_lr=9e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.17it/s, pg=-0.0669, ret=-0.000311, glen=69.2, tlen=229, kl=0.00899, act_lr=9e-7, ent=1.66]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:26,  1.17it/s, pg=-0.0669, ret=-0.000311, glen=69.2, tlen=229, kl=0.00899, act_lr=9e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:26,  1.17it/s, pg=0.0786, ret=-0.00125, glen=75.5, tlen=236, kl=0.00747, act_lr=9e-7, ent=1.98]  Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:25,  1.16it/s, pg=0.0786, ret=-0.00125, glen=75.5, tlen=236, kl=0.00747, act_lr=9e-7, ent=1.98]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:25,  1.16it/s, pg=-0.169, ret=0.000882, glen=78.1, tlen=239, kl=0.00742, act_lr=9e-7, ent=1.74]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:25,  1.16it/s, pg=-0.169, ret=0.000882, glen=78.1, tlen=239, kl=0.00742, act_lr=9e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:25,  1.16it/s, pg=-0.128, ret=0.000741, glen=80.7, tlen=241, kl=0.00677, act_lr=9e-7, ent=2.13]Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:24,  1.16it/s, pg=-0.128, ret=0.000741, glen=80.7, tlen=241, kl=0.00677, act_lr=9e-7, ent=2.13]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:24,  1.16it/s, pg=-0.25, ret=0.00122, glen=73.8, tlen=234, kl=0.00939, act_lr=9e-7, ent=1.78]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:19<00:23,  1.17it/s, pg=-0.25, ret=0.00122, glen=73.8, tlen=234, kl=0.00939, act_lr=9e-7, ent=1.78]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.17it/s, pg=-0.0202, ret=-5.94e-5, glen=75.4, tlen=236, kl=0.00671, act_lr=9e-7, ent=1.7]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.17it/s, pg=-0.0202, ret=-5.94e-5, glen=75.4, tlen=236, kl=0.00671, act_lr=9e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.17it/s, pg=0.188, ret=0.000527, glen=157, tlen=318, kl=0.00525, act_lr=9e-7, ent=2]     Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.15it/s, pg=0.188, ret=0.000527, glen=157, tlen=318, kl=0.00525, act_lr=9e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.15it/s, pg=-0.0969, ret=0.00051, glen=73.1, tlen=234, kl=0.00779, act_lr=9e-7, ent=1.79]Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.16it/s, pg=-0.0969, ret=0.00051, glen=73.1, tlen=234, kl=0.00779, act_lr=9e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:23<00:20,  1.16it/s, pg=0.118, ret=-0.00148, glen=72.2, tlen=233, kl=0.00836, act_lr=9e-7, ent=1.8]  Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:23<00:23,  1.03s/it, pg=0.118, ret=-0.00148, glen=72.2, tlen=233, kl=0.00836, act_lr=9e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:23,  1.03s/it, pg=0.41, ret=0.00151, glen=266, tlen=426, kl=0.00623, act_lr=9e-7, ent=1.71]  Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:22,  1.00s/it, pg=0.41, ret=0.00151, glen=266, tlen=426, kl=0.00623, act_lr=9e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:22,  1.00s/it, pg=-0.115, ret=0.000213, glen=75.8, tlen=237, kl=0.00762, act_lr=9e-7, ent=1.82]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:20,  1.05it/s, pg=-0.115, ret=0.000213, glen=75.8, tlen=237, kl=0.00762, act_lr=9e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:20,  1.05it/s, pg=-0.245, ret=0.000663, glen=74.3, tlen=235, kl=0.00725, act_lr=9e-7, ent=1.82]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:18,  1.08it/s, pg=-0.245, ret=0.000663, glen=74.3, tlen=235, kl=0.00725, act_lr=9e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:18,  1.08it/s, pg=-0.233, ret=0.00134, glen=68.4, tlen=229, kl=0.00713, act_lr=9e-7, ent=1.67] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:17,  1.10it/s, pg=-0.233, ret=0.00134, glen=68.4, tlen=229, kl=0.00713, act_lr=9e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:28<00:17,  1.10it/s, pg=-0.0664, ret=-0.000185, glen=71.4, tlen=232, kl=0.00811, act_lr=9e-7, ent=1.87]Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.13it/s, pg=-0.0664, ret=-0.000185, glen=71.4, tlen=232, kl=0.00811, act_lr=9e-7, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:29<00:15,  1.13it/s, pg=-0.069, ret=0.000461, glen=71.8, tlen=233, kl=0.00832, act_lr=9e-7, ent=1.73]  Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.14it/s, pg=-0.069, ret=0.000461, glen=71.8, tlen=233, kl=0.00832, act_lr=9e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.14it/s, pg=-0.287, ret=0.000866, glen=82.4, tlen=243, kl=0.00787, act_lr=9e-7, ent=1.88]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:29<00:13,  1.15it/s, pg=-0.287, ret=0.000866, glen=82.4, tlen=243, kl=0.00787, act_lr=9e-7, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:13,  1.15it/s, pg=0.052, ret=-0.000129, glen=73, tlen=234, kl=0.00845, act_lr=9e-7, ent=1.55]  Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:12,  1.16it/s, pg=0.052, ret=-0.000129, glen=73, tlen=234, kl=0.00845, act_lr=9e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:12,  1.16it/s, pg=0.0444, ret=-0.000594, glen=78.6, tlen=239, kl=0.0087, act_lr=9e-7, ent=1.76]Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.16it/s, pg=0.0444, ret=-0.000594, glen=78.6, tlen=239, kl=0.0087, act_lr=9e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.16it/s, pg=-0.127, ret=0.000112, glen=77.4, tlen=238, kl=0.00721, act_lr=9e-7, ent=1.71]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.17it/s, pg=-0.127, ret=0.000112, glen=77.4, tlen=238, kl=0.00721, act_lr=9e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.17it/s, pg=-0.152, ret=0.00102, glen=72.2, tlen=233, kl=0.00889, act_lr=9e-7, ent=1.69] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=-0.152, ret=0.00102, glen=72.2, tlen=233, kl=0.00889, act_lr=9e-7, ent=1.69]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:34<00:10,  1.17it/s, pg=-0.0101, ret=-0.000168, glen=72.6, tlen=234, kl=0.00832, act_lr=9e-7, ent=1.77]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=-0.0101, ret=-0.000168, glen=72.6, tlen=234, kl=0.00832, act_lr=9e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:35<00:09,  1.17it/s, pg=0.0344, ret=-0.000388, glen=68, tlen=229, kl=0.00891, act_lr=9e-7, ent=1.74]   Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.14it/s, pg=0.0344, ret=-0.000388, glen=68, tlen=229, kl=0.00891, act_lr=9e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.14it/s, pg=-0.0758, ret=-1.91e-5, glen=82, tlen=243, kl=0.00689, act_lr=9e-7, ent=1.99]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:35<00:07,  1.15it/s, pg=-0.0758, ret=-1.91e-5, glen=82, tlen=243, kl=0.00689, act_lr=9e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.15it/s, pg=-0.141, ret=0.000299, glen=77, tlen=238, kl=0.00853, act_lr=9e-7, ent=1.72] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.16it/s, pg=-0.141, ret=0.000299, glen=77, tlen=238, kl=0.00853, act_lr=9e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.16it/s, pg=-0.174, ret=0.000748, glen=72.6, tlen=234, kl=0.00813, act_lr=9e-7, ent=1.73]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:06,  1.16it/s, pg=-0.174, ret=0.000748, glen=72.6, tlen=234, kl=0.00813, act_lr=9e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:06,  1.16it/s, pg=-0.102, ret=0.00121, glen=86.5, tlen=247, kl=0.00674, act_lr=9e-7, ent=1.7]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.16it/s, pg=-0.102, ret=0.00121, glen=86.5, tlen=247, kl=0.00674, act_lr=9e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.16it/s, pg=0.0512, ret=-0.000893, glen=75.4, tlen=235, kl=0.00855, act_lr=9e-7, ent=1.75]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=0.0512, ret=-0.000893, glen=75.4, tlen=235, kl=0.00855, act_lr=9e-7, ent=1.75]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:40<00:04,  1.17it/s, pg=-0.0686, ret=-0.000564, glen=74, tlen=234, kl=0.00847, act_lr=9e-7, ent=1.77] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=-0.0686, ret=-0.000564, glen=74, tlen=234, kl=0.00847, act_lr=9e-7, ent=1.77]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:41<00:03,  1.17it/s, pg=0.0714, ret=-0.000869, glen=71.7, tlen=233, kl=0.00977, act_lr=9e-7, ent=1.71]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=0.0714, ret=-0.000869, glen=71.7, tlen=233, kl=0.00977, act_lr=9e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.101, ret=0.00119, glen=111, tlen=272, kl=0.00652, act_lr=9e-7, ent=2.47]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:41<00:01,  1.16it/s, pg=-0.101, ret=0.00119, glen=111, tlen=272, kl=0.00652, act_lr=9e-7, ent=2.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.16it/s, pg=-0.0385, ret=-0.000716, glen=71.4, tlen=232, kl=0.00793, act_lr=9e-7, ent=1.8]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.17it/s, pg=-0.0385, ret=-0.000716, glen=71.4, tlen=232, kl=0.00793, act_lr=9e-7, ent=1.8]
2025-07-23 14:36:01.515 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 43.77s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.17it/s, pg=-0.142, ret=0.000864, glen=74.5, tlen=235, kl=0.00774, act_lr=9.2e-7, ent=1.65]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.12it/s, pg=-0.142, ret=0.000864, glen=74.5, tlen=235, kl=0.00774, act_lr=9.2e-7, ent=1.65]
2025-07-23 14:36:02.170 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-23 14:36:04.158 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 1.99s
2025-07-23 14:36:04.472 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 46.85s
2025-07-23 14:36:04.478 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.030992431640625, 'actor_lr': 9.003999753076642e-07, 'clip_ratio': 0.0, 'entropy': 1.7983129930496216, 'kl': 0.007795543670654297, 'response_length': 82.43257339477539, 'total_length': 243.13618743896484, 'return': -1.3776709965895862e-06, 'policy_update_steps': 1.0}

Episode [4/20]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [24:27<20:18, 203.16s/it][A2025-07-23 14:36:04.511 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:38:24.173 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:38:24.368 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:38:24.368 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 139.86s
2025-07-23 14:38:26.220 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0159,avg_reflection_pattern_score: 0.0077,avg_pass_at_n: 1.0000,avg_num_tokens: 74.4792,std_num_tokens: 103.6584,avg_correct_num_tokens: 71.1381,std_correct_num_tokens: 47.5168,avg_incorrect_num_tokens: 80.4892,std_incorrect_num_tokens: 161.1069
2025-07-23 14:38:26.542 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.17s
2025-07-23 14:38:27.943 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.40s
2025-07-23 14:38:53.026 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 194
2025-07-23 14:38:53.027 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.08s
2025-07-23 14:38:53.862 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.83s
2025-07-23 14:38:53.863 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0005507239288489632, avg_kl: 0.012690396653008215, avg_response_length: 78.36366602317574, avg_orm_score: 0.0, avg_custom_rewards: -0.0005507239288489632
2025-07-23 14:38:53.896 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter46_replay_buffer.jsonl
2025-07-23 14:38:55.099 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.20s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s, pg=0.042, ret=-0.00051, glen=73.7, tlen=234, kl=0.0107, act_lr=9.2e-7, ent=1.67]Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:00<00:47,  1.02it/s, pg=0.042, ret=-0.00051, glen=73.7, tlen=234, kl=0.0107, act_lr=9.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:01<00:47,  1.02it/s, pg=-0.18, ret=0.00129, glen=71.7, tlen=232, kl=0.0126, act_lr=9.2e-7, ent=1.53] Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:01<00:42,  1.10it/s, pg=-0.18, ret=0.00129, glen=71.7, tlen=232, kl=0.0126, act_lr=9.2e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:02<00:42,  1.10it/s, pg=-0.068, ret=1.81e-5, glen=75.6, tlen=236, kl=0.0122, act_lr=9.2e-7, ent=1.67]Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:02<00:40,  1.13it/s, pg=-0.068, ret=1.81e-5, glen=75.6, tlen=236, kl=0.0122, act_lr=9.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:03<00:40,  1.13it/s, pg=0.129, ret=-8.55e-5, glen=81.5, tlen=242, kl=0.0118, act_lr=9.2e-7, ent=1.85]Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:03<00:39,  1.15it/s, pg=0.129, ret=-8.55e-5, glen=81.5, tlen=242, kl=0.0118, act_lr=9.2e-7, ent=1.85]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:04<00:39,  1.15it/s, pg=0.0366, ret=-6.28e-5, glen=80.7, tlen=241, kl=0.0115, act_lr=9.2e-7, ent=1.73]Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:04<00:38,  1.16it/s, pg=0.0366, ret=-6.28e-5, glen=80.7, tlen=241, kl=0.0115, act_lr=9.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:05<00:38,  1.16it/s, pg=-0.0284, ret=5.16e-5, glen=77.1, tlen=238, kl=0.0119, act_lr=9.2e-7, ent=1.92]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:05<00:37,  1.14it/s, pg=-0.0284, ret=5.16e-5, glen=77.1, tlen=238, kl=0.0119, act_lr=9.2e-7, ent=1.92]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:06<00:37,  1.14it/s, pg=-0.0688, ret=1.95e-5, glen=73.6, tlen=234, kl=0.0145, act_lr=9.2e-7, ent=1.74]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:06<00:36,  1.15it/s, pg=-0.0688, ret=1.95e-5, glen=73.6, tlen=234, kl=0.0145, act_lr=9.2e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:07<00:36,  1.15it/s, pg=0.0315, ret=-9.22e-5, glen=75.3, tlen=236, kl=0.0124, act_lr=9.2e-7, ent=1.82]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:36,  1.13it/s, pg=0.0315, ret=-9.22e-5, glen=75.3, tlen=236, kl=0.0124, act_lr=9.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:36,  1.13it/s, pg=-0.015, ret=-0.000832, glen=70.5, tlen=231, kl=0.0149, act_lr=9.2e-7, ent=1.51]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:07<00:35,  1.12it/s, pg=-0.015, ret=-0.000832, glen=70.5, tlen=231, kl=0.0149, act_lr=9.2e-7, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:35,  1.12it/s, pg=-0.11, ret=0.00122, glen=68.3, tlen=229, kl=0.0139, act_lr=9.2e-7, ent=1.65]   Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:08<00:34,  1.14it/s, pg=-0.11, ret=0.00122, glen=68.3, tlen=229, kl=0.0139, act_lr=9.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:09<00:34,  1.14it/s, pg=0.272, ret=-0.00533, glen=293, tlen=453, kl=0.00968, act_lr=9.2e-7, ent=2.56]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:09<00:34,  1.11it/s, pg=0.272, ret=-0.00533, glen=293, tlen=453, kl=0.00968, act_lr=9.2e-7, ent=2.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:10<00:34,  1.11it/s, pg=-0.0575, ret=0.000206, glen=70.7, tlen=231, kl=0.0127, act_lr=9.2e-7, ent=1.58]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:10<00:32,  1.13it/s, pg=-0.0575, ret=0.000206, glen=70.7, tlen=231, kl=0.0127, act_lr=9.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:11<00:32,  1.13it/s, pg=0.0332, ret=-0.000233, glen=70, tlen=231, kl=0.0115, act_lr=9.2e-7, ent=1.63]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:11<00:31,  1.14it/s, pg=0.0332, ret=-0.000233, glen=70, tlen=231, kl=0.0115, act_lr=9.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:12<00:31,  1.14it/s, pg=-0.0756, ret=0.0004, glen=67.4, tlen=228, kl=0.0131, act_lr=9.2e-7, ent=1.52]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:12<00:30,  1.15it/s, pg=-0.0756, ret=0.0004, glen=67.4, tlen=228, kl=0.0131, act_lr=9.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:13<00:30,  1.15it/s, pg=-0.0398, ret=0.00082, glen=80.9, tlen=241, kl=0.0117, act_lr=9.2e-7, ent=1.76]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:13<00:29,  1.16it/s, pg=-0.0398, ret=0.00082, glen=80.9, tlen=241, kl=0.0117, act_lr=9.2e-7, ent=1.76]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:14<00:29,  1.16it/s, pg=-0.0759, ret=0.000693, glen=81.6, tlen=243, kl=0.0138, act_lr=9.2e-7, ent=1.68]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.17it/s, pg=-0.0759, ret=0.000693, glen=81.6, tlen=243, kl=0.0138, act_lr=9.2e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.17it/s, pg=-0.0108, ret=-0.000229, glen=74.6, tlen=235, kl=0.011, act_lr=9.2e-7, ent=1.59]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:14<00:27,  1.17it/s, pg=-0.0108, ret=-0.000229, glen=74.6, tlen=235, kl=0.011, act_lr=9.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:15<00:27,  1.17it/s, pg=-0.0992, ret=0.000332, glen=72.9, tlen=233, kl=0.0119, act_lr=9.2e-7, ent=1.67]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:15<00:26,  1.17it/s, pg=-0.0992, ret=0.000332, glen=72.9, tlen=233, kl=0.0119, act_lr=9.2e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:16<00:26,  1.17it/s, pg=0.0601, ret=-0.000254, glen=75.5, tlen=236, kl=0.0127, act_lr=9.2e-7, ent=1.63]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:16<00:25,  1.17it/s, pg=0.0601, ret=-0.000254, glen=75.5, tlen=236, kl=0.0127, act_lr=9.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:17<00:25,  1.17it/s, pg=0.0702, ret=-0.000309, glen=69.6, tlen=230, kl=0.0105, act_lr=9.2e-7, ent=1.62]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:17<00:24,  1.17it/s, pg=0.0702, ret=-0.000309, glen=69.6, tlen=230, kl=0.0105, act_lr=9.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:18<00:24,  1.17it/s, pg=-0.0414, ret=6.23e-5, glen=76.3, tlen=237, kl=0.0124, act_lr=9.2e-7, ent=1.7]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:18<00:23,  1.17it/s, pg=-0.0414, ret=6.23e-5, glen=76.3, tlen=237, kl=0.0124, act_lr=9.2e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:19<00:23,  1.17it/s, pg=-0.0248, ret=-5.25e-5, glen=70.2, tlen=230, kl=0.0148, act_lr=9.2e-7, ent=1.6]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:19<00:22,  1.18it/s, pg=-0.0248, ret=-5.25e-5, glen=70.2, tlen=230, kl=0.0148, act_lr=9.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:19<00:22,  1.18it/s, pg=0.096, ret=-0.000892, glen=69.1, tlen=230, kl=0.0186, act_lr=9.2e-7, ent=1.6] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:19<00:22,  1.18it/s, pg=0.096, ret=-0.000892, glen=69.1, tlen=230, kl=0.0186, act_lr=9.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.18it/s, pg=-0.174, ret=0.000861, glen=77, tlen=237, kl=0.0122, act_lr=9.2e-7, ent=1.72] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:20<00:21,  1.17it/s, pg=-0.174, ret=0.000861, glen=77, tlen=237, kl=0.0122, act_lr=9.2e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:21<00:21,  1.17it/s, pg=-0.0884, ret=0.000529, glen=71.3, tlen=231, kl=0.0131, act_lr=9.2e-7, ent=1.59]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:21<00:20,  1.17it/s, pg=-0.0884, ret=0.000529, glen=71.3, tlen=231, kl=0.0131, act_lr=9.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:22<00:20,  1.17it/s, pg=0.0697, ret=-0.000573, glen=73.2, tlen=234, kl=0.0118, act_lr=9.2e-7, ent=1.73]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:22<00:19,  1.17it/s, pg=0.0697, ret=-0.000573, glen=73.2, tlen=234, kl=0.0118, act_lr=9.2e-7, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:23<00:19,  1.17it/s, pg=0.143, ret=-0.000963, glen=74.6, tlen=235, kl=0.0136, act_lr=9.2e-7, ent=1.63] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:23<00:22,  1.03s/it, pg=0.143, ret=-0.000963, glen=74.6, tlen=235, kl=0.0136, act_lr=9.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.03s/it, pg=-0.0405, ret=0.000312, glen=83.7, tlen=244, kl=0.0112, act_lr=9.2e-7, ent=2]  Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:24<00:20,  1.02it/s, pg=-0.0405, ret=0.000312, glen=83.7, tlen=244, kl=0.0112, act_lr=9.2e-7, ent=2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:25<00:20,  1.02it/s, pg=-0.073, ret=5.66e-5, glen=70.6, tlen=231, kl=0.0156, act_lr=9.2e-7, ent=1.62]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:25<00:18,  1.06it/s, pg=-0.073, ret=5.66e-5, glen=70.6, tlen=231, kl=0.0156, act_lr=9.2e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:26<00:18,  1.06it/s, pg=-0.0198, ret=-0.000303, glen=81.3, tlen=242, kl=0.0128, act_lr=9.2e-7, ent=2.06]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:26<00:17,  1.09it/s, pg=-0.0198, ret=-0.000303, glen=81.3, tlen=242, kl=0.0128, act_lr=9.2e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:27<00:17,  1.09it/s, pg=0.0745, ret=-0.000121, glen=71, tlen=231, kl=0.0127, act_lr=9.2e-7, ent=1.56]   Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:27<00:16,  1.12it/s, pg=0.0745, ret=-0.000121, glen=71, tlen=231, kl=0.0127, act_lr=9.2e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:28<00:16,  1.12it/s, pg=0.17, ret=-0.000607, glen=76.9, tlen=236, kl=0.0107, act_lr=9.2e-7, ent=1.66]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:28<00:14,  1.13it/s, pg=0.17, ret=-0.000607, glen=76.9, tlen=236, kl=0.0107, act_lr=9.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:29<00:14,  1.13it/s, pg=-0.062, ret=-9.1e-5, glen=74.7, tlen=235, kl=0.0133, act_lr=9.2e-7, ent=1.82]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:13,  1.15it/s, pg=-0.062, ret=-9.1e-5, glen=74.7, tlen=235, kl=0.0133, act_lr=9.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:13,  1.15it/s, pg=0.0542, ret=-0.000614, glen=69, tlen=229, kl=0.0137, act_lr=9.2e-7, ent=1.58]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:29<00:12,  1.15it/s, pg=0.0542, ret=-0.000614, glen=69, tlen=229, kl=0.0137, act_lr=9.2e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:12,  1.15it/s, pg=0.058, ret=-0.000214, glen=71.8, tlen=232, kl=0.0129, act_lr=9.2e-7, ent=1.65]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:30<00:12,  1.16it/s, pg=0.058, ret=-0.000214, glen=71.8, tlen=232, kl=0.0129, act_lr=9.2e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:31<00:12,  1.16it/s, pg=-0.0526, ret=-3.32e-5, glen=74.8, tlen=236, kl=0.0123, act_lr=9.2e-7, ent=1.82]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:31<00:11,  1.17it/s, pg=-0.0526, ret=-3.32e-5, glen=74.8, tlen=236, kl=0.0123, act_lr=9.2e-7, ent=1.82]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:32<00:11,  1.17it/s, pg=-0.0706, ret=0.000309, glen=67.7, tlen=228, kl=0.013, act_lr=9.2e-7, ent=1.61] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:32<00:10,  1.17it/s, pg=-0.0706, ret=0.000309, glen=67.7, tlen=228, kl=0.013, act_lr=9.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:33<00:10,  1.17it/s, pg=0.0766, ret=-0.000609, glen=71.4, tlen=231, kl=0.015, act_lr=9.2e-7, ent=1.55]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:33<00:09,  1.17it/s, pg=0.0766, ret=-0.000609, glen=71.4, tlen=231, kl=0.015, act_lr=9.2e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:34<00:09,  1.17it/s, pg=-0.13, ret=0.0015, glen=70, tlen=231, kl=0.0121, act_lr=9.2e-7, ent=1.52]     Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:34<00:08,  1.17it/s, pg=-0.13, ret=0.0015, glen=70, tlen=231, kl=0.0121, act_lr=9.2e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:35<00:08,  1.17it/s, pg=0.0724, ret=-0.000396, glen=73.2, tlen=234, kl=0.0123, act_lr=9.2e-7, ent=1.66]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=0.0724, ret=-0.000396, glen=73.2, tlen=234, kl=0.0123, act_lr=9.2e-7, ent=1.66]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=-0.0181, ret=0.000589, glen=75.7, tlen=236, kl=0.0133, act_lr=9.2e-7, ent=1.63]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:35<00:06,  1.17it/s, pg=-0.0181, ret=0.000589, glen=75.7, tlen=236, kl=0.0133, act_lr=9.2e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.17it/s, pg=-0.0361, ret=-0.000151, glen=69.5, tlen=230, kl=0.0146, act_lr=9.2e-7, ent=1.59]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:36<00:05,  1.17it/s, pg=-0.0361, ret=-0.000151, glen=69.5, tlen=230, kl=0.0146, act_lr=9.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:37<00:05,  1.17it/s, pg=0.0668, ret=-0.000276, glen=75.2, tlen=236, kl=0.0126, act_lr=9.2e-7, ent=1.54] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:37<00:05,  1.17it/s, pg=0.0668, ret=-0.000276, glen=75.2, tlen=236, kl=0.0126, act_lr=9.2e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:38<00:05,  1.17it/s, pg=-0.123, ret=0.000215, glen=71.6, tlen=232, kl=0.0129, act_lr=9.2e-7, ent=1.61] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:38<00:04,  1.18it/s, pg=-0.123, ret=0.000215, glen=71.6, tlen=232, kl=0.0129, act_lr=9.2e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:39<00:04,  1.18it/s, pg=-0.172, ret=0.0006, glen=75.9, tlen=236, kl=0.0111, act_lr=9.2e-7, ent=1.59]  Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:39<00:03,  1.18it/s, pg=-0.172, ret=0.0006, glen=75.9, tlen=236, kl=0.0111, act_lr=9.2e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:40<00:03,  1.18it/s, pg=0.374, ret=-0.00157, glen=79.3, tlen=240, kl=0.0106, act_lr=9.2e-7, ent=1.83]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:40<00:02,  1.18it/s, pg=0.374, ret=-0.00157, glen=79.3, tlen=240, kl=0.0106, act_lr=9.2e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:41<00:02,  1.18it/s, pg=-0.0691, ret=0.000521, glen=76.7, tlen=237, kl=0.0125, act_lr=9.2e-7, ent=1.6]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=-0.0691, ret=0.000521, glen=76.7, tlen=237, kl=0.0125, act_lr=9.2e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=-0.0612, ret=0.00046, glen=71.7, tlen=232, kl=0.0116, act_lr=9.2e-7, ent=1.59]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:41<00:00,  1.17it/s, pg=-0.0612, ret=0.00046, glen=71.7, tlen=232, kl=0.0116, act_lr=9.2e-7, ent=1.59]
2025-07-23 14:39:38.087 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 42.82s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.17it/s, pg=0.0362, ret=-9.42e-5, glen=68.8, tlen=229, kl=0.0131, act_lr=9.4e-7, ent=1.61]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.12it/s, pg=0.0362, ret=-9.42e-5, glen=68.8, tlen=229, kl=0.0131, act_lr=9.4e-7, ent=1.61]
2025-07-23 14:39:38.930 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.77s
2025-07-23 14:39:41.475 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.54s
2025-07-23 14:39:41.781 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 46.64s
2025-07-23 14:39:41.786 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0024174281529017855, 'actor_lr': 9.204081544451081e-07, 'clip_ratio': 0.0, 'entropy': 1.686296976342493, 'kl': 0.012684218737543846, 'response_length': 78.28099480453803, 'total_length': 238.67730183504065, 'return': -9.025120974057449e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [28:05<17:18, 207.67s/it][A2025-07-23 14:39:41.818 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:41:38.961 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:41:39.148 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:41:39.149 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 117.33s
2025-07-23 14:41:41.121 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0161,avg_reflection_pattern_score: 0.0077,avg_pass_at_n: 0.9922,avg_num_tokens: 72.2152,std_num_tokens: 100.5321,avg_correct_num_tokens: 71.5526,std_correct_num_tokens: 108.7499,avg_incorrect_num_tokens: 72.9795,std_incorrect_num_tokens: 90.1207
2025-07-23 14:41:41.547 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.40s
2025-07-23 14:41:42.997 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.45s
2025-07-23 14:42:08.003 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 193
2025-07-23 14:42:08.003 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.00s
2025-07-23 14:42:08.739 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.73s
2025-07-23 14:42:08.740 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.00040532420136015184, avg_kl: 0.018280740846624028, avg_response_length: 74.1583645282014, avg_orm_score: 0.0, avg_custom_rewards: 0.00040532420136015184
2025-07-23 14:42:08.768 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter47_replay_buffer.jsonl
2025-07-23 14:42:09.947 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.18s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s, pg=-0.147, ret=0.00122, glen=72, tlen=232, kl=0.0149, act_lr=9.4e-7, ent=1.6]Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:00<00:46,  1.02it/s, pg=-0.147, ret=0.00122, glen=72, tlen=232, kl=0.0149, act_lr=9.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:01<00:46,  1.02it/s, pg=0.0575, ret=-0.000275, glen=75.5, tlen=235, kl=0.0164, act_lr=9.4e-7, ent=1.68]Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:01<00:42,  1.10it/s, pg=0.0575, ret=-0.000275, glen=75.5, tlen=235, kl=0.0164, act_lr=9.4e-7, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:02<00:42,  1.10it/s, pg=0.046, ret=-0.0005, glen=64.1, tlen=224, kl=0.021, act_lr=9.4e-7, ent=1.56]    Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:02<00:40,  1.13it/s, pg=0.046, ret=-0.0005, glen=64.1, tlen=224, kl=0.021, act_lr=9.4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:03<00:40,  1.13it/s, pg=0.00186, ret=0.000143, glen=67.7, tlen=228, kl=0.0166, act_lr=9.4e-7, ent=1.6]Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:03<00:39,  1.15it/s, pg=0.00186, ret=0.000143, glen=67.7, tlen=228, kl=0.0166, act_lr=9.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:04<00:39,  1.15it/s, pg=-0.0652, ret=0.000569, glen=69.3, tlen=230, kl=0.0196, act_lr=9.4e-7, ent=1.56]Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:04<00:37,  1.16it/s, pg=-0.0652, ret=0.000569, glen=69.3, tlen=230, kl=0.0196, act_lr=9.4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:05<00:37,  1.16it/s, pg=0.0103, ret=-0.000407, glen=63.7, tlen=224, kl=0.0212, act_lr=9.4e-7, ent=1.49]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:05<00:37,  1.13it/s, pg=0.0103, ret=-0.000407, glen=63.7, tlen=224, kl=0.0212, act_lr=9.4e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:06<00:37,  1.13it/s, pg=-0.00159, ret=0.000669, glen=79.6, tlen=240, kl=0.0166, act_lr=9.4e-7, ent=1.8]Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:06<00:36,  1.14it/s, pg=-0.00159, ret=0.000669, glen=79.6, tlen=240, kl=0.0166, act_lr=9.4e-7, ent=1.8]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:07<00:36,  1.14it/s, pg=-0.0953, ret=0.00058, glen=70.1, tlen=230, kl=0.0205, act_lr=9.4e-7, ent=1.55] Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:35,  1.15it/s, pg=-0.0953, ret=0.00058, glen=70.1, tlen=230, kl=0.0205, act_lr=9.4e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:35,  1.15it/s, pg=-0.0838, ret=0.00115, glen=70.8, tlen=231, kl=0.0138, act_lr=9.4e-7, ent=1.67]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:07<00:34,  1.16it/s, pg=-0.0838, ret=0.00115, glen=70.8, tlen=231, kl=0.0138, act_lr=9.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:34,  1.16it/s, pg=-0.00348, ret=0.000484, glen=68.4, tlen=229, kl=0.02, act_lr=9.4e-7, ent=1.64]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:08<00:34,  1.15it/s, pg=-0.00348, ret=0.000484, glen=68.4, tlen=229, kl=0.02, act_lr=9.4e-7, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:09<00:34,  1.15it/s, pg=0.108, ret=-0.000161, glen=120, tlen=280, kl=0.0147, act_lr=9.4e-7, ent=1.34] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:09<00:34,  1.12it/s, pg=0.108, ret=-0.000161, glen=120, tlen=280, kl=0.0147, act_lr=9.4e-7, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:10<00:34,  1.12it/s, pg=0.00454, ret=-0.000313, glen=76, tlen=237, kl=0.0199, act_lr=9.4e-7, ent=1.62]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:10<00:32,  1.13it/s, pg=0.00454, ret=-0.000313, glen=76, tlen=237, kl=0.0199, act_lr=9.4e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:11<00:32,  1.13it/s, pg=-0.0279, ret=-0.00042, glen=75.3, tlen=235, kl=0.0169, act_lr=9.4e-7, ent=1.72]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:11<00:31,  1.14it/s, pg=-0.0279, ret=-0.00042, glen=75.3, tlen=235, kl=0.0169, act_lr=9.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:12<00:31,  1.14it/s, pg=-0.055, ret=0.00165, glen=60.6, tlen=221, kl=0.0238, act_lr=9.4e-7, ent=1.47]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:12<00:30,  1.15it/s, pg=-0.055, ret=0.00165, glen=60.6, tlen=221, kl=0.0238, act_lr=9.4e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:13<00:30,  1.15it/s, pg=-0.0679, ret=0.000728, glen=69.9, tlen=230, kl=0.0157, act_lr=9.4e-7, ent=1.61]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:13<00:29,  1.16it/s, pg=-0.0679, ret=0.000728, glen=69.9, tlen=230, kl=0.0157, act_lr=9.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:13<00:29,  1.16it/s, pg=0.0567, ret=-0.000688, glen=70, tlen=230, kl=0.0222, act_lr=9.4e-7, ent=1.63]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:13<00:28,  1.16it/s, pg=0.0567, ret=-0.000688, glen=70, tlen=230, kl=0.0222, act_lr=9.4e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.16it/s, pg=0.0666, ret=-0.000619, glen=67.3, tlen=227, kl=0.021, act_lr=9.4e-7, ent=1.48]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:14<00:27,  1.17it/s, pg=0.0666, ret=-0.000619, glen=67.3, tlen=227, kl=0.021, act_lr=9.4e-7, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:15<00:27,  1.17it/s, pg=0.0416, ret=-0.000628, glen=71.7, tlen=232, kl=0.018, act_lr=9.4e-7, ent=1.7] Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:15<00:26,  1.17it/s, pg=0.0416, ret=-0.000628, glen=71.7, tlen=232, kl=0.018, act_lr=9.4e-7, ent=1.7]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:16<00:26,  1.17it/s, pg=0.1, ret=-0.000521, glen=66.2, tlen=227, kl=0.0216, act_lr=9.4e-7, ent=1.55] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:16<00:25,  1.15it/s, pg=0.1, ret=-0.000521, glen=66.2, tlen=227, kl=0.0216, act_lr=9.4e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:17<00:25,  1.15it/s, pg=0.0149, ret=8.46e-5, glen=68.4, tlen=228, kl=0.0241, act_lr=9.4e-7, ent=1.57]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:17<00:25,  1.16it/s, pg=0.0149, ret=8.46e-5, glen=68.4, tlen=228, kl=0.0241, act_lr=9.4e-7, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:18<00:25,  1.16it/s, pg=-0.113, ret=0.000265, glen=70.9, tlen=231, kl=0.0157, act_lr=9.4e-7, ent=1.46]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:18<00:24,  1.16it/s, pg=-0.113, ret=0.000265, glen=70.9, tlen=231, kl=0.0157, act_lr=9.4e-7, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:19<00:24,  1.16it/s, pg=0.0839, ret=-0.000604, glen=72.2, tlen=232, kl=0.0201, act_lr=9.4e-7, ent=1.6]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:19<00:23,  1.15it/s, pg=0.0839, ret=-0.000604, glen=72.2, tlen=232, kl=0.0201, act_lr=9.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:20<00:23,  1.15it/s, pg=-0.0845, ret=0.000497, glen=69, tlen=229, kl=0.0164, act_lr=9.4e-7, ent=1.52] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.15it/s, pg=-0.0845, ret=0.000497, glen=69, tlen=229, kl=0.0164, act_lr=9.4e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.15it/s, pg=0.021, ret=-0.000289, glen=74.9, tlen=235, kl=0.0169, act_lr=9.4e-7, ent=1.67]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:20<00:21,  1.16it/s, pg=0.021, ret=-0.000289, glen=74.9, tlen=235, kl=0.0169, act_lr=9.4e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:21<00:21,  1.16it/s, pg=0.242, ret=-0.000629, glen=76.2, tlen=236, kl=0.0193, act_lr=9.4e-7, ent=1.65]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:21<00:20,  1.16it/s, pg=0.242, ret=-0.000629, glen=76.2, tlen=236, kl=0.0193, act_lr=9.4e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:22<00:20,  1.16it/s, pg=-0.00476, ret=0.000189, glen=67.6, tlen=227, kl=0.0172, act_lr=9.4e-7, ent=1.51]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:22<00:19,  1.16it/s, pg=-0.00476, ret=0.000189, glen=67.6, tlen=227, kl=0.0172, act_lr=9.4e-7, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:24<00:19,  1.16it/s, pg=-0.0602, ret=-5.95e-5, glen=65.9, tlen=226, kl=0.0181, act_lr=9.4e-7, ent=1.53] Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.03s/it, pg=-0.0602, ret=-5.95e-5, glen=65.9, tlen=226, kl=0.0181, act_lr=9.4e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.03s/it, pg=0.000397, ret=0.000347, glen=73, tlen=233, kl=0.0175, act_lr=9.4e-7, ent=1.56] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:24<00:20,  1.03it/s, pg=0.000397, ret=0.000347, glen=73, tlen=233, kl=0.0175, act_lr=9.4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:25<00:20,  1.03it/s, pg=-0.0628, ret=0.00032, glen=76, tlen=236, kl=0.0171, act_lr=9.4e-7, ent=1.61]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:25<00:18,  1.06it/s, pg=-0.0628, ret=0.00032, glen=76, tlen=236, kl=0.0171, act_lr=9.4e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:26<00:18,  1.06it/s, pg=-0.0115, ret=8.3e-6, glen=70.4, tlen=231, kl=0.0195, act_lr=9.4e-7, ent=1.52]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:26<00:17,  1.10it/s, pg=-0.0115, ret=8.3e-6, glen=70.4, tlen=231, kl=0.0195, act_lr=9.4e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:27<00:17,  1.10it/s, pg=-0.0364, ret=0.000224, glen=70.5, tlen=231, kl=0.0162, act_lr=9.4e-7, ent=1.54]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:27<00:16,  1.12it/s, pg=-0.0364, ret=0.000224, glen=70.5, tlen=231, kl=0.0162, act_lr=9.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:28<00:16,  1.12it/s, pg=-0.0465, ret=-0.00014, glen=67.2, tlen=227, kl=0.0158, act_lr=9.4e-7, ent=1.54]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:28<00:14,  1.14it/s, pg=-0.0465, ret=-0.00014, glen=67.2, tlen=227, kl=0.0158, act_lr=9.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:29<00:14,  1.14it/s, pg=0.114, ret=-0.000392, glen=72.5, tlen=232, kl=0.0173, act_lr=9.4e-7, ent=1.56] Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:13,  1.15it/s, pg=0.114, ret=-0.000392, glen=72.5, tlen=232, kl=0.0173, act_lr=9.4e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:13,  1.15it/s, pg=-0.0735, ret=0.000276, glen=80.8, tlen=241, kl=0.0198, act_lr=9.4e-7, ent=2.01]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:29<00:12,  1.15it/s, pg=-0.0735, ret=0.000276, glen=80.8, tlen=241, kl=0.0198, act_lr=9.4e-7, ent=2.01]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:12,  1.15it/s, pg=0.048, ret=-0.000208, glen=77.9, tlen=238, kl=0.0155, act_lr=9.4e-7, ent=1.72] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:30<00:12,  1.16it/s, pg=0.048, ret=-0.000208, glen=77.9, tlen=238, kl=0.0155, act_lr=9.4e-7, ent=1.72]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:31<00:12,  1.16it/s, pg=0.206, ret=-0.000961, glen=70.7, tlen=231, kl=0.0164, act_lr=9.4e-7, ent=1.74]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:31<00:11,  1.16it/s, pg=0.206, ret=-0.000961, glen=70.7, tlen=231, kl=0.0164, act_lr=9.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:32<00:11,  1.16it/s, pg=-0.0762, ret=0.00019, glen=70.5, tlen=231, kl=0.0195, act_lr=9.4e-7, ent=1.54]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:32<00:10,  1.17it/s, pg=-0.0762, ret=0.00019, glen=70.5, tlen=231, kl=0.0195, act_lr=9.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:33<00:10,  1.17it/s, pg=0.0544, ret=-0.000828, glen=65.3, tlen=225, kl=0.0208, act_lr=9.4e-7, ent=1.58]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:33<00:09,  1.17it/s, pg=0.0544, ret=-0.000828, glen=65.3, tlen=225, kl=0.0208, act_lr=9.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:34<00:09,  1.17it/s, pg=-0.0281, ret=-1.37e-5, glen=69.5, tlen=229, kl=0.0166, act_lr=9.4e-7, ent=1.58]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:34<00:08,  1.17it/s, pg=-0.0281, ret=-1.37e-5, glen=69.5, tlen=229, kl=0.0166, act_lr=9.4e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:35<00:08,  1.17it/s, pg=-0.0913, ret=0.0012, glen=73.3, tlen=234, kl=0.0162, act_lr=9.4e-7, ent=1.53]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=-0.0913, ret=0.0012, glen=73.3, tlen=234, kl=0.0162, act_lr=9.4e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:36<00:07,  1.17it/s, pg=-0.0481, ret=1.5e-5, glen=169, tlen=328, kl=0.0138, act_lr=9.4e-7, ent=2.61] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.15it/s, pg=-0.0481, ret=1.5e-5, glen=169, tlen=328, kl=0.0138, act_lr=9.4e-7, ent=2.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.15it/s, pg=0.144, ret=-0.000641, glen=73.8, tlen=234, kl=0.0204, act_lr=9.4e-7, ent=1.79]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:36<00:06,  1.16it/s, pg=0.144, ret=-0.000641, glen=73.8, tlen=234, kl=0.0204, act_lr=9.4e-7, ent=1.79]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:37<00:06,  1.16it/s, pg=-0.13, ret=8.99e-5, glen=76.6, tlen=237, kl=0.0229, act_lr=9.4e-7, ent=2.21]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:37<00:05,  1.16it/s, pg=-0.13, ret=8.99e-5, glen=76.6, tlen=237, kl=0.0229, act_lr=9.4e-7, ent=2.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:38<00:05,  1.16it/s, pg=-0.0499, ret=0.000489, glen=67.5, tlen=228, kl=0.0215, act_lr=9.4e-7, ent=1.54]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:38<00:04,  1.17it/s, pg=-0.0499, ret=0.000489, glen=67.5, tlen=228, kl=0.0215, act_lr=9.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:39<00:04,  1.17it/s, pg=-0.00446, ret=-0.000772, glen=71.7, tlen=231, kl=0.0186, act_lr=9.4e-7, ent=1.6]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:39<00:03,  1.17it/s, pg=-0.00446, ret=-0.000772, glen=71.7, tlen=231, kl=0.0186, act_lr=9.4e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:40<00:03,  1.17it/s, pg=-0.0533, ret=4.43e-5, glen=70.1, tlen=230, kl=0.0175, act_lr=9.4e-7, ent=1.54]  Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:40<00:02,  1.17it/s, pg=-0.0533, ret=4.43e-5, glen=70.1, tlen=230, kl=0.0175, act_lr=9.4e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:41<00:02,  1.17it/s, pg=0.00467, ret=0.000159, glen=74.8, tlen=235, kl=0.0163, act_lr=9.4e-7, ent=1.74]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=0.00467, ret=0.000159, glen=74.8, tlen=235, kl=0.0163, act_lr=9.4e-7, ent=1.74]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=-0.107, ret=0.000445, glen=70.7, tlen=231, kl=0.0198, act_lr=9.4e-7, ent=1.53] Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:41<00:00,  1.17it/s, pg=-0.107, ret=0.000445, glen=70.7, tlen=231, kl=0.0198, act_lr=9.4e-7, ent=1.53]
2025-07-23 14:42:53.034 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 42.89s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.17it/s, pg=0.0437, ret=-0.000807, glen=74.5, tlen=234, kl=0.0146, act_lr=9.6e-7, ent=1.62]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.12it/s, pg=0.0437, ret=-0.000807, glen=74.5, tlen=234, kl=0.0146, act_lr=9.6e-7, ent=1.62]
2025-07-23 14:42:53.883 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.79s
2025-07-23 14:42:56.465 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.58s
2025-07-23 14:42:56.777 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 46.75s
2025-07-23 14:42:56.783 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0032326834542410715, 'actor_lr': 9.404081691507005e-07, 'clip_ratio': 0.0, 'entropy': 1.6326173592586906, 'kl': 0.018289916369379784, 'response_length': 74.0585158990354, 'total_length': 234.14883236009248, 'return': 2.3646549528407656e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [31:20<13:34, 203.71s/it][A2025-07-23 14:42:56.816 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:44:07.134 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:44:07.315 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:44:07.316 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 70.50s
2025-07-23 14:44:09.144 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0154,avg_reflection_pattern_score: 0.0083,avg_pass_at_n: 1.0000,avg_num_tokens: 71.4685,std_num_tokens: 55.5752,avg_correct_num_tokens: 69.5153,std_correct_num_tokens: 42.3859,avg_incorrect_num_tokens: 73.9936,std_incorrect_num_tokens: 68.9027
2025-07-23 14:44:09.575 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.26s
2025-07-23 14:44:10.950 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.37s
2025-07-23 14:44:35.510 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 192
2025-07-23 14:44:35.510 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 24.56s
2025-07-23 14:44:36.268 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.76s
2025-07-23 14:44:36.269 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00020527252036117716, avg_kl: 0.021581490834554035, avg_response_length: 71.87927671273549, avg_orm_score: 0.0, avg_custom_rewards: -0.00020527252036117716
2025-07-23 14:44:36.295 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter48_replay_buffer.jsonl
2025-07-23 14:44:37.482 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.19s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/48 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/48 [00:00<?, ?it/s, pg=0.0225, ret=-0.000128, glen=72, tlen=232, kl=0.0169, act_lr=9.6e-7, ent=1.6]Actor Train epoch [1/1]:   2%|‚ñè         | 1/48 [00:00<00:46,  1.02it/s, pg=0.0225, ret=-0.000128, glen=72, tlen=232, kl=0.0169, act_lr=9.6e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/48 [00:01<00:46,  1.02it/s, pg=0.0806, ret=-0.000401, glen=68.4, tlen=228, kl=0.0215, act_lr=9.6e-7, ent=1.62]Actor Train epoch [1/1]:   4%|‚ñç         | 2/48 [00:01<00:42,  1.09it/s, pg=0.0806, ret=-0.000401, glen=68.4, tlen=228, kl=0.0215, act_lr=9.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/48 [00:02<00:42,  1.09it/s, pg=0.0602, ret=-0.000242, glen=66.2, tlen=226, kl=0.0218, act_lr=9.6e-7, ent=1.56]Actor Train epoch [1/1]:   6%|‚ñã         | 3/48 [00:02<00:39,  1.13it/s, pg=0.0602, ret=-0.000242, glen=66.2, tlen=226, kl=0.0218, act_lr=9.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñã         | 3/48 [00:03<00:39,  1.13it/s, pg=0.0139, ret=0.000197, glen=74.5, tlen=235, kl=0.0216, act_lr=9.6e-7, ent=1.63] Actor Train epoch [1/1]:   8%|‚ñä         | 4/48 [00:03<00:39,  1.12it/s, pg=0.0139, ret=0.000197, glen=74.5, tlen=235, kl=0.0216, act_lr=9.6e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/48 [00:04<00:39,  1.12it/s, pg=0.0618, ret=-0.000371, glen=69.7, tlen=230, kl=0.0207, act_lr=9.6e-7, ent=1.47]Actor Train epoch [1/1]:  10%|‚ñà         | 5/48 [00:04<00:38,  1.12it/s, pg=0.0618, ret=-0.000371, glen=69.7, tlen=230, kl=0.0207, act_lr=9.6e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/48 [00:05<00:38,  1.12it/s, pg=-0.00681, ret=-3.08e-5, glen=67, tlen=228, kl=0.0247, act_lr=9.6e-7, ent=1.47] Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 6/48 [00:05<00:37,  1.13it/s, pg=-0.00681, ret=-3.08e-5, glen=67, tlen=228, kl=0.0247, act_lr=9.6e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñé        | 6/48 [00:06<00:37,  1.13it/s, pg=-0.0722, ret=0.00059, glen=71, tlen=231, kl=0.0212, act_lr=9.6e-7, ent=1.65]  Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/48 [00:06<00:35,  1.15it/s, pg=-0.0722, ret=0.00059, glen=71, tlen=231, kl=0.0212, act_lr=9.6e-7, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñç        | 7/48 [00:07<00:35,  1.15it/s, pg=0.138, ret=-0.00122, glen=70.2, tlen=231, kl=0.021, act_lr=9.6e-7, ent=1.55]Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/48 [00:07<00:35,  1.13it/s, pg=0.138, ret=-0.00122, glen=70.2, tlen=231, kl=0.021, act_lr=9.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  17%|‚ñà‚ñã        | 8/48 [00:07<00:35,  1.13it/s, pg=-0.18, ret=0.000706, glen=68.5, tlen=229, kl=0.0191, act_lr=9.6e-7, ent=1.56]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/48 [00:07<00:33,  1.15it/s, pg=-0.18, ret=0.000706, glen=68.5, tlen=229, kl=0.0191, act_lr=9.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 9/48 [00:08<00:33,  1.15it/s, pg=0.118, ret=-0.00129, glen=75.8, tlen=236, kl=0.027, act_lr=9.6e-7, ent=1.49] Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 10/48 [00:08<00:32,  1.16it/s, pg=0.118, ret=-0.00129, glen=75.8, tlen=236, kl=0.027, act_lr=9.6e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  21%|‚ñà‚ñà        | 10/48 [00:09<00:32,  1.16it/s, pg=0.0687, ret=-0.000635, glen=71.4, tlen=231, kl=0.0255, act_lr=9.6e-7, ent=1.53]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/48 [00:09<00:31,  1.16it/s, pg=0.0687, ret=-0.000635, glen=71.4, tlen=231, kl=0.0255, act_lr=9.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 11/48 [00:10<00:31,  1.16it/s, pg=0.0608, ret=-0.000622, glen=79.6, tlen=240, kl=0.0184, act_lr=9.6e-7, ent=1.53]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 12/48 [00:10<00:30,  1.17it/s, pg=0.0608, ret=-0.000622, glen=79.6, tlen=240, kl=0.0184, act_lr=9.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 12/48 [00:11<00:30,  1.17it/s, pg=-0.00327, ret=0.000144, glen=70.3, tlen=231, kl=0.019, act_lr=9.6e-7, ent=1.49]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/48 [00:11<00:29,  1.17it/s, pg=-0.00327, ret=0.000144, glen=70.3, tlen=231, kl=0.019, act_lr=9.6e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/48 [00:12<00:29,  1.17it/s, pg=0.00845, ret=-7.26e-5, glen=73.6, tlen=234, kl=0.0252, act_lr=9.6e-7, ent=1.62]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 14/48 [00:12<00:29,  1.17it/s, pg=0.00845, ret=-7.26e-5, glen=73.6, tlen=234, kl=0.0252, act_lr=9.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 14/48 [00:13<00:29,  1.17it/s, pg=0.0967, ret=-0.000165, glen=78.2, tlen=238, kl=0.0233, act_lr=9.6e-7, ent=1.99]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [00:13<00:28,  1.17it/s, pg=0.0967, ret=-0.000165, glen=78.2, tlen=238, kl=0.0233, act_lr=9.6e-7, ent=1.99]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [00:13<00:28,  1.17it/s, pg=-0.0597, ret=0.000644, glen=71.8, tlen=232, kl=0.024, act_lr=9.6e-7, ent=1.52] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [00:13<00:27,  1.17it/s, pg=-0.0597, ret=0.000644, glen=71.8, tlen=232, kl=0.024, act_lr=9.6e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [00:14<00:27,  1.17it/s, pg=-0.0298, ret=0.000396, glen=71.6, tlen=232, kl=0.0183, act_lr=9.6e-7, ent=1.54]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [00:14<00:27,  1.14it/s, pg=-0.0298, ret=0.000396, glen=71.6, tlen=232, kl=0.0183, act_lr=9.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [00:15<00:27,  1.14it/s, pg=-0.173, ret=0.000577, glen=71.4, tlen=232, kl=0.0209, act_lr=9.6e-7, ent=1.39] Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [00:15<00:26,  1.15it/s, pg=-0.173, ret=0.000577, glen=71.4, tlen=232, kl=0.0209, act_lr=9.6e-7, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [00:16<00:26,  1.15it/s, pg=-0.122, ret=0.000765, glen=71.3, tlen=232, kl=0.0226, act_lr=9.6e-7, ent=1.67]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [00:16<00:25,  1.16it/s, pg=-0.122, ret=0.000765, glen=71.3, tlen=232, kl=0.0226, act_lr=9.6e-7, ent=1.67]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [00:17<00:25,  1.16it/s, pg=-0.00159, ret=-0.000519, glen=71.6, tlen=231, kl=0.0282, act_lr=9.6e-7, ent=1.53]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20/48 [00:17<00:24,  1.16it/s, pg=-0.00159, ret=-0.000519, glen=71.6, tlen=231, kl=0.0282, act_lr=9.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20/48 [00:18<00:24,  1.16it/s, pg=-0.0661, ret=0.000543, glen=96.6, tlen=257, kl=0.0176, act_lr=9.6e-7, ent=2.25]  Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [00:18<00:23,  1.16it/s, pg=-0.0661, ret=0.000543, glen=96.6, tlen=257, kl=0.0176, act_lr=9.6e-7, ent=2.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [00:19<00:23,  1.16it/s, pg=0.0322, ret=-9.27e-5, glen=74.8, tlen=235, kl=0.0177, act_lr=9.6e-7, ent=1.52] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [00:19<00:22,  1.14it/s, pg=0.0322, ret=-9.27e-5, glen=74.8, tlen=235, kl=0.0177, act_lr=9.6e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [00:20<00:22,  1.14it/s, pg=0.0191, ret=9.67e-5, glen=73.1, tlen=233, kl=0.0199, act_lr=9.6e-7, ent=1.56] Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 23/48 [00:20<00:21,  1.15it/s, pg=0.0191, ret=9.67e-5, glen=73.1, tlen=233, kl=0.0199, act_lr=9.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 23/48 [00:20<00:21,  1.15it/s, pg=-0.0743, ret=0.000638, glen=70.9, tlen=231, kl=0.0201, act_lr=9.6e-7, ent=1.57]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/48 [00:20<00:20,  1.16it/s, pg=-0.0743, ret=0.000638, glen=70.9, tlen=231, kl=0.0201, act_lr=9.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 24/48 [00:21<00:20,  1.16it/s, pg=0.16, ret=-0.00096, glen=67.3, tlen=228, kl=0.0186, act_lr=9.6e-7, ent=1.54]   Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25/48 [00:21<00:19,  1.16it/s, pg=0.16, ret=-0.00096, glen=67.3, tlen=228, kl=0.0186, act_lr=9.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25/48 [00:22<00:19,  1.16it/s, pg=-0.00684, ret=0.000282, glen=66.3, tlen=227, kl=0.0213, act_lr=9.6e-7, ent=1.55]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26/48 [00:22<00:18,  1.17it/s, pg=-0.00684, ret=0.000282, glen=66.3, tlen=227, kl=0.0213, act_lr=9.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26/48 [00:23<00:18,  1.17it/s, pg=0.056, ret=-0.00048, glen=66.9, tlen=227, kl=0.0226, act_lr=9.6e-7, ent=1.56]   Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/48 [00:23<00:20,  1.01it/s, pg=0.056, ret=-0.00048, glen=66.9, tlen=227, kl=0.0226, act_lr=9.6e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 27/48 [00:24<00:20,  1.01it/s, pg=0.0502, ret=0.000262, glen=71.8, tlen=232, kl=0.0218, act_lr=9.6e-7, ent=1.49]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 28/48 [00:24<00:18,  1.05it/s, pg=0.0502, ret=0.000262, glen=71.8, tlen=232, kl=0.0218, act_lr=9.6e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 28/48 [00:25<00:18,  1.05it/s, pg=0.0253, ret=-0.00059, glen=75.7, tlen=236, kl=0.0175, act_lr=9.6e-7, ent=1.51]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 29/48 [00:25<00:17,  1.09it/s, pg=0.0253, ret=-0.00059, glen=75.7, tlen=236, kl=0.0175, act_lr=9.6e-7, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 29/48 [00:26<00:17,  1.09it/s, pg=-0.116, ret=0.001, glen=67.5, tlen=228, kl=0.0231, act_lr=9.6e-7, ent=1.52]   Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 30/48 [00:26<00:16,  1.11it/s, pg=-0.116, ret=0.001, glen=67.5, tlen=228, kl=0.0231, act_lr=9.6e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 30/48 [00:27<00:16,  1.11it/s, pg=-0.0703, ret=-0.000142, glen=69, tlen=229, kl=0.0266, act_lr=9.6e-7, ent=1.52]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 31/48 [00:27<00:15,  1.13it/s, pg=-0.0703, ret=-0.000142, glen=69, tlen=229, kl=0.0266, act_lr=9.6e-7, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 31/48 [00:28<00:15,  1.13it/s, pg=-0.00188, ret=-9.93e-5, glen=69.2, tlen=230, kl=0.0219, act_lr=9.6e-7, ent=1.5]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 32/48 [00:28<00:13,  1.15it/s, pg=-0.00188, ret=-9.93e-5, glen=69.2, tlen=230, kl=0.0219, act_lr=9.6e-7, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 32/48 [00:28<00:13,  1.15it/s, pg=0.0654, ret=-0.000208, glen=69.5, tlen=229, kl=0.0256, act_lr=9.6e-7, ent=1.59]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 33/48 [00:28<00:12,  1.16it/s, pg=0.0654, ret=-0.000208, glen=69.5, tlen=229, kl=0.0256, act_lr=9.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 33/48 [00:29<00:12,  1.16it/s, pg=-0.0632, ret=0.000181, glen=69.4, tlen=230, kl=0.0202, act_lr=9.6e-7, ent=1.54]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 34/48 [00:29<00:12,  1.16it/s, pg=-0.0632, ret=0.000181, glen=69.4, tlen=230, kl=0.0202, act_lr=9.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 34/48 [00:30<00:12,  1.16it/s, pg=-0.152, ret=0.000955, glen=71.3, tlen=231, kl=0.0179, act_lr=9.6e-7, ent=1.54] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 35/48 [00:30<00:11,  1.17it/s, pg=-0.152, ret=0.000955, glen=71.3, tlen=231, kl=0.0179, act_lr=9.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 35/48 [00:31<00:11,  1.17it/s, pg=-0.0651, ret=0.000164, glen=75.3, tlen=235, kl=0.0204, act_lr=9.6e-7, ent=1.58]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 36/48 [00:31<00:10,  1.17it/s, pg=-0.0651, ret=0.000164, glen=75.3, tlen=235, kl=0.0204, act_lr=9.6e-7, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 36/48 [00:32<00:10,  1.17it/s, pg=0.2, ret=-0.000149, glen=73, tlen=233, kl=0.0204, act_lr=9.6e-7, ent=1.54]     Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 37/48 [00:32<00:09,  1.17it/s, pg=0.2, ret=-0.000149, glen=73, tlen=233, kl=0.0204, act_lr=9.6e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 37/48 [00:33<00:09,  1.17it/s, pg=-0.0461, ret=9.47e-5, glen=71.5, tlen=232, kl=0.0221, act_lr=9.6e-7, ent=1.53]Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 38/48 [00:33<00:08,  1.17it/s, pg=-0.0461, ret=9.47e-5, glen=71.5, tlen=232, kl=0.0221, act_lr=9.6e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 38/48 [00:34<00:08,  1.17it/s, pg=-0.141, ret=0.000564, glen=75.1, tlen=235, kl=0.0202, act_lr=9.6e-7, ent=1.62]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [00:34<00:07,  1.18it/s, pg=-0.141, ret=0.000564, glen=75.1, tlen=235, kl=0.0202, act_lr=9.6e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [00:34<00:07,  1.18it/s, pg=0.037, ret=1.84e-5, glen=73.1, tlen=234, kl=0.0234, act_lr=9.6e-7, ent=1.49]  Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 40/48 [00:34<00:06,  1.18it/s, pg=0.037, ret=1.84e-5, glen=73.1, tlen=234, kl=0.0234, act_lr=9.6e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 40/48 [00:35<00:06,  1.18it/s, pg=0.119, ret=-0.000457, glen=70.3, tlen=230, kl=0.0206, act_lr=9.6e-7, ent=1.57]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 41/48 [00:35<00:05,  1.18it/s, pg=0.119, ret=-0.000457, glen=70.3, tlen=230, kl=0.0206, act_lr=9.6e-7, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 41/48 [00:36<00:05,  1.18it/s, pg=0.0837, ret=-0.000381, glen=66.7, tlen=227, kl=0.0211, act_lr=9.6e-7, ent=1.48]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 42/48 [00:36<00:05,  1.18it/s, pg=0.0837, ret=-0.000381, glen=66.7, tlen=227, kl=0.0211, act_lr=9.6e-7, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 42/48 [00:37<00:05,  1.18it/s, pg=0.0439, ret=-0.000814, glen=75.3, tlen=235, kl=0.0196, act_lr=9.6e-7, ent=1.5] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 43/48 [00:37<00:04,  1.18it/s, pg=0.0439, ret=-0.000814, glen=75.3, tlen=235, kl=0.0196, act_lr=9.6e-7, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 43/48 [00:38<00:04,  1.18it/s, pg=0.0864, ret=-0.00057, glen=69.9, tlen=230, kl=0.0201, act_lr=9.6e-7, ent=1.61]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/48 [00:38<00:03,  1.18it/s, pg=0.0864, ret=-0.00057, glen=69.9, tlen=230, kl=0.0201, act_lr=9.6e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 44/48 [00:39<00:03,  1.18it/s, pg=-0.0831, ret=0.000594, glen=80.9, tlen=241, kl=0.027, act_lr=9.6e-7, ent=1.59]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [00:39<00:02,  1.18it/s, pg=-0.0831, ret=0.000594, glen=80.9, tlen=241, kl=0.027, act_lr=9.6e-7, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [00:40<00:02,  1.18it/s, pg=0.083, ret=-0.000557, glen=66.5, tlen=227, kl=0.0225, act_lr=9.6e-7, ent=1.55]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 46/48 [00:40<00:01,  1.18it/s, pg=0.083, ret=-0.000557, glen=66.5, tlen=227, kl=0.0225, act_lr=9.6e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 46/48 [00:40<00:01,  1.18it/s, pg=-0.118, ret=0.000501, glen=66.1, tlen=226, kl=0.0277, act_lr=9.6e-7, ent=1.46]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:40<00:00,  1.18it/s, pg=-0.118, ret=0.000501, glen=66.1, tlen=226, kl=0.0277, act_lr=9.6e-7, ent=1.46]
2025-07-23 14:45:19.436 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 41.79s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:41<00:00,  1.18it/s, pg=-0.106, ret=0.000715, glen=73.4, tlen=233, kl=0.0175, act_lr=9.8e-7, ent=1.51]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 47/48 [00:41<00:00,  1.13it/s, pg=-0.106, ret=0.000715, glen=73.4, tlen=233, kl=0.0175, act_lr=9.8e-7, ent=1.51]
2025-07-23 14:45:20.269 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.78s
2025-07-23 14:45:22.834 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.56s
2025-07-23 14:45:23.139 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 45.62s
2025-07-23 14:45:23.144 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': 0.0006452401479085287, 'actor_lr': 9.604166327411197e-07, 'clip_ratio': 0.0, 'entropy': 1.5665663704276085, 'kl': 0.021581490834554035, 'response_length': 71.87927707036336, 'total_length': 232.12357807159424, 'return': -1.1812752897337001e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [33:46<09:18, 186.00s/it][A2025-07-23 14:45:23.174 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:46:28.918 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:46:29.104 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:46:29.105 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 65.93s
2025-07-23 14:46:30.972 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0156,avg_reflection_pattern_score: 0.0055,avg_pass_at_n: 1.0000,avg_num_tokens: 73.5160,std_num_tokens: 56.3402,avg_correct_num_tokens: 70.9434,std_correct_num_tokens: 50.0403,avg_incorrect_num_tokens: 76.8533,std_incorrect_num_tokens: 63.4341
2025-07-23 14:46:31.405 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.30s
2025-07-23 14:46:32.590 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.18s
2025-07-23 14:46:58.231 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 194
2025-07-23 14:46:58.231 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.61s
2025-07-23 14:46:58.951 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.72s
2025-07-23 14:46:58.952 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.00016325110279758143, avg_kl: 0.027114081628543816, avg_response_length: 73.89251539879238, avg_orm_score: 0.0, avg_custom_rewards: 0.00016325110279758143
2025-07-23 14:46:58.982 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter49_replay_buffer.jsonl
2025-07-23 14:47:00.195 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.22s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s, pg=-0.143, ret=0.000633, glen=88.7, tlen=249, kl=0.0164, act_lr=9.8e-7, ent=1.61]Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:00<00:46,  1.03it/s, pg=-0.143, ret=0.000633, glen=88.7, tlen=249, kl=0.0164, act_lr=9.8e-7, ent=1.61]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:01<00:46,  1.03it/s, pg=0.144, ret=-0.000482, glen=67.1, tlen=228, kl=0.0329, act_lr=9.8e-7, ent=1.47]Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:01<00:42,  1.10it/s, pg=0.144, ret=-0.000482, glen=67.1, tlen=228, kl=0.0329, act_lr=9.8e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:02<00:42,  1.10it/s, pg=0.218, ret=-0.00146, glen=74.5, tlen=235, kl=0.018, act_lr=9.8e-7, ent=1.43]  Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:02<00:44,  1.03it/s, pg=0.218, ret=-0.00146, glen=74.5, tlen=235, kl=0.018, act_lr=9.8e-7, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:03<00:44,  1.03it/s, pg=0.00574, ret=0.000117, glen=72.9, tlen=233, kl=0.0326, act_lr=9.8e-7, ent=1.6]Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:03<00:41,  1.08it/s, pg=0.00574, ret=0.000117, glen=72.9, tlen=233, kl=0.0326, act_lr=9.8e-7, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:04<00:41,  1.08it/s, pg=-0.0203, ret=-2.69e-5, glen=73.4, tlen=234, kl=0.026, act_lr=9.8e-7, ent=1.42]Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:04<00:40,  1.09it/s, pg=-0.0203, ret=-2.69e-5, glen=73.4, tlen=234, kl=0.026, act_lr=9.8e-7, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:05<00:40,  1.09it/s, pg=0.0358, ret=-0.000139, glen=72.9, tlen=234, kl=0.0313, act_lr=9.8e-7, ent=1.4]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:05<00:38,  1.11it/s, pg=0.0358, ret=-0.000139, glen=72.9, tlen=234, kl=0.0313, act_lr=9.8e-7, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:06<00:38,  1.11it/s, pg=0.0971, ret=-0.000284, glen=75.3, tlen=236, kl=0.027, act_lr=9.8e-7, ent=1.5] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:06<00:37,  1.11it/s, pg=0.0971, ret=-0.000284, glen=75.3, tlen=236, kl=0.027, act_lr=9.8e-7, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:07<00:37,  1.11it/s, pg=0.22, ret=-0.000664, glen=83.2, tlen=244, kl=0.0179, act_lr=9.8e-7, ent=1.71]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:36,  1.13it/s, pg=0.22, ret=-0.000664, glen=83.2, tlen=244, kl=0.0179, act_lr=9.8e-7, ent=1.71]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:08<00:36,  1.13it/s, pg=0.0396, ret=-5.72e-5, glen=71, tlen=232, kl=0.0273, act_lr=9.8e-7, ent=1.48] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:35,  1.14it/s, pg=0.0396, ret=-5.72e-5, glen=71, tlen=232, kl=0.0273, act_lr=9.8e-7, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:09<00:35,  1.14it/s, pg=0.0357, ret=-0.000259, glen=75.4, tlen=236, kl=0.0257, act_lr=9.8e-7, ent=1.39]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:09<00:34,  1.14it/s, pg=0.0357, ret=-0.000259, glen=75.4, tlen=236, kl=0.0257, act_lr=9.8e-7, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:09<00:34,  1.14it/s, pg=0.092, ret=-0.000114, glen=73.3, tlen=234, kl=0.023, act_lr=9.8e-7, ent=1.43]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:09<00:33,  1.15it/s, pg=0.092, ret=-0.000114, glen=73.3, tlen=234, kl=0.023, act_lr=9.8e-7, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:10<00:33,  1.15it/s, pg=0.0436, ret=0.00372, glen=72.1, tlen=232, kl=0.0196, act_lr=9.8e-7, ent=1.37]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:10<00:32,  1.15it/s, pg=0.0436, ret=0.00372, glen=72.1, tlen=232, kl=0.0196, act_lr=9.8e-7, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:11<00:32,  1.15it/s, pg=-0.11, ret=2.71e-5, glen=72.1, tlen=233, kl=0.0227, act_lr=9.8e-7, ent=1.42] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:11<00:31,  1.16it/s, pg=-0.11, ret=2.71e-5, glen=72.1, tlen=233, kl=0.0227, act_lr=9.8e-7, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:12<00:31,  1.16it/s, pg=-0.191, ret=0.00166, glen=76.5, tlen=237, kl=0.0324, act_lr=9.8e-7, ent=1.53]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:12<00:30,  1.16it/s, pg=-0.191, ret=0.00166, glen=76.5, tlen=237, kl=0.0324, act_lr=9.8e-7, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:13<00:30,  1.16it/s, pg=-0.0349, ret=0.000337, glen=71.5, tlen=232, kl=0.0209, act_lr=9.8e-7, ent=1.5]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:13<00:29,  1.17it/s, pg=-0.0349, ret=0.000337, glen=71.5, tlen=232, kl=0.0209, act_lr=9.8e-7, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:14<00:29,  1.17it/s, pg=-0.144, ret=0.00121, glen=82.8, tlen=244, kl=0.0295, act_lr=9.8e-7, ent=1.62] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.17it/s, pg=-0.144, ret=0.00121, glen=82.8, tlen=244, kl=0.0295, act_lr=9.8e-7, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.17it/s, pg=-0.159, ret=0.00125, glen=73.5, tlen=234, kl=0.0178, act_lr=9.8e-7, ent=1.35]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:14<00:27,  1.17it/s, pg=-0.159, ret=0.00125, glen=73.5, tlen=234, kl=0.0178, act_lr=9.8e-7, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:15<00:27,  1.17it/s, pg=0.0948, ret=-0.000238, glen=77.1, tlen=237, kl=0.0219, act_lr=9.8e-7, ent=1.63]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:15<00:26,  1.17it/s, pg=0.0948, ret=-0.000238, glen=77.1, tlen=237, kl=0.0219, act_lr=9.8e-7, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:16<00:26,  1.17it/s, pg=0.031, ret=-0.000416, glen=79.9, tlen=241, kl=0.0223, act_lr=9.8e-7, ent=1.43] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:16<00:25,  1.17it/s, pg=0.031, ret=-0.000416, glen=79.9, tlen=241, kl=0.0223, act_lr=9.8e-7, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:17<00:25,  1.17it/s, pg=0.0785, ret=-0.000959, glen=68.5, tlen=229, kl=0.0277, act_lr=9.8e-7, ent=1.46]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:17<00:24,  1.18it/s, pg=0.0785, ret=-0.000959, glen=68.5, tlen=229, kl=0.0277, act_lr=9.8e-7, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:18<00:24,  1.18it/s, pg=0.0666, ret=-0.000569, glen=72.2, tlen=233, kl=0.022, act_lr=9.8e-7, ent=1.47] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:18<00:23,  1.18it/s, pg=0.0666, ret=-0.000569, glen=72.2, tlen=233, kl=0.022, act_lr=9.8e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:19<00:23,  1.18it/s, pg=-0.0592, ret=0.000152, glen=79.9, tlen=240, kl=0.0263, act_lr=9.8e-7, ent=1.55]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:19<00:22,  1.18it/s, pg=-0.0592, ret=0.000152, glen=79.9, tlen=240, kl=0.0263, act_lr=9.8e-7, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:20<00:22,  1.18it/s, pg=-0.0387, ret=0.000205, glen=73.6, tlen=234, kl=0.0211, act_lr=9.8e-7, ent=1.42]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.17it/s, pg=-0.0387, ret=0.000205, glen=73.6, tlen=234, kl=0.0211, act_lr=9.8e-7, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.17it/s, pg=-0.0899, ret=0.000975, glen=72.9, tlen=234, kl=0.024, act_lr=9.8e-7, ent=1.44] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:20<00:21,  1.17it/s, pg=-0.0899, ret=0.000975, glen=72.9, tlen=234, kl=0.024, act_lr=9.8e-7, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:21<00:21,  1.17it/s, pg=-0.223, ret=0.000917, glen=70.7, tlen=231, kl=0.0208, act_lr=9.8e-7, ent=1.45]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:21<00:20,  1.17it/s, pg=-0.223, ret=0.000917, glen=70.7, tlen=231, kl=0.0208, act_lr=9.8e-7, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:22<00:20,  1.17it/s, pg=-0.0913, ret=0.000449, glen=70.7, tlen=232, kl=0.0259, act_lr=9.8e-7, ent=1.4]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:22<00:19,  1.17it/s, pg=-0.0913, ret=0.000449, glen=70.7, tlen=232, kl=0.0259, act_lr=9.8e-7, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:24<00:19,  1.17it/s, pg=-0.0601, ret=0.00073, glen=75.8, tlen=236, kl=0.0229, act_lr=9.8e-7, ent=1.56]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.03s/it, pg=-0.0601, ret=0.00073, glen=75.8, tlen=236, kl=0.0229, act_lr=9.8e-7, ent=1.56]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.03s/it, pg=0.0703, ret=-0.000755, glen=68.1, tlen=229, kl=0.0294, act_lr=9.8e-7, ent=1.37]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:24<00:20,  1.02it/s, pg=0.0703, ret=-0.000755, glen=68.1, tlen=229, kl=0.0294, act_lr=9.8e-7, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:25<00:20,  1.02it/s, pg=0.129, ret=-0.000394, glen=70.4, tlen=231, kl=0.023, act_lr=9.8e-7, ent=1.5]   Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:25<00:18,  1.07it/s, pg=0.129, ret=-0.000394, glen=70.4, tlen=231, kl=0.023, act_lr=9.8e-7, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:26<00:18,  1.07it/s, pg=0.0332, ret=-0.000353, glen=70.8, tlen=232, kl=0.0183, act_lr=9.8e-7, ent=1.49]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:26<00:17,  1.10it/s, pg=0.0332, ret=-0.000353, glen=70.8, tlen=232, kl=0.0183, act_lr=9.8e-7, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:27<00:17,  1.10it/s, pg=0.0652, ret=7.23e-5, glen=69.5, tlen=230, kl=0.0255, act_lr=9.8e-7, ent=1.42]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:27<00:16,  1.12it/s, pg=0.0652, ret=7.23e-5, glen=69.5, tlen=230, kl=0.0255, act_lr=9.8e-7, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:28<00:16,  1.12it/s, pg=0.172, ret=-0.00141, glen=74.1, tlen=234, kl=0.0487, act_lr=9.8e-7, ent=1.46]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:28<00:14,  1.14it/s, pg=0.172, ret=-0.00141, glen=74.1, tlen=234, kl=0.0487, act_lr=9.8e-7, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:29<00:14,  1.14it/s, pg=0.0518, ret=7.83e-6, glen=69.1, tlen=230, kl=0.0279, act_lr=9.8e-7, ent=1.39]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:13,  1.15it/s, pg=0.0518, ret=7.83e-6, glen=69.1, tlen=230, kl=0.0279, act_lr=9.8e-7, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:30<00:13,  1.15it/s, pg=-0.0596, ret=0.000524, glen=64, tlen=224, kl=0.0332, act_lr=9.8e-7, ent=1.46]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:12,  1.16it/s, pg=-0.0596, ret=0.000524, glen=64, tlen=224, kl=0.0332, act_lr=9.8e-7, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:12,  1.16it/s, pg=0.0867, ret=-0.00106, glen=69.7, tlen=230, kl=0.025, act_lr=9.8e-7, ent=1.39]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:30<00:12,  1.16it/s, pg=0.0867, ret=-0.00106, glen=69.7, tlen=230, kl=0.025, act_lr=9.8e-7, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:31<00:12,  1.16it/s, pg=0.0133, ret=0.000103, glen=66.1, tlen=227, kl=0.0245, act_lr=9.8e-7, ent=1.39]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:31<00:11,  1.17it/s, pg=0.0133, ret=0.000103, glen=66.1, tlen=227, kl=0.0245, act_lr=9.8e-7, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:32<00:11,  1.17it/s, pg=-0.126, ret=0.000713, glen=74.9, tlen=235, kl=0.0319, act_lr=9.8e-7, ent=1.45]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:32<00:10,  1.17it/s, pg=-0.126, ret=0.000713, glen=74.9, tlen=235, kl=0.0319, act_lr=9.8e-7, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:33<00:10,  1.17it/s, pg=-0.205, ret=0.000616, glen=84.8, tlen=245, kl=0.0215, act_lr=9.8e-7, ent=1.83]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:33<00:09,  1.17it/s, pg=-0.205, ret=0.000616, glen=84.8, tlen=245, kl=0.0215, act_lr=9.8e-7, ent=1.83]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:34<00:09,  1.17it/s, pg=-0.164, ret=0.000923, glen=65.5, tlen=226, kl=0.0257, act_lr=9.8e-7, ent=1.36]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:34<00:08,  1.17it/s, pg=-0.164, ret=0.000923, glen=65.5, tlen=226, kl=0.0257, act_lr=9.8e-7, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:35<00:08,  1.17it/s, pg=0.128, ret=-0.00117, glen=68.6, tlen=229, kl=0.0865, act_lr=9.8e-7, ent=1.38] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=0.128, ret=-0.00117, glen=68.6, tlen=229, kl=0.0865, act_lr=9.8e-7, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=-0.0256, ret=0.000304, glen=76.1, tlen=237, kl=0.0428, act_lr=9.8e-7, ent=1.51]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:35<00:06,  1.17it/s, pg=-0.0256, ret=0.000304, glen=76.1, tlen=237, kl=0.0428, act_lr=9.8e-7, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.17it/s, pg=-0.141, ret=0.000349, glen=73.2, tlen=234, kl=0.0243, act_lr=9.8e-7, ent=1.54] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:36<00:05,  1.18it/s, pg=-0.141, ret=0.000349, glen=73.2, tlen=234, kl=0.0243, act_lr=9.8e-7, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:37<00:05,  1.18it/s, pg=-0.0766, ret=-0.000318, glen=77.2, tlen=238, kl=0.0197, act_lr=9.8e-7, ent=1.48]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:37<00:05,  1.18it/s, pg=-0.0766, ret=-0.000318, glen=77.2, tlen=238, kl=0.0197, act_lr=9.8e-7, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:38<00:05,  1.18it/s, pg=-0.0145, ret=0.000261, glen=74.2, tlen=235, kl=0.0197, act_lr=9.8e-7, ent=1.42] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:38<00:04,  1.18it/s, pg=-0.0145, ret=0.000261, glen=74.2, tlen=235, kl=0.0197, act_lr=9.8e-7, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:39<00:04,  1.18it/s, pg=-0.038, ret=-1.49e-5, glen=73.6, tlen=234, kl=0.022, act_lr=9.8e-7, ent=1.45]  Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:39<00:03,  1.18it/s, pg=-0.038, ret=-1.49e-5, glen=73.6, tlen=234, kl=0.022, act_lr=9.8e-7, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:40<00:03,  1.18it/s, pg=0.0322, ret=0.00149, glen=92.3, tlen=253, kl=0.0225, act_lr=9.8e-7, ent=2.06]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:40<00:02,  1.17it/s, pg=0.0322, ret=0.00149, glen=92.3, tlen=253, kl=0.0225, act_lr=9.8e-7, ent=2.06]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:41<00:02,  1.17it/s, pg=-0.0864, ret=0.000615, glen=78.9, tlen=240, kl=0.0288, act_lr=9.8e-7, ent=1.47]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=-0.0864, ret=0.000615, glen=78.9, tlen=240, kl=0.0288, act_lr=9.8e-7, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.17it/s, pg=0.164, ret=-0.00158, glen=75.2, tlen=236, kl=0.0215, act_lr=9.8e-7, ent=1.52]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:41<00:00,  1.17it/s, pg=0.164, ret=-0.00158, glen=75.2, tlen=236, kl=0.0215, act_lr=9.8e-7, ent=1.52]
2025-07-23 14:47:43.228 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 42.88s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.17it/s, pg=0.0217, ret=-0.000737, glen=73.3, tlen=234, kl=0.0492, act_lr=1e-6, ent=1.44]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.12it/s, pg=0.0217, ret=-0.000737, glen=73.3, tlen=234, kl=0.0492, act_lr=1e-6, ent=1.44]
2025-07-23 14:47:43.907 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 14:47:46.199 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-23 14:47:46.524 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 46.29s
2025-07-23 14:47:46.529 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0026620553464305643, 'actor_lr': 9.804081451986758e-07, 'clip_ratio': 0.0, 'entropy': 1.4858107664147202, 'kl': 0.02708964445153061, 'response_length': 74.01970197716538, 'total_length': 234.62658971669723, 'return': 9.96121647292558e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [36:09<05:45, 172.96s/it][A2025-07-23 14:47:52.743 | INFO     | orz.ppo.trainer:train:182 - Successfully save model weights, training continue.
2025-07-23 14:47:52.744 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:   1%|          | 1/172 [00:00<01:38,  1.73it/s, est. speed input: 306.10 toks/s, output: 32.86 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  18%|‚ñà‚ñä        | 31/171 [00:01<00:04, 28.72it/s, est. speed input: 3949.10 toks/s, output: 686.61 toks/s]Processed prompts:  20%|‚ñà‚ñà        | 35/171 [00:01<00:04, 30.65it/s, est. speed input: 4135.21 toks/s, output: 759.35 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/172 [00:03<00:00, 49.79it/s, est. speed input: 8259.83 toks/s, output: 3165.91 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884799)[0m Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:03<00:00, 42.79it/s, est. speed input: 8223.74 toks/s, output: 3077.67 toks/s][32m [repeated 78x across cluster][0m
[36m(LLMActor pid=884798)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:11<00:00, 14.88it/s, est. speed input: 2699.84 toks/s, output: 1165.02 toks/s][32m [repeated 14x across cluster][0m
2025-07-23 14:48:05.075 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 350.9039,strategyqa_test/accuracy: 0.5721,eval_accuracy: 0.5721
2025-07-23 14:48:05.361 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:48:59.708 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:48:59.887 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:48:59.887 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 54.53s
2025-07-23 14:49:01.650 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0155,avg_reflection_pattern_score: 0.0065,avg_pass_at_n: 0.9922,avg_num_tokens: 73.6873,std_num_tokens: 52.7884,avg_correct_num_tokens: 71.8027,std_correct_num_tokens: 46.3229,avg_incorrect_num_tokens: 76.1577,std_incorrect_num_tokens: 60.1321
2025-07-23 14:49:02.047 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.16s
2025-07-23 14:49:03.459 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.41s
2025-07-23 14:49:28.521 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 194
2025-07-23 14:49:28.521 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.06s
2025-07-23 14:49:29.234 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.71s
2025-07-23 14:49:29.235 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 3.021664211776146e-05, avg_kl: 0.033403848864368556, avg_response_length: 74.11050757182014, avg_orm_score: 0.0, avg_custom_rewards: 3.021664211776146e-05
2025-07-23 14:49:29.263 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter50_replay_buffer.jsonl
2025-07-23 14:49:30.469 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.21s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/49 [00:00<?, ?it/s, pg=-0.133, ret=-0.000151, glen=72.5, tlen=233, kl=0.0255, act_lr=1e-6, ent=1.48]Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:00<00:47,  1.02it/s, pg=-0.133, ret=-0.000151, glen=72.5, tlen=233, kl=0.0255, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/49 [00:01<00:47,  1.02it/s, pg=0.0688, ret=-0.000501, glen=74.4, tlen=235, kl=0.0551, act_lr=1e-6, ent=1.51]Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:01<00:42,  1.10it/s, pg=0.0688, ret=-0.000501, glen=74.4, tlen=235, kl=0.0551, act_lr=1e-6, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/49 [00:02<00:42,  1.10it/s, pg=0.191, ret=-0.000376, glen=96.8, tlen=257, kl=0.0311, act_lr=1e-6, ent=1.87] Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:02<00:40,  1.13it/s, pg=0.191, ret=-0.000376, glen=96.8, tlen=257, kl=0.0311, act_lr=1e-6, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/49 [00:03<00:40,  1.13it/s, pg=-0.0107, ret=-1.37e-5, glen=72.6, tlen=233, kl=0.0253, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:03<00:41,  1.10it/s, pg=-0.0107, ret=-1.37e-5, glen=72.6, tlen=233, kl=0.0253, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/49 [00:04<00:41,  1.10it/s, pg=-0.0321, ret=0.000345, glen=73.6, tlen=234, kl=0.0277, act_lr=1e-6, ent=1.41]Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:04<00:40,  1.09it/s, pg=-0.0321, ret=0.000345, glen=73.6, tlen=234, kl=0.0277, act_lr=1e-6, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/49 [00:05<00:40,  1.09it/s, pg=0.0743, ret=-0.000331, glen=72.2, tlen=232, kl=0.0448, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:05<00:38,  1.11it/s, pg=0.0743, ret=-0.000331, glen=72.2, tlen=232, kl=0.0448, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/49 [00:06<00:38,  1.11it/s, pg=-0.0464, ret=-6.1e-5, glen=70.2, tlen=230, kl=0.0258, act_lr=1e-6, ent=1.58] Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:06<00:37,  1.13it/s, pg=-0.0464, ret=-6.1e-5, glen=70.2, tlen=230, kl=0.0258, act_lr=1e-6, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/49 [00:07<00:37,  1.13it/s, pg=-0.0699, ret=-0.000633, glen=75.2, tlen=235, kl=0.0233, act_lr=1e-6, ent=1.64]Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:07<00:36,  1.11it/s, pg=-0.0699, ret=-0.000633, glen=75.2, tlen=235, kl=0.0233, act_lr=1e-6, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñã        | 8/49 [00:08<00:36,  1.11it/s, pg=-0.127, ret=0.000975, glen=70.9, tlen=232, kl=0.0208, act_lr=1e-6, ent=1.38]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:35,  1.13it/s, pg=-0.127, ret=0.000975, glen=70.9, tlen=232, kl=0.0208, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/49 [00:08<00:35,  1.13it/s, pg=-0.0791, ret=0.000541, glen=70.5, tlen=231, kl=0.0308, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:08<00:34,  1.14it/s, pg=-0.0791, ret=0.000541, glen=70.5, tlen=231, kl=0.0308, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/49 [00:09<00:34,  1.14it/s, pg=-0.00851, ret=7.88e-5, glen=69.9, tlen=230, kl=0.03, act_lr=1e-6, ent=1.38]  Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:09<00:33,  1.15it/s, pg=-0.00851, ret=7.88e-5, glen=69.9, tlen=230, kl=0.03, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/49 [00:10<00:33,  1.15it/s, pg=-0.0652, ret=0.000696, glen=70.8, tlen=231, kl=0.0224, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:10<00:31,  1.16it/s, pg=-0.0652, ret=0.000696, glen=70.8, tlen=231, kl=0.0224, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/49 [00:11<00:31,  1.16it/s, pg=0.306, ret=-0.00246, glen=72.6, tlen=233, kl=0.0338, act_lr=1e-6, ent=1.53]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:11<00:30,  1.16it/s, pg=0.306, ret=-0.00246, glen=72.6, tlen=233, kl=0.0338, act_lr=1e-6, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 13/49 [00:12<00:30,  1.16it/s, pg=-0.103, ret=-0.000159, glen=69.3, tlen=230, kl=0.0549, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:12<00:30,  1.16it/s, pg=-0.103, ret=-0.000159, glen=69.3, tlen=230, kl=0.0549, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñä       | 14/49 [00:13<00:30,  1.16it/s, pg=0.0948, ret=-0.000346, glen=74.4, tlen=234, kl=0.0236, act_lr=1e-6, ent=1.47]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:13<00:29,  1.17it/s, pg=0.0948, ret=-0.000346, glen=74.4, tlen=234, kl=0.0236, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 15/49 [00:14<00:29,  1.17it/s, pg=-0.00691, ret=6.07e-5, glen=75.6, tlen=236, kl=0.0247, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.17it/s, pg=-0.00691, ret=6.07e-5, glen=75.6, tlen=236, kl=0.0247, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/49 [00:14<00:28,  1.17it/s, pg=-0.0109, ret=0.000401, glen=78.6, tlen=239, kl=0.028, act_lr=1e-6, ent=1.54] Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:14<00:27,  1.17it/s, pg=-0.0109, ret=0.000401, glen=78.6, tlen=239, kl=0.028, act_lr=1e-6, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 17/49 [00:15<00:27,  1.17it/s, pg=-0.0839, ret=0.000895, glen=71.2, tlen=232, kl=0.0275, act_lr=1e-6, ent=1.48]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:15<00:26,  1.17it/s, pg=-0.0839, ret=0.000895, glen=71.2, tlen=232, kl=0.0275, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 18/49 [00:16<00:26,  1.17it/s, pg=0.0616, ret=-0.000276, glen=72.3, tlen=232, kl=0.0229, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:16<00:25,  1.17it/s, pg=0.0616, ret=-0.000276, glen=72.3, tlen=232, kl=0.0229, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19/49 [00:17<00:25,  1.17it/s, pg=-0.12, ret=0.000449, glen=68.9, tlen=229, kl=0.0275, act_lr=1e-6, ent=1.44]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:17<00:24,  1.17it/s, pg=-0.12, ret=0.000449, glen=68.9, tlen=229, kl=0.0275, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 20/49 [00:18<00:24,  1.17it/s, pg=-0.00336, ret=9.18e-5, glen=66.8, tlen=227, kl=0.0282, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:18<00:23,  1.17it/s, pg=-0.00336, ret=9.18e-5, glen=66.8, tlen=227, kl=0.0282, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21/49 [00:19<00:23,  1.17it/s, pg=-0.00706, ret=-0.000338, glen=65.7, tlen=226, kl=0.026, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:19<00:23,  1.17it/s, pg=-0.00706, ret=-0.000338, glen=65.7, tlen=226, kl=0.026, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/49 [00:20<00:23,  1.17it/s, pg=-0.136, ret=0.000884, glen=82.3, tlen=243, kl=0.0316, act_lr=1e-6, ent=1.4]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.17it/s, pg=-0.136, ret=0.000884, glen=82.3, tlen=243, kl=0.0316, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23/49 [00:20<00:22,  1.17it/s, pg=0.135, ret=-9.34e-5, glen=72, tlen=232, kl=0.029, act_lr=1e-6, ent=1.62]   Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:20<00:21,  1.17it/s, pg=0.135, ret=-9.34e-5, glen=72, tlen=232, kl=0.029, act_lr=1e-6, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24/49 [00:21<00:21,  1.17it/s, pg=0.08, ret=-0.000255, glen=70.5, tlen=231, kl=0.0242, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:21<00:20,  1.17it/s, pg=0.08, ret=-0.000255, glen=70.5, tlen=231, kl=0.0242, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:22<00:20,  1.17it/s, pg=-0.136, ret=0.000995, glen=76.8, tlen=237, kl=0.0266, act_lr=1e-6, ent=1.53]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:22<00:19,  1.17it/s, pg=-0.136, ret=0.000995, glen=76.8, tlen=237, kl=0.0266, act_lr=1e-6, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26/49 [00:24<00:19,  1.17it/s, pg=0.109, ret=-0.000866, glen=79.3, tlen=240, kl=0.0224, act_lr=1e-6, ent=1.59]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.04s/it, pg=0.109, ret=-0.000866, glen=79.3, tlen=240, kl=0.0224, act_lr=1e-6, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/49 [00:24<00:22,  1.04s/it, pg=0.0272, ret=-0.000582, glen=73.5, tlen=234, kl=0.026, act_lr=1e-6, ent=1.4] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:24<00:20,  1.02it/s, pg=0.0272, ret=-0.000582, glen=73.5, tlen=234, kl=0.026, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28/49 [00:25<00:20,  1.02it/s, pg=0.0921, ret=-0.000779, glen=73.6, tlen=234, kl=0.022, act_lr=1e-6, ent=1.41]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:25<00:18,  1.06it/s, pg=0.0921, ret=-0.000779, glen=73.6, tlen=234, kl=0.022, act_lr=1e-6, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29/49 [00:26<00:18,  1.06it/s, pg=-0.124, ret=0.000751, glen=74.7, tlen=235, kl=0.0543, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:26<00:17,  1.09it/s, pg=-0.124, ret=0.000751, glen=74.7, tlen=235, kl=0.0543, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/49 [00:27<00:17,  1.09it/s, pg=-0.0646, ret=0.000122, glen=77.3, tlen=238, kl=0.0222, act_lr=1e-6, ent=1.65]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:27<00:16,  1.12it/s, pg=-0.0646, ret=0.000122, glen=77.3, tlen=238, kl=0.0222, act_lr=1e-6, ent=1.65]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31/49 [00:28<00:16,  1.12it/s, pg=-0.0388, ret=0.000204, glen=73.3, tlen=234, kl=0.0253, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:28<00:15,  1.13it/s, pg=-0.0388, ret=0.000204, glen=73.3, tlen=234, kl=0.0253, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32/49 [00:29<00:15,  1.13it/s, pg=0.148, ret=-0.000811, glen=73.3, tlen=234, kl=0.037, act_lr=1e-6, ent=1.41]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:29<00:13,  1.14it/s, pg=0.148, ret=-0.000811, glen=73.3, tlen=234, kl=0.037, act_lr=1e-6, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33/49 [00:30<00:13,  1.14it/s, pg=-0.00772, ret=0.000264, glen=83.3, tlen=243, kl=0.039, act_lr=1e-6, ent=1.88]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:13,  1.15it/s, pg=-0.00772, ret=0.000264, glen=83.3, tlen=243, kl=0.039, act_lr=1e-6, ent=1.88]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34/49 [00:30<00:13,  1.15it/s, pg=-0.00478, ret=0.00038, glen=73.6, tlen=234, kl=0.0401, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:30<00:12,  1.16it/s, pg=-0.00478, ret=0.00038, glen=73.6, tlen=234, kl=0.0401, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35/49 [00:31<00:12,  1.16it/s, pg=0.0117, ret=-2.29e-5, glen=80.2, tlen=241, kl=0.0287, act_lr=1e-6, ent=1.6]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:31<00:11,  1.16it/s, pg=0.0117, ret=-2.29e-5, glen=80.2, tlen=241, kl=0.0287, act_lr=1e-6, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36/49 [00:32<00:11,  1.16it/s, pg=-0.019, ret=0.000224, glen=71.4, tlen=231, kl=0.0294, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:32<00:10,  1.17it/s, pg=-0.019, ret=0.000224, glen=71.4, tlen=231, kl=0.0294, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37/49 [00:33<00:10,  1.17it/s, pg=-0.113, ret=0.000997, glen=67.4, tlen=228, kl=0.0259, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:33<00:09,  1.17it/s, pg=-0.113, ret=0.000997, glen=67.4, tlen=228, kl=0.0259, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 38/49 [00:34<00:09,  1.17it/s, pg=-0.0263, ret=-1.72e-5, glen=72.5, tlen=233, kl=0.0486, act_lr=1e-6, ent=1.47]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:34<00:08,  1.17it/s, pg=-0.0263, ret=-1.72e-5, glen=72.5, tlen=233, kl=0.0486, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39/49 [00:35<00:08,  1.17it/s, pg=0.144, ret=-0.00139, glen=71.9, tlen=232, kl=0.0346, act_lr=1e-6, ent=1.44]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=0.144, ret=-0.00139, glen=71.9, tlen=232, kl=0.0346, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40/49 [00:35<00:07,  1.17it/s, pg=-0.153, ret=0.000606, glen=81.8, tlen=242, kl=0.0463, act_lr=1e-6, ent=1.53]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:35<00:06,  1.17it/s, pg=-0.153, ret=0.000606, glen=81.8, tlen=242, kl=0.0463, act_lr=1e-6, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41/49 [00:36<00:06,  1.17it/s, pg=0.0106, ret=0.000594, glen=70.6, tlen=231, kl=0.0327, act_lr=1e-6, ent=1.46]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:36<00:05,  1.17it/s, pg=0.0106, ret=0.000594, glen=70.6, tlen=231, kl=0.0327, act_lr=1e-6, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42/49 [00:37<00:05,  1.17it/s, pg=0.115, ret=-0.000695, glen=79, tlen=239, kl=0.0319, act_lr=1e-6, ent=1.46]  Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:37<00:05,  1.17it/s, pg=0.115, ret=-0.000695, glen=79, tlen=239, kl=0.0319, act_lr=1e-6, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 43/49 [00:38<00:05,  1.17it/s, pg=-0.0557, ret=4.15e-5, glen=75.4, tlen=236, kl=0.0239, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:38<00:04,  1.17it/s, pg=-0.0557, ret=4.15e-5, glen=75.4, tlen=236, kl=0.0239, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44/49 [00:39<00:04,  1.17it/s, pg=-0.025, ret=0.000155, glen=71.8, tlen=232, kl=0.0415, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:39<00:03,  1.15it/s, pg=-0.025, ret=0.000155, glen=71.8, tlen=232, kl=0.0415, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45/49 [00:40<00:03,  1.15it/s, pg=-0.146, ret=0.00111, glen=72.9, tlen=233, kl=0.0325, act_lr=1e-6, ent=1.4]  Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:40<00:02,  1.16it/s, pg=-0.146, ret=0.00111, glen=72.9, tlen=233, kl=0.0325, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46/49 [00:41<00:02,  1.16it/s, pg=-0.00348, ret=0.000102, glen=76.6, tlen=237, kl=0.0254, act_lr=1e-6, ent=1.55]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:41<00:01,  1.16it/s, pg=-0.00348, ret=0.000102, glen=76.6, tlen=237, kl=0.0254, act_lr=1e-6, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47/49 [00:42<00:01,  1.16it/s, pg=0.0459, ret=3.9e-5, glen=70.7, tlen=231, kl=0.14, act_lr=1e-6, ent=1.44]      Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.16it/s, pg=0.0459, ret=3.9e-5, glen=70.7, tlen=231, kl=0.14, act_lr=1e-6, ent=1.44]
2025-07-23 14:50:13.565 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 42.94s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.16it/s, pg=-0.0146, ret=0.000112, glen=78.8, tlen=239, kl=0.0308, act_lr=1e-6, ent=1.56]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 48/49 [00:42<00:00,  1.12it/s, pg=-0.0146, ret=0.000112, glen=78.8, tlen=239, kl=0.0308, act_lr=1e-6, ent=1.56]
2025-07-23 14:50:14.248 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 14:50:16.518 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.27s
2025-07-23 14:50:16.827 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 46.31s
2025-07-23 14:50:16.832 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.005337372118112992, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.4871708130349919, 'kl': 0.033299037388392856, 'response_length': 74.068849836077, 'total_length': 234.33756801060267, 'return': 1.9487524102204383e-05, 'policy_update_steps': 1.0}

Episode [4/20]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [38:40<02:46, 166.07s/it][A2025-07-23 14:50:16.837 | INFO     | playground.orz_7b_ppo:eval:428 - Start evaluating on val set
[36m(LLMActor pid=884796)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:   1%|          | 1/172 [00:00<01:31,  1.88it/s, est. speed input: 347.11 toks/s, output: 33.77 toks/s]
[36m(LLMActor pid=884799)[0m Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:02<00:00, 98.85it/s, est. speed input: 8387.87 toks/s, output: 2453.24 toks/s]Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:02<00:00, 105.86it/s, est. speed input: 9015.46 toks/s, output: 2706.04 toks/s]
[36m(LLMActor pid=884798)[0m Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 163/172 [00:03<00:00, 56.08it/s, est. speed input: 9025.27 toks/s, output: 3378.06 toks/s]
[36m(LLMActor pid=884796)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:04<00:00, 15.37it/s, est. speed input: 6387.98 toks/s, output: 2834.46 toks/s]Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:04<00:00, 35.15it/s, est. speed input: 6387.98 toks/s, output: 2834.46 toks/s]
[36m(LLMActor pid=884797)[0m Processed prompts:   0%|          | 0/172 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=884796)[0m Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/172 [00:03<00:00, 58.04it/s, est. speed input: 8266.29 toks/s, output: 3204.01 toks/s][32m [repeated 78x across cluster][0m
2025-07-23 14:50:25.993 | INFO     | playground.orz_7b_ppo:eval:533 - strategyqa_test/response_len_in_char: 343.6012,strategyqa_test/accuracy: 0.5619,eval_accuracy: 0.5619
2025-07-23 14:50:26.248 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:51:01.198 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:51:01.383 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 14:51:01.383 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 35.14s
2025-07-23 14:51:02.591 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0150,avg_reflection_pattern_score: 0.0051,avg_pass_at_n: 1.0000,avg_num_tokens: 76.1119,std_num_tokens: 48.8640,avg_correct_num_tokens: 76.0330,std_correct_num_tokens: 50.8310,avg_incorrect_num_tokens: 76.2273,std_incorrect_num_tokens: 45.8372
2025-07-23 14:51:02.902 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 1.52s
2025-07-23 14:51:03.512 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 0.61s
2025-07-23 14:51:17.069 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 103
2025-07-23 14:51:17.070 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 13.56s
2025-07-23 14:51:17.514 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.44s
2025-07-23 14:51:17.514 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.000682528058374391, avg_kl: 0.031927905036407765, avg_response_length: 76.27770196118401, avg_orm_score: 0.0, avg_custom_rewards: -0.000682528058374391
2025-07-23 14:51:17.536 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter51_replay_buffer.jsonl
2025-07-23 14:51:18.188 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 0.65s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/26 [00:00<?, ?it/s]
[36m(LLMActor pid=884797)[0m Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:08<00:00, 20.64it/s, est. speed input: 3739.92 toks/s, output: 1547.65 toks/s][32m [repeated 10x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/26 [00:00<?, ?it/s, pg=0.0728, ret=-0.000358, glen=71.1, tlen=231, kl=0.0293, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:   4%|‚ñç         | 1/26 [00:00<00:23,  1.07it/s, pg=0.0728, ret=-0.000358, glen=71.1, tlen=231, kl=0.0293, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 1/26 [00:01<00:23,  1.07it/s, pg=0.0156, ret=3.66e-5, glen=80.7, tlen=241, kl=0.0305, act_lr=1e-6, ent=1.52]  Actor Train epoch [1/1]:   8%|‚ñä         | 2/26 [00:01<00:21,  1.12it/s, pg=0.0156, ret=3.66e-5, glen=80.7, tlen=241, kl=0.0305, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 2/26 [00:02<00:21,  1.12it/s, pg=-0.0315, ret=0.000226, glen=74.5, tlen=234, kl=0.0286, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 3/26 [00:02<00:20,  1.14it/s, pg=-0.0315, ret=0.000226, glen=74.5, tlen=234, kl=0.0286, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 3/26 [00:03<00:20,  1.14it/s, pg=-0.122, ret=0.000524, glen=77.6, tlen=238, kl=0.0329, act_lr=1e-6, ent=1.39] Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 4/26 [00:03<00:19,  1.16it/s, pg=-0.122, ret=0.000524, glen=77.6, tlen=238, kl=0.0329, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  15%|‚ñà‚ñå        | 4/26 [00:04<00:19,  1.16it/s, pg=-0.0451, ret=0.00021, glen=71.8, tlen=232, kl=0.0301, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 5/26 [00:04<00:18,  1.14it/s, pg=-0.0451, ret=0.00021, glen=71.8, tlen=232, kl=0.0301, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  19%|‚ñà‚ñâ        | 5/26 [00:05<00:18,  1.14it/s, pg=0.0723, ret=-0.00133, glen=78.2, tlen=239, kl=0.0292, act_lr=1e-6, ent=1.41]Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:05<00:17,  1.13it/s, pg=0.0723, ret=-0.00133, glen=78.2, tlen=239, kl=0.0292, act_lr=1e-6, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:06<00:17,  1.13it/s, pg=-0.0519, ret=0.000472, glen=77.2, tlen=238, kl=0.028, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 7/26 [00:06<00:16,  1.15it/s, pg=-0.0519, ret=0.000472, glen=77.2, tlen=238, kl=0.028, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 7/26 [00:06<00:16,  1.15it/s, pg=0.0665, ret=-0.000707, glen=72.5, tlen=232, kl=0.0445, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:06<00:15,  1.16it/s, pg=0.0665, ret=-0.000707, glen=72.5, tlen=232, kl=0.0445, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:07<00:15,  1.16it/s, pg=-0.0309, ret=8.67e-5, glen=71.5, tlen=232, kl=0.0298, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:07<00:14,  1.16it/s, pg=-0.0309, ret=8.67e-5, glen=71.5, tlen=232, kl=0.0298, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:08<00:14,  1.16it/s, pg=-0.14, ret=0.0016, glen=97.2, tlen=257, kl=0.0226, act_lr=1e-6, ent=1.63]   Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:08<00:13,  1.17it/s, pg=-0.14, ret=0.0016, glen=97.2, tlen=257, kl=0.0226, act_lr=1e-6, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:09<00:13,  1.17it/s, pg=0.0683, ret=-0.000299, glen=73.8, tlen=233, kl=0.0597, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [00:09<00:12,  1.17it/s, pg=0.0683, ret=-0.000299, glen=73.8, tlen=233, kl=0.0597, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [00:10<00:12,  1.17it/s, pg=0.0809, ret=-0.00676, glen=72.4, tlen=233, kl=0.0248, act_lr=1e-6, ent=1.4]  Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:10<00:11,  1.17it/s, pg=0.0809, ret=-0.00676, glen=72.4, tlen=233, kl=0.0248, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:11<00:11,  1.17it/s, pg=0.0269, ret=-0.00014, glen=79.4, tlen=239, kl=0.0229, act_lr=1e-6, ent=1.48]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [00:11<00:11,  1.17it/s, pg=0.0269, ret=-0.00014, glen=79.4, tlen=239, kl=0.0229, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [00:12<00:11,  1.17it/s, pg=-0.0663, ret=0.000431, glen=81.3, tlen=242, kl=0.0249, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:12<00:10,  1.17it/s, pg=-0.0663, ret=0.000431, glen=81.3, tlen=242, kl=0.0249, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:12<00:10,  1.17it/s, pg=0.0349, ret=0.000285, glen=75.2, tlen=235, kl=0.0313, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:12<00:09,  1.15it/s, pg=0.0349, ret=0.000285, glen=75.2, tlen=235, kl=0.0313, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:13<00:09,  1.15it/s, pg=-0.0568, ret=0.00027, glen=80.9, tlen=241, kl=0.024, act_lr=1e-6, ent=1.39] Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:13<00:08,  1.16it/s, pg=-0.0568, ret=0.00027, glen=80.9, tlen=241, kl=0.024, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:14<00:08,  1.16it/s, pg=0.0203, ret=0.000358, glen=77.3, tlen=237, kl=0.0233, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:14<00:07,  1.16it/s, pg=0.0203, ret=0.000358, glen=77.3, tlen=237, kl=0.0233, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:15<00:07,  1.16it/s, pg=0.0853, ret=-0.000664, glen=77.7, tlen=238, kl=0.0447, act_lr=1e-6, ent=1.44]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:15<00:06,  1.17it/s, pg=0.0853, ret=-0.000664, glen=77.7, tlen=238, kl=0.0447, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:16<00:06,  1.17it/s, pg=-0.0246, ret=0.000116, glen=75.8, tlen=236, kl=0.0302, act_lr=1e-6, ent=1.4] Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:16<00:05,  1.17it/s, pg=-0.0246, ret=0.000116, glen=75.8, tlen=236, kl=0.0302, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:17<00:05,  1.17it/s, pg=0.0554, ret=-0.00015, glen=71.4, tlen=232, kl=0.0364, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:17<00:05,  1.17it/s, pg=0.0554, ret=-0.00015, glen=71.4, tlen=232, kl=0.0364, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:18<00:05,  1.17it/s, pg=0.111, ret=-0.000439, glen=75.9, tlen=236, kl=0.0241, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:18<00:04,  1.18it/s, pg=0.111, ret=-0.000439, glen=75.9, tlen=236, kl=0.0241, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:18<00:04,  1.18it/s, pg=-0.00574, ret=-0.000101, glen=72.5, tlen=232, kl=0.0281, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:18<00:03,  1.17it/s, pg=-0.00574, ret=-0.000101, glen=72.5, tlen=232, kl=0.0281, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:19<00:03,  1.17it/s, pg=-0.12, ret=0.000895, glen=74.5, tlen=234, kl=0.0314, act_lr=1e-6, ent=1.44]    Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:19<00:02,  1.18it/s, pg=-0.12, ret=0.000895, glen=74.5, tlen=234, kl=0.0314, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:20<00:02,  1.18it/s, pg=-0.0181, ret=0.000232, glen=72.6, tlen=233, kl=0.0427, act_lr=1e-6, ent=1.47]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:20<00:01,  1.17it/s, pg=-0.0181, ret=0.000232, glen=72.6, tlen=233, kl=0.0427, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:21<00:01,  1.17it/s, pg=0.117, ret=-0.000554, glen=76.1, tlen=236, kl=0.0497, act_lr=1e-6, ent=1.44] Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:21<00:00,  1.18it/s, pg=0.117, ret=-0.000554, glen=76.1, tlen=236, kl=0.0497, act_lr=1e-6, ent=1.44]
2025-07-23 14:51:40.784 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 22.41s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:22<00:00,  1.18it/s, pg=0.00607, ret=-0.000499, glen=73.1, tlen=233, kl=0.0259, act_lr=1e-6, ent=1.41]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:22<00:00,  1.12it/s, pg=0.00607, ret=-0.000499, glen=73.1, tlen=233, kl=0.0259, act_lr=1e-6, ent=1.41]
2025-07-23 14:51:41.649 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.81s
2025-07-23 14:51:44.116 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.47s
2025-07-23 14:51:44.419 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 26.16s
2025-07-23 14:51:44.422 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': 0.0046135829045222355, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.4228196235803456, 'kl': 0.03191669170673077, 'response_length': 76.2417135972243, 'total_length': 236.3378912118765, 'return': -0.0002405006444650308, 'policy_update_steps': 1.0}

Episode [4/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [40:07<00:00, 142.29s/it][A[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=885532)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=885532)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=885532)[0m     "load_path": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,659] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 2.95 GB         CA 3.97 GB         Max_CA 4 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,659] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 105.95 GB, percent = 21.0%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,660] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:11:36,661] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=885532)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=885532)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=885532)[0m         "stage": 3, 
[36m(RefRayActorBase pid=885532)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=885532)[0m             "pin_memory": true
[36m(RefRayActorBase pid=885532)[0m         }
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "bf16": {
[36m(RefRayActorBase pid=885532)[0m         "enabled": true
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=885532)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=885532)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=885532)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=885532)[0m }
2025-07-23 14:51:53.202 | INFO     | orz.ppo.trainer:train:193 - Successfully update ref model with policy model, training continue.
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=886178)[0m [2025-07-23 14:11:36,120] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:19:30,575] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:23:24,582] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:25:43,626] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:29:39,339] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:32:21,426] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:36:01,507] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:42:53,027] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:45:19,429] [WARNING] [stage3.py:2118:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:47:43,221] [WARNING] [stage3.py:2118:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m [2025-07-23 14:50:13,559] [WARNING] [stage3.py:2118:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(PolicyRayActorBase pid=885360)[0m Broadcast actor weights to vllm engines done
[36m(RefRayActorBase pid=886177)[0m [2025-07-23 14:51:51,029] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:51,245] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1695, num_elems = 8.89B
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:52,774] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:52,774] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:52,781] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:52,783] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,004] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,005] [INFO] [utils.py:782:see_memory_usage] MA 3.68 GB         Max_MA 8.64 GB         CA 4.7 GB         Max_CA 41 GB 
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,005] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 106.44 GB, percent = 21.1%
[36m(RefRayActorBase pid=885532)[0m Parameter Offload: Total persistent parameters: 144896 in 141 params
Episode [5/20]:   0%|          | 0/13 [00:00<?, ?it/s]Episode [4/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [40:16<00:00, 185.89s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,195] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,196] [INFO] [utils.py:782:see_memory_usage] MA 3.68 GB         Max_MA 3.68 GB         CA 4.7 GB         Max_CA 5 GB huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,196] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 106.43 GB, percent = 21.1%
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=885532)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=885532)[0m     "contiguous_memory_optimization": false, huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

[36m(RefRayActorBase pid=885532)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=885532)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=885532)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=885532)[0m     "profile": false
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "start_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "end_step": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=885532)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=885532)[0m     "model_info": null, 
[36m(RefRayActorBase pid=885532)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=885532)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=885532)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=885532)[0m     "fast": true, 
[36m(RefRayActorBase pid=885532)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=885532)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=885532)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=885532)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=885532)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=885532)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=885532)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,197] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x764c21752ea0>
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=885532)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=885532)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=885532)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=885532)[0m     "detailed": true, 
[36m(RefRayActorBase pid=885532)[0m     "output_file": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=885532)[0m     "enabled": false, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=885532)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=885532)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=885532)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=885532)[0m     "load_path": null
[36m(RefRayActorBase pid=885532)[0m }
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   train_batch_size ............. 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,198] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   world_size ................... 4
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=885532)[0m [2025-07-23 14:51:53,199] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=885532)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=885532)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=885532)[0m         "stage": 3, 
[36m(RefRayActorBase pid=885532)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=885532)[0m         "offload_param": {
[36m(RefRayActorBase pid=885532)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=885532)[0m             "pin_memory": true
[36m(RefRayActorBase pid=885532)[0m         }
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "bf16": {
[36m(RefRayActorBase pid=885532)[0m         "enabled": true
[36m(RefRayActorBase pid=885532)[0m     }, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=885532)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=885532)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=885532)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=885532)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=885532)[0m }
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-07-23 14:51:53.481 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:54:19.975 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:54:20.164 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:54:20.164 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 146.68s
2025-07-23 14:54:22.074 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0001,avg_repeat_score: 0.0145,avg_reflection_pattern_score: 0.0066,avg_pass_at_n: 1.0000,avg_num_tokens: 81.4260,std_num_tokens: 103.2904,avg_correct_num_tokens: 77.3099,std_correct_num_tokens: 45.7656,avg_incorrect_num_tokens: 86.6272,std_incorrect_num_tokens: 146.4758
2025-07-23 14:54:22.410 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.25s
2025-07-23 14:54:23.882 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.47s
2025-07-23 14:54:49.999 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 200
2025-07-23 14:54:50.000 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.12s
2025-07-23 14:54:50.781 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.78s
2025-07-23 14:54:50.782 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.00031769698121934196, avg_kl: 0.0, avg_response_length: 85.74659782409668, avg_orm_score: 0.0, avg_custom_rewards: -0.00031769698121934196
2025-07-23 14:54:50.831 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter52_replay_buffer.jsonl
2025-07-23 14:54:52.147 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.32s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/50 [00:01<?, ?it/s, pg=0.145, ret=-0.000198, glen=88.2, tlen=249, kl=0, act_lr=1e-6, ent=1.49]Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:50,  1.02s/it, pg=0.145, ret=-0.000198, glen=88.2, tlen=249, kl=0, act_lr=1e-6, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/50 [00:01<00:50,  1.02s/it, pg=0.0519, ret=-0.00121, glen=86.3, tlen=247, kl=0, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:01<00:44,  1.08it/s, pg=0.0519, ret=-0.00121, glen=86.3, tlen=247, kl=0, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/50 [00:02<00:44,  1.08it/s, pg=-0.105, ret=0.000579, glen=81.5, tlen=242, kl=0, act_lr=1e-6, ent=1.4] Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:02<00:41,  1.12it/s, pg=-0.105, ret=0.000579, glen=81.5, tlen=242, kl=0, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/50 [00:03<00:41,  1.12it/s, pg=-0.116, ret=0.000408, glen=79.3, tlen=239, kl=0, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:03<00:40,  1.14it/s, pg=-0.116, ret=0.000408, glen=79.3, tlen=239, kl=0, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/50 [00:04<00:40,  1.14it/s, pg=0.0439, ret=-0.000909, glen=81.2, tlen=242, kl=0, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:04<00:40,  1.12it/s, pg=0.0439, ret=-0.000909, glen=81.2, tlen=242, kl=0, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñà         | 5/50 [00:05<00:40,  1.12it/s, pg=-0.126, ret=0.000811, glen=77.4, tlen=238, kl=0, act_lr=1e-6, ent=1.35] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:05<00:38,  1.14it/s, pg=-0.126, ret=0.000811, glen=77.4, tlen=238, kl=0, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/50 [00:06<00:38,  1.14it/s, pg=-0.094, ret=0.0005, glen=74.3, tlen=234, kl=0, act_lr=1e-6, ent=1.35]  Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:06<00:37,  1.15it/s, pg=-0.094, ret=0.0005, glen=74.3, tlen=234, kl=0, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñç        | 7/50 [00:07<00:37,  1.15it/s, pg=0.4, ret=-0.00146, glen=329, tlen=490, kl=0, act_lr=1e-6, ent=2.04]  Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:37,  1.12it/s, pg=0.4, ret=-0.00146, glen=329, tlen=490, kl=0, act_lr=1e-6, ent=2.04]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/50 [00:07<00:37,  1.12it/s, pg=-0.137, ret=0.000986, glen=73.3, tlen=234, kl=0, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:07<00:36,  1.14it/s, pg=-0.137, ret=0.000986, glen=73.3, tlen=234, kl=0, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/50 [00:08<00:36,  1.14it/s, pg=-0.0789, ret=0.000304, glen=75.8, tlen=236, kl=0, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:08<00:34,  1.15it/s, pg=-0.0789, ret=0.000304, glen=75.8, tlen=236, kl=0, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñà        | 10/50 [00:09<00:34,  1.15it/s, pg=-0.0785, ret=-0.000198, glen=81.8, tlen=242, kl=0, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:09<00:33,  1.16it/s, pg=-0.0785, ret=-0.000198, glen=81.8, tlen=242, kl=0, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/50 [00:10<00:33,  1.16it/s, pg=-0.21, ret=0.00104, glen=72.7, tlen=233, kl=0, act_lr=1e-6, ent=1.35]    Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:10<00:34,  1.11it/s, pg=-0.21, ret=0.00104, glen=72.7, tlen=233, kl=0, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñç       | 12/50 [00:11<00:34,  1.11it/s, pg=-0.03, ret=-0.00014, glen=76, tlen=236, kl=0, act_lr=1e-6, ent=1.31] Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:11<00:32,  1.13it/s, pg=-0.03, ret=-0.00014, glen=76, tlen=236, kl=0, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  26%|‚ñà‚ñà‚ñå       | 13/50 [00:12<00:32,  1.13it/s, pg=-0.01, ret=-0.000364, glen=83.5, tlen=244, kl=0, act_lr=1e-6, ent=1.44]Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:12<00:32,  1.11it/s, pg=-0.01, ret=-0.000364, glen=83.5, tlen=244, kl=0, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  28%|‚ñà‚ñà‚ñä       | 14/50 [00:13<00:32,  1.11it/s, pg=0.189, ret=-0.00167, glen=88.9, tlen=249, kl=0, act_lr=1e-6, ent=1.38] Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:13<00:30,  1.13it/s, pg=0.189, ret=-0.00167, glen=88.9, tlen=249, kl=0, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  30%|‚ñà‚ñà‚ñà       | 15/50 [00:14<00:30,  1.13it/s, pg=-0.0307, ret=4.62e-5, glen=85.6, tlen=246, kl=0, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:14<00:29,  1.14it/s, pg=-0.0307, ret=4.62e-5, glen=85.6, tlen=246, kl=0, act_lr=1e-6, ent=1.52]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:15<00:29,  1.14it/s, pg=-0.0557, ret=0.000155, glen=73.8, tlen=234, kl=0, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.15it/s, pg=-0.0557, ret=0.000155, glen=73.8, tlen=234, kl=0, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:15<00:28,  1.15it/s, pg=0.147, ret=-0.000997, glen=78.4, tlen=239, kl=0, act_lr=1e-6, ent=1.41] Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:15<00:27,  1.16it/s, pg=0.147, ret=-0.000997, glen=78.4, tlen=239, kl=0, act_lr=1e-6, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:16<00:27,  1.16it/s, pg=-0.0948, ret=0.000681, glen=84.4, tlen=245, kl=0, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:16<00:26,  1.16it/s, pg=-0.0948, ret=0.000681, glen=84.4, tlen=245, kl=0, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:17<00:26,  1.16it/s, pg=0.0422, ret=-0.000425, glen=85.4, tlen=245, kl=0, act_lr=1e-6, ent=1.59]Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:25,  1.17it/s, pg=0.0422, ret=-0.000425, glen=85.4, tlen=245, kl=0, act_lr=1e-6, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:18<00:25,  1.17it/s, pg=0.00848, ret=-0.000797, glen=75.3, tlen=235, kl=0, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:18<00:24,  1.17it/s, pg=0.00848, ret=-0.000797, glen=75.3, tlen=235, kl=0, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:19<00:24,  1.17it/s, pg=0.0439, ret=-0.000169, glen=84.1, tlen=245, kl=0, act_lr=1e-6, ent=1.48] Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:19<00:23,  1.17it/s, pg=0.0439, ret=-0.000169, glen=84.1, tlen=245, kl=0, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:20<00:23,  1.17it/s, pg=-0.0228, ret=5.51e-5, glen=82.9, tlen=243, kl=0, act_lr=1e-6, ent=1.47] Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.17it/s, pg=-0.0228, ret=5.51e-5, glen=82.9, tlen=243, kl=0, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:20<00:23,  1.17it/s, pg=-0.117, ret=0.000472, glen=79.7, tlen=240, kl=0, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:22,  1.17it/s, pg=-0.117, ret=0.000472, glen=79.7, tlen=240, kl=0, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:21<00:22,  1.17it/s, pg=0.0137, ret=-0.000223, glen=76.8, tlen=237, kl=0, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:21<00:21,  1.17it/s, pg=0.0137, ret=-0.000223, glen=76.8, tlen=237, kl=0, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:22<00:21,  1.17it/s, pg=0.0372, ret=-0.00146, glen=84.6, tlen=245, kl=0, act_lr=1e-6, ent=1.5]  Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:22<00:20,  1.18it/s, pg=0.0372, ret=-0.00146, glen=84.6, tlen=245, kl=0, act_lr=1e-6, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:23<00:20,  1.18it/s, pg=-0.0859, ret=0.000165, glen=77.6, tlen=238, kl=0, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:23<00:22,  1.02it/s, pg=-0.0859, ret=0.000165, glen=77.6, tlen=238, kl=0, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:24<00:22,  1.02it/s, pg=0.0121, ret=-0.000837, glen=79.3, tlen=239, kl=0, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:24<00:20,  1.06it/s, pg=0.0121, ret=-0.000837, glen=79.3, tlen=239, kl=0, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:25<00:20,  1.06it/s, pg=-0.0807, ret=0.000339, glen=79, tlen=239, kl=0, act_lr=1e-6, ent=1.37]  Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:25<00:19,  1.07it/s, pg=-0.0807, ret=0.000339, glen=79, tlen=239, kl=0, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:26<00:19,  1.07it/s, pg=-0.0583, ret=-0.000283, glen=84.1, tlen=244, kl=0, act_lr=1e-6, ent=1.48]Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:26<00:18,  1.10it/s, pg=-0.0583, ret=-0.000283, glen=84.1, tlen=244, kl=0, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:27<00:18,  1.10it/s, pg=0.0255, ret=-0.00052, glen=78, tlen=238, kl=0, act_lr=1e-6, ent=1.29]    Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:27<00:16,  1.12it/s, pg=0.0255, ret=-0.00052, glen=78, tlen=238, kl=0, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:28<00:16,  1.12it/s, pg=-0.1, ret=0.00109, glen=79.3, tlen=239, kl=0, act_lr=1e-6, ent=1.38] Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:28<00:15,  1.14it/s, pg=-0.1, ret=0.00109, glen=79.3, tlen=239, kl=0, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:29<00:15,  1.14it/s, pg=-0.122, ret=0.000814, glen=78.6, tlen=239, kl=0, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.15it/s, pg=-0.122, ret=0.000814, glen=78.6, tlen=239, kl=0, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:29<00:14,  1.15it/s, pg=-0.0383, ret=-0.000156, glen=76.5, tlen=238, kl=0, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:29<00:13,  1.16it/s, pg=-0.0383, ret=-0.000156, glen=76.5, tlen=238, kl=0, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:30<00:13,  1.16it/s, pg=0.178, ret=-0.000868, glen=81.5, tlen=242, kl=0, act_lr=1e-6, ent=1.43] Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:30<00:12,  1.16it/s, pg=0.178, ret=-0.000868, glen=81.5, tlen=242, kl=0, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:31<00:12,  1.16it/s, pg=-0.0418, ret=0.000329, glen=82, tlen=242, kl=0, act_lr=1e-6, ent=1.39] Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:31<00:12,  1.17it/s, pg=-0.0418, ret=0.000329, glen=82, tlen=242, kl=0, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:32<00:12,  1.17it/s, pg=-0.00885, ret=-2.95e-5, glen=75.3, tlen=235, kl=0, act_lr=1e-6, ent=1.37]Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:32<00:11,  1.17it/s, pg=-0.00885, ret=-2.95e-5, glen=75.3, tlen=235, kl=0, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:33<00:11,  1.17it/s, pg=-0.0233, ret=0.000222, glen=84.3, tlen=245, kl=0, act_lr=1e-6, ent=1.38] Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:33<00:10,  1.17it/s, pg=-0.0233, ret=0.000222, glen=84.3, tlen=245, kl=0, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:34<00:10,  1.17it/s, pg=-0.202, ret=0.00141, glen=77.6, tlen=238, kl=0, act_lr=1e-6, ent=1.28]  Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:34<00:09,  1.17it/s, pg=-0.202, ret=0.00141, glen=77.6, tlen=238, kl=0, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:35<00:09,  1.17it/s, pg=-0.0312, ret=-0.000552, glen=80.7, tlen=241, kl=0, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.18it/s, pg=-0.0312, ret=-0.000552, glen=80.7, tlen=241, kl=0, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:35<00:08,  1.18it/s, pg=0.054, ret=0.00086, glen=101, tlen=262, kl=0, act_lr=1e-6, ent=1.63]     Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:35<00:07,  1.17it/s, pg=0.054, ret=0.00086, glen=101, tlen=262, kl=0, act_lr=1e-6, ent=1.63]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:36<00:07,  1.17it/s, pg=-0.0677, ret=0.000161, glen=81.3, tlen=241, kl=0, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:36<00:06,  1.17it/s, pg=-0.0677, ret=0.000161, glen=81.3, tlen=241, kl=0, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:37<00:06,  1.17it/s, pg=-0.0998, ret=0.000553, glen=79.2, tlen=240, kl=0, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:37<00:05,  1.17it/s, pg=-0.0998, ret=0.000553, glen=79.2, tlen=240, kl=0, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:38<00:05,  1.17it/s, pg=-0.0243, ret=1.33e-5, glen=81.4, tlen=242, kl=0, act_lr=1e-6, ent=1.46] Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:38<00:05,  1.17it/s, pg=-0.0243, ret=1.33e-5, glen=81.4, tlen=242, kl=0, act_lr=1e-6, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:39<00:05,  1.17it/s, pg=0.00574, ret=7.02e-5, glen=83.7, tlen=244, kl=0, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:39<00:04,  1.17it/s, pg=0.00574, ret=7.02e-5, glen=83.7, tlen=244, kl=0, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:40<00:04,  1.17it/s, pg=-0.0894, ret=-0.000196, glen=86.9, tlen=247, kl=0, act_lr=1e-6, ent=1.46]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:40<00:03,  1.17it/s, pg=-0.0894, ret=-0.000196, glen=86.9, tlen=247, kl=0, act_lr=1e-6, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:41<00:03,  1.17it/s, pg=-0.0127, ret=0.000175, glen=78.6, tlen=239, kl=0, act_lr=1e-6, ent=1.33] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.0127, ret=0.000175, glen=78.6, tlen=239, kl=0, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:41<00:02,  1.17it/s, pg=-0.0569, ret=0.000264, glen=76.1, tlen=236, kl=0, act_lr=1e-6, ent=1.37]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:41<00:01,  1.17it/s, pg=-0.0569, ret=0.000264, glen=76.1, tlen=236, kl=0, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:42<00:01,  1.17it/s, pg=0.11, ret=-0.00121, glen=81.7, tlen=242, kl=0, act_lr=1e-6, ent=1.37]   Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:42<00:00,  1.17it/s, pg=0.11, ret=-0.00121, glen=81.7, tlen=242, kl=0, act_lr=1e-6, ent=1.37]
2025-07-23 14:55:36.021 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 43.71s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.17it/s, pg=-0.0533, ret=0.000287, glen=82.9, tlen=243, kl=0, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:43<00:00,  1.12it/s, pg=-0.0533, ret=0.000287, glen=82.9, tlen=243, kl=0, act_lr=1e-6, ent=1.38]
2025-07-23 14:55:36.700 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 14:55:38.983 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.28s
2025-07-23 14:55:39.286 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.10s
2025-07-23 14:55:39.292 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.01988903045654297, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.4087387037277221, 'kl': 0.0, 'response_length': 85.74659759521484, 'total_length': 246.02910919189452, 'return': -4.161548957199557e-05, 'policy_update_steps': 1.0}
Episode [5/20]:   8%|‚ñä         | 1/13 [03:46<45:13, 226.09s/it]2025-07-23 14:55:39.327 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 14:56:36.617 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 14:56:36.807 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 14:56:36.808 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 57.48s
2025-07-23 14:56:38.647 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0139,avg_reflection_pattern_score: 0.0077,avg_pass_at_n: 1.0000,avg_num_tokens: 82.8801,std_num_tokens: 52.9851,avg_correct_num_tokens: 80.3326,std_correct_num_tokens: 52.6198,avg_incorrect_num_tokens: 86.4779,std_incorrect_num_tokens: 53.2897
2025-07-23 14:56:38.958 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.15s
2025-07-23 14:56:40.452 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.49s
2025-07-23 14:57:06.501 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 201
2025-07-23 14:57:06.501 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.05s
2025-07-23 14:57:07.235 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.73s
2025-07-23 14:57:07.236 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 5.8825165861568864e-05, avg_kl: 0.002310226212686567, avg_response_length: 83.1862550042755, avg_orm_score: 0.0, avg_custom_rewards: 5.8825165861568864e-05
2025-07-23 14:57:07.266 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter53_replay_buffer.jsonl
2025-07-23 14:57:08.601 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.34s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=-0.292, ret=0.00204, glen=85.6, tlen=246, kl=0.00217, act_lr=1e-6, ent=1.44]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:46,  1.07it/s, pg=-0.292, ret=0.00204, glen=85.6, tlen=246, kl=0.00217, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:46,  1.07it/s, pg=-0.0901, ret=0.001, glen=78.1, tlen=239, kl=0.00293, act_lr=1e-6, ent=1.33] Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:43,  1.12it/s, pg=-0.0901, ret=0.001, glen=78.1, tlen=239, kl=0.00293, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:43,  1.12it/s, pg=-0.0395, ret=0.000745, glen=81.3, tlen=242, kl=0.00212, act_lr=1e-6, ent=1.3]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:41,  1.15it/s, pg=-0.0395, ret=0.000745, glen=81.3, tlen=242, kl=0.00212, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:41,  1.15it/s, pg=0.0331, ret=1.98e-5, glen=82.6, tlen=244, kl=0.00197, act_lr=1e-6, ent=1.32] Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:40,  1.16it/s, pg=0.0331, ret=1.98e-5, glen=82.6, tlen=244, kl=0.00197, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:40,  1.16it/s, pg=0.0351, ret=-0.000161, glen=83.7, tlen=244, kl=0.00334, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:39,  1.16it/s, pg=0.0351, ret=-0.000161, glen=83.7, tlen=244, kl=0.00334, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:39,  1.16it/s, pg=-0.0287, ret=-0.000248, glen=81.4, tlen=242, kl=0.00205, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:38,  1.17it/s, pg=-0.0287, ret=-0.000248, glen=81.4, tlen=242, kl=0.00205, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:38,  1.17it/s, pg=0.0557, ret=0.000347, glen=84.2, tlen=245, kl=0.0022, act_lr=1e-6, ent=1.38]  Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:37,  1.17it/s, pg=0.0557, ret=0.000347, glen=84.2, tlen=245, kl=0.0022, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:37,  1.17it/s, pg=0.161, ret=-0.00093, glen=80.5, tlen=241, kl=0.00281, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:06<00:37,  1.15it/s, pg=0.161, ret=-0.00093, glen=80.5, tlen=241, kl=0.00281, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=0.0251, ret=0.000978, glen=84.4, tlen=244, kl=0.00194, act_lr=1e-6, ent=1.53]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.15it/s, pg=0.0251, ret=0.000978, glen=84.4, tlen=244, kl=0.00194, act_lr=1e-6, ent=1.53]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.15it/s, pg=0.0439, ret=0.000201, glen=88.7, tlen=249, kl=0.0019, act_lr=1e-6, ent=1.47] Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.16it/s, pg=0.0439, ret=0.000201, glen=88.7, tlen=249, kl=0.0019, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.16it/s, pg=0.0786, ret=-0.000839, glen=81.4, tlen=242, kl=0.00166, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.16it/s, pg=0.0786, ret=-0.000839, glen=81.4, tlen=242, kl=0.00166, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.16it/s, pg=0.0151, ret=4.1e-6, glen=83.7, tlen=244, kl=0.00275, act_lr=1e-6, ent=1.44]   Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:34,  1.14it/s, pg=0.0151, ret=4.1e-6, glen=83.7, tlen=244, kl=0.00275, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:34,  1.14it/s, pg=-0.0708, ret=0.000254, glen=79.3, tlen=240, kl=0.00196, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:33,  1.12it/s, pg=-0.0708, ret=0.000254, glen=79.3, tlen=240, kl=0.00196, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:33,  1.12it/s, pg=0.115, ret=-0.000773, glen=82.5, tlen=243, kl=0.00197, act_lr=1e-6, ent=1.36] Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:32,  1.14it/s, pg=0.115, ret=-0.000773, glen=82.5, tlen=243, kl=0.00197, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:32,  1.14it/s, pg=0.0172, ret=-0.000333, glen=88.1, tlen=249, kl=0.00214, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.15it/s, pg=0.0172, ret=-0.000333, glen=88.1, tlen=249, kl=0.00214, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.15it/s, pg=-0.0384, ret=0.000237, glen=81, tlen=242, kl=0.00189, act_lr=1e-6, ent=1.29]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:30,  1.16it/s, pg=-0.0384, ret=0.000237, glen=81, tlen=242, kl=0.00189, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.16it/s, pg=0.000427, ret=-0.000527, glen=82.7, tlen=243, kl=0.00276, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.16it/s, pg=0.000427, ret=-0.000527, glen=82.7, tlen=243, kl=0.00276, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.16it/s, pg=-0.129, ret=0.000319, glen=95, tlen=256, kl=0.00232, act_lr=1e-6, ent=1.6]      Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.16it/s, pg=-0.129, ret=0.000319, glen=95, tlen=256, kl=0.00232, act_lr=1e-6, ent=1.6]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.16it/s, pg=0.0679, ret=-0.000727, glen=95.7, tlen=256, kl=0.00246, act_lr=1e-6, ent=1.57]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.17it/s, pg=0.0679, ret=-0.000727, glen=95.7, tlen=256, kl=0.00246, act_lr=1e-6, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.17it/s, pg=-0.156, ret=0.00103, glen=89.4, tlen=250, kl=0.00196, act_lr=1e-6, ent=1.62]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=-0.156, ret=0.00103, glen=89.4, tlen=250, kl=0.00196, act_lr=1e-6, ent=1.62]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=0.188, ret=-0.00161, glen=78.3, tlen=239, kl=0.00227, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.188, ret=-0.00161, glen=78.3, tlen=239, kl=0.00227, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=-0.116, ret=0.000758, glen=83.8, tlen=244, kl=0.00302, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.116, ret=0.000758, glen=83.8, tlen=244, kl=0.00302, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.0224, ret=1.87e-5, glen=81.3, tlen=242, kl=0.0037, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:19<00:23,  1.17it/s, pg=-0.0224, ret=1.87e-5, glen=81.3, tlen=242, kl=0.0037, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=0.0633, ret=9.63e-5, glen=82.7, tlen=242, kl=0.00184, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.17it/s, pg=0.0633, ret=9.63e-5, glen=82.7, tlen=242, kl=0.00184, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=0.00742, ret=-8.81e-5, glen=74.3, tlen=235, kl=0.00247, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.16it/s, pg=0.00742, ret=-8.81e-5, glen=74.3, tlen=235, kl=0.00247, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.16it/s, pg=-0.116, ret=0.000926, glen=82.6, tlen=244, kl=0.00253, act_lr=1e-6, ent=1.3]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.16it/s, pg=-0.116, ret=0.000926, glen=82.6, tlen=244, kl=0.00253, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.16it/s, pg=-0.032, ret=0.00118, glen=87, tlen=247, kl=0.00247, act_lr=1e-6, ent=1.34]  Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:22,  1.07it/s, pg=-0.032, ret=0.00118, glen=87, tlen=247, kl=0.00247, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:22,  1.07it/s, pg=0.0436, ret=-0.000357, glen=81.5, tlen=243, kl=0.00225, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:20,  1.10it/s, pg=0.0436, ret=-0.000357, glen=81.5, tlen=243, kl=0.00225, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:20,  1.10it/s, pg=0.0149, ret=-0.00052, glen=85.3, tlen=246, kl=0.00233, act_lr=1e-6, ent=1.36] Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:19,  1.12it/s, pg=0.0149, ret=-0.00052, glen=85.3, tlen=246, kl=0.00233, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:19,  1.12it/s, pg=0.0112, ret=0.000377, glen=77.9, tlen=238, kl=0.00177, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.14it/s, pg=0.0112, ret=0.000377, glen=77.9, tlen=238, kl=0.00177, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.14it/s, pg=0.0492, ret=-0.000636, glen=78.4, tlen=239, kl=0.00225, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:26<00:17,  1.15it/s, pg=0.0492, ret=-0.000636, glen=78.4, tlen=239, kl=0.00225, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.15it/s, pg=-0.0695, ret=-0.000185, glen=76.8, tlen=237, kl=0.00257, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:27<00:16,  1.16it/s, pg=-0.0695, ret=-0.000185, glen=76.8, tlen=237, kl=0.00257, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.16it/s, pg=0.0383, ret=0.000188, glen=86.8, tlen=248, kl=0.0019, act_lr=1e-6, ent=1.32]   Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.16it/s, pg=0.0383, ret=0.000188, glen=86.8, tlen=248, kl=0.0019, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.16it/s, pg=0.196, ret=-0.00135, glen=80.2, tlen=241, kl=0.00216, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.14it/s, pg=0.196, ret=-0.00135, glen=80.2, tlen=241, kl=0.00216, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.14it/s, pg=-0.074, ret=0.00039, glen=83.5, tlen=244, kl=0.0019, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.15it/s, pg=-0.074, ret=0.00039, glen=83.5, tlen=244, kl=0.0019, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.15it/s, pg=0.117, ret=-0.000883, glen=83.9, tlen=244, kl=0.00244, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.16it/s, pg=0.117, ret=-0.000883, glen=83.9, tlen=244, kl=0.00244, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.16it/s, pg=-0.0203, ret=0.000206, glen=84.4, tlen=245, kl=0.00207, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:12,  1.16it/s, pg=-0.0203, ret=0.000206, glen=84.4, tlen=245, kl=0.00207, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:12,  1.16it/s, pg=0.0872, ret=-0.00124, glen=82, tlen=242, kl=0.00198, act_lr=1e-6, ent=1.36]   Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.0872, ret=-0.00124, glen=82, tlen=242, kl=0.00198, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.0728, ret=-0.000252, glen=82.3, tlen=243, kl=0.00256, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:33<00:10,  1.17it/s, pg=0.0728, ret=-0.000252, glen=82.3, tlen=243, kl=0.00256, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=-0.0536, ret=0.000384, glen=82.3, tlen=243, kl=0.00293, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.17it/s, pg=-0.0536, ret=0.000384, glen=82.3, tlen=243, kl=0.00293, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.094, ret=0.00132, glen=83.5, tlen=244, kl=0.00267, act_lr=1e-6, ent=1.37]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.17it/s, pg=-0.094, ret=0.00132, glen=83.5, tlen=244, kl=0.00267, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=-0.0579, ret=9.94e-5, glen=82.9, tlen=243, kl=0.0025, act_lr=1e-6, ent=1.3] Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.18it/s, pg=-0.0579, ret=9.94e-5, glen=82.9, tlen=243, kl=0.0025, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.18it/s, pg=-0.0731, ret=6.42e-5, glen=81.1, tlen=242, kl=0.00228, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.18it/s, pg=-0.0731, ret=6.42e-5, glen=81.1, tlen=242, kl=0.00228, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.18it/s, pg=-0.0741, ret=0.000866, glen=80.1, tlen=240, kl=0.00209, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.18it/s, pg=-0.0741, ret=0.000866, glen=80.1, tlen=240, kl=0.00209, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.18it/s, pg=-0.0326, ret=-0.00057, glen=80.4, tlen=241, kl=0.00216, act_lr=1e-6, ent=1.37]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:38<00:05,  1.18it/s, pg=-0.0326, ret=-0.00057, glen=80.4, tlen=241, kl=0.00216, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=0.0435, ret=-0.000898, glen=80.6, tlen=241, kl=0.00237, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:39<00:04,  1.18it/s, pg=0.0435, ret=-0.000898, glen=80.6, tlen=241, kl=0.00237, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.18it/s, pg=0.0309, ret=-0.000476, glen=83, tlen=244, kl=0.00222, act_lr=1e-6, ent=1.36]  Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.18it/s, pg=0.0309, ret=-0.000476, glen=83, tlen=244, kl=0.00222, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.18it/s, pg=0.0857, ret=-0.000391, glen=93.6, tlen=254, kl=0.00216, act_lr=1e-6, ent=1.51]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.18it/s, pg=0.0857, ret=-0.000391, glen=93.6, tlen=254, kl=0.00216, act_lr=1e-6, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.18it/s, pg=-0.136, ret=0.0013, glen=84.4, tlen=245, kl=0.00167, act_lr=1e-6, ent=1.3]    Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.18it/s, pg=-0.136, ret=0.0013, glen=84.4, tlen=245, kl=0.00167, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.18it/s, pg=0.0918, ret=-0.00108, glen=82.8, tlen=243, kl=0.00251, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.18it/s, pg=0.0918, ret=-0.00108, glen=82.8, tlen=243, kl=0.00251, act_lr=1e-6, ent=1.4]
2025-07-23 14:57:52.900 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.13s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.18it/s, pg=0.0328, ret=-0.000147, glen=82.6, tlen=243, kl=0.00202, act_lr=1e-6, ent=1.41]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=0.0328, ret=-0.000147, glen=82.6, tlen=243, kl=0.00202, act_lr=1e-6, ent=1.41]
2025-07-23 14:57:53.569 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 14:57:55.864 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.29s
2025-07-23 14:57:56.174 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.53s
2025-07-23 14:57:56.179 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': 0.00013331338471057368, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.3639553691826614, 'kl': 0.002301795809876685, 'response_length': 83.1284538717831, 'total_length': 243.66014757343368, 'return': 2.6826821661362534e-06, 'policy_update_steps': 1.0}
Episode [5/20]:  15%|‚ñà‚ñå        | 2/13 [06:02<31:49, 173.62s/it]2025-07-23 14:57:56.211 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 15:00:08.012 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 15:00:08.195 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 15:00:08.196 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 131.99s
2025-07-23 15:00:10.101 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0141,avg_reflection_pattern_score: 0.0063,avg_pass_at_n: 1.0000,avg_num_tokens: 85.0428,std_num_tokens: 105.8090,avg_correct_num_tokens: 82.2931,std_correct_num_tokens: 78.5793,avg_incorrect_num_tokens: 88.8433,std_incorrect_num_tokens: 134.5730
2025-07-23 15:00:10.534 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.34s
2025-07-23 15:00:12.012 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.48s
2025-07-23 15:00:37.967 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 204
2025-07-23 15:00:37.967 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.95s
2025-07-23 15:00:38.784 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.81s
2025-07-23 15:00:38.785 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: -0.0006888452055635771, avg_kl: 0.004286971746706495, avg_response_length: 87.03678026386336, avg_orm_score: 0.0, avg_custom_rewards: -0.0006888452055635771
2025-07-23 15:00:38.828 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter54_replay_buffer.jsonl
2025-07-23 15:00:40.186 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.36s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=-0.107, ret=0.00107, glen=79.7, tlen=240, kl=0.00597, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:49,  1.01it/s, pg=-0.107, ret=0.00107, glen=79.7, tlen=240, kl=0.00597, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:49,  1.01it/s, pg=0.179, ret=-0.000729, glen=90, tlen=250, kl=0.00317, act_lr=1e-6, ent=1.47] Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.10it/s, pg=0.179, ret=-0.000729, glen=90, tlen=250, kl=0.00317, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.10it/s, pg=0.0372, ret=-0.000461, glen=78.4, tlen=239, kl=0.00526, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:42,  1.12it/s, pg=0.0372, ret=-0.000461, glen=78.4, tlen=239, kl=0.00526, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:42,  1.12it/s, pg=0.0652, ret=-0.00102, glen=88.6, tlen=249, kl=0.00286, act_lr=1e-6, ent=1.4]  Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:41,  1.14it/s, pg=0.0652, ret=-0.00102, glen=88.6, tlen=249, kl=0.00286, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:41,  1.14it/s, pg=0.0677, ret=-0.000303, glen=81.5, tlen=242, kl=0.00434, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:41,  1.11it/s, pg=0.0677, ret=-0.000303, glen=81.5, tlen=242, kl=0.00434, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:41,  1.11it/s, pg=-0.0393, ret=-5.65e-5, glen=84.7, tlen=245, kl=0.00368, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.13it/s, pg=-0.0393, ret=-5.65e-5, glen=84.7, tlen=245, kl=0.00368, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.13it/s, pg=-0.0735, ret=0.000599, glen=79, tlen=239, kl=0.00416, act_lr=1e-6, ent=1.24]  Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.14it/s, pg=-0.0735, ret=0.000599, glen=79, tlen=239, kl=0.00416, act_lr=1e-6, ent=1.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.14it/s, pg=0.0891, ret=-0.00111, glen=79.6, tlen=240, kl=0.00407, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=0.0891, ret=-0.00111, glen=79.6, tlen=240, kl=0.00407, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=0.00311, ret=1.51e-5, glen=83, tlen=243, kl=0.00395, act_lr=1e-6, ent=1.38]  Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.16it/s, pg=0.00311, ret=1.51e-5, glen=83, tlen=243, kl=0.00395, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.16it/s, pg=-0.18, ret=0.00143, glen=78.4, tlen=239, kl=0.00411, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.14it/s, pg=-0.18, ret=0.00143, glen=78.4, tlen=239, kl=0.00411, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.14it/s, pg=-0.0103, ret=-0.00047, glen=84.7, tlen=245, kl=0.00375, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.15it/s, pg=-0.0103, ret=-0.00047, glen=84.7, tlen=245, kl=0.00375, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.15it/s, pg=-0.124, ret=0.00108, glen=82.2, tlen=242, kl=0.00497, act_lr=1e-6, ent=1.29]  Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.16it/s, pg=-0.124, ret=0.00108, glen=82.2, tlen=242, kl=0.00497, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.16it/s, pg=0.0403, ret=-6.9e-5, glen=88.3, tlen=249, kl=0.00534, act_lr=1e-6, ent=1.5] Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.16it/s, pg=0.0403, ret=-6.9e-5, glen=88.3, tlen=249, kl=0.00534, act_lr=1e-6, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.16it/s, pg=-0.086, ret=0.0007, glen=80.3, tlen=240, kl=0.00504, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:32,  1.15it/s, pg=-0.086, ret=0.0007, glen=80.3, tlen=240, kl=0.00504, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:32,  1.15it/s, pg=0.0125, ret=-0.000921, glen=77, tlen=237, kl=0.00467, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.16it/s, pg=0.0125, ret=-0.000921, glen=77, tlen=237, kl=0.00467, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.16it/s, pg=-0.12, ret=0.000787, glen=81.9, tlen=242, kl=0.00457, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:30,  1.16it/s, pg=-0.12, ret=0.000787, glen=81.9, tlen=242, kl=0.00457, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.16it/s, pg=0.0391, ret=-0.00128, glen=84.2, tlen=245, kl=0.00542, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.15it/s, pg=0.0391, ret=-0.00128, glen=84.2, tlen=245, kl=0.00542, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.15it/s, pg=0.0562, ret=0.000779, glen=87.2, tlen=247, kl=0.00344, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.16it/s, pg=0.0562, ret=0.000779, glen=87.2, tlen=247, kl=0.00344, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.16it/s, pg=-0.101, ret=-0.00101, glen=96.2, tlen=257, kl=0.00588, act_lr=1e-6, ent=1.64]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=-0.101, ret=-0.00101, glen=96.2, tlen=257, kl=0.00588, act_lr=1e-6, ent=1.64]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=0.0741, ret=-0.000698, glen=81.6, tlen=242, kl=0.00357, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.16it/s, pg=0.0741, ret=-0.000698, glen=81.6, tlen=242, kl=0.00357, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.16it/s, pg=0.0525, ret=-0.000659, glen=81.3, tlen=242, kl=0.00371, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.0525, ret=-0.000659, glen=81.3, tlen=242, kl=0.00371, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=-0.132, ret=0.000465, glen=101, tlen=261, kl=0.00446, act_lr=1e-6, ent=1.68]  Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=-0.132, ret=0.000465, glen=101, tlen=261, kl=0.00446, act_lr=1e-6, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:24,  1.17it/s, pg=-0.106, ret=0.000793, glen=83.2, tlen=244, kl=0.00354, act_lr=1e-6, ent=1.44]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:24,  1.17it/s, pg=-0.106, ret=0.000793, glen=83.2, tlen=244, kl=0.00354, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:24,  1.17it/s, pg=0.244, ret=-0.00157, glen=185, tlen=345, kl=0.00423, act_lr=1e-6, ent=2.15]  Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.14it/s, pg=0.244, ret=-0.00157, glen=185, tlen=345, kl=0.00423, act_lr=1e-6, ent=2.15]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.14it/s, pg=0.0908, ret=-0.000463, glen=82.2, tlen=242, kl=0.00471, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.15it/s, pg=0.0908, ret=-0.000463, glen=82.2, tlen=242, kl=0.00471, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.15it/s, pg=-0.216, ret=0.00032, glen=110, tlen=271, kl=0.00344, act_lr=1e-6, ent=1.14]   Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.15it/s, pg=-0.216, ret=0.00032, glen=110, tlen=271, kl=0.00344, act_lr=1e-6, ent=1.14]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.15it/s, pg=0.101, ret=-0.0013, glen=84.3, tlen=245, kl=0.00426, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:23,  1.00it/s, pg=0.101, ret=-0.0013, glen=84.3, tlen=245, kl=0.00426, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:23,  1.00it/s, pg=-0.0857, ret=-0.000805, glen=94.1, tlen=254, kl=0.00294, act_lr=1e-6, ent=1.73]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:21,  1.05it/s, pg=-0.0857, ret=-0.000805, glen=94.1, tlen=254, kl=0.00294, act_lr=1e-6, ent=1.73]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:21,  1.05it/s, pg=-0.0711, ret=-0.000193, glen=80.7, tlen=241, kl=0.00595, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.08it/s, pg=-0.0711, ret=-0.000193, glen=80.7, tlen=241, kl=0.00595, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.08it/s, pg=0.209, ret=-0.000362, glen=94.7, tlen=255, kl=0.00294, act_lr=1e-6, ent=1.68]  Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.11it/s, pg=0.209, ret=-0.000362, glen=94.7, tlen=255, kl=0.00294, act_lr=1e-6, ent=1.68]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:18,  1.11it/s, pg=-0.0375, ret=-2.87e-6, glen=81.7, tlen=242, kl=0.0053, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.13it/s, pg=-0.0375, ret=-2.87e-6, glen=81.7, tlen=242, kl=0.0053, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.13it/s, pg=-0.0769, ret=0.000151, glen=86.5, tlen=247, kl=0.0036, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.14it/s, pg=-0.0769, ret=0.000151, glen=86.5, tlen=247, kl=0.0036, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:29<00:16,  1.14it/s, pg=0.185, ret=-0.000597, glen=88.8, tlen=249, kl=0.00351, act_lr=1e-6, ent=1.47]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=0.185, ret=-0.000597, glen=88.8, tlen=249, kl=0.00351, act_lr=1e-6, ent=1.47]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.15it/s, pg=-0.0571, ret=0.000286, glen=80.2, tlen=241, kl=0.00558, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.16it/s, pg=-0.0571, ret=0.000286, glen=80.2, tlen=241, kl=0.00558, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=-0.176, ret=0.00134, glen=83.5, tlen=244, kl=0.00308, act_lr=1e-6, ent=1.49]  Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.17it/s, pg=-0.176, ret=0.00134, glen=83.5, tlen=244, kl=0.00308, act_lr=1e-6, ent=1.49]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.17it/s, pg=-0.116, ret=0.000944, glen=84.6, tlen=245, kl=0.0063, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=-0.116, ret=0.000944, glen=84.6, tlen=245, kl=0.0063, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=-0.0636, ret=8.53e-5, glen=90.1, tlen=250, kl=0.00555, act_lr=1e-6, ent=1.59]Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=-0.0636, ret=8.53e-5, glen=90.1, tlen=250, kl=0.00555, act_lr=1e-6, ent=1.59]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:11,  1.17it/s, pg=-0.079, ret=0.00142, glen=87.8, tlen=248, kl=0.0036, act_lr=1e-6, ent=1.48]  Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=-0.079, ret=0.00142, glen=87.8, tlen=248, kl=0.0036, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.17it/s, pg=-0.217, ret=0.00179, glen=86.9, tlen=247, kl=0.00462, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.18it/s, pg=-0.217, ret=0.00179, glen=86.9, tlen=247, kl=0.00462, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.18it/s, pg=-0.0676, ret=0.000726, glen=84.1, tlen=245, kl=0.00483, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.18it/s, pg=-0.0676, ret=0.000726, glen=84.1, tlen=245, kl=0.00483, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.18it/s, pg=-0.0564, ret=0.000139, glen=80.7, tlen=241, kl=0.00342, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.18it/s, pg=-0.0564, ret=0.000139, glen=80.7, tlen=241, kl=0.00342, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.18it/s, pg=-0.196, ret=0.00161, glen=80.3, tlen=241, kl=0.00357, act_lr=1e-6, ent=1.33]  Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.18it/s, pg=-0.196, ret=0.00161, glen=80.3, tlen=241, kl=0.00357, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.18it/s, pg=-0.0302, ret=8.59e-6, glen=84.9, tlen=245, kl=0.0049, act_lr=1e-6, ent=1.41]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.18it/s, pg=-0.0302, ret=8.59e-6, glen=84.9, tlen=245, kl=0.0049, act_lr=1e-6, ent=1.41]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.18it/s, pg=0.0464, ret=6.18e-5, glen=84.7, tlen=245, kl=0.0033, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.18it/s, pg=0.0464, ret=6.18e-5, glen=84.7, tlen=245, kl=0.0033, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.18it/s, pg=-0.0193, ret=0.000208, glen=79.7, tlen=240, kl=0.00535, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=-0.0193, ret=0.000208, glen=79.7, tlen=240, kl=0.00535, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.18it/s, pg=0.181, ret=-0.000373, glen=88.4, tlen=249, kl=0.00312, act_lr=1e-6, ent=1.58] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.18it/s, pg=0.181, ret=-0.000373, glen=88.4, tlen=249, kl=0.00312, act_lr=1e-6, ent=1.58]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.18it/s, pg=0.0634, ret=-0.000693, glen=75.1, tlen=235, kl=0.00616, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.18it/s, pg=0.0634, ret=-0.000693, glen=75.1, tlen=235, kl=0.00616, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.18it/s, pg=-0.00867, ret=-0.000757, glen=80.9, tlen=241, kl=0.00353, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.18it/s, pg=-0.00867, ret=-0.000757, glen=80.9, tlen=241, kl=0.00353, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.18it/s, pg=0.147, ret=-0.000853, glen=82.1, tlen=242, kl=0.00335, act_lr=1e-6, ent=1.39]   Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.18it/s, pg=0.147, ret=-0.000853, glen=82.1, tlen=242, kl=0.00335, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.18it/s, pg=0.241, ret=-0.00184, glen=84.9, tlen=245, kl=0.00447, act_lr=1e-6, ent=1.4]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.18it/s, pg=0.241, ret=-0.00184, glen=84.9, tlen=245, kl=0.00447, act_lr=1e-6, ent=1.4]
2025-07-23 15:01:24.770 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.42s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.18it/s, pg=0.0319, ret=-0.000187, glen=101, tlen=261, kl=0.00307, act_lr=1e-6, ent=1.52]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=0.0319, ret=-0.000187, glen=101, tlen=261, kl=0.00307, act_lr=1e-6, ent=1.52]
2025-07-23 15:01:25.457 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.62s
2025-07-23 15:01:27.612 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.15s
2025-07-23 15:01:27.923 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.69s
2025-07-23 15:01:27.929 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.007809896095126283, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.4162493359808828, 'kl': 0.004286971746706495, 'response_length': 87.03677981507545, 'total_length': 247.36754653033088, 'return': -3.85753244022364e-05, 'policy_update_steps': 1.0}
Episode [5/20]:  23%|‚ñà‚ñà‚ñé       | 3/13 [09:34<31:50, 191.03s/it]2025-07-23 15:01:27.961 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 15:02:38.156 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 15:02:38.338 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 15:02:38.338 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 70.38s
2025-07-23 15:02:40.469 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0143,avg_reflection_pattern_score: 0.0089,avg_pass_at_n: 1.0000,avg_num_tokens: 83.4087,std_num_tokens: 56.8902,avg_correct_num_tokens: 81.2592,std_correct_num_tokens: 48.7460,avg_incorrect_num_tokens: 86.5887,std_incorrect_num_tokens: 67.0249
2025-07-23 15:02:40.927 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.59s
2025-07-23 15:02:42.187 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.26s
2025-07-23 15:03:08.164 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 202
2025-07-23 15:03:08.165 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 25.98s
2025-07-23 15:03:08.925 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.76s
2025-07-23 15:03:08.926 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.00026480204801508547, avg_kl: 0.005828177574837562, avg_response_length: 83.78981033174118, avg_orm_score: 0.0, avg_custom_rewards: 0.00026480204801508547
2025-07-23 15:03:08.957 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter55_replay_buffer.jsonl
2025-07-23 15:03:10.310 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.35s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=-0.039, ret=0.000353, glen=88.2, tlen=249, kl=0.0065, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:48,  1.03it/s, pg=-0.039, ret=0.000353, glen=88.2, tlen=249, kl=0.0065, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:48,  1.03it/s, pg=0.105, ret=-0.00163, glen=82.5, tlen=243, kl=0.0052, act_lr=1e-6, ent=1.31] Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.10it/s, pg=0.105, ret=-0.00163, glen=82.5, tlen=243, kl=0.0052, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.10it/s, pg=-0.0349, ret=0.000101, glen=78.9, tlen=239, kl=0.00461, act_lr=1e-6, ent=1.23]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:42,  1.13it/s, pg=-0.0349, ret=0.000101, glen=78.9, tlen=239, kl=0.00461, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:42,  1.13it/s, pg=0.06, ret=0.000674, glen=92.2, tlen=253, kl=0.00446, act_lr=1e-6, ent=1.28]   Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:40,  1.15it/s, pg=0.06, ret=0.000674, glen=92.2, tlen=253, kl=0.00446, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:40,  1.15it/s, pg=-0.0334, ret=-0.000363, glen=77.3, tlen=238, kl=0.00624, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.13it/s, pg=-0.0334, ret=-0.000363, glen=77.3, tlen=238, kl=0.00624, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.13it/s, pg=-0.00409, ret=3.91e-5, glen=84, tlen=245, kl=0.0062, act_lr=1e-6, ent=1.37]    Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:40,  1.12it/s, pg=-0.00409, ret=3.91e-5, glen=84, tlen=245, kl=0.0062, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:40,  1.12it/s, pg=-0.00244, ret=-0.000186, glen=86.1, tlen=247, kl=0.00546, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.13it/s, pg=-0.00244, ret=-0.000186, glen=86.1, tlen=247, kl=0.00546, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.13it/s, pg=0.0393, ret=-7.89e-5, glen=79.5, tlen=241, kl=0.00617, act_lr=1e-6, ent=1.31]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=0.0393, ret=-7.89e-5, glen=79.5, tlen=241, kl=0.00617, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=-0.0532, ret=0.000872, glen=79.9, tlen=241, kl=0.00664, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.16it/s, pg=-0.0532, ret=0.000872, glen=79.9, tlen=241, kl=0.00664, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.16it/s, pg=0.0137, ret=-0.0006, glen=92.5, tlen=253, kl=0.00674, act_lr=1e-6, ent=1.51]  Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.16it/s, pg=0.0137, ret=-0.0006, glen=92.5, tlen=253, kl=0.00674, act_lr=1e-6, ent=1.51]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.16it/s, pg=-0.105, ret=0.00124, glen=81.4, tlen=242, kl=0.00496, act_lr=1e-6, ent=1.21]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:35,  1.14it/s, pg=-0.105, ret=0.00124, glen=81.4, tlen=242, kl=0.00496, act_lr=1e-6, ent=1.21]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:35,  1.14it/s, pg=-0.22, ret=0.00154, glen=88.3, tlen=249, kl=0.00582, act_lr=1e-6, ent=1.32] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.15it/s, pg=-0.22, ret=0.00154, glen=88.3, tlen=249, kl=0.00582, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.15it/s, pg=-0.0784, ret=0.000647, glen=84.4, tlen=245, kl=0.00459, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.16it/s, pg=-0.0784, ret=0.000647, glen=84.4, tlen=245, kl=0.00459, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.16it/s, pg=0.158, ret=-0.00115, glen=81.8, tlen=242, kl=0.00473, act_lr=1e-6, ent=1.25]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:31,  1.16it/s, pg=0.158, ret=-0.00115, glen=81.8, tlen=242, kl=0.00473, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:31,  1.16it/s, pg=-0.00638, ret=-0.000183, glen=86.6, tlen=247, kl=0.00458, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:30,  1.17it/s, pg=-0.00638, ret=-0.000183, glen=86.6, tlen=247, kl=0.00458, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:30,  1.17it/s, pg=-0.0891, ret=0.000479, glen=80.9, tlen=241, kl=0.00797, act_lr=1e-6, ent=1.29]  Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:13<00:29,  1.17it/s, pg=-0.0891, ret=0.000479, glen=80.9, tlen=241, kl=0.00797, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:29,  1.17it/s, pg=0.0132, ret=-4.42e-5, glen=79.1, tlen=240, kl=0.00555, act_lr=1e-6, ent=1.18] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.17it/s, pg=0.0132, ret=-4.42e-5, glen=79.1, tlen=240, kl=0.00555, act_lr=1e-6, ent=1.18]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.17it/s, pg=0.147, ret=-0.00181, glen=85, tlen=246, kl=0.00491, act_lr=1e-6, ent=1.25]   Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.17it/s, pg=0.147, ret=-0.00181, glen=85, tlen=246, kl=0.00491, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.17it/s, pg=-0.0103, ret=-0.000576, glen=87.7, tlen=248, kl=0.00623, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.17it/s, pg=-0.0103, ret=-0.000576, glen=87.7, tlen=248, kl=0.00623, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.17it/s, pg=0.123, ret=-0.000855, glen=83.5, tlen=244, kl=0.00534, act_lr=1e-6, ent=1.44]  Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=0.123, ret=-0.000855, glen=83.5, tlen=244, kl=0.00534, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=-0.124, ret=0.000471, glen=84.7, tlen=246, kl=0.00807, act_lr=1e-6, ent=1.24]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.18it/s, pg=-0.124, ret=0.000471, glen=84.7, tlen=246, kl=0.00807, act_lr=1e-6, ent=1.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.18it/s, pg=0.159, ret=-0.00131, glen=85.2, tlen=246, kl=0.00609, act_lr=1e-6, ent=1.28] Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.18it/s, pg=0.159, ret=-0.00131, glen=85.2, tlen=246, kl=0.00609, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.18it/s, pg=0.124, ret=-0.00157, glen=77.3, tlen=238, kl=0.00555, act_lr=1e-6, ent=1.25]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:19<00:24,  1.15it/s, pg=0.124, ret=-0.00157, glen=77.3, tlen=238, kl=0.00555, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:24,  1.15it/s, pg=0.268, ret=-0.00156, glen=90.5, tlen=251, kl=0.00744, act_lr=1e-6, ent=1.5] Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.16it/s, pg=0.268, ret=-0.00156, glen=90.5, tlen=251, kl=0.00744, act_lr=1e-6, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.16it/s, pg=0.0988, ret=0.00132, glen=91.4, tlen=252, kl=0.00475, act_lr=1e-6, ent=1.2]Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.16it/s, pg=0.0988, ret=0.00132, glen=91.4, tlen=252, kl=0.00475, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.16it/s, pg=-0.153, ret=0.00177, glen=82.8, tlen=243, kl=0.00565, act_lr=1e-6, ent=1.25]Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.16it/s, pg=-0.153, ret=0.00177, glen=82.8, tlen=243, kl=0.00565, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.16it/s, pg=-0.106, ret=0.000441, glen=75.9, tlen=236, kl=0.00499, act_lr=1e-6, ent=1.22]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:22,  1.07it/s, pg=-0.106, ret=0.000441, glen=75.9, tlen=236, kl=0.00499, act_lr=1e-6, ent=1.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:22,  1.07it/s, pg=-0.0811, ret=0.00116, glen=93.9, tlen=255, kl=0.00539, act_lr=1e-6, ent=1.54]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:20,  1.10it/s, pg=-0.0811, ret=0.00116, glen=93.9, tlen=255, kl=0.00539, act_lr=1e-6, ent=1.54]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:20,  1.10it/s, pg=-0.0336, ret=0.000372, glen=84.6, tlen=245, kl=0.00483, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:19,  1.12it/s, pg=-0.0336, ret=0.000372, glen=84.6, tlen=245, kl=0.00483, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:19,  1.12it/s, pg=-0.0206, ret=0.000242, glen=86.6, tlen=247, kl=0.00811, act_lr=1e-6, ent=1.22]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.14it/s, pg=-0.0206, ret=0.000242, glen=86.6, tlen=247, kl=0.00811, act_lr=1e-6, ent=1.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:18,  1.14it/s, pg=0.148, ret=-0.00148, glen=76.3, tlen=237, kl=0.00531, act_lr=1e-6, ent=1.2]   Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.15it/s, pg=0.148, ret=-0.00148, glen=76.3, tlen=237, kl=0.00531, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.15it/s, pg=-0.0576, ret=0.000674, glen=76.9, tlen=237, kl=0.00698, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:27<00:16,  1.16it/s, pg=-0.0576, ret=0.000674, glen=76.9, tlen=237, kl=0.00698, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.16it/s, pg=-0.112, ret=0.000945, glen=83.8, tlen=244, kl=0.00616, act_lr=1e-6, ent=1.3]  Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.16it/s, pg=-0.112, ret=0.000945, glen=83.8, tlen=244, kl=0.00616, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.16it/s, pg=-0.00244, ret=-0.000286, glen=80.5, tlen=241, kl=0.00488, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.16it/s, pg=-0.00244, ret=-0.000286, glen=80.5, tlen=241, kl=0.00488, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=-0.00383, ret=-0.000572, glen=78.7, tlen=239, kl=0.00655, act_lr=1e-6, ent=1.23]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.17it/s, pg=-0.00383, ret=-0.000572, glen=78.7, tlen=239, kl=0.00655, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.17it/s, pg=0.000977, ret=0.000653, glen=83.3, tlen=244, kl=0.00584, act_lr=1e-6, ent=1.36] Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.17it/s, pg=0.000977, ret=0.000653, glen=83.3, tlen=244, kl=0.00584, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.17it/s, pg=0.0333, ret=-0.00104, glen=87.5, tlen=249, kl=0.00493, act_lr=1e-6, ent=1.37]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=0.0333, ret=-0.00104, glen=87.5, tlen=249, kl=0.00493, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:11,  1.17it/s, pg=-0.217, ret=0.00176, glen=80.1, tlen=241, kl=0.00581, act_lr=1e-6, ent=1.24] Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:32<00:11,  1.18it/s, pg=-0.217, ret=0.00176, glen=80.1, tlen=241, kl=0.00581, act_lr=1e-6, ent=1.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.18it/s, pg=0.0779, ret=-0.000943, glen=86.3, tlen=246, kl=0.00652, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:33<00:10,  1.17it/s, pg=0.0779, ret=-0.000943, glen=86.3, tlen=246, kl=0.00652, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=0.0181, ret=-2.14e-5, glen=83.4, tlen=244, kl=0.00584, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.18it/s, pg=0.0181, ret=-2.14e-5, glen=83.4, tlen=244, kl=0.00584, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.18it/s, pg=-0.0951, ret=0.000559, glen=78.3, tlen=239, kl=0.00668, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.18it/s, pg=-0.0951, ret=0.000559, glen=78.3, tlen=239, kl=0.00668, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.18it/s, pg=-0.0964, ret=0.000588, glen=89, tlen=250, kl=0.00504, act_lr=1e-6, ent=1.4]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.18it/s, pg=-0.0964, ret=0.000588, glen=89, tlen=250, kl=0.00504, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.18it/s, pg=-0.0755, ret=0.000223, glen=82, tlen=242, kl=0.0048, act_lr=1e-6, ent=1.2] Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.18it/s, pg=-0.0755, ret=0.000223, glen=82, tlen=242, kl=0.0048, act_lr=1e-6, ent=1.2]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.18it/s, pg=0.0205, ret=0.00126, glen=95.3, tlen=256, kl=0.00837, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=0.0205, ret=0.00126, glen=95.3, tlen=256, kl=0.00837, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=0.00577, ret=-0.000125, glen=78.2, tlen=239, kl=0.00622, act_lr=1e-6, ent=1.23]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:38<00:05,  1.18it/s, pg=0.00577, ret=-0.000125, glen=78.2, tlen=239, kl=0.00622, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=-0.0576, ret=0.000231, glen=84.7, tlen=245, kl=0.00538, act_lr=1e-6, ent=1.32] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:39<00:04,  1.16it/s, pg=-0.0576, ret=0.000231, glen=84.7, tlen=245, kl=0.00538, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.16it/s, pg=0.111, ret=-0.00132, glen=85.3, tlen=246, kl=0.00642, act_lr=1e-6, ent=1.23]  Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.16it/s, pg=0.111, ret=-0.00132, glen=85.3, tlen=246, kl=0.00642, act_lr=1e-6, ent=1.23]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.16it/s, pg=0.122, ret=-0.00137, glen=84.4, tlen=245, kl=0.0048, act_lr=1e-6, ent=1.26] Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=0.122, ret=-0.00137, glen=84.4, tlen=245, kl=0.0048, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=0.00281, ret=0.000556, glen=80.8, tlen=241, kl=0.00557, act_lr=1e-6, ent=1.25]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=0.00281, ret=0.000556, glen=80.8, tlen=241, kl=0.00557, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.172, ret=0.00197, glen=91.3, tlen=253, kl=0.00505, act_lr=1e-6, ent=1.34]  Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=-0.172, ret=0.00197, glen=91.3, tlen=253, kl=0.00505, act_lr=1e-6, ent=1.34]
2025-07-23 15:03:54.620 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.15s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=0.0634, ret=-0.00112, glen=79.2, tlen=239, kl=0.00549, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=0.0634, ret=-0.00112, glen=79.2, tlen=239, kl=0.00549, act_lr=1e-6, ent=1.29]
2025-07-23 15:03:55.486 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.80s
2025-07-23 15:03:57.999 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.51s
2025-07-23 15:03:58.326 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.98s
2025-07-23 15:03:58.331 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0033552880380667894, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.3010662536995083, 'kl': 0.005811728683172488, 'response_length': 83.88522159352021, 'total_length': 244.5438486735026, 'return': 1.8386979397345224e-05, 'policy_update_steps': 1.0}
Episode [5/20]:  31%|‚ñà‚ñà‚ñà       | 4/13 [12:05<26:14, 174.99s/it]2025-07-23 15:03:58.363 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 15:05:09.421 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 15:05:09.608 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.19s
2025-07-23 15:05:09.608 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 71.25s
2025-07-23 15:05:11.410 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0138,avg_reflection_pattern_score: 0.0074,avg_pass_at_n: 1.0000,avg_num_tokens: 84.9160,std_num_tokens: 58.7275,avg_correct_num_tokens: 82.8133,std_correct_num_tokens: 49.8237,avg_incorrect_num_tokens: 88.1946,std_incorrect_num_tokens: 70.2729
2025-07-23 15:05:11.713 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.10s
2025-07-23 15:05:13.192 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.48s
2025-07-23 15:05:39.258 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 203
2025-07-23 15:05:39.259 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.06s
2025-07-23 15:05:40.131 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.87s
2025-07-23 15:05:40.132 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0004931289973333845, avg_kl: 0.008530038918180419, avg_response_length: 85.3187415212246, avg_orm_score: 0.0, avg_custom_rewards: 0.0004931289973333845
2025-07-23 15:05:40.162 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter56_replay_buffer.jsonl
2025-07-23 15:05:41.525 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.36s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=0.0311, ret=-2.64e-5, glen=88.5, tlen=248, kl=0.008, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:49,  1.01it/s, pg=0.0311, ret=-2.64e-5, glen=88.5, tlen=248, kl=0.008, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:49,  1.01it/s, pg=-0.0473, ret=0.000778, glen=81.4, tlen=242, kl=0.014, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.09it/s, pg=-0.0473, ret=0.000778, glen=81.4, tlen=242, kl=0.014, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.09it/s, pg=0.0547, ret=-0.000144, glen=92.5, tlen=253, kl=0.00811, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:44,  1.07it/s, pg=0.0547, ret=-0.000144, glen=92.5, tlen=253, kl=0.00811, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:44,  1.07it/s, pg=0.164, ret=-0.00151, glen=82.6, tlen=243, kl=0.008, act_lr=1e-6, ent=1.35]    Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:42,  1.11it/s, pg=0.164, ret=-0.00151, glen=82.6, tlen=243, kl=0.008, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:42,  1.11it/s, pg=0.105, ret=-0.000996, glen=85.5, tlen=246, kl=0.00681, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.13it/s, pg=0.105, ret=-0.000996, glen=85.5, tlen=246, kl=0.00681, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.13it/s, pg=0.012, ret=-3.73e-5, glen=83.5, tlen=244, kl=0.00909, act_lr=1e-6, ent=1.31] Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.13it/s, pg=0.012, ret=-3.73e-5, glen=83.5, tlen=244, kl=0.00909, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.13it/s, pg=0.0302, ret=-0.000266, glen=80.9, tlen=241, kl=0.00816, act_lr=1e-6, ent=1.34]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.14it/s, pg=0.0302, ret=-0.000266, glen=80.9, tlen=241, kl=0.00816, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.14it/s, pg=0.123, ret=-0.00111, glen=86.3, tlen=247, kl=0.0084, act_lr=1e-6, ent=1.32]   Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=0.123, ret=-0.00111, glen=86.3, tlen=247, kl=0.0084, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:37,  1.15it/s, pg=-0.218, ret=0.00171, glen=88.8, tlen=249, kl=0.006, act_lr=1e-6, ent=1.48] Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:07<00:36,  1.16it/s, pg=-0.218, ret=0.00171, glen=88.8, tlen=249, kl=0.006, act_lr=1e-6, ent=1.48]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:36,  1.16it/s, pg=-0.0753, ret=0.00024, glen=87.1, tlen=248, kl=0.00851, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:35,  1.16it/s, pg=-0.0753, ret=0.00024, glen=87.1, tlen=248, kl=0.00851, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:35,  1.16it/s, pg=-0.136, ret=0.000567, glen=94, tlen=255, kl=0.00558, act_lr=1e-6, ent=1.55] Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.17it/s, pg=-0.136, ret=0.000567, glen=94, tlen=255, kl=0.00558, act_lr=1e-6, ent=1.55]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.17it/s, pg=0.00455, ret=0.000405, glen=78, tlen=238, kl=0.00816, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:34,  1.13it/s, pg=0.00455, ret=0.000405, glen=78, tlen=238, kl=0.00816, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:34,  1.13it/s, pg=-0.142, ret=0.00103, glen=81.5, tlen=242, kl=0.00806, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:33,  1.14it/s, pg=-0.142, ret=0.00103, glen=81.5, tlen=242, kl=0.00806, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:33,  1.14it/s, pg=0.102, ret=-0.00128, glen=80.2, tlen=240, kl=0.0107, act_lr=1e-6, ent=1.3]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:33,  1.12it/s, pg=0.102, ret=-0.00128, glen=80.2, tlen=240, kl=0.0107, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:33,  1.12it/s, pg=0.063, ret=-0.000392, glen=82.9, tlen=243, kl=0.00875, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:31,  1.14it/s, pg=0.063, ret=-0.000392, glen=82.9, tlen=243, kl=0.00875, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:14<00:31,  1.14it/s, pg=-0.158, ret=0.00131, glen=80.6, tlen=241, kl=0.00722, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.15it/s, pg=-0.158, ret=0.00131, glen=80.6, tlen=241, kl=0.00722, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.15it/s, pg=0.173, ret=-0.00063, glen=86.7, tlen=247, kl=0.00647, act_lr=1e-6, ent=1.4] Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.16it/s, pg=0.173, ret=-0.00063, glen=86.7, tlen=247, kl=0.00647, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.16it/s, pg=-0.0695, ret=0.00107, glen=81.5, tlen=242, kl=0.00886, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.16it/s, pg=-0.0695, ret=0.00107, glen=81.5, tlen=242, kl=0.00886, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.16it/s, pg=0.134, ret=-0.000564, glen=93.8, tlen=254, kl=0.00591, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=0.134, ret=-0.000564, glen=93.8, tlen=254, kl=0.00591, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=-0.0707, ret=9.63e-5, glen=82.4, tlen=243, kl=0.00895, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=-0.0707, ret=9.63e-5, glen=82.4, tlen=243, kl=0.00895, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=-0.0709, ret=0.000604, glen=83.7, tlen=244, kl=0.0118, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=-0.0709, ret=0.000604, glen=83.7, tlen=244, kl=0.0118, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=0.0208, ret=-0.00061, glen=82.3, tlen=243, kl=0.00727, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=0.0208, ret=-0.00061, glen=82.3, tlen=243, kl=0.00727, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:24,  1.17it/s, pg=0.0382, ret=-0.00141, glen=84.2, tlen=245, kl=0.00771, act_lr=1e-6, ent=1.37]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=0.0382, ret=-0.00141, glen=84.2, tlen=245, kl=0.00771, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=0.0322, ret=0.000229, glen=82.8, tlen=243, kl=0.00821, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.17it/s, pg=0.0322, ret=0.000229, glen=82.8, tlen=243, kl=0.00821, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=-0.0944, ret=0.00114, glen=90.2, tlen=250, kl=0.00668, act_lr=1e-6, ent=1.4] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.18it/s, pg=-0.0944, ret=0.00114, glen=90.2, tlen=250, kl=0.00668, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.18it/s, pg=-0.106, ret=0.00121, glen=84.9, tlen=246, kl=0.0278, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.18it/s, pg=-0.106, ret=0.00121, glen=84.9, tlen=246, kl=0.0278, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.18it/s, pg=-0.0282, ret=0.00068, glen=83.9, tlen=244, kl=0.00639, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:23,  1.02it/s, pg=-0.0282, ret=0.00068, glen=83.9, tlen=244, kl=0.00639, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:23,  1.02it/s, pg=-0.121, ret=0.000568, glen=86.8, tlen=247, kl=0.012, act_lr=1e-6, ent=1.38]  Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:21,  1.07it/s, pg=-0.121, ret=0.000568, glen=86.8, tlen=247, kl=0.012, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:21,  1.07it/s, pg=0.154, ret=-0.000557, glen=85.1, tlen=246, kl=0.00976, act_lr=1e-6, ent=1.4]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:20,  1.10it/s, pg=0.154, ret=-0.000557, glen=85.1, tlen=246, kl=0.00976, act_lr=1e-6, ent=1.4]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:20,  1.10it/s, pg=-0.0721, ret=0.000609, glen=90.8, tlen=251, kl=0.00632, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.12it/s, pg=-0.0721, ret=0.000609, glen=90.8, tlen=251, kl=0.00632, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:18,  1.12it/s, pg=-0.00659, ret=-0.000251, glen=81.5, tlen=242, kl=0.00654, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.14it/s, pg=-0.00659, ret=-0.000251, glen=81.5, tlen=242, kl=0.00654, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:28<00:17,  1.14it/s, pg=0.0509, ret=-0.000849, glen=83.6, tlen=244, kl=0.00622, act_lr=1e-6, ent=1.32]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.15it/s, pg=0.0509, ret=-0.000849, glen=83.6, tlen=244, kl=0.00622, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.15it/s, pg=-0.0397, ret=-0.000204, glen=86.1, tlen=246, kl=0.00772, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.16it/s, pg=-0.0397, ret=-0.000204, glen=86.1, tlen=246, kl=0.00772, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.16it/s, pg=0.0336, ret=9.36e-5, glen=85.6, tlen=246, kl=0.0197, act_lr=1e-6, ent=1.3]     Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.16it/s, pg=0.0336, ret=9.36e-5, glen=85.6, tlen=246, kl=0.0197, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.16it/s, pg=0.157, ret=-0.00138, glen=77.1, tlen=237, kl=0.00735, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.15it/s, pg=0.157, ret=-0.00138, glen=77.1, tlen=237, kl=0.00735, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.15it/s, pg=-0.0215, ret=0.000897, glen=85.1, tlen=245, kl=0.00616, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.16it/s, pg=-0.0215, ret=0.000897, glen=85.1, tlen=245, kl=0.00616, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.16it/s, pg=0.16, ret=0.00134, glen=118, tlen=278, kl=0.00497, act_lr=1e-6, ent=1.87]     Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:12,  1.16it/s, pg=0.16, ret=0.00134, glen=118, tlen=278, kl=0.00497, act_lr=1e-6, ent=1.87]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:12,  1.16it/s, pg=-0.0431, ret=0.000835, glen=82, tlen=242, kl=0.00771, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.16it/s, pg=-0.0431, ret=0.000835, glen=82, tlen=242, kl=0.00771, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:34<00:11,  1.16it/s, pg=0.0312, ret=0.000441, glen=80.6, tlen=241, kl=0.00783, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=0.0312, ret=0.000441, glen=80.6, tlen=241, kl=0.00783, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:35<00:10,  1.17it/s, pg=-0.0238, ret=8.19e-5, glen=91.2, tlen=252, kl=0.00571, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.0238, ret=8.19e-5, glen=91.2, tlen=252, kl=0.00571, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.00565, ret=-0.000896, glen=88.8, tlen=250, kl=0.0071, act_lr=1e-6, ent=1.45]Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.17it/s, pg=-0.00565, ret=-0.000896, glen=88.8, tlen=250, kl=0.0071, act_lr=1e-6, ent=1.45]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.17it/s, pg=-0.0273, ret=-8.92e-6, glen=80, tlen=240, kl=0.00658, act_lr=1e-6, ent=1.28]   Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.17it/s, pg=-0.0273, ret=-8.92e-6, glen=80, tlen=240, kl=0.00658, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.17it/s, pg=-0.129, ret=0.00058, glen=80, tlen=241, kl=0.00843, act_lr=1e-6, ent=1.31]  Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.18it/s, pg=-0.129, ret=0.00058, glen=80, tlen=241, kl=0.00843, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.18it/s, pg=0.095, ret=-0.000928, glen=82.5, tlen=243, kl=0.00713, act_lr=1e-6, ent=1.3]Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.18it/s, pg=0.095, ret=-0.000928, glen=82.5, tlen=243, kl=0.00713, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.18it/s, pg=-0.0536, ret=0.000684, glen=82.2, tlen=243, kl=0.00659, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=-0.0536, ret=0.000684, glen=82.2, tlen=243, kl=0.00659, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:40<00:05,  1.18it/s, pg=0.0104, ret=-0.000536, glen=85.9, tlen=246, kl=0.0081, act_lr=1e-6, ent=1.31] Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.18it/s, pg=0.0104, ret=-0.000536, glen=85.9, tlen=246, kl=0.0081, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.18it/s, pg=-0.146, ret=0.000315, glen=89.6, tlen=250, kl=0.0129, act_lr=1e-6, ent=1.37] Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.18it/s, pg=-0.146, ret=0.000315, glen=89.6, tlen=250, kl=0.0129, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.18it/s, pg=-0.1, ret=0.00102, glen=88, tlen=248, kl=0.00734, act_lr=1e-6, ent=1.31]    Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=-0.1, ret=0.00102, glen=88, tlen=248, kl=0.00734, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=0.0342, ret=-0.0011, glen=83.3, tlen=243, kl=0.00712, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=0.0342, ret=-0.0011, glen=83.3, tlen=243, kl=0.00712, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=-0.0402, ret=0.000108, glen=81.1, tlen=241, kl=0.00919, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.18it/s, pg=-0.0402, ret=0.000108, glen=81.1, tlen=241, kl=0.00919, act_lr=1e-6, ent=1.29]
2025-07-23 15:06:26.124 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.44s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.18it/s, pg=0.0767, ret=-0.000777, glen=85.3, tlen=246, kl=0.00687, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=0.0767, ret=-0.000777, glen=85.3, tlen=246, kl=0.00687, act_lr=1e-6, ent=1.28]
2025-07-23 15:06:26.792 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.60s
2025-07-23 15:06:29.008 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.22s
2025-07-23 15:06:29.316 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.75s
2025-07-23 15:06:29.321 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.0030577416513480394, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.35127120625739, 'kl': 0.008527419146369486, 'response_length': 85.3121581732058, 'total_length': 245.68874493767234, 'return': 4.284358774562932e-05, 'policy_update_steps': 1.0}
Episode [5/20]:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [14:36<22:10, 166.34s/it]2025-07-23 15:06:29.354 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
2025-07-23 15:07:34.813 | INFO     | orz.ppo.trainer:make_experience:230 - generate local rollout batch done
2025-07-23 15:07:34.990 | INFO     | orz.ppo.trainer:make_experience:234 - Offload vllm engines to cpu, time cost: 0.18s
2025-07-23 15:07:34.990 | INFO     | orz.ppo.trainer:make_experience:215 - Generate sequences via vllm engines, time cost: 65.64s
2025-07-23 15:07:37.045 | INFO     | playground.orz_7b_ppo:custom_reward_fn:322 - avg_non_stop_count: 0.0000,avg_repeat_score: 0.0139,avg_reflection_pattern_score: 0.0065,avg_pass_at_n: 1.0000,avg_num_tokens: 83.6211,std_num_tokens: 54.0590,avg_correct_num_tokens: 82.0262,std_correct_num_tokens: 50.8822,avg_incorrect_num_tokens: 85.9251,std_incorrect_num_tokens: 58.2667
2025-07-23 15:07:37.483 | INFO     | orz.ppo.trainer:make_experience:245 - Calculate custom rewards, time cost: 2.49s
2025-07-23 15:07:38.733 | INFO     | orz.ppo.trainer:make_experience:260 - Packing samples, time cost: 1.25s
2025-07-23 15:08:04.732 | INFO     | orz.ppo.trainer:make_experience:282 - experiences size: 202
2025-07-23 15:08:04.733 | INFO     | orz.ppo.trainer:make_experience:273 - Inference and calculate values, log probs, rewards, kl divergence, time cost: 26.00s
2025-07-23 15:08:05.456 | INFO     | orz.ppo.trainer:make_experience:299 - Calculate advantages and returns, time cost: 0.72s
2025-07-23 15:08:05.456 | INFO     | orz.ppo.trainer:make_experience:317 - avg_raw_rewards: 0.0031767573017721516, avg_kl: 0.009541596516524212, avg_response_length: 87.3038369358176, avg_orm_score: 0.0, avg_custom_rewards: 0.0031767573017721516
2025-07-23 15:08:05.486 | INFO     | orz.ppo.trainer:train:129 - dumping replay buffer to /home/a/anokhin/links/scratch/orz_ckpt/binary_orz_1p5b_ppo_grpo-4gpu-v0/dumped_replay_buffer/iter57_replay_buffer.jsonl
2025-07-23 15:08:06.831 | INFO     | orz.ppo.trainer:train:124 - Dumping replay buffer, time cost: 1.35s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   0%|          | 0/51 [00:00<?, ?it/s, pg=0.0679, ret=-0.00063, glen=86, tlen=246, kl=0.00887, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:00<00:48,  1.02it/s, pg=0.0679, ret=-0.00063, glen=86, tlen=246, kl=0.00887, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   2%|‚ñè         | 1/51 [00:01<00:48,  1.02it/s, pg=-0.0505, ret=0.000412, glen=79, tlen=240, kl=0.00818, act_lr=1e-6, ent=1.25]Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:01<00:44,  1.10it/s, pg=-0.0505, ret=0.000412, glen=79, tlen=240, kl=0.00818, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   4%|‚ñç         | 2/51 [00:02<00:44,  1.10it/s, pg=-0.0867, ret=0.000728, glen=85.7, tlen=246, kl=0.00963, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:02<00:43,  1.09it/s, pg=-0.0867, ret=0.000728, glen=85.7, tlen=246, kl=0.00963, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   6%|‚ñå         | 3/51 [00:03<00:43,  1.09it/s, pg=-0.0493, ret=0.000704, glen=83.5, tlen=244, kl=0.0089, act_lr=1e-6, ent=1.29] Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:03<00:42,  1.12it/s, pg=-0.0493, ret=0.000704, glen=83.5, tlen=244, kl=0.0089, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:   8%|‚ñä         | 4/51 [00:04<00:42,  1.12it/s, pg=0.0306, ret=-2.1e-5, glen=80.8, tlen=241, kl=0.00858, act_lr=1e-6, ent=1.34] Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:04<00:40,  1.14it/s, pg=0.0306, ret=-2.1e-5, glen=80.8, tlen=241, kl=0.00858, act_lr=1e-6, ent=1.34]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  10%|‚ñâ         | 5/51 [00:05<00:40,  1.14it/s, pg=-0.0947, ret=0.00078, glen=83.5, tlen=243, kl=0.00851, act_lr=1e-6, ent=1.3]Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:05<00:39,  1.15it/s, pg=-0.0947, ret=0.00078, glen=83.5, tlen=243, kl=0.00851, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  12%|‚ñà‚ñè        | 6/51 [00:06<00:39,  1.15it/s, pg=0.188, ret=-0.000948, glen=99.6, tlen=260, kl=0.00743, act_lr=1e-6, ent=1.5]Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:06<00:38,  1.14it/s, pg=0.188, ret=-0.000948, glen=99.6, tlen=260, kl=0.00743, act_lr=1e-6, ent=1.5]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  14%|‚ñà‚ñé        | 7/51 [00:07<00:38,  1.14it/s, pg=0.0523, ret=-6.11e-5, glen=82.8, tlen=243, kl=0.00692, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:07<00:38,  1.13it/s, pg=0.0523, ret=-6.11e-5, glen=82.8, tlen=243, kl=0.00692, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  16%|‚ñà‚ñå        | 8/51 [00:08<00:38,  1.13it/s, pg=-0.201, ret=0.0015, glen=81.6, tlen=241, kl=0.0292, act_lr=1e-6, ent=1.22]   Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:37,  1.12it/s, pg=-0.201, ret=0.0015, glen=81.6, tlen=241, kl=0.0292, act_lr=1e-6, ent=1.22]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  18%|‚ñà‚ñä        | 9/51 [00:08<00:37,  1.12it/s, pg=0.0196, ret=-0.000434, glen=85.5, tlen=246, kl=0.00724, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:08<00:36,  1.13it/s, pg=0.0196, ret=-0.000434, glen=85.5, tlen=246, kl=0.00724, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  20%|‚ñà‚ñâ        | 10/51 [00:09<00:36,  1.13it/s, pg=-0.0216, ret=-0.000761, glen=84.8, tlen=245, kl=0.00676, act_lr=1e-6, ent=1.35]Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:09<00:34,  1.14it/s, pg=-0.0216, ret=-0.000761, glen=84.8, tlen=245, kl=0.00676, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  22%|‚ñà‚ñà‚ñè       | 11/51 [00:10<00:34,  1.14it/s, pg=-0.0641, ret=0.000478, glen=84.9, tlen=245, kl=0.00816, act_lr=1e-6, ent=1.31] Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:10<00:33,  1.15it/s, pg=-0.0641, ret=0.000478, glen=84.9, tlen=245, kl=0.00816, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  24%|‚ñà‚ñà‚ñé       | 12/51 [00:11<00:33,  1.15it/s, pg=-0.0785, ret=0.000623, glen=82.7, tlen=243, kl=0.00853, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:11<00:32,  1.16it/s, pg=-0.0785, ret=0.000623, glen=82.7, tlen=243, kl=0.00853, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  25%|‚ñà‚ñà‚ñå       | 13/51 [00:12<00:32,  1.16it/s, pg=-0.17, ret=0.000765, glen=90.1, tlen=250, kl=0.00894, act_lr=1e-6, ent=1.46]  Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:12<00:31,  1.16it/s, pg=-0.17, ret=0.000765, glen=90.1, tlen=250, kl=0.00894, act_lr=1e-6, ent=1.46]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  27%|‚ñà‚ñà‚ñã       | 14/51 [00:13<00:31,  1.16it/s, pg=0.065, ret=5.18e-5, glen=83.7, tlen=244, kl=0.0109, act_lr=1e-6, ent=1.35]  Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:13<00:30,  1.17it/s, pg=0.065, ret=5.18e-5, glen=83.7, tlen=244, kl=0.0109, act_lr=1e-6, ent=1.35]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  29%|‚ñà‚ñà‚ñâ       | 15/51 [00:14<00:30,  1.17it/s, pg=-0.0104, ret=0.000221, glen=83.4, tlen=243, kl=0.00981, act_lr=1e-6, ent=1.31]Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.14it/s, pg=-0.0104, ret=0.000221, glen=83.4, tlen=243, kl=0.00981, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:14<00:30,  1.14it/s, pg=-0.0209, ret=0.000532, glen=79, tlen=239, kl=0.00715, act_lr=1e-6, ent=1.29]  Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:14<00:29,  1.15it/s, pg=-0.0209, ret=0.000532, glen=79, tlen=239, kl=0.00715, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  33%|‚ñà‚ñà‚ñà‚ñé      | 17/51 [00:15<00:29,  1.15it/s, pg=0.175, ret=-0.00111, glen=77.6, tlen=238, kl=0.00734, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:15<00:28,  1.15it/s, pg=0.175, ret=-0.00111, glen=77.6, tlen=238, kl=0.00734, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:16<00:28,  1.15it/s, pg=-0.0149, ret=-0.000588, glen=80.9, tlen=241, kl=0.00977, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:16<00:27,  1.16it/s, pg=-0.0149, ret=-0.000588, glen=80.9, tlen=241, kl=0.00977, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  37%|‚ñà‚ñà‚ñà‚ñã      | 19/51 [00:17<00:27,  1.16it/s, pg=-0.00531, ret=-0.000158, glen=85, tlen=245, kl=0.00752, act_lr=1e-6, ent=1.32] Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:17<00:26,  1.17it/s, pg=-0.00531, ret=-0.000158, glen=85, tlen=245, kl=0.00752, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:18<00:26,  1.17it/s, pg=0.142, ret=-0.00113, glen=81.5, tlen=241, kl=0.00771, act_lr=1e-6, ent=1.36]  Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:18<00:25,  1.17it/s, pg=0.142, ret=-0.00113, glen=81.5, tlen=241, kl=0.00771, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  41%|‚ñà‚ñà‚ñà‚ñà      | 21/51 [00:19<00:25,  1.17it/s, pg=0.0754, ret=-0.000716, glen=83.8, tlen=243, kl=0.0166, act_lr=1e-6, ent=1.38]Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:19<00:24,  1.17it/s, pg=0.0754, ret=-0.000716, glen=83.8, tlen=243, kl=0.0166, act_lr=1e-6, ent=1.38]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:20<00:24,  1.17it/s, pg=0.024, ret=-0.000387, glen=85.2, tlen=245, kl=0.00772, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=0.024, ret=-0.000387, glen=85.2, tlen=245, kl=0.00772, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/51 [00:20<00:23,  1.17it/s, pg=-0.0118, ret=-0.000125, glen=87.1, tlen=247, kl=0.00807, act_lr=1e-6, ent=1.57]Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:20<00:23,  1.17it/s, pg=-0.0118, ret=-0.000125, glen=87.1, tlen=247, kl=0.00807, act_lr=1e-6, ent=1.57]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:21<00:23,  1.17it/s, pg=-0.0894, ret=0.000852, glen=83.4, tlen=244, kl=0.00774, act_lr=1e-6, ent=1.27] Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:21<00:22,  1.17it/s, pg=-0.0894, ret=0.000852, glen=83.4, tlen=244, kl=0.00774, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25/51 [00:22<00:22,  1.17it/s, pg=0.0254, ret=9.93e-5, glen=91.6, tlen=252, kl=0.00772, act_lr=1e-6, ent=1.44]  Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:22<00:21,  1.17it/s, pg=0.0254, ret=9.93e-5, glen=91.6, tlen=252, kl=0.00772, act_lr=1e-6, ent=1.44]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26/51 [00:23<00:21,  1.17it/s, pg=0.185, ret=-0.00203, glen=81.8, tlen=242, kl=0.0089, act_lr=1e-6, ent=1.28] Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:23<00:22,  1.08it/s, pg=0.185, ret=-0.00203, glen=81.8, tlen=242, kl=0.0089, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:24<00:22,  1.08it/s, pg=-0.124, ret=0.00173, glen=84.8, tlen=245, kl=0.00788, act_lr=1e-6, ent=1.28]Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:24<00:20,  1.10it/s, pg=-0.124, ret=0.00173, glen=84.8, tlen=245, kl=0.00788, act_lr=1e-6, ent=1.28]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/51 [00:25<00:20,  1.10it/s, pg=0.0654, ret=-0.000219, glen=85.2, tlen=245, kl=0.00834, act_lr=1e-6, ent=1.39]Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:25<00:19,  1.12it/s, pg=0.0654, ret=-0.000219, glen=85.2, tlen=245, kl=0.00834, act_lr=1e-6, ent=1.39]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29/51 [00:26<00:19,  1.12it/s, pg=0.112, ret=-0.00119, glen=81, tlen=241, kl=0.00858, act_lr=1e-6, ent=1.25]    Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:26<00:18,  1.14it/s, pg=0.112, ret=-0.00119, glen=81, tlen=241, kl=0.00858, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:27<00:18,  1.14it/s, pg=-0.0889, ret=0.000969, glen=80.1, tlen=240, kl=0.00786, act_lr=1e-6, ent=1.25]Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.15it/s, pg=-0.0889, ret=0.000969, glen=80.1, tlen=240, kl=0.00786, act_lr=1e-6, ent=1.25]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31/51 [00:27<00:17,  1.15it/s, pg=0.0714, ret=-0.000657, glen=86, tlen=246, kl=0.00688, act_lr=1e-6, ent=1.31]  Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:27<00:16,  1.16it/s, pg=0.0714, ret=-0.000657, glen=86, tlen=246, kl=0.00688, act_lr=1e-6, ent=1.31]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:28<00:16,  1.16it/s, pg=0.000183, ret=0.000203, glen=78.9, tlen=239, kl=0.00824, act_lr=1e-6, ent=1.24]Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:28<00:15,  1.17it/s, pg=0.000183, ret=0.000203, glen=78.9, tlen=239, kl=0.00824, act_lr=1e-6, ent=1.24]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33/51 [00:29<00:15,  1.17it/s, pg=-0.0819, ret=0.00017, glen=85.1, tlen=246, kl=0.00688, act_lr=1e-6, ent=1.37]  Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:29<00:14,  1.15it/s, pg=-0.0819, ret=0.00017, glen=85.1, tlen=246, kl=0.00688, act_lr=1e-6, ent=1.37]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:30<00:14,  1.15it/s, pg=-0.0558, ret=0.000687, glen=84.5, tlen=244, kl=0.0103, act_lr=1e-6, ent=1.42]Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:30<00:13,  1.15it/s, pg=-0.0558, ret=0.000687, glen=84.5, tlen=244, kl=0.0103, act_lr=1e-6, ent=1.42]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35/51 [00:31<00:13,  1.15it/s, pg=0.113, ret=-0.000633, glen=82.8, tlen=243, kl=0.00957, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:31<00:12,  1.16it/s, pg=0.113, ret=-0.000633, glen=82.8, tlen=243, kl=0.00957, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:32<00:12,  1.16it/s, pg=-0.0532, ret=9.03e-5, glen=89, tlen=249, kl=0.00732, act_lr=1e-6, ent=1.29]  Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:32<00:12,  1.17it/s, pg=-0.0532, ret=9.03e-5, glen=89, tlen=249, kl=0.00732, act_lr=1e-6, ent=1.29]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 37/51 [00:33<00:12,  1.17it/s, pg=0.132, ret=-0.0014, glen=81.3, tlen=242, kl=0.00979, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=0.132, ret=-0.0014, glen=81.3, tlen=242, kl=0.00979, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:33<00:11,  1.17it/s, pg=-0.0623, ret=0.00039, glen=82.8, tlen=243, kl=0.0178, act_lr=1e-6, ent=1.33]Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:33<00:10,  1.17it/s, pg=-0.0623, ret=0.00039, glen=82.8, tlen=243, kl=0.0178, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 39/51 [00:34<00:10,  1.17it/s, pg=0.171, ret=-0.00183, glen=80.1, tlen=240, kl=0.0162, act_lr=1e-6, ent=1.27] Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:34<00:09,  1.17it/s, pg=0.171, ret=-0.00183, glen=80.1, tlen=240, kl=0.0162, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:35<00:09,  1.17it/s, pg=-0.404, ret=0.165, glen=261, tlen=422, kl=0.00597, act_lr=1e-6, ent=1.93]  Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:35<00:08,  1.18it/s, pg=-0.404, ret=0.165, glen=261, tlen=422, kl=0.00597, act_lr=1e-6, ent=1.93]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 41/51 [00:36<00:08,  1.18it/s, pg=-0.156, ret=0.00117, glen=84.8, tlen=245, kl=0.0138, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:36<00:07,  1.17it/s, pg=-0.156, ret=0.00117, glen=84.8, tlen=245, kl=0.0138, act_lr=1e-6, ent=1.36]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42/51 [00:37<00:07,  1.17it/s, pg=-0.0154, ret=0.00042, glen=84.9, tlen=245, kl=0.00731, act_lr=1e-6, ent=1.32]Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:37<00:06,  1.17it/s, pg=-0.0154, ret=0.00042, glen=84.9, tlen=245, kl=0.00731, act_lr=1e-6, ent=1.32]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:38<00:06,  1.17it/s, pg=0.0193, ret=-0.00068, glen=81.4, tlen=241, kl=0.0129, act_lr=1e-6, ent=1.33] Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:38<00:05,  1.17it/s, pg=0.0193, ret=-0.00068, glen=81.4, tlen=241, kl=0.0129, act_lr=1e-6, ent=1.33]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 44/51 [00:39<00:05,  1.17it/s, pg=0.0908, ret=-0.000745, glen=80.5, tlen=240, kl=0.0193, act_lr=1e-6, ent=1.26]Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=0.0908, ret=-0.000745, glen=80.5, tlen=240, kl=0.0193, act_lr=1e-6, ent=1.26]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45/51 [00:39<00:05,  1.18it/s, pg=-0.0337, ret=0.00015, glen=86.5, tlen=247, kl=0.00786, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:39<00:04,  1.16it/s, pg=-0.0337, ret=0.00015, glen=86.5, tlen=247, kl=0.00786, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:40<00:04,  1.16it/s, pg=-0.0975, ret=0.000918, glen=80.6, tlen=240, kl=0.00899, act_lr=1e-6, ent=1.43]Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:40<00:03,  1.16it/s, pg=-0.0975, ret=0.000918, glen=80.6, tlen=240, kl=0.00899, act_lr=1e-6, ent=1.43]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47/51 [00:41<00:03,  1.16it/s, pg=-0.208, ret=0.00213, glen=83, tlen=243, kl=0.00887, act_lr=1e-6, ent=1.3]     Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:41<00:02,  1.17it/s, pg=-0.208, ret=0.00213, glen=83, tlen=243, kl=0.00887, act_lr=1e-6, ent=1.3]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:42<00:02,  1.17it/s, pg=0.0179, ret=-0.000476, glen=83, tlen=244, kl=0.00796, act_lr=1e-6, ent=1.27]Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:42<00:01,  1.17it/s, pg=0.0179, ret=-0.000476, glen=83, tlen=244, kl=0.00796, act_lr=1e-6, ent=1.27]
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 49/51 [00:43<00:01,  1.17it/s, pg=0.12, ret=-0.000505, glen=82.4, tlen=242, kl=0.00701, act_lr=1e-6, ent=1.29]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:43<00:00,  1.17it/s, pg=0.12, ret=-0.000505, glen=82.4, tlen=242, kl=0.00701, act_lr=1e-6, ent=1.29]
2025-07-23 15:08:51.307 | INFO     | orz.ppo.trainer:ppo_local_train_policy:732 - Policy model training, time cost: 44.32s
[36m(PolicyRayActorBase pid=885360)[0m Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.17it/s, pg=0.0085, ret=0.000327, glen=84.8, tlen=245, kl=0.00798, act_lr=1e-6, ent=1.36]Actor Train epoch [1/1]:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:44<00:00,  1.13it/s, pg=0.0085, ret=0.000327, glen=84.8, tlen=245, kl=0.00798, act_lr=1e-6, ent=1.36]
2025-07-23 15:08:51.964 | INFO     | orz.ppo.trainer:ppo_local_train_policy:739 - Backload vllm engines to gpu, time cost: 0.61s
2025-07-23 15:08:54.189 | INFO     | orz.ppo.trainer:ppo_local_train_policy:741 - Broadcast actor weights to vllm engines, time cost: 2.22s
2025-07-23 15:08:54.510 | INFO     | orz.ppo.trainer:train:148 - Actor model training, time cost: 47.64s
2025-07-23 15:08:54.516 | INFO     | orz.ppo.trainer:train:173 - {'policy_loss': -0.007432002647250306, 'actor_lr': 9.999999974752427e-07, 'clip_ratio': 0.0, 'entropy': 1.3401624688915177, 'kl': 0.009539005803126915, 'response_length': 87.24000265084061, 'total_length': 247.3127710678998, 'return': 0.0032267293604045155, 'policy_update_steps': 1.0}
Episode [5/20]:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [17:01<18:34, 159.15s/it]2025-07-23 15:08:54.552 | INFO     | orz.ppo.trainer:make_experience:227 - start generation
